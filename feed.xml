<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Leo's blog</title><link href="https://leonis.cc/" rel="alternate"></link><link href="https://leonis.cc/feed.xml" rel="self"></link><id>https://leonis.cc/</id><updated>2023-11-17T00:00:00+08:00</updated><subtitle>A nook to hoard my manuscripts.</subtitle><entry><title>Cloudflare + Backblaze 实现免费的博客图床方案</title><link href="https://leonis.cc/sui-sui-nian/2023-11-17-deploy-backblaze-image-hosting.html" rel="alternate"></link><published>2023-11-17T00:00:00+08:00</published><updated>2023-11-17T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-11-17:/sui-sui-nian/2023-11-17-deploy-backblaze-image-hosting.html</id><summary type="html">&lt;p&gt;图床一直是困扰 Markdown 以及静态博客用户的麻烦事，&lt;a href="https://weilining.github.io/177.html" rel="noopener" target="_blank"&gt;Ln's Blog&lt;/a&gt; 总结了一些免费图床服务，还分别列出了测试链接，可以比较主观地比较各图床的速度，也可以判断在所处网络环境下该图床是否可用。&lt;/p&gt;
&lt;p&gt;我对图床的要求只有访问速度可靠、数据受控几点，遗 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;图床一直是困扰 Markdown 以及静态博客用户的麻烦事，&lt;a href="https://weilining.github.io/177.html" rel="noopener" target="_blank"&gt;Ln's Blog&lt;/a&gt; 总结了一些免费图床服务，还分别列出了测试链接，可以比较主观地比较各图床的速度，也可以判断在所处网络环境下该图床是否可用。&lt;/p&gt;
&lt;p&gt;我对图床的要求只有访问速度可靠、数据受控几点，遗憾的是尝试过的众多图床服务都不能满足我的要求，唯一适合我的方案只能是使用 OSS 搭建图床。于是我调查了阿里、腾迅等多家厂商提供的 OSS 服务，极复杂的收费规则首先就劝退了我。&lt;/p&gt;
&lt;p&gt;辗转之下我发现了 Backblaze 提供的存储服务，B2 云存储提供 10 GB 的免费空间，同时 Cloudflare 与 Backblaze 之间的流量不计费，用作为图床是完全足够了，就算超出免费额度，$0.006 GB/Month 的价格也很合适。&lt;/p&gt;
&lt;p&gt;使用 Backblaze B2 作为图床的唯一要求就是拥有一条托管在 Cloudflare 上的域名。若不知道如何将域名转移到 Cloudflare 上，可以参考先前写的&lt;a href="https://leonis.cc/sui-sui-nian/2023-10-31-cloudflare-dns-of-blog.html" rel="noopener" target="_blank"&gt;迁移教程&lt;/a&gt;，完成后就可以按照本文的步骤操作了。&lt;/p&gt;
&lt;h2 id="chuang-jian-tong"&gt;创建桶&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Backblaze homepage" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12725?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Backblaze homepage" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12725?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;打开 &lt;a href="https://www.backblaze.com/" rel="noopener" target="_blank"&gt;Backblaze 官网&lt;/a&gt;很容易就能找到 B2 Cloud Storage 产品，完成注册与邮箱验证后，登录即可免费创建 B2 云存储的桶。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Create Bucket" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12726?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Create Bucket" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12726?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;{note begin}Backblaze 提供的部分机翻中文根本看不懂，建议在网站的右下角切换语言为英文。{note end}&lt;/p&gt;
&lt;p&gt;选择 &lt;code&gt;Create a Bucket&lt;/code&gt;，在 Bucket Unique Name 一栏填入桶名称，桶名决定了源站的 URL，应尽可能复杂避免被他人猜测到。若源站 URL 泄露，绕过 Cloudflare 的直接访问就会产生额外流量了。其余项如下图保持默认即可：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Bucket config" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12727?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Bucket config" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12727?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;创建完成后，选择 &lt;code&gt;Upload / Download&lt;/code&gt; 尝试在桶中上传一张图片，查看图片的详细信息，其中 Friendly URL 一项就是生成的图片链接。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Image URL" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12729?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Image URL" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12729?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;以 &lt;code&gt;f000.backblazeb2.com/file/a-complicated-name/hokciu.jpg&lt;/code&gt; 为例，图片链接可以都分成以下几个部分：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;主机名&lt;/th&gt;
&lt;th style="text-align: center;"&gt;后缀&lt;/th&gt;
&lt;th style="text-align: center;"&gt;桶名&lt;/th&gt;
&lt;th style="text-align: center;"&gt;图片路径&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;code&gt;f000.backblze.com&lt;/code&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;code&gt;file&lt;/code&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;code&gt;a-complicated-name&lt;/code&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;code&gt;hokciu.jpg&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;因为 Friendly URL 中包含了桶名，不宜直接引用。假设想要将链接改写为 &lt;code&gt;img.leonis.cc/hokciu.jpg&lt;/code&gt;，显然要修改主机名、隐藏固定的后缀和桶名，再拼接上图片路径，URL 的改写就通过 Cloudflare 实现。&lt;/p&gt;
&lt;h2 id="tian-jia-dns-ji-lu"&gt;添加 DNS 记录&lt;/h2&gt;
&lt;p&gt;改写的目标 URL 必须使用 Cloudflare CDN，打开 Cloudflare 控制台，添加名称为 &lt;code&gt;img&lt;/code&gt; 目标为 &lt;code&gt;f000.backblazeb2.com&lt;/code&gt; 的 CNAME 记录，并&lt;dot&gt;将代理状态设为打开&lt;/dot&gt;。待 DNS 记录生效后，就实现了 &lt;code&gt;img.leonis.cc&lt;/code&gt; &amp;rarr; &lt;code&gt;f000.backblazeb2.com&lt;/code&gt; 的跳转。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="DNS record" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12730?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="DNS record" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12730?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="pei-zhi-zhuan-huan-gui-ze"&gt;配置转换规则&lt;/h2&gt;
&lt;p&gt;同样在 Cloudflare 控制台中，找到 &lt;code&gt;规则&lt;/code&gt; - &lt;code&gt;转换规则&lt;/code&gt; 页面并创建新规则，填写规则自定义名称后就来处理 URL 的转换问题。&lt;/p&gt;
&lt;p&gt;第一次接触 Cloudflare 的转换规则功能时，我被界面上各个选项弄得很迷糊，所以我在这里介绍一下转换规则各个功能的使用方法，读者理解了就能根据自己的想法配置图片链接了。&lt;/p&gt;
&lt;p&gt;&lt;dot&gt;规则页面上的「传入请求」是指访客对托管站点发起的请求&lt;/dot&gt;，例如访客所浏览的页面上有一条 &lt;code&gt;img.leonis.cc/hokciu.jpg&lt;/code&gt; 链接，该请求先进入到 Cloudflare 的服务器，再根据设定的规则前往 &lt;code&gt;f000.backblazeb2.com/file/a-complicated-name/hokciu.jpg&lt;/code&gt; 取出图片资源，最终呈现在页面上。&lt;/p&gt;
&lt;p&gt;前文为了表述简单，说的是将 &lt;code&gt;f000.backblazeb2.com/*&lt;/code&gt; 改为 &lt;code&gt;img.leonis.cc/*&lt;/code&gt;，实则是我们要设定一个规则，让访客能通过 &lt;code&gt;img.leonis.cc/*&lt;/code&gt; 到 &lt;code&gt;f000.backblazeb2.com/*&lt;/code&gt; 中取得需要的图片。&lt;/p&gt;
&lt;p&gt;在规则页面中的设置项可以参考下图：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Transform rule" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12731?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Transform rule" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12731?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;该规则筛选得到所有主机名为 &lt;code&gt;img.leonis.cc&lt;/code&gt; 的请求，将其 URL 重写到 &lt;code&gt;concat("/file/a-complicated-name", http.request.uri.path)&lt;/code&gt;，也就是把所有对 &lt;code&gt;img.leonis.cc/*&lt;/code&gt; 的请求指向 &lt;code&gt;img.leonis.cc/file/a-complicated-name/*&lt;/code&gt;。而因为 &lt;code&gt;img.leonis.cc&lt;/code&gt; 已经通过 CNAME 指向了 &lt;code&gt;f000.backblazeb2.com&lt;/code&gt;，最终请求都到达 &lt;code&gt;f000.backblazeb2.com/file/a-complicated-name/*&lt;/code&gt; 并取得图片资源。&lt;/p&gt;
&lt;p&gt;上述请求过程可以表示成&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-txt"&gt;GET: https://img.leonis.cc/hokciu.jpg
&amp;rarr; https://img.leonis.cc/file/a-complicated-name/hokciu.jpg
&amp;rarr; https://f000.backblazeb2.com/file/a-complicated-name/hokciu.jpg
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;需要注意的是，因为这里使用的是&lt;strong&gt;重写&lt;/strong&gt;（rewrite）而非&lt;strong&gt;重定向&lt;/strong&gt;（redirect），请求的改变发生在服务端而非客户端，&lt;dot&gt;整个过程中用户都不会看见 URL 发生变化&lt;/dot&gt;，所以也就达到了隐藏桶名的目的。&lt;/p&gt;
&lt;p&gt;若设置全部无误，这时候就可以通过 &lt;code&gt;https://img.leonis.cc/example.jpg&lt;/code&gt; 打开先前上传的图片了，由于 Backblaze 只支持 HTTPS，若打开 &lt;code&gt;http://img.leonis.cc/example.jpg&lt;/code&gt; 则会弹出无效页面，用户体验不太好，所以接下来我们还需要通过 Cloudflare 页面规则完成 HTTPS 重写和缓存的相关设置。&lt;/p&gt;
&lt;h2 id="she-zhi-ye-mian-gui-ze"&gt;设置页面规则&lt;/h2&gt;
&lt;p&gt;回到 Backblaze 找到 Bucket Settings 一项，在 Bucket Info 中填入 &lt;code&gt;{"cache-control":"max-age=720000"}&lt;/code&gt;，该项将 Cloudflare 回到源站获取资源的周期设定为 720000 s，用于避免回源次数过多导致加载速度过慢。当然，该周期过长也会导致源文件更改后不能及时更新，可以按自己的需求更改。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Bucket cache" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12732?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Bucket cache" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12732?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在 Cloudflare 中打开 &lt;code&gt;规则&lt;/code&gt; - &lt;code&gt;页面规则&lt;/code&gt;，新建一条页面规则，在 URL 一栏中填入 &lt;code&gt;img.leonis.cc/*&lt;/code&gt;，按下图设置设置缓存和 HTTPS 即可。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Page rule" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12733?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Page rule" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12733?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;{note begin}暂时不确定边缘缓存 TTL 和缓存级别两个设置项有什么作用，发现在未设置时图片就能命中缓存。不过既然官方文档提到了这两项配置就先给开启了，回头找找有没有详细些的资料。{note end}&lt;/p&gt;
&lt;p&gt;再打开样例图片的链接，查看浏览器的开发者工具，在响应头中有一项 &lt;code&gt;cd-cache-status&lt;/code&gt;，其值若为 &lt;code&gt;HIT&lt;/code&gt;，则表示 Cloudflare 命中了缓存，该图片是由缓存中取出的。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Cache response" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12734?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Cache response" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12734?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;至此关于 Backblaze + Cloudflare 的图床就设置完了，接下来还可以借助 PicGo 等第三方工具更方便地上传图片并获取图片链接，这部分内容可以根据章节标题向后文寻找。&lt;/p&gt;
&lt;h2 id="zheng-he-jing-tai-zi-yuan"&gt;整合静态资源&lt;/h2&gt;
&lt;p&gt;由于博客通常会使用到包括图片、字体在内的多种静态资源，我希望将他们都整合到相同的子域名下。当某些静态资源由于各种原由突然挂掉的时候&lt;del&gt;（说的就是 jsDelivr 和 Google Fonts）&lt;/del&gt;，我就可以直接在 Cloudflare 控制台上将其指向备用服务而不用去网页中一个个修改引用的链接，在管理维护上更方便。如果读者没有此需求，就可以完整跳过这一节了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="URL design" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12735?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="URL design" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12735?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在我的设想中，所有静态资源都由 &lt;code&gt;cdn.leonis.cc&lt;/code&gt; 分发，通过 URL 路径转向不同的子域名取得目标资源，后面就以图片资源为例实现这个构想。&lt;/p&gt;
&lt;h4&gt;添加 CDN 子域名&lt;/h4&gt;
&lt;p&gt;先在 Cloudflare 中添加子域名 &lt;code&gt;cdn.leonis.cc&lt;/code&gt; 的 DNS 记录，暂时任意设置一个解析目标，能让 Cloudflare 获取缓存即可。&lt;/p&gt;
&lt;h4&gt;处理 URL 重定向&lt;/h4&gt;
&lt;p&gt;接着要实现对 URL 路径的处理，例如将 &lt;code&gt;cdn.leonis.cc/img/*&lt;/code&gt; 重定向到 &lt;code&gt;img.leonis.cc/*&lt;/code&gt;，这种重定向可以通过 Cloudflare 规则功能下的页面规则或重定向规则实现。&lt;/p&gt;
&lt;p&gt;若使用&lt;strong&gt;页面规则&lt;/strong&gt;，可以使用下图中的方案，用通配符实现 URL 解析：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Page rule" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12736?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Page rule" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12736?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;该方案的一个小缺点在于无法将规则应用于 &lt;code&gt;cdn.leonis.cc/img&lt;/code&gt; 等不带后一个 &lt;code&gt;/&lt;/code&gt; 的页面。使用&lt;strong&gt;重定向规则&lt;/strong&gt;可以解决这个问题，但重定向规则中的正则匹配是收费功能，无法批量处理，每种后缀都必须添加一条规则，配置方案可以参考下图：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Redirect rule" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12737?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Redirect rule" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12737?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;表达式 &lt;code&gt;concat("https://img.leonis.cc", substring(http.request.uri.path, 4))&lt;/code&gt; 中的 &lt;code&gt;substring()&lt;/code&gt; 用于除去 &lt;code&gt;/img/*&lt;/code&gt; 的前 4 个字符，若是用于处理 &lt;code&gt;/js/*&lt;/code&gt; 等不同的 URL 则需要根据字符数量更改该数值。以上两种方案各有优劣，读者可以根据自己的需求选择。&lt;/p&gt;
&lt;h2 id="she-zhi-fang-dao-lian"&gt;设置防盗链&lt;/h2&gt;
&lt;p&gt;防盗链是用于屏蔽其他站点对静态资源引用的常用手段，倒不是不愿意分享资源，至少本站内的各种照片都可随意使用，而是个人站点的服务容量有限，很难做到再向外提供服务。除此以外，设置防盗链对于避免流量被恶意浪费也很有必要。防盗链的功能可以通过 Cloudflare 的防火墙规则实现，打开 &lt;code&gt;安全性&lt;/code&gt; - &lt;code&gt;WAF&lt;/code&gt; 页面即可创建规则。&lt;/p&gt;
&lt;p&gt;防盗链功能一般通过请求头中的 &lt;code&gt;Referer&lt;/code&gt; 字段判断是否允许请求，例如允许自己的站点引用图片（&lt;strong&gt;Referer 为本站&lt;/strong&gt; &lt;code&gt;leonis.cc&lt;/code&gt;），不允许他人的站点引用图片（&lt;strong&gt;Referer 为外站&lt;/strong&gt; &lt;code&gt;bing.com&lt;/code&gt;）。另外还有一种&lt;strong&gt;没有 Referer&lt;/strong&gt; 的情况，例如直接打开图片、在各种 Markdown 编辑器中使用图片都属于这一类。&lt;/p&gt;
&lt;p&gt;为了不影响正常使用，我使用的防盗链规则是&lt;dot&gt;允许无 Referer 与白名单站点访问&lt;/dot&gt;。很棘手的是，Cloudflare 没有提供判断有无 Referer 的功能，所以我使用了比较曲折的方法实现该方案。&lt;/p&gt;
&lt;p&gt;首先新建一条防火墙规则，对于静态资源的 URL，阻止所有 Referer 中包含 &lt;code&gt;"http"&lt;/code&gt; 的请求：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="WAF rule" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12738?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="WAF rule" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12738?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;{note begin}该规则实际上阻止了所有具有 Referer 的请求，由于无法使用通配符才用 &lt;code&gt;"http"&lt;/code&gt; 作为匹配内容。需要注意的是，没有 Referer 的请求不在该匹配范围内，设置后仍可访问。{note end}&lt;/p&gt;
&lt;p&gt;再新建一条规则，这条规则用于根据 Referer 放行请求，作用等同于白名单，设置项如下：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Whitelist" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12739?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Whitelist" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12739?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;设置生效后可以发现，先前的图片链接可以直接打开，却不能在其他网站上引用了。Cloudflare 阻止了白名单以外站点的引用请求，在防火墙事件中还可以查看阻止请求的来源 IP 等具体信息。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Blocking" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12740?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Blocking" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12740?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;{note begin}后来发现在 Cloudflare 控制台中的 &lt;code&gt;Scrape Shield&lt;/code&gt; 页面中有一项 &lt;strong&gt;Hotlink 保护&lt;/strong&gt;功能，一键即可开启防盗链，在 &lt;code&gt;Configuration Rules&lt;/code&gt; 中添加规则即为白名单，该配置方案更简单，以上 WAF 方案也留作参考。{note end}&lt;/p&gt;
&lt;h2 id="picgo-she-zhi"&gt;PicGo 设置&lt;/h2&gt;
&lt;p&gt;若每次上传图片都要打开 Backblaze 网站终归还是很麻烦，好在 PicGo 能够让整个过程自动化。PicGo 还提供了丰富的插件，可以实现自定义文件路径、文件名哈希化等功能。&lt;/p&gt;
&lt;p&gt;设置 PicGo 作为 Backblaze 的图片上传工具，需要先打开 Backblaze Buckets 页面，在桶信息中记录下 &lt;code&gt;Endpoint&lt;/code&gt; 的内容：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Endpoint" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12741?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Endpoint" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12741?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;再在页面中找到 &lt;code&gt;Application Keys&lt;/code&gt; 界面，选择 &lt;code&gt;Add a New Application Key&lt;/code&gt;，填入 key 的名字：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Key config" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12743?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Key config" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12743?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;Duration&lt;/code&gt; 一项可以设置 key 的有效期，过期后需要重新申请。选择提交后，页面就会给出生成的 &lt;code&gt;keyID&lt;/code&gt; 和 &lt;code&gt;applicationKey&lt;/code&gt;，将内容复制保存下来，一凡离开该页面就再也无法查看了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Generated key" href="https://storage.live.com/items/4D18B16B8E0B1EDB!12744?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Generated key" src="https://storage.live.com/items/4D18B16B8E0B1EDB!12744?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;安装好 PicGo 后，搜索并安装 s3 插件，打开 Amazon S3 的设置界面，填入先前保存下的信息，我的设置如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;"aws-s3": {
    "accessKeyID": "Backblaze keyID",
    "secretAccessKey": "Backblaze applicationKey",
    "endpoint": "https://s3.us-west-000.backblazeb2.com",
    "bucketName": "a-complicated-name",
    "uploadPath": "{year}/{month}/{sha256}.{extName}",
    "urlPrefix": "https://cdn.leonis.cc/img/"
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中比较关键的是 &lt;code&gt;accessKeyID&lt;/code&gt;、&lt;code&gt;secretAccessKey&lt;/code&gt;、&lt;code&gt;endpoint&lt;/code&gt; 三项，确保填写正确，另外不要忘了在 endpoint 前加上 &lt;code&gt;https://&lt;/code&gt;。其余项则用于自定义图片路径和得到的 URL，具体配置可以参考&lt;a href="https://github.com/wayjam/picgo-plugin-s3" rel="noopener" target="_blank"&gt;插件仓库&lt;/a&gt;中的说明。&lt;/p&gt;
&lt;p&gt;到这里就大功告成了，下面两张图片都存放在 Backblaze 上，一张是前文手动上传的示例图片，另一张则是通过 PicGo 上传。关于图片的加载速度和链接，不用我多说，诸君查看这两张图片即可自明。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="PicGo demo" href="https://cdn.leonis.cc/img/2023/11/9c341684e296247e896e1f4131fc36f8da3e897335572206adc8774849f2fa8b.jpg" rel="noopener" target="_blank"&gt;&lt;img alt="PicGo demo" src="https://cdn.leonis.cc/img/2023/11/9c341684e296247e896e1f4131fc36f8da3e897335572206adc8774849f2fa8b.jpg"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Demo" href="https://cdn.leonis.cc/img/hokciu.jpg" rel="noopener" target="_blank"&gt;&lt;img alt="Demo" src="https://cdn.leonis.cc/img/hokciu.jpg"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.backblaze.com/docs/cloud-storage-deliver-public-backblaze-b2-content-through-cloudflare-cdn" rel="noopener" target="_blank"&gt;Deliver Public Backblaze B2 Content Through Cloudflare CDN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.mitsea.com/67b8601211284a25b68bb8afe65b80a7/" rel="noopener" target="_blank"&gt;Backblaze B2 + CloudFlare 搭建图床 - Mitsea Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.winer.website/archives/use_blackblaze_b2_and_cloudflare_cdn_to_bulid_a_free_oss.html" rel="noopener" target="_blank"&gt;使用 Backblaze B2 + Cloudflare CDN + PicGo 实现可自定义域名的 10G 免费图床解决方案 - winer's Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="Cloudflare"></category><category term="Blog"></category></entry><entry><title>新服务器必做的基本设置——服务器迁移之记录</title><link href="https://leonis.cc/sui-sui-nian/2023-11-11-necessary-config-of-new-server.html" rel="alternate"></link><published>2023-11-11T00:00:00+08:00</published><updated>2023-11-11T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-11-11:/sui-sui-nian/2023-11-11-necessary-config-of-new-server.html</id><summary type="html">&lt;p&gt;最近各大服务器厂商都开始做年末的促销了，不满于先前服务器时断时续的网络质量，我也趁着优惠换了一家供应商租赁了服务器，着手将所有服务迁移到新服务器上来。新购置的服务器空空如也，各种设置不免繁琐，于是我把过程记录下来 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;最近各大服务器厂商都开始做年末的促销了，不满于先前服务器时断时续的网络质量，我也趁着优惠换了一家供应商租赁了服务器，着手将所有服务迁移到新服务器上来。新购置的服务器空空如也，各种设置不免繁琐，于是我把过程记录下来，在又遇上新服务器时就能方便查阅。&lt;/p&gt;
&lt;h2 id="ji-ben-she-zhi"&gt;基本设置&lt;/h2&gt;
&lt;h3 id="wang-luo-lian-tong-xing"&gt;网络连通性&lt;/h3&gt;
&lt;p&gt;我租赁的都是海外服务器，可以免去很多麻烦，也会带来很多麻烦。海外服务器最首要的麻烦就是网络问题，厂商提供的 IP 可能会被防火墙污染，一买来就无法连接。比较方便的方法是在网站 &lt;a href="https://ping.pe/" rel="noopener" target="_blank"&gt;https://ping.pe/&lt;/a&gt; 上输入服务器 IP 检查服务器在全球范围内的连通状态，如果在大陆地区一片飘红，那么就必须联系客服申请更换 IP 了。&lt;/p&gt;
&lt;p&gt;我在 RackNerd 上更换过 IP，客服的回应很快，更换也是免费的。但至于其他 IP 余量比较紧张的厂商，可能就会收取少许的额外费用。&lt;/p&gt;
&lt;h3 id="fu-wu-qi-can-shu"&gt;服务器参数&lt;/h3&gt;
&lt;p&gt;GitHub 上可以找到很多用于测试服务器参数的 bash 脚本，我比较常用的是 &lt;a href="https://bench.sh/" rel="noopener" target="_blank"&gt;Bench.sh&lt;/a&gt;。使用 SSH 连接并登录 root 用户后，输入 &lt;code&gt;wget -qO- bench.sh | bash&lt;/code&gt;，自动下载脚本并开始测试。给出的测试结果包括系统信息、I/O 读写速度、网络速度：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-txt"&gt;-------------------- A Bench.sh Script By Teddysun -------------------
 Version            : v2023-10-15
 Usage              : wget -qO- bench.sh | bash
----------------------------------------------------------------------
 CPU Model          : Intel(R) Xeon(R) CPU E5-2697 v2 @ 2.70GHz
 CPU Cores          : 2 @ 2699.998 MHz
 CPU Cache          : 30720 KB
 AES-NI             : &amp;checkmark; Enabled
 VM-x/AMD-V         : &amp;cross; Disabled
 Total Disk         : 49.2 GB (1.9 GB Used)
 Total Mem          : 976.2 MB (96.0 MB Used)
 Total Swap         : 1023.0 MB (340.0 KB Used)
 System uptime      : 23 days, 4 hour 40 min
 Load average       : 0.03, 0.01, 0.00
 OS                 : Debian GNU/Linux 11
 Arch               : x86_64 (64 Bit)
 Kernel             : 5.10.0-8-amd64
 TCP CC             : 
 Virtualization     : Dedicated
 IPv4/IPv6          : &amp;checkmark; Online / &amp;cross; Offline
 Organization       : AS35916 MULTACOM CORPORATION
 Location           : Los Angeles / US
 Region             : California
----------------------------------------------------------------------
 I/O Speed(1st run) : 133 MB/s
 I/O Speed(2nd run) : 264 MB/s
 I/O Speed(3rd run) : 296 MB/s
 I/O Speed(average) : 231.0 MB/s
----------------------------------------------------------------------
 Node Name        Upload Speed      Download Speed      Latency
 Speedtest.net    917.95 Mbps       911.80 Mbps         0.48 ms
 Los Angeles, US  917.27 Mbps       906.25 Mbps         1.04 ms
 Dallas, US       919.58 Mbps       129.57 Mbps         31.06 ms
 Montreal, CA     792.01 Mbps       674.81 Mbps         72.70 ms
 Paris, FR        567.70 Mbps       655.98 Mbps         144.64 ms
 Amsterdam, NL    584.61 Mbps       271.38 Mbps         139.27 ms
 Shanghai, CN     387.34 Mbps       25.47 Mbps          187.49 ms
 Chongqing, CN    27.54 Mbps        0.65 Mbps           224.27 ms
 Hongkong, CN     528.81 Mbps       23.52 Mbps          145.39 ms
 Mumbai, IN       358.96 Mbps       430.29 Mbps         235.43 ms
 Singapore, SG    366.75 Mbps       638.25 Mbps         184.59 ms
 Tokyo, JP        375.91 Mbps       198.06 Mbps         118.29 ms
----------------------------------------------------------------------
 Finished in        : 6 min 31 sec
 Timestamp          : 2023-11-10 03:01:34 EST
----------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在测试结果中主要比对提供的服务器参数是否与购买时的配置清单匹配、网络状况是否满足要求，同时也可对服务器的具体工作状况加深印象。我的测试结果没有什么问题，相比旧服务器硬件有所升级，网络的连接比顺畅很多。新服务器来自于 CloudCone，这款 2 核 1 GB 的服务器售价是每年 $16.5，还是比较划算的。&lt;/p&gt;
&lt;p&gt;由于我对 Debian 系统有着特殊的感情，不管 PC 设备还是服务器的首选 Linux OS 都是 Debian。后续涉及的安装软件等操作在不同 OS 上可能有所不同，读者自行留意，不再反复重提。&lt;/p&gt;
&lt;h3 id="lu-you-ce-shi"&gt;路由测试&lt;/h3&gt;
&lt;p&gt;购买海外服务器的用户一般会比较看重线路，即去回程的数据需要经过哪些路由的转发，例如拥有 CN2 GIA 线路的海外服务器在大陆访问也十分通畅，很受追捧。我购买的廉价服务器自然没有这样的线路，不过研究研究数据如何穿越海底光缆到达大洋彼岸也是很有意思的事。&lt;/p&gt;
&lt;p&gt;网站 &lt;a href="https://tools.ipip.net/traceroute.php" rel="noopener" target="_blank"&gt;https://tools.ipip.net/traceroute.php&lt;/a&gt; 提供了各地区节点，可以查询各地去往服务器的数据线路。另一种方法是使用系统自带的测试工具，可以追踪由本机发出的数据。&lt;/p&gt;
&lt;p&gt;后文的例子中，&lt;dot&gt;&lt;b&gt;将 8.8.8.8 当作为服务器的 IP 地址，将 1.1.1.1 当作为本地 IP 地址&lt;/b&gt;&lt;/dot&gt;，读者需要根据自己的实际情况修改。&lt;/p&gt;
&lt;p&gt;在本地电脑上打开终端，使用 &lt;code&gt;tracert&lt;/code&gt; 命令追踪住服务器需要经过的路由：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;&amp;gt; tracert 8.8.8.8

通过最多 30 个跃点跟踪到 [8.8.8.8] 的路由

  1     3 ms     2 ms     1 ms  10.131.192.1
  2     3 ms     2 ms     3 ms  202.113.18.233
  3     1 ms     1 ms     1 ms  202.113.18.102
  4     3 ms     4 ms     2 ms  117.131.219.1
  5     6 ms     3 ms     3 ms  117.131.131.13
  6     *        *        *     请求超时。
  7     *        *        *     请求超时。
  8     7 ms     8 ms     7 ms  221.183.89.121
  9     *        *        *     请求超时。
 10     *        *        *     请求超时。
 11     *        *        *     请求超时。
 12     *        *        *     请求超时。
 13   193 ms   192 ms   192 ms  eth-0-19.10g.cr1.ny1.ip.coresite.com [206.223.143.40]
 14     *        *        *     请求超时。
 15   195 ms   195 ms   195 ms  8.8.8.8

跟踪完成。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查询一下 IP 的归属就能知道，去程数据先往北京，走寻常不过的 &lt;code&gt;221.183.*.*&lt;/code&gt; 的 AS9808 路由再跨过大洋。&lt;/p&gt;
&lt;p&gt;在服务器端可以查看回程线路，使用 mtr 命令向本地 IP 传递数据：&lt;/p&gt;
&lt;p&gt;{note begin}有的服务器厂商可能没有在 OS 里预装 mtr 工具，可以通过 &lt;code&gt;apt-get install mtr-tiny&lt;/code&gt; 安装。{note end}&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;# mtr 1.1.1.1 -r
HOST: cc.server                   Loss%   Snt   Last   Avg  Best  Wrst StDev
  1.|-- undefined.hostname.localh  0.0%    10    9.1   5.3   0.7  15.1   6.1
  2.|-- multacom.com               0.0%    10    0.8   0.8   0.7   1.1   0.1
  3.|-- 182.54.129.88              0.0%    10    0.5   6.7   0.5  61.5  19.2
  4.|-- 218.30.54.189              0.0%    10    5.9   5.3   2.5   7.9   1.8
  5.|-- ???                       100.0    10    0.0   0.0   0.0   0.0   0.0
  6.|-- 202.97.58.121              0.0%    10  154.4 155.1 152.7 156.3   1.2
  7.|-- 202.97.48.209             90.0%    10  175.8 175.8 175.8 175.8   0.0
  8.|-- 202.97.108.126             0.0%    10  178.8 175.0 166.5 178.8   5.4
  9.|-- 219.150.49.154            20.0%    10  179.2 177.2 172.5 182.1   3.5
 10.|-- 221.238.222.118           10.0%    10  169.8 167.8 159.2 175.3   5.1
 11.|-- 218.69.12.90              10.0%    10  188.9 184.8 176.4 189.0   5.1
 12.|-- ???                       100.0    10    0.0   0.0   0.0   0.0   0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;线路中的路由主要是 &lt;code&gt;202.97.*.*&lt;/code&gt;，也就是传统的 163 骨干网。据说在网络较空闲时，该款服务器线路会动态切换为 CN2，我对此也不是很在意就是了。&lt;/p&gt;
&lt;p&gt;{warn begin}测试结果中的路由 IP 地址会暴露设备所处的地理位置，在社交平台上公开前务必三思。{warn end}&lt;/p&gt;
&lt;h2 id="chuang-jian-xin-yong-hu_1"&gt;创建新用户&lt;/h2&gt;
&lt;p&gt;在购买服务器后，除了服务器 IP 地址外，供应商还会提供 root 用户的密码，用户可以通过 SSH 连接服务器。但 root 用户的权限太高，误操作容易造成不可逆的结果。在 Linux 的使用中，不论是服务器还是本地 PC，通常都是新建普通用户供日常使用，在权限不足时通过 &lt;code&gt;sudo&lt;/code&gt; 命令提权，完成操作后自动「尽早」地退出 root 模式。&lt;/p&gt;
&lt;p&gt;首先是使用 &lt;code&gt;adduser 用户名&lt;/code&gt; 新建用户，创建用户的过程中会提示设定并确认密码，按提示输入即可：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;# adduser leo
Adding user `leo' ...
Adding new group `leo' (1000) ...
Adding new user `leo' (1000) with group `leo' ...
Creating home directory `/home/leo' ...
Copying files from `/etc/skel' ...
New password:
Retype new password:
passwd: password updated successfully
Changing the user information for leo
Enter the new value, or press ENTER for the default
        Full Name []: Leo
        Room Number []:
        Work Phone []:
        Home Phone []:
        Other []:
Is the information correct? [Y/n] y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;接着安装 &lt;code&gt;sudo&lt;/code&gt; 命令，以后就用 &lt;code&gt;sudo&lt;/code&gt; 命令管理 root 权限。安装完成后用 &lt;code&gt;visudo&lt;/code&gt; 进入配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;# apt-get install sudo
# visudo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在配置文件中找到以下片段：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;# User privilege specification
root    ALL=(ALL:ALL) ALL
leo     ALL=(ALL:ALL) ALL
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 root 用户的下一行填上新建用户的用户名，同样填上 &lt;code&gt;ALL=(ALL:ALL) ALL&lt;/code&gt;。根据窗口下方的快捷键提示，依次摁 &lt;code&gt;CTRL&lt;/code&gt; + &lt;code&gt;O&lt;/code&gt; 保存，摁 &lt;code&gt;ENTER&lt;/code&gt; 确认，摁 &lt;code&gt;CTRL&lt;/code&gt; + &lt;code&gt;X&lt;/code&gt; 退出。&lt;/p&gt;
&lt;h2 id="ssh-she-zhi"&gt;SSH 设置&lt;/h2&gt;
&lt;p&gt;几乎所有远端服务器都是通过 SSH 与用户相连接，当服务器暴露在公网上时，就有无数人尝试爆破 SSH 口令盗取控制权，所以 SSH 的安全是保护服务器的第一道关口。为了避免服务器变成肉鸡，最为基础且最为有效的方法就是修改 SSH 的默认配置。&lt;/p&gt;
&lt;h3 id="geng-gai-ssh-duan-kou"&gt;更改 SSH 端口&lt;/h3&gt;
&lt;p&gt;SSH 的默认端口是 22，将其改为非常见端口就可以躲过大量定向的爆破。上文中新建的用户名为 &lt;code&gt;leo&lt;/code&gt;，通过 &lt;code&gt;ssh leo@8.8.8.8&lt;/code&gt; 换用新用户登录 SSH。&lt;/p&gt;
&lt;p&gt;打开 SSH 的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo vim /etc/ssh/sshd_config
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将文件中 &lt;code&gt;Port&lt;/code&gt; 一项改为自定义端口，并将 &lt;code&gt;PermitRootLogin&lt;/code&gt; 一项改为 &lt;code&gt;no&lt;/code&gt;，禁止直接使用 root 用户登录，修改后例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-txt"&gt;Port 2222
PermitRootLogin no
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后重启 SSH 服务加载配置：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo service sshd restart
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时可以测试是否可以通过 &lt;code&gt;ssh  -p 2222 leo@8.8.8.8&lt;/code&gt; 登录，若配置无误，以下两种方式都会失效：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;ssh leo@8.8.8.8
ssh root@8.8.8.8
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="jin-zhi-mi-ma-deng-lu"&gt;禁止密码登录&lt;/h3&gt;
&lt;p&gt;凡使用密码作为登录口令，终究有被爆破的可能，况且长密码也很难记忆。更为安全有效的方法是禁止使用密码登录 SSH，使用公私钥完成用户的验证。&lt;/p&gt;
&lt;p&gt;在服务器上生成公私钥：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/home/leo/.ssh/id_rsa):    # 此处摁回车，存储在默认位置
Created directory '/home/leo/.ssh'.
Enter passphrase (empty for no passphrase):                     # 输入 passphrase，若不设置则直接摁回车
Enter same passphrase again:                                    # 重复 passphrase
Your identification has been saved in /home/leo/.ssh/id_rsa     # 私钥保存路径
Your public key has been saved in /home/leo/.ssh/id_rsa.pub     # 公钥保存路径
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在生成过程中会提示用户输入 passphrase，若设置了该口令，在私钥验证通过后还需要通过该口令的验证。在私钥被他人盗取的情况下，对方不知道该口令也无法登录，安全性更高。&lt;/p&gt;
&lt;p&gt;公钥相当于一把锁，存放在服务器上，私钥相当于一把钥匙，存放在本地。服务器上的授权文件则决定了使不使用该公钥完成验证，所以还要按下列步骤为新生成的公钥添加授权：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ cd .ssh
$ cat id_rsa.pub &amp;gt;&amp;gt; authorized_keys
$ chmod 600 authorized_keys
$ chmod 700 ~/.ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;打开 SSH 的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo vim /etc/ssh/sshd_config
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;找到以下项目，编辑设置开启公钥验证：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-txt"&gt;PubkeyAuthentication yes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用 &lt;code&gt;cat /home/leo/.ssh/id_rsa&lt;/code&gt; 在终端中输出私钥内容，将其复制后写入到本地的记事本中，将文件保存为 &lt;code&gt;id_rsa&lt;/code&gt;，存放在自定义的目录下。接着在本地打开终端，尝试使用私钥连接服务器：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;ssh -p 端口号 -i "私钥路径" 用户名@主机名
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{note begin}相信许多读者使用的是 PuTTY 等更为便捷的 SSH 客户端，在设置项中一定也可以使用私钥的方式完成登录，各种客户端的设置方式不尽相同，就不在此罗列了。 {note end}&lt;/p&gt;
&lt;p&gt;成功登录后，SSH 的公私钥设置就没有问题了。再次打开 SSH 配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo vim /etc/ssh/sshd_config
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将密码登录关闭，以后全部使用私钥登录：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-txt"&gt;PasswordAuthentication no
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后重启 SSH 服务，SSH 的设置内容就全部完成了：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo service sshd restart
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="an-zhuang-fail2ban_1"&gt;安装 Fail2Ban&lt;/h2&gt;
&lt;p&gt;更改 SSH 的默认设置提升了防御等级，但只顾着防守而没有反制措施，暴露在外的防护手段在积年累月的攻击下，始终有被攻破的风险。Fail2Ban 是用于反制非法访问的有力工具，Fail2Ban 能够根据服务器的访问日志找出密码失败次数过多等具有风险的 IP 并自动封禁，是避免暴力攻击的有效手段。&lt;/p&gt;
&lt;p&gt;Fail2Ban 亦可设置邮件通知等功能，读者如有兴趣可以自行搜索，在这里仅介绍基础的 SSH 安全设置。首先在服务器上安装 Fail2Ban：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo apt-get install fail2ban
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fail2Ban 的默认设置文件为 &lt;code&gt;/etc/fail2ban/jail.conf&lt;/code&gt;，一般不改写该文件，而是在同目录下新建 &lt;code&gt;jail.local&lt;/code&gt;，其中的设置项会添加入 &lt;code&gt;jail.conf&lt;/code&gt; 并覆盖同名设置项。使用 Vim 新建 &lt;code&gt;jail.local&lt;/code&gt; 文件：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo vim /etc/fail2ban/jail.local
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在文件中写入针对 SSH 服务的封禁规则：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-ini"&gt;[sshd]
enable = ture
filter = sshd
port = 2222                     # SSH 服务对应的端口
logpath = /var/log/auth.log     # 日志路径
maxretry = 3                    # 最大允许试错次数
bantime = -1                    # IP 封禁时间（无限）
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;保存设置后通过 &lt;code&gt;sudo systemctl start fail2ban&lt;/code&gt; 启动，Fail2Ban 就开始保护服务器了。以下罗列了在维护时经常需要用到的命令：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ w                                             # 查看当前服务器登录的用户
$ last                                          # 查看过去一段时间的登录用户
$ sudo systemctl enable fail2ban.service        # 开机启动
$ sudo systemctl status fail2ban.service        # 查看服务运行状态
$ sudo cat /var/log/fail2ban.log                # 查看日志文件
$ sudo fail2ban-client status                   # 查看 fail2ban 的运行状态
$ sudo fail2ban-client status sshd              # 查看 sshd 的详细信息，包括封禁 IP 等
$ sudo fail2ban-client set sshd unbanip 1.1.1.1 # 解封指定 IP 1.1.1.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;或许有读者认为，有必要这么麻烦地折腾 SSH 安全吗？也没见有什么人来连接我的服务器。事实并非如此，当服务器以公网 IP 直接接入互联网后，每天都要面临大量连接请求，多亏了厂商默认设置的强密码，将很多隐患挡在了外头。&lt;/p&gt;
&lt;p&gt;诸君如若不信，可以通过以下命令查询指定日期的失败访问：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ lastb -s 2023-11-6 -t 2023-11-7&amp;ZeroWidthSpace;
administ ssh:notty    185.224.128.160  Mon Nov  6 04:53 - 04:53  (00:00)
esroot   ssh:notty    170.64.161.15    Mon Nov  6 04:53 - 04:53  (00:00)
administ ssh:notty    185.224.128.160  Mon Nov  6 04:53 - 04:53  (00:00)
admin    ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
esroot   ssh:notty    170.64.161.15    Mon Nov  6 04:52 - 04:52  (00:00)
admin    ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
root     ssh:notty    180.101.88.222   Mon Nov  6 04:52 - 04:52  (00:00)
root     ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
root     ssh:notty    180.101.88.222   Mon Nov  6 04:52 - 04:52  (00:00)
root     ssh:notty    180.101.88.222   Mon Nov  6 04:52 - 04:52  (00:00)
root     ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
odoo     ssh:notty    170.64.161.15    Mon Nov  6 04:52 - 04:52  (00:00)
odoo     ssh:notty    170.64.161.15    Mon Nov  6 04:52 - 04:52  (00:00)
Admin    ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
Admin    ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
root     ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
opc      ssh:notty    170.64.161.15    Mon Nov  6 04:52 - 04:52  (00:00)
Admin    ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
opc      ssh:notty    170.64.161.15    Mon Nov  6 04:52 - 04:52  (00:00)
Admin    ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
root     ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
user     ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
user     ssh:notty    185.224.128.160  Mon Nov  6 04:52 - 04:52  (00:00)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我查询了还未修改 SSH 默认设置时的失败访问，这里仅截取了很小一部分结果。可以看见，全世界各地都有人在很频繁地尝试连接，爆破 &lt;code&gt;root&lt;/code&gt;、&lt;code&gt;Admin&lt;/code&gt; 等常见用户的密码，由此也可见以上安全措施的重要性。&lt;/p&gt;
&lt;h2 id="ufw-fang-huo-qiang-she-zhi"&gt;UFW 防火墙设置&lt;/h2&gt;
&lt;p&gt;UFW 可以用于很方便地管理服务器上的端口，关闭无用的端口也是保证服务器安全的基本措施。安装 UFW 后仅打开需要的服务端口：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo apt-get install ufw
$ sudo ufw allow ssh
$ sudo ufw allow http
$ sudo ufw allow https
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;千万别忘了我们已经修改了 SSH 的默认端口，再将自定义端口打开并尝看规则是否有误：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo ufw allow 2222
$ sudo ufw status
Status: active

To                         Action      From
--                         ------      ----
22/tcp                     ALLOW       Anywhere
80/tcp                     ALLOW       Anywhere
443                        ALLOW       Anywhere
2222                       ALLOW       Anywhere
22/tcp (v6)                ALLOW       Anywhere (v6)
80/tcp (v6)                ALLOW       Anywhere (v6)
443 (v6)                   ALLOW       Anywhere (v6)
2222 (v6)                  ALLOW       Anywhere (v6)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;用 systemd 打开 UFW 服务并设定自动启动，服务器上的端口就受 UFW 规则控制了：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo systemctl start ufw
$ sudo systemctl enable ufw
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="kai-qi-bbr"&gt;开启 BBR&lt;/h2&gt;
&lt;p&gt;BBR（Bottleneck Bandwidth and Round-trip propagation time）是 Google 提出的一种拥塞控制算法，能够保证在有丢包率的不良网络环境下的连接，这对于海外服务器是一项比较重要的功能。&lt;/p&gt;
&lt;p&gt;有些服务器默认开启了 BBR，可以通过以下命令检查：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo sysctl net.ipv4.tcp_available_congestion_control | grep bbr
$ sudo sysctl net.ipv4.tcp_congestion_control | grep bbr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;若没有输出，就需要通过以下方式手动开启：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo sh -c 'echo "net.core.default_qdisc=fq" &amp;gt;&amp;gt; /etc/sysctl.conf'
$ sudo sh -c 'echo "net.ipv4.tcp_congestion_control=bbr" &amp;gt;&amp;gt; /etc/sysctl.conf'
$ sudo sysctl -p
net.core.default_qdisc = fq
net.ipv4.tcp_congestion_control = bbr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;至此，新购买服务器的配置就差不多完成了，大部分都是和网络安全相关的设置，虽显得繁琐却又不得不做。若服务器厂商另外提供备份和 DDoS 防御等功能也应选择开启，因为廉价服务器不会提供此类服务且各厂商的设置方法都不相同，这类功能就超出本文的范围了。不过将文中的基础功能配置下来，后续就已经可以在服务器上放心地部署服务了。&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kiku.vip/2021/10/13/VPS%20%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE/" rel="noopener" target="_blank"&gt;VPS 初体验（一）基础配置 - Kiku 的个人博客&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ivo-wang.github.io/2019/04/08/vps-%E6%9C%8D%E5%8A%A1%E5%99%A8-%E5%AE%89%E5%85%A8%E9%98%B2%E6%8A%A4%E8%AE%BE%E7%BD%AE/" rel="noopener" target="_blank"&gt;VPS 服务器 安全防护设置 - 老王的自留地 | ivo Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.logcg.com/archives/884.html" rel="noopener" target="_blank"&gt;购买了 VPS 之后你应该做足的安全措施 - 落格博客&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="Linux"></category><category term="VPS"></category><category term="Blog"></category><category term="SSH"></category></entry><entry><title>把博客站点交给了 Cloudflare 托管</title><link href="https://leonis.cc/sui-sui-nian/2023-10-31-cloudflare-dns-of-blog.html" rel="alternate"></link><published>2023-10-31T00:00:00+08:00</published><updated>2023-10-31T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-10-31:/sui-sui-nian/2023-10-31-cloudflare-dns-of-blog.html</id><summary type="html">&lt;p&gt;因为博客域名是在阿里云购买的，先前一直顺理成章地用着阿里云的 DNS 解析。阿里云的 DNS 解析在各方面的体验都很不错，例如修改配置后就能很快更新、配置平台访问速度快、站点不会被国内的运营商污染等等，这些优点反过来可是说尽是 Cloudflare …&lt;/p&gt;</summary><content type="html">&lt;p&gt;因为博客域名是在阿里云购买的，先前一直顺理成章地用着阿里云的 DNS 解析。阿里云的 DNS 解析在各方面的体验都很不错，例如修改配置后就能很快更新、配置平台访问速度快、站点不会被国内的运营商污染等等，这些优点反过来可是说尽是 Cloudflare 的缺点。&lt;/p&gt;
&lt;p&gt;但由于 Cloudflare 为网站提供的各种免费服务十分诱人，加之我想利用 Cloudflare 的 CDN 搭建博客图床，终究是把站点交给了 Cloudflare 管理。本文记录了从阿里云迁移站点的过程和一些必要的 Nginx 配置。&lt;/p&gt;
&lt;h2 id="cloudflare-zhu-ce-zhan-dian"&gt;Cloudflare 注册站点&lt;/h2&gt;
&lt;p&gt;打开 &lt;a href="https://www.cloudflare-cn.com/" rel="noopener" target="_blank"&gt;Cloudflare 官网&lt;/a&gt;，注册帐号后选择添加站点，输入域名后点击继续。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Cloudflare" href="https://storage.live.com/items/4D18B16B8E0B1EDB!11535?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Cloudflare" src="https://storage.live.com/items/4D18B16B8E0B1EDB!11535?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;按需选择计划，对于普通的小站点来说，Free 计划足矣。点击继续后，Cloudflare 会检测站点目前已有的部分 DNS 记录，其余未检测出的记录日后再手动添加，最关键的是检查域名指向服务器 IP 地址的 A 记录是否正确。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="DNS records" href="https://storage.live.com/items/4D18B16B8E0B1EDB!11536?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="DNS records" src="https://storage.live.com/items/4D18B16B8E0B1EDB!11536?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在「代理状态」一列可以选择该 DNS 记录是否使用 Cloudflare 的 CDN，激活后图标显示一朵黄色的云。Cloudflare 的 CDN 在国内速度很慢，一直被称为减速 CDN，所以我都选择「仅 DNS」。此前我也担心 Cloudflare 的 DNS 解析会不会也像其 CDN 一样龟速，幸好解析速度并不慢，我的担心是多虑了。&lt;/p&gt;
&lt;p&gt;提交 DNS 记录后，Cloudflare 会提示删除阿里云的 DNS 服务器，以 Cloudflare 的 DNS 服务器代替之，接着就转到阿里云的控制中心操作。&lt;/p&gt;
&lt;h2 id="geng-huan-dns-fu-wu-qi"&gt;更换 DNS 服务器&lt;/h2&gt;
&lt;p&gt;登录&lt;a href="https://www.aliyun.com" rel="noopener" target="_blank"&gt;阿里云&lt;/a&gt;，进入控制台。在云解析 DNS - 域名解析下找到迁移的域名，在解析设置中保存了站点的 DNS 记录。将记录备份，后续要将所有记录导入 Cloudflare。站点交由 Cloudflare 解析后，阿里云中的解析设置也会失效，所以也在解析设置中将所有解析都停用。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="aliyun DNS records" href="https://storage.live.com/items/4D18B16B8E0B1EDB!11537?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="aliyun DNS records" src="https://storage.live.com/items/4D18B16B8E0B1EDB!11537?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在阿里云控制台中来到域名控制台 - 域名列表，选择域名的管理 - DNS 管理 - DNS 修改 - 修改 DNS 服务器，将 Cloudflare 提供的两个 DNS 服务器地址填入其中。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="DNS server" href="https://storage.live.com/items/4D18B16B8E0B1EDB!11538?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="DNS server" src="https://storage.live.com/items/4D18B16B8E0B1EDB!11538?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;修改 DNS 服务器一般需要 24-48 h 生效，生效后 Cloudflare 会发送邮件通知。如果迟迟没有收到邮件，也可以到 Cloudflare 手动验证网站。验证成功后 Cloudflare 会指引是否开启 Brotli 压缩等功能，按需选择即可。至此，站点已经交由 Cloudflare 托管。如果站点是由 Nginx 搭建的，那么就还需要考虑 Nginx 的 SSL 设置是否与 Cloudflare 兼容。&lt;/p&gt;
&lt;h2 id="nginx-zhong-de-ssl-xiang-guan-pei-zhi"&gt;Nginx 中的 SSL 相关配置&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Cloudflare SSL" href="https://storage.live.com/items/4D18B16B8E0B1EDB!11539?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Cloudflare SSL" src="https://storage.live.com/items/4D18B16B8E0B1EDB!11539?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在 Cloudflare 的 SSL/TLS 设置界面可以看到，用户访问由 Cloudflare 托管的站点的过程中有 3 个实体，根据实体间通信安全等级的不同可以分为 4 种模式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;关闭：浏览器-Cloudflare 间和 Cloudflare-服务器间都使用 HTTP；&lt;/li&gt;
&lt;li&gt;灵活：浏览器-Cloudflare 间使用 HTTPS，Cloudflare-服务器间使用 HTTP；&lt;/li&gt;
&lt;li&gt;完全：浏览器-Cloudflare 间和 Cloudflare-服务器间都使用 HTTPS，需要 SSL 证书；&lt;/li&gt;
&lt;li&gt;完全（严格）：浏览器-Cloudflare 间和 Cloudflare-服务器间都使用 HTTPS，需要非自签名 SSL 证书。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;现在的站点一般都使用了 HTTPS&lt;del&gt;，还在使用 HTTP 的站长快去申请个 SSL 证书吧&lt;/del&gt;，同时通过 Nginx 将访问 80 端口的 HTTP 流量强制重定向到 HTTPS 入口。若使用这样的 Nginx 配置又开启的「灵活」模式，用户发起访问请求后，Cloudflare 使用 HTTP 交由 Nginx，Nginx 告知用户重定向为 HTTPS，但Cloudflare 仍使用 HTTP 与 Nginx 通信，该过程无限循环，出现 &lt;strong&gt;301 重定向次数过多&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;为了保证站点的安全性和避免以上问题，推荐配置好站点的 HTTPS 后，在 Cloudflare 的 SSL/TLS 中&lt;dot&gt;使用完全或完全（严格）两种模式。&lt;/dot&gt;&lt;/p&gt;
&lt;p&gt;最后附上我的 Nginx 配置供参考：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-nginx"&gt;server {
    listen                              443 ssl http2;
    server_name                         leonis.cc;
    root                                /home/Leo/web/blog;

    # SSL 配置
    ssl_certificate                     /etc/nginx/cert/leonis.cc.cer;
    ssl_certificate_key                 /etc/nginx/cert/leonis.cc.key;
    ssl_session_timeout                 5m;
    ssl_ciphers                         ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;
    ssl_protocols                       TLSv1 TLSv1.1 TLSv1.2;
    ssl_prefer_server_ciphers           on;

    location / {
        index index.html;
    }
}

server {
    listen                              80;
    server_name                         leonis.cc
    # 重定向至 HTTPS，开启 Cloudflare 完全模式后不会访问 80 端口，也不会用上此处的重定向
    rewrite ^/(.*)$ https://leonis.cc:443/$1 permanent;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="hou-ji"&gt;后记&lt;/h2&gt;
&lt;p&gt;Cloudflare 总体来说还是很好用的，提供了很多有意思的功能，很便利地就能体验，免去了自己动手配置的烦恼。Cloudflare 的不足仅在于在国内有时访问不畅，添加 DNS 记录后也要等比较长的时间才会更新到国内网络上，若能接受这两点，Cloudflare 的可玩性还是比其他平台更高的。&lt;/p&gt;</content><category term="碎碎念"></category><category term="Cloudflare"></category><category term="DNS"></category><category term="Nginx"></category><category term="Blog"></category></entry><entry><title>RIME 脚本食用方法举隅：以输入苏州码为例</title><link href="https://leonis.cc/sui-sui-nian/2023-09-14-rime-script-simple-tutorial.html" rel="alternate"></link><published>2023-09-14T00:00:00+08:00</published><updated>2023-09-14T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-09-14:/sui-sui-nian/2023-09-14-rime-script-simple-tutorial.html</id><summary type="html">&lt;p&gt;RIME 或称中州韵输入法，另一个更风行的名字是小狼毫输入法，当然这并不准确，因为只有 Windows 平台上的 RIME 才称为小狼毫。不过也无妨，作为一款开源输入法，RIME 可以部署在 Windows、MacOS、Linux、Android 等多个平台上，实现大同小异的功能，大部分配置文件也都通用，用不 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;RIME 或称中州韵输入法，另一个更风行的名字是小狼毫输入法，当然这并不准确，因为只有 Windows 平台上的 RIME 才称为小狼毫。不过也无妨，作为一款开源输入法，RIME 可以部署在 Windows、MacOS、Linux、Android 等多个平台上，实现大同小异的功能，大部分配置文件也都通用，用不着很仔细区分。&lt;/p&gt;
&lt;p&gt;我很早就听说了 RIME，作为开源输入法，用户可以自己构建码表、输入方案，因而一问世就很受方言、汉字、打字爱好者的青睐。方言爱好者用 RIME 实现各种方言输入方案，汉字爱好者用来输入扩展区汉字，打字爱好者则是用来改进各种音码、形码方案，不一而足。&lt;/p&gt;
&lt;p&gt;但早年间 RIME 的 bug 比较多，入门的门槛高，一直只在小圈子内流行。经过数次版本迭代后，现而今的 RIME 可以说是非常好用，哪怕是仅追求不窃取用户资料的「圈外人」也可以轻松体验。&lt;/p&gt;
&lt;p&gt;网络上关于配置 RIME 的入门教程很多，我不在此赘言。这篇文章主要谈谈如何用 RIME 的 Lua 脚本实现一些高级输入，也是我最近折腾 RIME 的一些心得。&lt;/p&gt;
&lt;h2 id="su-zhou-ma"&gt;苏州码&lt;/h2&gt;
&lt;p&gt;苏州码也称苏州码子、花码等，是中国传统的记数符号，对照如下表所示：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;0&lt;/th&gt;
&lt;th style="text-align: center;"&gt;1&lt;/th&gt;
&lt;th style="text-align: center;"&gt;2&lt;/th&gt;
&lt;th style="text-align: center;"&gt;3&lt;/th&gt;
&lt;th style="text-align: center;"&gt;4&lt;/th&gt;
&lt;th style="text-align: center;"&gt;5&lt;/th&gt;
&lt;th style="text-align: center;"&gt;6&lt;/th&gt;
&lt;th style="text-align: center;"&gt;7&lt;/th&gt;
&lt;th style="text-align: center;"&gt;8&lt;/th&gt;
&lt;th style="text-align: center;"&gt;9&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;〇&lt;/td&gt;
&lt;td style="text-align: center;"&gt;〡&lt;/td&gt;
&lt;td style="text-align: center;"&gt;〢&lt;/td&gt;
&lt;td style="text-align: center;"&gt;〣&lt;/td&gt;
&lt;td style="text-align: center;"&gt;〤&lt;/td&gt;
&lt;td style="text-align: center;"&gt;〥&lt;/td&gt;
&lt;td style="text-align: center;"&gt;〦&lt;/td&gt;
&lt;td style="text-align: center;"&gt;〧&lt;/td&gt;
&lt;td style="text-align: center;"&gt;〨&lt;/td&gt;
&lt;td style="text-align: center;"&gt;〩&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;在表示数字时，苏州码用一个符号表示一位数，从左向右书写，这与阿拉伯数字的计数方式相同。&lt;/p&gt;
&lt;p&gt;苏州码还有一条规则，当「〡」「〢」「〣」中任意两者相邻时，首个用竖式，次一个用横式，再次一个又用回竖式，如此循环。&lt;dot&gt;仅「〡」「〢」「〣」三个数字具有横式苏州码&lt;/dot&gt;，其所谓横式就是汉字的「一」「二」「三」，可以想知这是为了避免「〡」「〢」粘连成「〣」。&lt;/p&gt;
&lt;p&gt;知道以上的规则就会识读苏州码了，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;18590&lt;/code&gt; ➔ 〡〨〥〩〇&lt;/li&gt;
&lt;li&gt;&lt;code&gt;51203&lt;/code&gt; ➔ 〥〡二〇〣&lt;/li&gt;
&lt;li&gt;&lt;code&gt;72132&lt;/code&gt; ➔ 〧〢一〣二&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再来看几个加上单位的具体例子：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="癸亥年更流部" href="https://storage.live.com/items/4D18B16B8E0B1EDB!11200?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="癸亥年更流部" src="https://storage.live.com/items/4D18B16B8E0B1EDB!11200?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;实际使用时，还会将&lt;dot&gt;最大数位用汉字着于最高位数字下方，数量单位着于个位数字下方&lt;/dot&gt;。可以看出，苏州码完美兼容中文直排的书写传统，阅读时从左至右逐列读出即可。在遇到大数时，这种能直接呼读的优势更为明显，例如&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;〡〨〥〣〤〦〥&lt;br/&gt;
万　　　块&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可以直接读「一万八千五百三十四块六五」。由于排版不便，苏州码在互联网时代已经难觅踪迹了，但似乎在民间手写的场合还有孑余。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="手写的苏州码" href="https://storage.live.com/items/4D18B16B8E0B1EDB!11201?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="手写的苏州码" src="https://storage.live.com/items/4D18B16B8E0B1EDB!11201?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="rime"&gt;RIME&lt;/h2&gt;
&lt;p&gt;言归正传，一个个复制输入苏州码太不现实，那么如何优雅地用 RIME 输入苏州码呢？&lt;/p&gt;
&lt;h3 id="gua-zai-yi-ge-shu-ru-fang-an"&gt;挂载一个输入方案&lt;/h3&gt;
&lt;p&gt;从头构建输入方案太过复杂，我们可以通过修改现成的输入方案实现我们的想法。在 &lt;a href="https://github.com/rime/plum" rel="noopener" target="_blank"&gt;RIME 的官方仓库&lt;/a&gt;中就能找到很多输入方案，可以下载一个最熟悉的。&lt;/p&gt;
&lt;p&gt;以 Windows 平台为例，正确安装 RIME 后，在右下角的任务栏中理应出现 RIME 图标。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;右击 RIME 图标，选择 &lt;code&gt;用户文件夹&lt;/code&gt;，将下载的输入方案移入该文件夹中，文件夹中应具有许多 &lt;code&gt;.yaml&lt;/code&gt; 文件；&lt;/li&gt;
&lt;li&gt;右击 RIME 图标，选择 &lt;code&gt;重新部署&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;再右击 RIME 图标，选择 &lt;code&gt;输入法设定&lt;/code&gt;，就能找到下载的输入方案了。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="shen-ru-shu-ru-fang-an"&gt;深入输入方案&lt;/h3&gt;
&lt;p&gt;输入方案最基本的两个文件是 &lt;code&gt;*.schema.yaml&lt;/code&gt; 和 &lt;code&gt;*.dict.yaml&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;*.schema.yaml&lt;/code&gt; 用于实现输入功能，例如模糊音、中英文混打等功能都通过它实现；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;*.dict.yaml&lt;/code&gt; 是码表文件，用户一般不需要动它。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;打开输入方案的 &lt;code&gt;*.schema.yaml&lt;/code&gt;，可以看到里面有一个名为 &lt;code&gt;translators&lt;/code&gt; 的模块，该模块决定了打字时击入的编码如何转化为候选词。&lt;/p&gt;
&lt;p&gt;我们要通过 Lua 脚本将输入的数字转为苏州码，在该模块下添加一项 &lt;code&gt;lua_translator@number_translator&lt;/code&gt;。&lt;code&gt;lua_translator&lt;/code&gt; 告诉 RIME 我们要使用 Lua 生成候选词，&lt;code&gt;number_translator&lt;/code&gt; 是函数名称。我修改后的 &lt;code&gt;translators&lt;/code&gt; 模块为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;translators:
  - punct_translator
  - table_translator@custom_phrase
  - reverse_lookup_translator
  - script_translator
  - lua_translator@number_translator
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{warn begin}YAML 文件对缩进敏感，一定要检查缩进是否正确。{warn end}&lt;/p&gt;
&lt;h3 id="lua-jiao-ben"&gt;Lua 脚本&lt;/h3&gt;
&lt;p&gt;接着在用户文件夹，即 &lt;code&gt;*.schema.yaml&lt;/code&gt; 所在文件夹中新建一个名为 &lt;code&gt;rime.lua&lt;/code&gt; 的文件，写入&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lua"&gt;number_translator = require("number")
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上述代码将 &lt;code&gt;number.lua&lt;/code&gt; 脚本注册为 &lt;code&gt;number_translator&lt;/code&gt; 函数。&lt;code&gt;rime.lua&lt;/code&gt; 文件管理着接入 RIME 的所有 Lua 脚本，将相应脚本注释去，其功能就被禁用。&lt;/p&gt;
&lt;p&gt;在用户文件夹中新建名为 &lt;code&gt;lua&lt;/code&gt; 的文件夹，所有 Lua 脚本就存放在该目录下，在该目录中新建一个 &lt;code&gt;number.lua&lt;/code&gt; 文件。如果仅列举关键文件，文件结构应为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-txt"&gt;RIME
&amp;boxvr;&amp;boxh;*.dict.yaml
&amp;boxvr;&amp;boxh;*.schema.yaml
&amp;boxvr;&amp;boxh;lua
&amp;boxv;  &amp;boxur;&amp;boxh;number.lua
&amp;boxur;&amp;boxh;rime.lua
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 &lt;code&gt;number.lua&lt;/code&gt; 写入将数字字符串转为苏州码的核心函数：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lua"&gt;local function contains(array, element)
    for _, value in pairs(array) do
        if value == element then
            return true
        end
    end
    return false
end

local function num2suzhou(num)
    local suzhou = {"〇", "〡", "〢", "〣", "〤", "〥", "〦", "〧", "〨", "〩"}
    local horizontalSuzhou = {"一", "二", "三"}
    local oneTwoThree = {table.unpack(suzhou, 2, 4)}  -- {"〡", "〢", "〣"}
    local result = ""
    if num == nil then return "" end
    -- 遍历整个字符串
    for pos = 1, string.len(num) do
        -- 将每个字符转为数字
        digit = tonumber(string.sub(num, pos, pos))
        if pos &amp;gt; 1 then
            -- 数字若为 {"〡", "〢", "〣"}
            if digit &amp;gt; 0 and digit &amp;lt; 4 then
                -- 且前一个字符也为 {"〡", "〢", "〣"}
                -- `-3` 即取末一个汉字，utf-8 中一个汉字 3 字节
                if contains(oneTwoThree, string.sub(result, -3)) then
                    -- 就使用横式的 {"一", "二", "三"}
                    result = result .. horizontalSuzhou[digit]
                    goto continue
                end
            end
        end
        -- 其他情况或其他数字都使用竖式
        result = result .. suzhou[digit + 1]
        ::continue::
    end
    return result
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;num2suzhou()&lt;/code&gt; 实现了前文提到的数字与苏州码映射和横竖式转换两个规则，接下来要将封装成 RIME 的接口：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lua"&gt;-- 若输入数字带有小数，将其切分为整数、小数点、小数 3 个部分
local function splitNumPart(str)
    local part = {}
    part.int, part.dot, part.dec = string.match(str, "^(%d*)(%.?)(%d*)")
    return part
end

-- 字符串处理流程
function numberTranslatorFunc(num)
    -- 切分小数
    local numberPart = splitNumPart(num)
    local result = {}
    -- 整数和小数部分分别用 num2suzhou() 转换，再将整数、小数点、小数三者连起来
    -- 最后将结果存入 result
    table.insert(
        result,
        {
            -- 候选结果
            num2suzhou(numberPart.int) .. numberPart.dot .. num2suzhou(numberPart.dec),
            -- 候选备注
            "〔蘇州碼〕"
        }
    )
    return result
end

-- 接入 RIME 引擎
function translator(input, seg)
    local str, num, numberPart
    -- 匹配 "S + 数字 + 小数点（可有可无） + 数字（可有可无）" 的模版
    if string.match(input, "^(S%d+)(%.?)(%d*)$") ~= nil then
        -- 去除字符串首的字母
        str = string.gsub(input, "^(%a+)", "")
        numberPart = numberTranslatorFunc(str)
        if #numberPart &amp;gt; 0 then
            for i = 1, #numberPart do
                -- numberTranslatorFunc()
                yield(
                    Candidate(
                        input,
                        seg.start,
                        seg._end,
                        numberPart[i][1],   -- 候选结果
                        numberPart[i][2]    -- 候选备注
                    )
                )
            end
        end
    end
end

return translator
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;处理字符串的过程都写在注释中了，这里仅具体说一下接入 RIME 的 &lt;code&gt;translator()&lt;/code&gt; 函数。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;translator(input, seg)&lt;/code&gt; 接受两个参数，&lt;code&gt;input&lt;/code&gt; 为用户击入的字符，&lt;code&gt;seg&lt;/code&gt; 推测是分词信息，一般用不到，可以当作固定模版。&lt;/p&gt;
&lt;p&gt;正则 &lt;code&gt;"^(S%d+)(%.?)(%d*)$"&lt;/code&gt; 用于匹配用户的 &lt;code&gt;input&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;S&lt;/code&gt; 匹配大写字母「S」，作用类似于快捷键，也可以改为自己喜欢的键位；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;%d+&lt;/code&gt; 匹配一至多个数字；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;^&lt;/code&gt; 表示匹配句首，&lt;code&gt;^(S%d+)&lt;/code&gt; 就表示只有以「S」和若干数字开头时才会转换；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;%.&lt;/code&gt; 匹配字符「.」，&lt;code&gt;%.?&lt;/code&gt; 表示「.」可有可无；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;%d*&lt;/code&gt; 匹配零至多个数字。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户输入的字符符合匹配规则，字符串经处理后用 &lt;code&gt;yield(Candidate())&lt;/code&gt; 生成候选词。&lt;code&gt;Candidate()&lt;/code&gt; 需要填入 5 个参数，不过其实也只用更改后两个参数就好。&lt;/p&gt;
&lt;p&gt;完成后仍然要重新部署一下，就可以试试输入效果了~&lt;/p&gt;
&lt;p&gt;&lt;img alt="demo" src="https://storage.live.com/items/4D18B16B8E0B1EDB!11206?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;了解在 RIME 上套用 Lua 脚本的方法后，相信编写自己的脚本也不觉得困难了，参考模版就能实现自己的奇思妙想。&lt;a href="https://github.com/hchunhui/librime-lua/tree/master" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; librime-lua&lt;/a&gt; 提供了许多 Lua 脚本，已经实现了很多有意思的想法，供额外参考。&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references_1"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Suzhou_numerals" rel="noopener" target="_blank"&gt;Suzhou numerals - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;李文化 &amp;amp; 陈虹. (2020).《癸亥年更流部》苏州码子释读. 南海学刊(04), 38-46.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/LEOYoon-Tsaw/Rime_collections/blob/master/Rime_description.md" rel="noopener" target="_blank"&gt;LEOYoon-Tsaw / Rime_collections - GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="RIME"></category><category term="Lua"></category></entry><entry><title>通过 SSH 在 Pycharm 上使用 Docker 容器中的 Python 解释器</title><link href="https://leonis.cc/sui-sui-nian/2023-08-05-connect-docker-container-by-pycharm.html" rel="alternate"></link><published>2023-08-05T00:00:00+08:00</published><updated>2023-08-05T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-08-05:/sui-sui-nian/2023-08-05-connect-docker-container-by-pycharm.html</id><summary type="html">&lt;p&gt;配置工程的运行环境一直是一件麻烦事，尽管 Anaconda 等工具提供的虚拟环境能够提够相对隔离的 Python 环境，但在调用更为底层硬件资源时难免会遇到冲突。例如我所遇到的情况是，需要使用的 Mindspore 最高仅支持 CUDA 11.6，而设备上已经安装了 CUDA 11.8，卸载 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;配置工程的运行环境一直是一件麻烦事，尽管 Anaconda 等工具提供的虚拟环境能够提够相对隔离的 Python 环境，但在调用更为底层硬件资源时难免会遇到冲突。例如我所遇到的情况是，需要使用的 Mindspore 最高仅支持 CUDA 11.6，而设备上已经安装了 CUDA 11.8，卸载又担心导致先前的项目出问题，这样冲突就只能靠 Docker 来解决了。&lt;/p&gt;
&lt;p&gt;我的解决方案很简单，直接从 Docker Hub 上拉取 CUDA 11.6 的 Mindspore 镜像，镜像中已经做好了相应的配置且与宿主机的环境隔离，运行该镜像的容器后就可以运行工程代码。但通过 Docker 运行容器呈现出的内容并非图形化的，都是以命令行形式在终端上展示、交互，调试代码时很不方便。那么是否能用 IDE 连接容器中的 Python 解释器，在图形化界面里调试代码呢？&lt;/p&gt;
&lt;p&gt;巧的是 Pycharm 的确提供这个功能，在选择项目的解释器时的确可以选择 Docker，不巧的是在 Pycharm 的工作逻辑中，该配置项&lt;dot&gt;只能选择镜像，而不能选择容器&lt;/dot&gt;。点击运行代码后，Pycharm 先用所选择的镜像构建一个临时容器，再用该容器中的解释器来运行代码。&lt;/p&gt;
&lt;p&gt;运行 Docker 容器更为通用的方法是使用 &lt;code&gt;docker run&lt;/code&gt; 命令，该命令还可以接收很多其他复杂的参数，例如通过 &lt;code&gt;docker run --gpus all&lt;/code&gt; 挂载 GPU 等。Pycharm 略过这个配置项就导致生成的容器存在多多少少的问题，例如无法调用 GPU、没有挂载硬盘等等。&lt;/p&gt;
&lt;p&gt;那么是否有通过 IDE 使用容器中的解释器调试代码的方法呢？有的，那就是&lt;dot&gt;不使用 Pycharm，而使用 JetBrains Gateway 连接解释器&lt;/dot&gt;。虽说有些标题党，但 Gateway 与 Pycharm 毕竟是同一家公司的产品，且 Gateway 集成了 Pycharm 的 IDE，完成能达到使用要求。尽管 Gateway 还在 Beta 版本，我试用了很久仍觉得十分好用，我认为这大概是最「优雅」的 Docker 环境使用方式。&lt;/p&gt;
&lt;p&gt;在宿主机上安装 Gateway 后，通过 SSH 连接到容器内，Gateway 会在容器中下载后台程序。宿主机上的操作都会经由 SSH 通过后台在容器中执行，所产生的反馈也由 SSH 传达并渲染到宿主机的界面上。所以使用 Gateway 调试、运行容器中代码的感觉就几乎和在本地一样，尽管无声的来去之间已经在 SSH 上交换了无数数据。如果能通过 SSH 连接远程服务器，同样也可以使用 Gateway 调试，十分便捷。&lt;/p&gt;
&lt;p&gt;下文就以 Mindspore 为例，介绍在 Linux 上配置 Docker 容器的 SSH 服务并使用 Gateway 连接容器中解释器的方法。Mindspore 是相当麻烦的 AI 框架，如果 Mindspore 都能装上，相信 Pytorch 和 TensorFlow 之类用户友好的框架就完全不成问题了。&lt;/p&gt;
&lt;h2 id="an-zhuang-jetbrains-gateway"&gt;安装 JetBrains Gateway&lt;/h2&gt;
&lt;p&gt;在 &lt;a href="https://www.jetbrains.com/remote-development/gateway/" rel="noopener" target="_blank"&gt;JetBrains Gateway 官网&lt;/a&gt;下载压缩包，解压后挪到 &lt;code&gt;/opt&lt;/code&gt; 目录下，在终端中可以用 &lt;code&gt;/opt/Gateway/bin/gateway.sh&lt;/code&gt; 启动。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;# 在官网上可以找到最新版的下载链接
$ wget https://download.jetbrains.com/idea/gateway/JetBrainsGateway-2023.2.tar.gz?_gl=1*1b4kr34*_ga*MTkzNDYxNzI1MS4xNjc2Njg1NzQx*_ga_9J976DJZ68*MTY5MTE0NTQwMy4yMC4xLjE2OTExNDc2NzkuNTguMC4w -O Gateway.tar.gz
$ tar -zxvf Gateway.tar.gz
$ sudo mv -f JetBrainsGateway-232.8660.185 /opt/Gateway
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;亦可以通过 Gateway 的欢迎界面创建桌面图标：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="创建图标" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9928?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="创建图标" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9928?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="pei-zhi-rong-qi-ssh"&gt;配置容器 SSH&lt;/h2&gt;
&lt;h4&gt;拉取镜像&lt;/h4&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;# 从 Docker Hub 上拉取需要的镜像（Ubuntu X86）
$ docker pull mindspore/mindspore-gpu-11.6:2.0.0-alpha
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;构建镜像&lt;/h4&gt;
&lt;p&gt;在空文件夹中新建 Dockerfile 文件，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-docker"&gt;# 使用前一步骤拉取的镜像作为基础镜像
FROM mindspore/mindspore-gpu-cuda11.6:2.0.0-alpha

# 切换到 root 用户
USER root

# 设置 root 用户密码为 12345（连接 SSH 时使用）
RUN echo "root:12345"|chpasswd

# 安装 vim supervisor openssh-server
RUN apt-get update &amp;amp;&amp;amp; \
    apt-get install -y vim supervisor openssh-server

# 修改 SSH 设置，允许使用 root 用户连接
RUN echo "PermitRootLogin yes" &amp;gt;&amp;gt; /etc/ssh/sshd_config

# 设置 supervisor，将 SSH 作为其子进程，用 supervisor 管理 SSH 服务
RUN echo -e \
"[supervisord]\n\
nodaemon=true\n\
\n\
[program:sshd]\n\
command=/usr/sbin/sshd -D\n\
autostart=true\n\
autorestart=true\n\
startsecs=3\n" &amp;gt; /etc/supervisor/conf.d/sshd.conf

# 在 Ubuntu 需要创建该文件夹
RUN mkdir -p /var/run/sshd

# 将 /usr/bin/supervisord -c /etc/supervisor/supervisord.conf 命令作为容器启动的入口，即让 supervisor 启动 SSH
CMD ["/usr/bin/supervisord", "-c", "/etc/supervisor/supervisord.conf"]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{warn begin}在 Ubuntu 系统下，使用 &lt;code&gt;/usr/sbin/sshd -D&lt;/code&gt; 命令启动 SSH 服务会出现错误，提示找不到文件夹 &lt;code&gt;Missing privilege separation directory: /var/run/sshd&lt;/code&gt;，我检索到的解决方法是用 &lt;code&gt;mkdir -p /var/run/sshd&lt;/code&gt; 创建该文件夹，所以在 Dockerfile 中加上了这行命令。我不确定其他系统是否有这个错误，文末附上了关于这个错误的两个链接。{warn end}&lt;/p&gt;
&lt;p&gt;用终端进入 Dockerfile 所在文件夹，用下列命令构建镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;$ docker build -t ms:200a-cu116 .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;完成后使用 &lt;code&gt;docker image ls&lt;/code&gt; 就能看到构建的镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;$ docker image ls
REPOSITORY                         TAG                 IMAGE ID            CREATED             SIZE
ms                                 200a-cu116          f9029d1ecae2        5 seconds ago       10.7 GB
mindspore/mindspore-gpu-cuda11.6   2.0.0-alpha         01db14982624        6 months ago        10.5 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="lian-jie-rong-qi-jie-shi-qi"&gt;连接容器解释器&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;$ docker run -d -p 2222:22 -v /dev/shm:/dev/shm -v /home/code:/home/code --name=work --runtime=nvidia ms:200a-cu116
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-d&lt;/code&gt; 参数使容器在后台运行，不打开终端；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-p 2222:22&lt;/code&gt; 参数令容器的 &lt;code&gt;22&lt;/code&gt; 端口（默认的 SSH 端口）映射到宿主机的 &lt;code&gt;2222&lt;/code&gt; 端口；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-v&lt;/code&gt; 参数是将本地的硬盘路径挂载到容器中，其中 &lt;code&gt;-v /dev/shm:/dev/shm&lt;/code&gt; 是 Mindspore 的要求，&lt;code&gt;-v /home/code:/home/code&lt;/code&gt; 则是将工程文件挂载到容器里，这两个目录双向同步；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--runtime=nvidia&lt;/code&gt; 参数使容器能够使用宿主机的 GPU 硬件。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;{info begin}有时运行调用 GPU 资源的容器会遇到问题，提示 &lt;code&gt;Error response from daemon: Unknown runtime specified nvidia&lt;/code&gt;。而我则是更换了内核和驱动版本后，尝试重启容器时出现了类似的错误，提示 &lt;code&gt;Error response from daemon: Cannot restart container or invalid runtime name: nvdia&lt;/code&gt;，暂时还不确定原因。在 &lt;a href="https://github.com/NVIDIA/nvidia-docker/issues/838" rel="noopener" target="_blank"&gt;GitHub&lt;/a&gt; 上有关于该问题的讨论，其中的方法都可以尝试一下，将 &lt;code&gt;--runtime=nvidia&lt;/code&gt; 参数替换为 &lt;code&gt;--gpus all&lt;/code&gt; 普遍可以解决问题。{info end}&lt;/p&gt;
&lt;p&gt;容器运行后可以在终端尝试用 SSH 连接容器，输入 &lt;code&gt;yes&lt;/code&gt; 再输入用户密码（前文 Dockerfile 中设置为 &lt;code&gt;12345&lt;/code&gt;）后若能成功连接就表示 SSH 服务正常。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;$ ssh root@127.0.0.1 -p 2222
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;还可以在连接上的终端中输入 &lt;code&gt;nvidia-smi&lt;/code&gt; 检查容器是否连接上 GPU 硬件。&lt;/p&gt;
&lt;p&gt;如果在这一步中，没有显示 SSH 成功连接的提示，多半是因为容器中的 SSH 服务没有成功启动。用 &lt;code&gt;docker exec -it work /bin/bash&lt;/code&gt; 进入容器的交互界面，用 &lt;code&gt;service ssh status&lt;/code&gt; 检查服务是否已经启动。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="登录" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9929?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="登录" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9929?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;确保容器一切正常后，打开 Gateway，选择 &lt;code&gt;New Connection&lt;/code&gt;，输入用户名、IP 地址和端口号，选择 &lt;code&gt;Check Connection and Continue&lt;/code&gt;，Gateway 使用 SHH 成功连接后就可以选择需要的 IDE。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="选择 IDE" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9930?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="选择 IDE" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9930?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;不知道为什么 Linux 上可选择的 IDE 这么少，好在可以通过从官网手动下载安装包的方式安装。例如&lt;a href="https://www.jetbrains.com/pycharm/download/?section=linux" rel="noopener" target="_blank"&gt;下载 Pycharm&lt;/a&gt; 的安装包后，选择 &lt;code&gt;Installation options&lt;/code&gt; - &lt;code&gt;Upload installer file&lt;/code&gt;，Gateway 就会在远端（容器中）安装指定的 IDE。&lt;/p&gt;
&lt;p&gt;{warn begin}目前 Gateway 的远端只支持 Linux 系统，所以下载的 IDE 安装包也应为 Linux 版本，这与上文构建的 Linux 镜像匹配。{warn end}&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="解释器" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9931?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="解释器" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9931?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;进入 IDE 后需要选择 Python 解释器，注意此时 Gateway 已经连接到容器，local 指的也是容器内，所以要选择的解释器正是本地解释器。Gateway 检测到的 Python 路径可能不正确，需要额外确认一下。在 Docker 中一般直接使用系统的 Python，不需要使用 Anaconda 一类的虚拟环境，可以通过以下命设查找系统 Python 路径：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;$ which python
/usr/local/bin/python
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;一切设置都正确的话，Gateway 就能读取到 Python 中的包了，此时无论运行还是调试代码，所使用的也都是容器中的 Python。在 Gateway 中打开终端，进入的也是容器中的终端，在终端中检查 Mindspore 是否成功安装：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;$ python -c "import mindspore;mindspore.set_context(device_target='GPU');mindspore.run_check()"
MindSpore version: 2.0.0a0
The result of multiplication calculation is correct, MindSpore has been installed successfully!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出上述信息即表示在 GPU 平台上成功安装 Mindspore。&lt;/p&gt;
&lt;h2 id="rong-qi-de-guan-bi-yu-zhong-qi"&gt;容器的关闭与重启&lt;/h2&gt;
&lt;p&gt;创建容器时指定了 &lt;code&gt;-d&lt;/code&gt; 参数，容器只在后台运行，一般也不需要关闭。如果需要开关容器，以下列出一些常用的 Docker 命令：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;# 列出所有容器，可以查询容器的运行状态、名称和 ID 等信息
$ docker ps -a

# 关闭指定容器，停止容器中的进程，内容不会消失
$ docker stop {容器名称或 ID}

# 重启容器，例如创建容器时已经指定了运行参数 -d，重启的容器同样在后台运行
$ docker restart {容器名称或 ID}

# 删除容器，若删除失败需要确定容易是否在运行
$ docker rm {容器名称或 ID}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr/&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/jesse131/p/13543308.html" rel="noopener" target="_blank"&gt;用 ssh 连接 docker 容器 - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/laolieren/p/launch_service_with_supervisor.html" rel="noopener" target="_blank"&gt;安装使用 supervisor 来启动服务 - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zhihu.com/question/436422410/answer/1647611960" rel="noopener" target="_blank"&gt;如何让操作系统为 ubuntu 的 docker 容器在启动时自动重启 sshd 服务? - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bugs.launchpad.net/ubuntu/+source/openssh/+bug/45234" rel="noopener" target="_blank"&gt;Bug #45234 &amp;ldquo;Missing privilege separation directory: /var/run/ssh...&amp;rdquo; - Launchpad&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ansible/ansible-container/issues/141" rel="noopener" target="_blank"&gt;Missing privilege separation directory: /var/run/sshd - GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="Docker"></category><category term="Linux"></category><category term="Mindspore"></category><category term="SSH"></category><category term="Pycharm"></category><category term="Python"></category></entry><entry><title>如何在 X86 设备上使用 Docker 构建 ARM 镜像</title><link href="https://leonis.cc/sui-sui-nian/2023-07-28-build-arm-docker-image-on-x86.html" rel="alternate"></link><published>2023-07-28T00:00:00+08:00</published><updated>2023-07-28T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-07-28:/sui-sui-nian/2023-07-28-build-arm-docker-image-on-x86.html</id><summary type="html">&lt;p&gt;最近一直在使用华为 ModelArts 的计算平台，使用这类计算平台的一般流程是先在本地用 Docker 构建镜像，再上传至云端，然后就可以在该环境下部署具体的计算作业了。使用 Docker 构建环境非常方便，基于官方或其他用户提供的基础镜像安装上自己所需要 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;最近一直在使用华为 ModelArts 的计算平台，使用这类计算平台的一般流程是先在本地用 Docker 构建镜像，再上传至云端，然后就可以在该环境下部署具体的计算作业了。使用 Docker 构建环境非常方便，基于官方或其他用户提供的基础镜像安装上自己所需要的依赖就可以直接上传使用了，完全不用跟驱动安装等等令人头疼又心累的事情打交道。&lt;/p&gt;
&lt;p&gt;但在使用 Docker 构建镜像时，有一个挺棘手的问题：计算平台或是服务器所使用的设备一般是 ARM 架构，个人电脑使用基本上是 X86 架构。由于二者 CPU 指令集不同，尽管可以在 X86 设备上用 &lt;code&gt;docker pull --platform=linux/arm64&lt;/code&gt; 拉取用于 ARM 设备的镜像，但无法使用 &lt;code&gt;docker run&lt;/code&gt; 或 &lt;code&gt;docker build&lt;/code&gt; 运行或是通过构建的方法修改该镜像。&lt;/p&gt;
&lt;h2 id="qemu-user-static"&gt;qemu-user-static&lt;/h2&gt;
&lt;p&gt;去寻找 ARM 设备再使用 Docker 构建镜像就太麻烦了，幸好找到了一个工具 &lt;a href="https://github.com/multiarch/qemu-user-static" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; qemu-user-static&lt;/a&gt;，专门用于解决这个问题。先来看看仓库中给出的示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;$ uname -m
x86_64

$ docker run --rm -t arm64v8/ubuntu uname -m
standard_init_linux.go:211: exec user process caused "exec format error"

$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes

$ docker run --rm -t arm64v8/ubuntu uname -m
aarch64
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;第一行的 &lt;code&gt;uname -m&lt;/code&gt; 用于检测宿主机的架构，终端给出的信息表明这是一台 X86 设备。&lt;/li&gt;
&lt;li&gt;第二行命令用 Docker 运行 &lt;code&gt;arm64v8/ubuntu&lt;/code&gt; 镜像，并运行同样的 &lt;code&gt;uname -m&lt;/code&gt;，当然由于架构不同，无法运行该镜像，给出了 &lt;code&gt;standard_init_linux.go:211: exec user process caused "exec format error"&lt;/code&gt; 错误。在使用 Dockerfile 构建镜像时，遇到类似的 &lt;code&gt;exec /bin/bash: exec format error&lt;/code&gt; 错误也需要考虑是不是架构的问题。&lt;/li&gt;
&lt;li&gt;运行 &lt;code&gt;qemu-user-static&lt;/code&gt; 镜像后，&lt;code&gt;arm64v8/ubuntu&lt;/code&gt; 就可以成功运行了，终端给出的信息表明 &lt;code&gt;arm64v8/ubuntu&lt;/code&gt; 是一个用于 ARM 设备的镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简单来说，qemu-user-static 通过 QEMU 模拟器模拟出了 ARM 设备，从而实现在 X86 设备上运行或是构建 ARM 镜像。当然，qemu-user-static 能模拟的硬件不仅限于 ARM，对于支持的硬件，官网上有更详细的介绍。&lt;/p&gt;
&lt;p&gt;qemu-user-static 的安装和使用都可以通过以下命令完成，若本地不存在该镜像，Docker 会自动从云端拉取：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;也有人会让 qemu-user-static 在后台一直运行，我嫌维护起来麻烦，就直接使用上面的命令，如果后台挂掉了，再运行一次就好。&lt;/p&gt;
&lt;h2 id="docker-chang-yong-ming-ling"&gt;Docker 常用命令&lt;/h2&gt;
&lt;p&gt;最后再记录几个创建环境时常用的 Docker 命令：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;# 检查镜像的架构
$ docker inspect {image_name}:{tag} | grep "Architecture"

# 用终端交互模式进入镜像的 /bin/bash
$ docker run -it {image_name}:{tag} /bin/bash

# 使用当前文件夹中的 Dockerfile 构建镜像，不使用缓存并输出详细信息
$ docker build -t {image_name}:{tag} . --progress=plain --no-cache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dockerfile 中记录了配置镜像的所有步骤，其他人也可以通过分享出去的 Dockerfile 构建相同的环境。而在撰写 Dockerfile 时，由于不熟悉基本镜像，一般都需要参考着终端给出的反馈来修改 Dockerfile 中的命令。这时候使用 &lt;code&gt;docker run -it&lt;/code&gt; 就很方便，特别是运行 qemu-user-static 后，可以直接进入 ARM 镜像的交互终端中，一步步安装依赖后再保存命令。&lt;/p&gt;
&lt;p&gt;上面的方法在简单的镜像中尚可，有的基本镜像做了特别复杂的操作，就算使用 qemu-user-static 也无法执行 &lt;code&gt;docker run&lt;/code&gt;，这种情况下就必须根据 &lt;code&gt;docker build&lt;/code&gt; 给出的错误信息修改 Dockerfile 了。在对 Dockerfile Debug 时，指定 &lt;code&gt;--progress=plain --no-cache&lt;/code&gt; 两个参数能输出更为完整的错误。&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/chen2ha/p/17180287.html" rel="noopener" target="_blank"&gt;x86 平台利用 qemu-user-static 实现 arm64 平台 docker 镜像的运行和构建 - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="Docker"></category><category term="Linux"></category></entry><entry><title>文献总结｜结构诱导的预训练</title><link href="https://leonis.cc/sui-sui-nian/2023-06-23-summary-doi.org/10.1038/s42256-023-00647-z.html" rel="alternate"></link><published>2023-06-23T00:00:00+08:00</published><updated>2023-06-23T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-06-23:/sui-sui-nian/2023-06-23-summary-doi.org/10.1038/s42256-023-00647-z.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年 MIT 研究团队在 Nature Machine Intelligence 发表上的一篇文章，文章原标题为 Structure-inducing pre-training，文章调查了目前广泛应用的多种预训练模型，设计了一种通过图结构在预训练过程中引入显式且深层结构约束的方法。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1038/s42256-023-00647-z" rel="noopener" target="_blank"&gt;doi.org/10.1038/s42256-023-00647-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年 MIT 研究团队在 Nature Machine Intelligence 发表上的一篇文章，文章原标题为 Structure-inducing pre-training，文章调查了目前广泛应用的多种预训练模型，设计了一种通过图结构在预训练过程中引入显式且深层结构约束的方法。&lt;/p&gt;
&lt;p&gt;预训练-微调的学习模式在自然语言处理及其他相关领域都已经得到广泛的应用，预训练通过在隐空间中提取样本的特征，从而提升模型在下游任务上的表现。但目前的预训练模型都没能在潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 上添加结构约束，从而获得既显式又深层的特征，这是目前预训练模型的一大缺陷。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;p&gt;对于数据集 &lt;span class="math"&gt;\(\boldsymbol{X}_\mathrm{PT}\in\mathcal{X}^{N_\mathrm{PT}}\)&lt;/span&gt;，预训练的目标就是从学习过程中得到编码器 &lt;span class="math"&gt;\(f_\theta:\mathcal{X}\rightarrow\mathcal{Z}\)&lt;/span&gt;，然后将 &lt;span class="math"&gt;\(f_\theta\)&lt;/span&gt; 用于各种各样的下游任务。&lt;/p&gt;
&lt;h3 id="xian-shi-he-shen-ceng-jie-gou-yue-shu"&gt;显式和深层结构约束&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;显示结构约束&lt;/strong&gt;：如果能从隐空间 &lt;span class="math"&gt;\(\mathcal{Z}\)&lt;/span&gt; 中的两个样本 &lt;span class="math"&gt;\(\boldsymbol{z}_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{z}_j\)&lt;/span&gt; 直接推导出两者间的关系（如距离），那么该预训练过程就有显示的结构约束。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深层结构约束&lt;/strong&gt;：预训练过程中所使用的信息越多（如维数），那么预训练过程所使用的结构约束越深。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前大部分的预训练模型都无法同时保证显式与深层的结构约束，调查目前超过 90 种的预训模型，其方法可以分为以下几类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;完全不使用样本间的关系，例如 prompt 训练，主要用于文本生成。&lt;/li&gt;
&lt;li&gt;使用显式，但浅层的监督预训练目标，例如 BERT 的 Next Sentence Prediction 训练模式。&lt;/li&gt;
&lt;li&gt;使用深层，但隐式的无监督或自监督预训练目标，例如通过添加噪声的数据强化方法。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9284?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9284?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;因此文章设计了一种同时使用显式与深层的结构约束的预训练框架，称这种方法为结构诱导的预训练。&lt;/p&gt;
&lt;p&gt;首先将预训练问题表示为图 &lt;span class="math"&gt;\(G_\mathrm{PT}=(V,E)\)&lt;/span&gt;，其中结点 &lt;span class="math"&gt;\(V\)&lt;/span&gt; 表示 &lt;span class="math"&gt;\(\boldsymbol{X}_\mathrm{PT}\)&lt;/span&gt; 中的预训练样本，&lt;span class="math"&gt;\(E\)&lt;/span&gt; 表示预先定义的样本间关系。&lt;/p&gt;
&lt;p&gt;接着预训练的损失函数就定义为&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathcal{L}_\mathrm{PT}=(1-\lambda_\mathrm{SI})\mathcal{L}_\mathrm{M}+\lambda_\mathrm{SI}\mathcal{L}_{SI}
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\mathcal{L}_\mathrm{M}\)&lt;/span&gt; 为传统预训练模型所使用的损失函数，&lt;span class="math"&gt;\(\mathcal{L}_\mathrm{SI}\)&lt;/span&gt; 是定义用于实现结构诱导目标的损失函数，使隐空间的各潜变量满足 &lt;span class="math"&gt;\(G_\mathrm{PT}\)&lt;/span&gt; 中的边（样本间关系）。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用了 3 类数据用于预训练：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proteins：来自 Stanford tree-of-life 数据集约 150 万条蛋白序列&lt;/li&gt;
&lt;li&gt;Abstracts：来自 Microsoft Academic Graph 数据集约 650,000 篇的生物医学相关的文本摘要&lt;/li&gt;
&lt;li&gt;Networks：来自文献的 70,000 条蛋白-蛋白相互作用网络的子图&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Proteins 与 Abstracts 预训练的编码器是 Transformer 架构，Networks 预训练所使用的模型是具有图同构网络（Graph Isomorphism Network, GIN）编码器的图卷积神经网络（graph convolutional neural network, GNN）。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9285?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9285?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;预训练模型在下游任务上的测试结果如上图所示，&amp;Delta; 一列中以 &amp;uarr; 表示相对传统预训练模型性质的提升，可以看出不管是相对于 per-token 还是 per-sample 的传统预训练策略，文中提出的结构诱导的预训练方法（structure-inducing pre-training, SIPT）在各下游任务上具有更好的表现。&lt;/p&gt;
&lt;p&gt;分析 Networks 任务得到的各种预训练模型在下游任务中微调的过程，SIPT 方法相比其他预训练方法得到的特征能够更快收敛，且在最后得到更好的效果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9286?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9286?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章调查了多种预训练模型，分析其训练目标发现大多数都没有引入显式且深层的结构约束，文章设计了一种预训练策略 SIPT，通过预训练图 &lt;span class="math"&gt;\(G_\mathrm{PT}\)&lt;/span&gt; 在隐空间中加入了显式且深层的结构约束，相比于传统的预训练方法，这种策略在下游任务的层次上提升上模型表现。&lt;/p&gt;
&lt;p&gt;文章借鉴了图结构来对样本与样本间的关系建模，但文中并未对得到「显式且深层」的特征做详尽的研究，只能推测这种方法更适用于蛋白-蛋白相互作用等更关注于样本间关系的任务，还不能证明 SIPT 得到的例如分子表示比传统预训练方法得到的分子表示更好。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category><category term="Transformer"></category></entry><entry><title>文献总结｜MTGL-ADMET：一种通过地位理论与最大流增强并用于 ADMET 预测的多任务图学习框架</title><link href="https://leonis.cc/sui-sui-nian/2023-06-09-summary-doi.org/10.1007/978-3-031-29119-7_6.html" rel="alternate"></link><published>2023-06-09T00:00:00+08:00</published><updated>2023-06-09T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-06-09:/sui-sui-nian/2023-06-09-summary-doi.org/10.1007/978-3-031-29119-7_6.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年 西北工业大学发表在 RECOMB 2023 上的一篇文章，文章原标题为 MTGL-ADMET: A Novel Multi-task Graph Learning Framework for ADMET Prediction Enhanced by Status-Theory and Maximum Flow，文章通过地位理论与最大流构造了由主要任务与辅助任务构成的多任务模型，相比单任务模型在预测准确性上有很大提高。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1007/978-3-031-29119-7_6" rel="noopener" target="_blank"&gt;doi.org/10.1007/978-3-031-29119-7_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年 西北工业大学发表在 RECOMB 2023 上的一篇文章，文章原标题为 MTGL-ADMET: A Novel Multi-task Graph Learning Framework for ADMET Prediction Enhanced by Status-Theory and Maximum Flow，文章通过地位理论与最大流构造了由主要任务与辅助任务构成的多任务模型，相比单任务模型在预测准确性上有很大提高。&lt;/p&gt;
&lt;p&gt;对于 ADMET 多种性质的预测，一般的方法是单任务学习，也就是一个模型只完成一种任务（预测一种性质），这种方法不仅繁琐，而且在缺少真实数据的情况下效果不佳。近年来出现的一种新范式是先通过预训练得到分子的通用表示，再将其用于多任务学习，使用一个模型完成所有预测任务，预训练的步骤弥补了缺少真实数据的问题。&lt;/p&gt;
&lt;p&gt;文章认为，现有基于多任务的 ADMET 模型都是通过一个模型完成所有预测任务，这样的共同学习很难保证模型能够共同学习到多种性质的信息，导致效果甚至不如单任务学习。文章设想以一个任务为主要任务，多个其他任务作为辅助任务，并通过地位理论找到最佳的任务搭配，改善模型效果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9255?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9255?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9256?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9256?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在文章设计的「一个主要任务，多个辅助任务」模式下，需要通过 3 个步骤找到这最佳的任务搭配，如上图中 &lt;strong&gt;a&lt;/strong&gt; 所示。&lt;/p&gt;
&lt;p&gt;首先先以各任务为单任务建立模型，例如对任务 &lt;span class="math"&gt;\(t_w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(t_k\)&lt;/span&gt; 分别建立单任务模型 &lt;span class="math"&gt;\(\mathcal{S}_w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\mathcal{S}_k\)&lt;/span&gt;，再为其建立多任务模型 &lt;span class="math"&gt;\(\mathcal{D}_{w,k}\)&lt;/span&gt;，那么 &lt;span class="math"&gt;\(t_w\)&lt;/span&gt; 对 &lt;span class="math"&gt;\(t_k\)&lt;/span&gt; 的影响就可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{Z}_{w\rightarrow k}=Z^{(d)}_{k|w}-Z^{(s)}_k
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(Z^{(s)}_k\)&lt;/span&gt; 就是 &lt;span class="math"&gt;\(\mathcal{S}_k\)&lt;/span&gt; 模型的表现，&lt;span class="math"&gt;\(Z^{(d)}_{k|w}\)&lt;/span&gt; 就是 &lt;span class="math"&gt;\(\mathcal{D}_{w,k}\)&lt;/span&gt; 模型的表现，从而可以得到类似下图中的结果：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9257?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9257?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;接着根据以上结果将相互增强的任务作为同一组的多任务，再通过地位理论决定各组任务中的主要任务，其他任务作为辅助任务。简单来说，地位理论就是将对模型表现提升最多的任务视为主要任务。&lt;/p&gt;
&lt;p&gt;最后通过最大流优化所选择的辅助任务。经过以上步骤，就可以将许多 ADMET 性质的预测任务分组，分别建立多任务模型。&lt;/p&gt;
&lt;p&gt;多任务模型的预测过程如上图 &lt;strong&gt;b&lt;/strong&gt; 所示，输入的分子通过两层 GCN 提取分子的信息，得到分子 embedding 表示，再在 Task-specific molecular embedding module 中得到适用于特定任务的分子表示。对于辅助任务，分子表示直接通过全连接层得到相应任务的预测结果。对于主要任务，除了针对于本任务的分子表示，还通过 Gating Network 通过可学习的权重融合来自于辅助任务的分子表示（图 &lt;strong&gt;c&lt;/strong&gt;），最后得到预测结果。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;模型所使用的 ADMET 数据来源于各文献中收集到的 24 种性质（18 个分类任务，6 个回归任务），共包含 43291 个类药的化合物。&lt;/p&gt;
&lt;p&gt;输入模型的分子以图的形式表示，分子图除了原子信息外，还添加了手性、电荷、芳香性、杂化等信息。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9258?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9258?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MTGL-ADMET 在 24 种性质上的预测结果如上图所示，括号中的数字代表辅助任务的数量。与其他图模型相比，MTGL-ADMET 在 20 个任务上表现最优，另外 4 个任务上表现仅次于最优。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9259?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9259?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在消融实验中，文章验证了「主要任务+辅助任务」策略的效果，测试结果如上图所示。与单任务（Single）、随机挑选 5 个辅助任务（Ran-5）和不使用地位理论与最大流而仅挑选对模型提升最大的 5 个辅助任务（Top-5）相比，MTGL-ADMET 在所有性质的预测上表现都是最佳的，说明了文章所设计多任务策略的优势。&lt;/p&gt;
&lt;p&gt;最后，文章展示了模型的可解释性，下图的案例展示了化合物结构片段与相应性质的相关性。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9260?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9260?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种用于构建 ADMET 多任务的策略，该策略主要使用地位理论与最大流分析了对主要任务具有增强作用的辅助任务，将主要任务与辅助任务一起构建多任务模型，使模型最后的预测效果好过很多完成类似任务的图模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;局限&lt;/strong&gt;：文章只评估了多任务模型中主要任务的预测结果，而没有全面评估模型包括辅助任务在内的多个预测结果，文章中的策略可以找到辅助提升主要任务结果的辅助任务，但这样的多任务模型不一定在多个任务上都表现得很好。文章中所测试的 ADMET 数据较少，在 ADMET 性质种类很多时，在两两任务间寻找是否具有性能提升的步骤就会变得繁琐。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category></entry><entry><title>文献总结｜探测图表示</title><link href="https://leonis.cc/sui-sui-nian/2023-06-02-summary-doi.org/10.48550/arXiv.2303.03951.html" rel="alternate"></link><published>2023-06-02T00:00:00+08:00</published><updated>2023-06-02T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-06-02:/sui-sui-nian/2023-06-02-summary-doi.org/10.48550/arXiv.2303.03951.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年德国亥姆霍兹信息安全中心研究团队发表在 AISTATS 2023 上的一篇文章，文章原标题为 Probing Graph Representations，文章设计了多种分子表示的探测模型，并通过探测模型研究了图模型在预训练后所编码分子信息。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2303.03951" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2303.03951&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年 德国亥姆霍兹信息安全中心研究团队发表在 AISTATS 2023 上的一篇文章，文章原标题为 Probing Graph Representations，文章设计了多种分子表示的探测模型，并通过探测模型研究了图模型在预训练后所编码分子信息。&lt;/p&gt;
&lt;p&gt;随着基于图的深度学习模型不断出现，亟需回答的一个问题是「图模型将什么信息编码进了表示中？」为了研究这一问题，文章构建了探测模型测试预训练图模型得到的分子表示。&lt;/p&gt;
&lt;p&gt;探测图表示的思路很简单，如果能从图模型输出的分子表示中提取出分子性质，那么就可以认为该性质被编码进分子表示中，所以文章的工作流程是「预训练-预测」（略不同于「预训练-微调」）。通过该流程，文章测试了传统 GNN 与基于 Transformer 的图模型等不同架构、不同数据集、不同优化算法等因素对于模型编码得到的潜变量的影响。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;p&gt;在分子性质预测中，对于分子 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 与其性质 &lt;span class="math"&gt;\(y\)&lt;/span&gt;，完成该任务的模型就是映射 &lt;span class="math"&gt;\(f:\boldsymbol{x}\mapsto y\)&lt;/span&gt;。取出 GNN 或图 Transformer 模型中 &lt;span class="math"&gt;\(d\)&lt;/span&gt; 维的 &lt;span class="math"&gt;\(l\)&lt;/span&gt; 层输出 &lt;span class="math"&gt;\(f_l(\boldsymbol{x})=\boldsymbol{z}\)&lt;/span&gt;，该潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 可以作为输入 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 的一种表示，进一步得到 &lt;span class="math"&gt;\(y\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;文章使用不同的图模型得到分子表示，再通过另一模型测试分子表示 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 预测分子性质 &lt;span class="math"&gt;\(y\)&lt;/span&gt; 的性能，从而对比不同图模型提取特征信息的能力。&lt;/p&gt;
&lt;p&gt;所构建的预测分子性质任务包括较为基础的判断是否具有某些官能团、更高层次的毒性、血脑屏障渗透性等。&lt;/p&gt;
&lt;h3 id="tan-ce-ce-lue"&gt;探测策略&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;线性探测（Linear Probing）：使用最简单的线性层，将分子表示映射为分子性质。&lt;/li&gt;
&lt;li&gt;贝叶斯探测（Bayesian Probing）：互信息可以用于 &lt;span class="math"&gt;\(Z\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(P\)&lt;/span&gt; 两个随机变量之间的依赖程度，文中通过计算潜变量与分子性质间的贝叶斯互信息进行评估。&lt;/li&gt;
&lt;li&gt;成对探测（Pairwise Probing）：将结构相近而性质差异大的分子构成一对 &lt;span class="math"&gt;\((\boldsymbol{x}_i,\boldsymbol{x}'_i)\)&lt;/span&gt;，通过主成分分析等方法分子潜变量与分子性质之间的关系。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9217?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9217?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先使用线性模型用 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 预测了分子中是否具有某种子结构，结果如上图所示，基于 Transformer 的一类图模型显然具有比 GCN 和 GIN 具有更好的表现，同时 GCN 模型得到的表示又比以 Morgan 指纹作为分子表示更好。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9218?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9218?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在更高层次的分子性质数据集上测试各种分子表示，结果如上图所示，以 Morgan 指纹作为分子表示的任务效果比部分图模型更好，Morgan 指纹作为一种可以简单获得的分子表示，仍然适合用于许多机器学习模型中完成预测任务。&lt;/p&gt;
&lt;p&gt;基于 Transformer 的图模型在更高层次的分子性质数据集上同样具有更好的表现，是具有潜力的新一代分子表示方式。这一点也可以从下图中看出，在左图中，基于 Transformer 图模型的结果都位于右上角，既能表示低层次的子结构信息，也能有效编码高层次的分子性质信息，而其他分子表示则位于左下角。右图使用贝叶斯互信息评估了样本数量与 &lt;span class="math"&gt;\(Z\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; 之间的依赖程度的关系，就整体趋势而言，仍然是基于 Transformer 图模型效果更好。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9219?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9219?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章通过主成分分析评估了相似分子间不同的分子表示，两个相似分子仅在官能团上有所不同，文中选择的官能团为硝基。结果如下图左侧一列所示，with FG 表示含硝基分子，w/o FG 表示去除该官能团的分子，可以明显看出，相比于 GCN，GraphGPS 这一基于 Transformer 的图模型所产生的特征中，两种结构相似的分子也具有较大的区分子，是更好的分子表示。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9220?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9220?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章设计探测模型研究了图模型在预训练后编码的分子信息，最终发现相比于使用消息传递聚合信息的传统 GNN 模型，基于 Transformer 的图模型能够学习到更多与化学相关的化学信息，得到更好的分子表示。文章中提出的分析方法为预训练模型的测试以及分子表示的评估提供了指导。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category></entry><entry><title>四月十二奉新纸一试</title><link href="https://leonis.cc/zai-lu-shang/2023-05-30-new-calligraphy-paper.html" rel="alternate"></link><published>2023-05-30T00:00:00+08:00</published><updated>2023-05-30T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-30:/zai-lu-shang/2023-05-30-new-calligraphy-paper.html</id><summary type="html">&lt;p&gt;常常划拉大字，练字的毛边纸用得很快，加之想多试试不同品种的纸，于是日前购买了一批新纸。从古至今，宣纸的价格都不算便宜，也少有人负担得起用宣纸练字，我所购买的纸也大多是毛边纸。新纸亦属于毛边纸，但与以前所买的毛边纸大不相 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;常常划拉大字，练字的毛边纸用得很快，加之想多试试不同品种的纸，于是日前购买了一批新纸。从古至今，宣纸的价格都不算便宜，也少有人负担得起用宣纸练字，我所购买的纸也大多是毛边纸。新纸亦属于毛边纸，但与以前所买的毛边纸大不相同，欣然提笔一试，果然令人惊喜。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="浓墨试纸" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9191?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="浓墨试纸" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9191?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 左为旧竹纸，单面粗糙且无帘纹，右为新纸&lt;/p&gt;
&lt;p&gt;新购之纸明显更为厚实，且颜色偏白，不像竹纸那样黄。取纸在灯下观之，帘纹新晰，料不是机器所制，人工捞纸才有这样的痕迹。向者识别毛边纸的方法是「若纸单面糙，则为机制纸；若双面糙，则为手工纸」，我也尝试用手指轻捻，发现竟两面粗糙。可我购买的的确是价廉的机制纸，取发货单审阅，上面也分明写着「机制」二字。疑惑这余，仔细摩挲再三，才发觉的确一面更滑，两面仅差毫厘。从这几点上看，虽说买的是毛边纸，却有下宣纸一等的做工了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="淡墨试纸" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9192?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="淡墨试纸" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9192?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 淡墨在新纸上晕开的痕迹，纸面上的帘纹清晰可见&lt;/p&gt;
&lt;p&gt;竹纸不吸墨，就算使用较稀的墨我也喜欢再加些水。在新纸上一试，墨水骤然晕开，分出浓淡的墨色，竹纸决没有这样的表现力，这种水汽氤氲之感特别适合用来写邓石如的篆书。我又兑上浓墨，虽然纸面抚摸着似乎并没有竹纸粗糙，行笔却要用更大的力气，也就是所谓「吃」得住笔。从浓淡墨的线条来看，新纸干湿两宜，行笔之间的迟滞顺滑又全然不同于竹纸，这种妙趣是在竹纸上完全找不到的。最重要的是这新纸仅比以前用的竹纸贵少许，但仍比宣纸便宜得多，很适合用来日常练字。&lt;/p&gt;
&lt;p&gt;在发现好物的欣喜之余，我不由地又惊异于科技的发展。旧时认为宣纸必须借由人工制作，制作过程还需要制纸师傅具有高超的捞纸技术，这些观点似乎正在被改写。费孝通先生在《乡土中国》一书中提到，在人们生于斯而长于斯的「乡土社会」中，一切都是那么的自然，一切生活中行之有效的法则都可以由口耳相传的经验得到，而当进入到原子化的「现代社会」后，无数人在世界范围发生着巨大规模的迁徙，在面对新事物时，那些法则就失效了。我自诩为年轻一代，在科技昌明的环境中成长，对那些流传下来的陋习也是弃如敝履，毫不惋惜，并自矜于终于能与老大帝国的积习切割。当我摩挲纸背发现我的经验失效时，内心竟也划过了一丝惶恐，原来我曾以为的法则也正在失效，我正迷惘地处在乡土社会与现代社会的间隙。一切都在改变，一切又都似也没变，似距离跨出乡土社会还甚遥远。这种种又何尝不是对我趋于「保守」的一种警醒？&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="书法"></category></entry><entry><title>文献总结｜可以同时完成分子语言序列回归和生成的 Regression Transformer</title><link href="https://leonis.cc/sui-sui-nian/2023-05-27-summary-doi.org/10.1038/s42256-023-00639-z.html" rel="alternate"></link><published>2023-05-27T00:00:00+08:00</published><updated>2023-05-27T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-27:/sui-sui-nian/2023-05-27-summary-doi.org/10.1038/s42256-023-00639-z.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年 IBM 研究团队发表在 &lt;em&gt;Nature Machine Intelligence&lt;/em&gt; 上的一篇文章，文章原标题为 Regression Transformer enables concurrent sequence regression and generation for molecular language modelling，文章提出了一种可以同时处理序列中的数值与文本并完成回归与生成的多任务的 Transformer 模型。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1038/s42256-023-00639-z" rel="noopener" target="_blank"&gt;doi.org/10.1038/s42256-023-00639-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年 IBM 研究团队发表在 &lt;em&gt;Nature Machine Intelligence&lt;/em&gt; 上的一篇文章，文章原标题为 Regression Transformer enables concurrent sequence regression and generation for molecular language modelling，文章提出了一种可以同时处理序列中的数值与文本并完成回归与生成的多任务的 Transformer 模型。&lt;/p&gt;
&lt;p&gt;基于 Transformer 的模型是化学任务中常用的模型，但由于 Transformer 最早是用于自然语言处理的模型，难以处理回归任务，这些模型只能完成性质预测或条件分子生成，无法同时完成指定结构的生成和性质预测。若要实现有约束的分子生成，即根据指定的性质生成分子，则不得不通过在多个模型间传递参数再得到反馈的方法不断调节并得到目标的分子，如下图中 &lt;strong&gt;a&lt;/strong&gt; 所示。&lt;/p&gt;
&lt;p&gt;文章尝试将回归任务融入到文本序列建模的过程中，提出了一种可以同时处理序列中的数值与文本并完成回归与生成的多任务模型，称为 回归 Transformer（Regression Transformer, RT）。在实验部分，文章使用化学领域中常见的分子生成、性质预测、化学反应预测、生物领域中蛋白质性质预测以及自然语言处理中的文本生成等多种任务测试了模型效果，证明 RT 是一种可以通用于多种任务且可以同时完成序列回归和生成的模型。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9166?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9166?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;Transformer 原为由左至右逐次由前一个 token 预测下一个 token 的自回归模型，而在分子语言，如 SMILES 中，序列中各原子的顺序是没有特定意义的，序列中的原子也并非由前一个原子决定，因此文章选择使用非自回归模型。BERT、XLNet 都是 Transformer 的变种，BERT 使用掩码的方式随机掩盖序列中的 token，并根据周围的 token 预测被掩盖的 token，因为这个过程使用周围信息编码掩盖的 token，这类模型称为自编码模型。&lt;/p&gt;
&lt;p&gt;XLNet 结合了自回归模型与自编码模型的优势，尽管 XLNet 还是由左至右预测 token，但它使用排列置换的方法将随机选择的待预测 token 放至序列末端，与 BERT 的掩码机制实际上相同，称为排列语言模型（Permutation language modeling, PLM）。文章使用 XLNet 作为主要的模型。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9167?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9167?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h4&gt;数值编码器&lt;/h4&gt;
&lt;p&gt;如上图所示，输入的数据格式为 &lt;code&gt;&amp;lt;ESOL&amp;gt;-2.92|SMILES&lt;/code&gt;，&lt;code&gt;&amp;lt;ESOL&amp;gt;&lt;/code&gt; 标识了预测的性质，&lt;code&gt;-2.92&lt;/code&gt; 为该性质的数值。由于 Transformer 无法识别数值，会将其识别为数字字符，文章设计了数值编码器（numeric encoder, NE）获取数值信息。&lt;/p&gt;
&lt;p&gt;先将 &lt;code&gt;-2.92&lt;/code&gt; 分为 &lt;code&gt;_-_&lt;/code&gt; &lt;code&gt;_2_0_&lt;/code&gt; &lt;code&gt;_._&lt;/code&gt; &lt;code&gt;_9_-1_&lt;/code&gt; &lt;code&gt;_2_-2_&lt;/code&gt; 几个 token，其中的 &lt;code&gt;_-_&lt;/code&gt; 与 &lt;code&gt;_._&lt;/code&gt; 分别表示负号与小数点，数字 &lt;code&gt;9&lt;/code&gt; 就以 &lt;code&gt;_9_-1_&lt;/code&gt; 表示，其中 &lt;code&gt;9&lt;/code&gt; 表示数值为 9，&lt;code&gt;-1&lt;/code&gt; 表示该值位于十分位（10&lt;sup&gt;-1&lt;/sup&gt;）。&lt;/p&gt;
&lt;p&gt;对于数值 token &lt;span class="math"&gt;\(t_{v,p}\)&lt;/span&gt;，&lt;span class="math"&gt;\(v\)&lt;/span&gt; 表示该 token 的数值，&lt;span class="math"&gt;\(p\)&lt;/span&gt; 表示该 token 数值的位置，词嵌入的第 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 维按下式计算：&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{NE_{Float}}(v,p,j)=(-1)^j\cdot\frac{v\cdot 10^p}{j+1}
$$&lt;/div&gt;
&lt;p&gt;然后与 SMILES 的常规词嵌入一起加上位置编码进入 XLNet 中进行计算。&lt;/p&gt;
&lt;h4&gt;XLNet&lt;/h4&gt;
&lt;p&gt;输入 RT 的 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 是由 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 个性质 token &lt;span class="math"&gt;\([\boldsymbol{x}^p]_k\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(l\)&lt;/span&gt; 个文本 token &lt;span class="math"&gt;\([\boldsymbol{x}^t]_l\)&lt;/span&gt; 拼接而成，即&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{x}=[\boldsymbol{x}^p,\boldsymbol{x}^t]_T=[x^p_1,\cdots,x^p_k,x^t_1,\cdots,x^t_l]
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(T=k+l\)&lt;/span&gt;，为整个序列的 token 数量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PLM objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{PLM}\)&lt;/span&gt;）：在原始的 XLNet 中，输入的序列就要做 &lt;span class="math"&gt;\(T!\)&lt;/span&gt; 次的排列，将掩盖的 token 放置到序列末端，训练目标是使模型能够预测出掩盖的 token。如上图中 PLM objective 所示，由于这种训练方法是随机选取，打断了整体的 &lt;span class="math"&gt;\(\boldsymbol{x}^p\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(\boldsymbol{x}^t\)&lt;/span&gt;，因而不适合该任务，仅用于预训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Property prediction objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{P}\)&lt;/span&gt;）：对于分子性质预测的回归任务，将表示分子性质的 &lt;span class="math"&gt;\(\boldsymbol{x}^p\)&lt;/span&gt; 全部掩盖并排列置换位置，使用分子的文本 &lt;span class="math"&gt;\(\boldsymbol{x}^t\)&lt;/span&gt; 预测被掩盖的分子性质。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conditional text generation objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{G}\)&lt;/span&gt;）：对于分子生成任务，正与上述过程相反，将表示分子的 &lt;span class="math"&gt;\(\boldsymbol{x}^t\)&lt;/span&gt; 全部掩盖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Self-consistency (SC) objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{SC}\)&lt;/span&gt;）：为了使 RT 能够同时完成回归和生成任务，文章设计了该训练目标：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{J}_\mathrm{SC}=\mathcal{J}_\mathrm{G}(\boldsymbol{x})+\alpha\cdot\mathcal{J}_\mathrm{P}(\hat{\boldsymbol{x}})$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; 为权重，&lt;span class="math"&gt;\(\hat{\boldsymbol{x}}=[\boldsymbol{x}^p,\hat{\boldsymbol{x}}^t]\)&lt;/span&gt; 为生成的样本。该训练任务就是先使用分子性质生成分子，再用生成的分子预测其性质。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;使用 SELFIES 作为分子表示，许多研究表明，相比 SMILES，SELFIES 在分子生成任务上更具有优势。&lt;/p&gt;
&lt;p&gt;Synthetic QED dataset：由 ChEMBL 得到的约 160 万个分子，约 140 万用于训练，1000 条数据用于验证，10000 条数据用于测试。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;p&gt;文章中使用 RT 在化学反应、蛋白质性质预测等任务上测试了模型性能，这里仅以分子生成与分子性质预测的任务为例。&lt;/p&gt;
&lt;p&gt;在 QED 数据集上，先使用 &lt;span class="math"&gt;\(\mathcal{J}_\mathrm{PLM}\)&lt;/span&gt; 训练模型，至验证集数据的指标收敛后，再每 50 轮用 &lt;span class="math"&gt;\(\mathcal{J}_\mathrm{P}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{G}\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(\mathcal{J}_\mathrm{SC}\)&lt;/span&gt; 轮流微调（Alternate），不同模型设定的结果如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9168?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9168?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;从实验结果中可以看出，（1）SELFIES 在生成任务上更有优势，但在回归任务上稍逊于 SMILES；（2）不论是回归还是生成任务，预训练使模型的表现提升；（3）设计的数值编码器有利于模型识别数值信息，提升模型表现；（4）在微调阶段轮流使用不同的训练任务，使模型在回归和生成两种任务上的泛化能力更好，在回归和生成单个任务上都具有与单任务模型接近甚至更优的表现。&lt;/p&gt;
&lt;p&gt;能够处理回归与生成两种任务的模型也可以用于实现分子的性质优化，具体过程是设定一个 seed 分子以及目标的性质（primer），模型随机掩盖分子中 token 再通过 primer 将 token 预测出来，得到优化后的新分子，再通过新分子计算其性质的预测值，下图展示了在两种不同的数据集上微调得到的模型实现分子性质优化的样例。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9169?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9169?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了回归 Transformer（RT）模型，该模型以 XLNet 为主要的结构，文章增加了数值编码器用于获取数值信息，并设计了不同的训练模式使模型在预训练-微调后能够完成数值回归与序列生成两种不同的任务。RT 设计用于数值回归与序列生成，因此也可以用于蛋白性质预测、反应预测等。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>旧书市场淘书记</title><link href="https://leonis.cc/zai-lu-shang/2023-05-22-wander-in-old-book-shop.html" rel="alternate"></link><published>2023-05-22T00:00:00+08:00</published><updated>2023-05-22T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-22:/zai-lu-shang/2023-05-22-wander-in-old-book-shop.html</id><summary type="html">&lt;p&gt;不得不说，在北方诸多城市中，天津的二手旧物市场可以说是相当火热的。我猜测的原因有二，一则是天津的老龄人口占比多，古玩旧物收藏有很大的受众；二则是得益于近代天津经济、文化的繁荣，许多官商士绅定居在此，天津民间仍流通有十分 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;不得不说，在北方诸多城市中，天津的二手旧物市场可以说是相当火热的。我猜测的原因有二，一则是天津的老龄人口占比多，古玩旧物收藏有很大的受众；二则是得益于近代天津经济、文化的繁荣，许多官商士绅定居在此，天津民间仍流通有十分具有价值的骨董。&lt;/p&gt;
&lt;p&gt;我对古玩是一窍不通，再加之俚谚「多看少买」的教育，更有许多低劣到我都能看出的赝品，常引得我在心中暗笑，所以我对那些地摊上的古玩也一点不感兴趣。但旧物中有一门类却是我的心头好，那就是旧书。&lt;/p&gt;
&lt;p&gt;我一向认为书应当是用来读的，次之才是历史等其他价值。由于许多好书由于各种原因不再出版了，或是更改了原来的版本，没有旧版本更好读了，于是有了「藏书」的群体去搜罗这些旧书。所以「藏书」的「藏」不该是像对待金银珠宝那样的「秘藏」，而是作「保存」解。这是我的藏书主张，也是我搜集旧书的信条。&lt;/p&gt;
&lt;p&gt;之所以提及以上原则，还是因为天津旧货市场上的旧书实在太多了，近乎可以同逛新书店一般，不带任何想法去，抱回好几摞的书，为了避免这种无谓的金钱开销，必须要有筛选的准绳。前些天在反复告诉自己&lt;dot&gt;买书是为了读书&lt;/dot&gt;后，终于敢大胆淘了几本书，对其中几本实在喜欢得紧，也算小有收获。&lt;/p&gt;
&lt;h2 id="qie-jie-ting-za-wen-qie-jie-ting-za-wen-mo-bian"&gt;《且介亭杂文》《且介亭杂文末编》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="且介亭杂文" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9071?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="且介亭杂文" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9071?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;人文社 1973 年出版的鲁迅作品集应该是最优良的鲁迅作品版本，因为印量大，价格也不贵，但有几本很少见，凑齐全套并不容易。所以我一般是遇见了品相较好且为手中所无才购买，一切都随缘，并不特意搜集。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="且介亭杂文内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9070?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="且介亭杂文内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9070?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《且介亭杂文末编》，是我在一堆未经整理的书堆中翻找出来的，售 5 元。人民文学出版社 1973 年 4 月北京 1 版 1 印，扉枼钤「天津市第一机械工业学校图书舘藏书」，内枼整洁，纸张坚韧泛黄。唯一不美的是封面钤「不外借」圆印且封面有墨渍污损。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="且介亭杂文扉枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9072?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="且介亭杂文扉枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9072?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《且介亭杂文》，由我在另一书摊上访得，要价 4 元。人民文学出版社 1973 年 6 月山西 1 版 1 印，扉枼有 82 年的购书识记，内枼整洁，纸张洁白，可惜曾遭水浸，整册书都有湿后的压痕。鲁迅冠以「且介亭杂文」为名的集子共有 3 册，那么我还差一册《且介亭杂文二集》就可成一小帙了。&lt;/p&gt;
&lt;h2 id="tang-shi-xuan-yi-wei-liu-zhao-shi-xuan"&gt;《唐诗选》《汉魏六朝诗选》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐诗选与汉魏六朝诗选" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9078?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐诗选与汉魏六朝诗选" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9078?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;人文社的文学类古籍也具有口碑，可这一套《中国古典文学读本丛书》让我颇为困惑。这一套丛书中的书籍都具有类似的封面和题签，古雅简洁，装帧精美，而且编者与注者都是各领域的权威，内容也很精良，我十分喜欢。可是这一套丛书中兼有简体横排本和繁体竖排本，例如《唐诗选》和《汉魏六朝诗选》就都是简体横排，对简体横排介怀者在挑选这一套书时务必留意。能翻看时一看便知，但有时书商将书用塑料纸包装起来，不允翻看内枼，这时可以根据书口方向分辨，书口向右者为简体横排，书口向左者为繁体竖排本。在读古典文学时，我当然更喜欢用繁体竖排，这套丛书夹杂的两种版本让我困惑又纠结。&lt;/p&gt;
&lt;p&gt;此一册《唐诗选（上）》，中国社科院文学研究所编，全套为上下两册，因此仅售我 5 元，待有机会再访下册，这套《唐诗选》印量很大，可货比三家，寻找品相好，更适合翻阅的版本。人民文学出版社 1978 年 4 月北京 1 版 1 印，内枼整洁，纸张泛黄。许多人认为这套集子选的诗并不好，但我只以其简体横排为遗憾。唐诗存世量极大，唐朝诗人又如群星璀璨，无论怎么选都有顾此失彼之嫌，这套集子已经尽可能选出唐朝代表性诗人的作品，注释详略得当，限于篇幅可能未选许多代表作，但对于业余的爱好者概览唐诗完全是足够的了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="汉魏六朝诗选内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9079?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="汉魏六朝诗选内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9079?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《汉魏六朝诗选》，余冠英选注，可能由于印量少些，再加上这家书商的眼光比其他家更利，竟要价 10 元，不过余冠英的注本，加之品相不错，这个价格并不亏。人民文学出版社 1979 年 3 月北京 1 版 2 印，内枼整洁，纸张微黄。&lt;/p&gt;
&lt;h2 id="du-fu-shi-xuan-song-shi-xuan-zhu"&gt;《杜甫诗选》《宋诗选注》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="杜甫诗选与宋诗选注" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9080?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="杜甫诗选与宋诗选注" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9080?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;同样是《中国古典文学读本丛书》，《杜甫诗选》和《宋诗选注》都是繁体竖排，版式相同，老铅字实在赏心悦目，我十分钟爱这两本。&lt;/p&gt;
&lt;p&gt;此一册《杜甫诗选》，冯至选，要价 10 元，还价不允，无奈购下。人民文学出版社 1987 年北京 1 版 11 印，内枼整洁，纸张洁白，摩挲纸面铅字凹痕明显，字画如新，真令人边不释手。《杜甫诗选》并不是最好的杜甫诗集，但又是读杜诗难以绕开的选本。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="杜甫诗选内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9081?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="杜甫诗选内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9081?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《宋诗选注》，钱锺书选注，售 2 元，可以说是捡到的最大漏。人民文学出版社 1982 年 7 月北京 1 版重庆 1 印，内枼整洁，纸张洁白柔韧，字画清晰，惜其封面有折痕。这本集子是由钱锺书选、钱锺书选的宋诗，读宋诗的人可能不多，但是这本集子是宋诗最好的选注本。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="宋诗选注内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9083?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="宋诗选注内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9083?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="tang-song-ci-xuan-shi"&gt;《唐宋詞選釋》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐宋詞選釋" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9082?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐宋詞選釋" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9082?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《唐宋詞選釋》，俞平伯编，售 10 元，叹无人识此宝又恐有人抢购，立马购入。人民文学出版社 1979 年北京 1 版 1 印，扉枼钤「天津自行车二厂图书舘」，内枼整洁，纸张洁白柔韧，铅字的字画虽不如前面两本清晰，但同样令人赏玩不忍释手。俞平伯的《唐宋詞選釋》是读宋词的入门，选、释皆精良，说是读宋词必读并不为过。其实家中已有一本《唐宋詞選釋》，但已经快要脱胶散枼，遇到品相如此好的一本，真令我欣喜！&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐宋詞選釋内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9084?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐宋詞選釋内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9084?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="tang-shi-san-bai-shou-xin-zhu"&gt;《唐詩三百首新注》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐詩三百首新注" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9094?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐詩三百首新注" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9094?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《唐詩三百首新注》，金性尧注，售 5 元。上海古籍出版社 1980 年上海 1 版 1 印，扉枼有 00 年购书识记，竟购于内蒙而流入我手。内枼如新，纸张洁白柔韧，适合翻阅。看多了总感觉上古的铅字整体比人文的更好，字画更清晰，字形也更优美，但若是真让我哪些细节上有差异则有些困难。《唐诗三百首》是家喻户晓的唐诗集子，中华书局前几年覆刻的《唐诗三百首》是更好的版本，版式古雅且价格低廉，现在也很容易买到，但肯定是激光排印而不是铅印了。这本《唐詩三百首新注》静静躺在一角，封面的金字闪耀动人，立马吸引了我的注意，展卷翻阅，心甚悦之，遂购入。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐詩三百首新注扉枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9093?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐詩三百首新注扉枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9093?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐詩三百首新注内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9098?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐詩三百首新注内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9098?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后以全部收获的合影作结吧，共计约 50 元，从堆积成山的书堆里挑出这几本，真可谓是如大浪淘沙一般的「淘」书。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="淘书收获" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9099?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="淘书收获" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9099?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="藏书"></category></entry><entry><title>文献总结｜一种用于基于结构药物设计的 3D 生成模型</title><link href="https://leonis.cc/sui-sui-nian/2023-05-19-summary-doi.org/10.48550/arXiv.2203.10446.html" rel="alternate"></link><published>2023-05-19T00:00:00+08:00</published><updated>2023-05-19T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-19:/sui-sui-nian/2023-05-19-summary-doi.org/10.48550/arXiv.2203.10446.html</id><summary type="html">&lt;p&gt;本文介绍于 2021 年彭健课题组发表在 NeurIPS 2021 上的一篇文章，文章原标题为 A 3D Generative Model for Structure-Based Drug Design，文章提出了一种能够针对指定的蛋白生成药物分子的 3D 生成模型，在利用蛋白空间信息的情况下生成分子，实现基于结构的药物设计。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2203.10446" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2203.10446&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2021 年彭健课题组发表在 NeurIPS 2021 上的一篇文章，文章原标题为 A 3D Generative Model for Structure-Based Drug Design。&lt;/p&gt;
&lt;p&gt;基于结构药物设计中的一个基本问题是针对指定的蛋白结合位点生成分子，目前解决这一问题的深度学习方法可以分为两类：基于字符序列与基于图的方法。但不论是基于字符序列的 1 维模型，还是基于图的 2 维模型，其本质上缺少蛋白质 3 维空间中的信息。为了获取空间信息，目前也出现了在 3D 空间中实现分子生成的模型，但这些模型只能生成较小的分子，无法有效生成类药的更大分子。&lt;/p&gt;
&lt;p&gt;因此，文章提出了一种能够针对指定的蛋白生成药物分子的 3D 生成模型，在利用蛋白空间信息的情况下生成分子，实现基于结构的药物设计。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;蛋白的结合位点可以定义为原子的集合 &lt;span class="math"&gt;\(\mathcal{C}=\{(\boldsymbol{a}_i,\boldsymbol{r}_i)\}^{N_b}_{i=1}\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(N_b\)&lt;/span&gt; 是结合位点原子的数量，&lt;span class="math"&gt;\(\boldsymbol{a}_i\)&lt;/span&gt; 是第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个原子的特征，&lt;span class="math"&gt;\(\boldsymbol{r}_i\)&lt;/span&gt; 是其空间坐标。可以将在结合位点生成原子的任务视作为模拟结合位点中各位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 上出现原子的概率，也就是模拟原子在结合位点上出现的概率密度 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(e\in\mathcal{E}=\{\mathrm{H},\mathrm{C},\mathrm{O},\cdots\}\)&lt;/span&gt; 代表生成分子中的原子。&lt;/p&gt;
&lt;p&gt;为了对 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt; 建模，文章设计了两个模块：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;上下文编码器（Context Encoder）：使用图神经网络（graph neural networks, GNN）学习环境 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 下各原子的表示；&lt;/li&gt;
&lt;li&gt;空间分类器（Spatial Classifier）：输入任意位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt;，集合该位置附近所有上下文原子的表示，输出预测结果 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;上下文编码器&lt;/h4&gt;
&lt;p&gt;上下文编码器用于提取特征，获得各原子的表示，在该任务中，对原子表示有两个要求：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;原子表示不应只具有本身的信息，还应具有环境中的信息；&lt;/li&gt;
&lt;li&gt;在旋转和平移变换后，原子性质的性质不会发生改变，原子表示应具有旋转和平移不变性。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基于以上两点要求，文章使用了旋转平移不变的图神经网络。&lt;/p&gt;
&lt;p&gt;首先，针对蛋白结合位点构建 k-近邻图，基于结合位点 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 中各原子的距离得到图 &lt;span class="math"&gt;\(\mathcal{G}=\langle\mathcal{C},\boldsymbol{A}\rangle\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\boldsymbol{A}\)&lt;/span&gt; 为邻接矩阵，将 k-近邻中的第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个原子记作 &lt;span class="math"&gt;\(N_k(\boldsymbol{r}_i)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;接着，编码器将 &lt;span class="math"&gt;\(\mathcal{G}\)&lt;/span&gt; 中所有结点原子的特征 &lt;span class="math"&gt;\(\{\boldsymbol{a}_i\}\)&lt;/span&gt; 转化为嵌入表示 &lt;span class="math"&gt;\(\{\boldsymbol{h}^{(0)}_i\}\)&lt;/span&gt;，然后进入消息传递层。&lt;/p&gt;
&lt;p&gt;一般的 GNN 消息传递过程定义为&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{h}^{(\ell+1)}_i=\sigma\left(\boldsymbol{W}^\ell_\mathrm{self}\boldsymbol{h}^{(\ell)}_i+\boldsymbol{W}^\ell_\mathrm{nergh}\sum_{j\in\mathcal{N}}\boldsymbol{h}^{(\ell)}_j\right)
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}\)&lt;/span&gt; 为模型需要训练的参数，&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; 为激活函数。从上式中可以看出，GNN 的消息传递是在将 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 结点周围临近的 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 结点的信息按权重聚集起来。&lt;/p&gt;
&lt;p&gt;在文章中所使用的消息传递过程为&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{h}^{(\ell+1)}_i=\sigma\left(\boldsymbol{W}^\ell_0\boldsymbol{h}^{(\ell)}_i+\sum_{j\in N_k(\boldsymbol{r}_i)}\boldsymbol{W}^\ell_\mathrm{1}\boldsymbol{w}(d_{ij})\odot\boldsymbol{W}^\ell_2\boldsymbol{h}^{(\ell)}_j\right)
$$&lt;/div&gt;
&lt;p&gt;相比原式，文章在第 2 项中做了一些改动，&lt;span class="math"&gt;\(\boldsymbol{w}(\cdot)\)&lt;/span&gt; 是一个权重网络，&lt;span class="math"&gt;\(d_{ij}\)&lt;/span&gt; 为 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 两个结点间的距离。上述过程就是在聚集信息时，根据距离的远近分配权重，逐个原子计算后得到 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 中所有原子的嵌入表示集合 &lt;span class="math"&gt;\(\{\boldsymbol{h}^{(L)}_i\}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4&gt;空间分类器&lt;/h4&gt;
&lt;p&gt;在空间中的任意位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 上，空间分类器聚集由上下文编码器得到的原子的嵌入表示：&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{v}=\sum_{j\in N_k(\boldsymbol{r})}\boldsymbol{W}_0\boldsymbol{w}_\mathrm{aggr}(||\boldsymbol{r}-\boldsymbol{r}_j||)\odot\boldsymbol{W}_i\boldsymbol{h}^{(L)}_j
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{w}_\mathrm{aggr}(\cdot)\)&lt;/span&gt; 同样是一个权重网络。在这一步中，类似地根据任意位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 与周围结点间的距离 &lt;span class="math"&gt;\(||\boldsymbol{r}-\boldsymbol{r}_j||\)&lt;/span&gt; 分配权重，聚集该位置附近出现过原子的信息，得到特征 &lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;最后通过多层感知机、归一化后得到所求概率分布：&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{c}=\mathrm{MLP}(\boldsymbol{v})\\
p(e|\boldsymbol{r},\mathcal{C})=\frac{\exp(\boldsymbol{c}[e])}{1+\sum_{e'\in\mathcal{E}}\exp(\boldsymbol{c}[e'])}
$$&lt;/div&gt;
&lt;h4&gt;取样&lt;/h4&gt;
&lt;p&gt;因为 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt; 需要指定结合位点 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 和位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 得到预测的原子 &lt;span class="math"&gt;\(e\)&lt;/span&gt;，而分子生成需要根据 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 自动分配各原子的位置，所以由 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt; 导出&lt;/p&gt;
&lt;div class="math"&gt;$$p(e,\boldsymbol{r}|\mathcal{C})=\frac{\exp(\boldsymbol{c}[e])}{Z}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(Z\)&lt;/span&gt; 为未定的归一化常数。&lt;/p&gt;
&lt;p&gt;分子生成的过程为，在 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 步骤，使用结合位点（环境） &lt;span class="math"&gt;\(\mathcal{C}_t\)&lt;/span&gt; 由 &lt;span class="math"&gt;\(p(e,\boldsymbol{r}|\mathcal{C}_t)\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\((e_{t+1},\boldsymbol{r}_{t+1})\)&lt;/span&gt;，将 &lt;span class="math"&gt;\((e_{t+1},\boldsymbol{r}_{t+1})\)&lt;/span&gt; 加入到环境 &lt;span class="math"&gt;\(\mathcal{C}_t\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\(\mathcal{C_{t+1}}\)&lt;/span&gt;，再用于预测下一个原子的种类和位置，即&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    &amp;amp;(e_{t+1},\boldsymbol{r}_{t+1})\sim p(e,\boldsymbol{r}|\mathcal{C}_t)\\
    &amp;amp;\mathcal{C}_{t+1}\leftarrow\mathcal{C}_t\cup\{(e_{t+1},\boldsymbol{r}_{t+1})\}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;再增加一个辅助的分类网络用于判断生成原子是否为末端原子，若为末端原子则结束分子生成过程。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9062?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9062?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;CrossDocked 数据集中有 2.25 千万条对接得到的蛋白-配体对数据，经数据清洗后，使用其中的 100000 条数据训练模型，100 条数据作为测试集。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9063?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9063?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章首先测试了模型根据蛋白结合位点生成分子的整体效果。结果如上图所示，模型生成分子的对接打分略差于参考分子，但要更好于同类模型 liGAN，生成分子的 QED 与 SA 甚至好过于参考分子，生成分子的所有指标均好于 liGAN。在各分子性质分布的对比中，相比另两个数据集，生成分子的 QED 向右偏移，具有更好的类药性。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9064?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9064?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;以上结果也可以从生成分子的样例中看出，上图展示了模型针对两个蛋白生成的多个分子，生成分子的对接打分、QED 都要好于参考分子，同时许多生成分子还具有参考分子的类似结构。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9066?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9066?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章测试了模型在 linker 预测上的应用。模型不需要经过重新训练或都微调，只需将结合位点与片段作为初始的环境 &lt;span class="math"&gt;\(\mathcal{C}_0\)&lt;/span&gt;，模型就会根据环境补足片段间的 linker。测试结果如上图所示，与设计用于 linker 预测任务的 DeLinker 相比，文章中的模型在各方面都具有优势。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9065?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9065?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章也列举了 linker 预测结果的样例，虽然模型不一定能预测并找回参考分子，但预测生成的分子中都包含了指定的片段，同时模型是根据蛋白的 3D 信息生成 linker，这在基于结构的药物设计上可以作为应用工具。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章使用 GNN 构建了一种用于基于结构药物设计的分子生成模型，该模型使用 GNN 通过设计用于蛋白 3D 信息的消息传递过程提取结合位点中的空间信息，根据配体各原子在结合位点中各位置出现的概率建立模型，从该概率中取样实现分子生成。模型生成的分子具有蛋白的空间信息，在各方面指标上都具有较好的表现。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category></entry><entry><title>文献总结｜对编码器-解码器模型学习过程中化学结构识别的研究</title><link href="https://leonis.cc/sui-sui-nian/2023-05-13-summary-doi.org/10.1186/s13321-023-00713-z.html" rel="alternate"></link><published>2023-05-13T00:00:00+08:00</published><updated>2023-05-13T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-13:/sui-sui-nian/2023-05-13-summary-doi.org/10.1186/s13321-023-00713-z.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年东京大学发表在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Investigation of chemical structure recognition by encoder–decoder models in learning progress，文章，文章研究了编码器-解码器模型训练过程中对化学结构识别的过程以及将其潜变量作为分子表示用于下游任务的效果。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1186/s13321-023-00713-z" rel="noopener" target="_blank"&gt;doi.org/10.1186/s13321-023-00713-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年东京大学发表在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Investigation of chemical structure recognition by encoder&amp;ndash;decoder models in learning progress，文章，文章研究了编码器-解码器模型训练过程中对化学结构识别的过程以及将其潜变量作为分子表示用于下游任务的效果。&lt;/p&gt;
&lt;p&gt;基于结构的分子表示又被称为描述符，如何获得更好的描述符是化学信息学中很重要的问题。在近年兴起的深度学习领域，编码器-解码器（encoder-decoder, ED）类模型广受关注，以分子的字符序列 SMILES 作为输入，编码器模型会将其转化为一连串蕴含化学学信息的描述符，解码器模型通过该中间变量还原出原来的分子，这种由 SMILES（或其他）分子表示编码至隐空间中的潜变量就可以用作为数字形式的分子表示，用于各种下游任务，这也是自然语言处理中「预训练-微调」的范式。&lt;/p&gt;
&lt;p&gt;在传统方法中，也有例如 ECFP、NFP 一类的分子指纹，但他们只描述了分子中所具有的特定结构，无法由描述符再还原出分子结构。只有更好的表示才能在下游任务得到更好的结果，所以文章研究了在 ED 模型中化学结构的识别过程，ED 模型对化学结构的识别是指模型获得反映化学结构的数字信息和将该信息还原为化学结构的能力。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9036?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9036?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;模型的编码器部分是 3 层的 GRU，后面的全连接层将 GRU 的输出映射到 256 维的潜空间，解码器部分以该潜变量为输入，进入全连接层后同样是 3 层 GRU，输出还原的 SMILES。模型以输出 SMILES 与目标 SMILES 的交叉熵损失作为损失函数，训练模型的过程是使其能通过输入的随机化 SMILES 输出标准 SMILES。&lt;/p&gt;
&lt;p&gt;为了评估将 ED 模型中潜变量作为分子表示的效果，文章使用 ToxCast 中 113 个任务分别训练了 XGBoost，使用其预测结果（即下游任务结果）判断分子表示的优劣。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;模型所使用的训练数据来源于 ZINC15，从中随机获取了 3 千万个分子，后续通过去除非有机物常见原子、去除重原子等方式清洗数据。&lt;/p&gt;
&lt;h3 id="zhi-biao"&gt;指标&lt;/h3&gt;
&lt;p&gt;文章定义了两个指标用于评估 ED 模型的准确率，完全准确率定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{perfect\ accuracy}=\frac 1n\sum^n_iI(t=p)$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 表示正确的 SMILES，&lt;span class="math"&gt;\(p\)&lt;/span&gt; 表示预测的 SMILES，即计算与标签值相同的输出所占比例。&lt;/p&gt;
&lt;p&gt;另一个指标部分准确率定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{partial\ accuracy}=\frac 1n\sum^n_i\left\{\frac{1}{\max(l(t),l(p))}\sum^{\min(l(t),l(p))}_jI(t_i=p_i)\right\}$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(t_i\)&lt;/span&gt; 表示 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 位置上正确的 SMILES 字符，&lt;span class="math"&gt;\(p_i\)&lt;/span&gt; 表示 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 位置上预测的 SMILES 字符，该式即计算所有 SMILES 字符中，预测结果与标签值相同位置上相同的字符所占比例。&lt;/p&gt;
&lt;p&gt;在 XGBoost 模型中，使用 ROC 曲线下面积（AUROC）与 Matthews 相关系数（MCC）评估模型准确率。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9035?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9035?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章首先分别使用 10 k、100 k、1 M 的测诫集测试了训练过程中的 ED 模型，结果如上图所示。随着训练轮次的增加，模型还原出化学结构的准确率也在上升，对比部分准确率与完全准确率，可以发现在相同时刻下，部分准确率要高于完全准确率，这说明模型先学习到了还原出分子中若干个字符组成的小片段，然后才将其拼合还原出整个分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9038?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9038?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;以训练后的还原准确率命名模型，例如 94% 的模型记为「Model_94」，测试各模型的分子表示在 ToxCast 任务上的效果，结果如上图（a）所示。可以看出，除了未经训练完全无法还原出分子的 Model_0，其他模型都能较好地完成分类任务，而且分类效果比较接近。接着文章选定了三类结构的分子，使用 UMAP 降维的方法绘制出了其分子表示在化学空间中的位置，如上图（b）所示，在 Model_0 中，三种结构混杂在一起，很难完成分类，而 ED 模型只需要经过训练，三种结构就区别开来，与前一个实验的结果也吻合，这说明 ED 模型生成的潜变量很适用于分子分类的任务。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9037?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9037?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章比较了训练过程中还原出分子的分子量与 SMILES 长度的准确关系，在上图中只有在黄线上的样本表明还原分子与标签值一致，可以看出虽然 DE 模型在训练的靠前阶段无法还原出分子的特定性质，但其潜变量在化学空间中已经有了区分度，这种区分度足可以完成分类任务，但其所蕴含的化学信息还不能使模型还原出结构。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章研究了 DE 模型训练过程中对化学结构的识别以及将其中的潜变量作为分子表示的效果，文章的实验结果展示了模型训练过程中分子表示的变化，证明了将其作为下游任务的分子表示的可行性。&lt;/p&gt;
&lt;p&gt;在目前，预训练-微调是广泛使用的模型训练范式，而在化学信息学领域，其中的关键步骤，也就是将潜变量作为分子表示尚缺乏研究。文章研究的是较早的 GRU（RNN）所构成的 DE 模型，还应该对 Transformer、GPT 等目前更广泛使用的模型进行潜变量的深入研究。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="RNN"></category></entry><entry><title>文献总结｜我们能用 Transformer 模型快速学会「翻译」活性分子吗？</title><link href="https://leonis.cc/sui-sui-nian/2023-04-28-summary-doi.org/10.1021/acs.jcim.2c01618.html" rel="alternate"></link><published>2023-04-28T00:00:00+08:00</published><updated>2023-04-28T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-28:/sui-sui-nian/2023-04-28-summary-doi.org/10.1021/acs.jcim.2c01618.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Can We Quickly Learn to “Translate” Bioactive Molecules with Transformer Models? 文章使用 MMP 数据训练 Transformer，使其生成具有活性的分子，文章结果表明 Transformer 对于未知靶点也能生成活性分子。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/acs.jcim.2c01618" rel="noopener" target="_blank"&gt;doi.org/10.1021/acs.jcim.2c01618&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Can We Quickly Learn to &amp;ldquo;Translate&amp;rdquo; Bioactive Molecules with Transformer Models? 文章使用 MMP 数据训练 Transformer，使其生成具有活性的分子，文章结果表明 Transformer 对于未知靶点也能生成活性分子。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章所使用的分子数据来源于 ChEMBL 29，包含有 950640 个分子。数据集中的分子都由 SMILES 表示，将其输入 MMP 软件匹配其中的相似分子。输出的数据中，每对数据都由两个 SMILES 构成，形成一个 MMP 对，用 SMIRK 表示两个 SMILES 间的化学转化，最后得到了约 5700 万条 MMP 对。&lt;/p&gt;
&lt;p&gt;接着文章对得到的 MMP 对数据进一步清洗，主要包括两个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;排除 SMIRK 出现次数少于 &lt;span class="math"&gt;\(N_1\)&lt;/span&gt; 的 MMP 对；&lt;/li&gt;
&lt;li&gt;在剩余的 MMP 对中，随机保留 &lt;span class="math"&gt;\(N_2\)&lt;/span&gt; 条数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第 1 步的清洗是为了除去数据中出现频率过少的化学转化，因为它们过于特殊，并不能普适地用于所有分子；第 2 步是为了避免数据中的极端偏向影响模型，因为在数据集中，简单的转化（如 -H &amp;rarr; -CH&lt;sub&gt;3&lt;/sub&gt;）出现频率极高，这会导致模型无法学习到那些复杂的转化。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8914?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8914?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;文章使用 OpenNMT 构建 Transformer 模型，在 SMILES 数据输入模型前，都将其转化为 SELFIES 形式。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8917?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8917?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在训练过程中，使用 OpenNMT 中默认的损失函数，使用困惑度（perplexity）评估模型训练效果。困惑度是自然语言处理中所使用的评估指标，它定义为 &lt;span class="math"&gt;\(ppl=\exp(L/N)\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(L\)&lt;/span&gt; 为损失函数，&lt;span class="math"&gt;\(N\)&lt;/span&gt; 为全部备选的 token 数量。在自然语言处理中，模型根据前一个 token 预测下一个 token，并将其连成句子，困惑度的意义就是模型在获取前一个 token 的情况下，概率较高的下一个 token 的数量，所以困惑度越小时，表明模型能在目前分布下生成更合理的句子。同样，将其应用于分子生成模型，困惑度就可以表示分枝原子上可以备选的连接原子。&lt;/p&gt;
&lt;p&gt;此外，文章测试了模型对未知靶点生成分子的效果。在上一步得到的数据中，分别除去对 COX2、DRD2 或 HERG 有活性的分子，分别用三种数据训练 Transformer，最后得到的各模型对指定靶点「不可知」。文章又将去除掉的活性分子根据活性大小分为前 5% 与后 95% 的子数据集，用后 95% 作为模型输入，测试模型是否能输出前 5% 的分子。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;h4&gt;Transformer 可以为未知靶点生成活性分子&lt;/h4&gt;
&lt;p&gt;分别从训练数据中除去 COX2、DRD2 或 HERG 的活性分子，训练了 3 个 Transformer 模型，模型无法得到关于特点靶点的信息。将对相应靶点具有活性的后 95% 分子作为输入，模型生成的分子不仅满足相应的化学规则，而且相当数量的活性分子，说明模型具有相当好的泛化能力。生成分子的结果如下图所示，横轴表示分子与前 5% 高活性分子的相似性，竖轴表示分子活性，其中蓝色圆点表示生成的分子，红色菱形表示输入模型的分子。图中只展示了一部分输入-输出的变换，用箭头表示，可以看出许多生成的分子都是往右上方向移动，也就是生成活性更高、与高活性分子更相似的结构。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8916?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8916?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h4&gt;Transformer 可以为苗头化合物发现生成新颖分子&lt;/h4&gt;
&lt;p&gt;文章统计了训练集与输入-输出分子中的 SMIRKS 化学转化，在模型生成的结果中有 1086 种化转化并未出现在训练集中，所以文章认为模型可以生成新颖的化学结构。一部分化学转化如下图所示，训练集中的化学转化都是 MMP 转化，而图中的化学换化显然并不符合规则，而是模型根据训练集中的信息所新造的化学转化。&lt;/p&gt;
&lt;p&gt;文章认为使用深度学习实现分子生成并不是将所有可能的 SMIRKS 规则放到输入分子上，这种排列组合的模式势必会大大增加生成的数据量，将有价值的信息淹没。Transformer 能够上下文相关地获取分子信息，并根据信息通过合适的 SMIRKS 规则构建新分子，这种分子生成的手段更有帮助，同时文章发现 Transformer 所使用的 SMIRKS 并不是完全照搬训练集中的数据，而是根据已有信息新构造出的转化规则，这一点可能也是提升 Transformer 分子生成效果的方向。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8915?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8915?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章还认为，相比于更常用的 SMILES，SELFIES 更有利于 Transformer 学习其中的化学信息，因为 SMILES 无法保证分子合法，必须先训练模型使其学习生成合乎规则的分子。相反，使用 SELFIES 的分子生成模型不需要先让模型学会表示分子的语法，极少出现分子不合法的情况，可能使用 SELFIES 表示分子也是能实现文章实验中效果的重要原因。&lt;/p&gt;
&lt;p&gt;文章也指出这只是一个尝试性的工作，还有很多的问题没有解决。首要的一点就是文章使用与活性分子的相似性来评价生成分子的效果，尽管两个分子十分相似，它们也可能具有十分悬殊的活性，这是使用深度学习手段进行药物发现尤需解决的问题。所以文章中的分子生成也只能起到「启发」的作用，并不能真接指导药物化学家找到活性更高的分子。另一点就是文章中并没有对模型做全面的评估与超参数的选择，只是验证的方法的可行性，并没有对比与其他模型的优势。文章中还推测 SELFIES 相比 SMILES 更具有优势，但也未对比两种模型的效果。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章将 Transformer 用于苗头化合物的发现，并且发现 Transformer 对于训练集中不存在的未知靶点也能生成相当数量的活性分子，其中一部分分子与高活性的配体具有很高的相似性。文章还发现，Transformer 生成的结果中，其化学变化并不局限于用于训练的 MMP 数据，这表明 Transformer 具有上下文相关的信息识别能力，利用好这一特性有利于实现活性分子的生成。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>津城海棠</title><link href="https://leonis.cc/zai-lu-shang/2023-04-27-crabapple-of-tianjin.html" rel="alternate"></link><published>2023-04-27T00:00:00+08:00</published><updated>2023-04-27T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-27:/zai-lu-shang/2023-04-27-crabapple-of-tianjin.html</id><summary type="html">&lt;p&gt;五大道的海棠花开了，天津又到了最美的时候。&lt;/p&gt;
&lt;p&gt;几年前初到天津时，给我印象最深刻的就是城中随处可见的海棠花。天津遍植海棠，却不是密密地栽种为若干排。而是漫步在街头时，走过几个街道，转过几个巷口，蓦地发现几枝洁白而间杂洋红的海 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;五大道的海棠花开了，天津又到了最美的时候。&lt;/p&gt;
&lt;p&gt;几年前初到天津时，给我印象最深刻的就是城中随处可见的海棠花。天津遍植海棠，却不是密密地栽种为若干排。而是漫步在街头时，走过几个街道，转过几个巷口，蓦地发现几枝洁白而间杂洋红的海棠花在那静静地开放，而无意间的拜访者也成了唯一的赏花者，独享这春光。至于如庆王府的几处，大概是因为人们喜爱海棠，几代经营之下竟植海棠如林。阳春三月，津城就被海棠花环抱，绝不是一句虚言。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8898?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8898?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;我在故乡不曾见过这样的景象，倒不是没有海棠，却没有天津的海棠这般而引人重视。我一度误把海棠当作天津的市花，我愿意把这一切都推作是天津与海棠有缘。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8906?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8906?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 向着窗口盛开的海棠&lt;/p&gt;
&lt;p&gt;我与朋友晃悠悠地骑着单车往五大道拜访海棠，过桂林路至大理道&amp;mdash;&amp;mdash;好一趟西南之行。天津与上海相似，在收回各国租界后便以国内诸大小城市为道路名，因而也成了一大特色。想必不少来天津的游人都玩过寻找家乡路牌的游戏，冥冥之中建立起了一种远跨大半个中国的联系。看着大理道旁盛开的海棠，万里之遥的大理也为这春景增色不少吧。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8900?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8900?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 大理道上的海棠花&lt;/p&gt;
&lt;p&gt;不及步入五大道，已经能望见一批批游人往前走去。天津市也乐于营造海棠的氛围，工作人员用围挡临时封闭了路段，仅允游人入内。但这并不意味着五大道内有多少宽敞余裕的空间，也休想从容地漫步海棠花下。汹涌的人潮占满了可及之处，我甚至难以看清路面，却能依人群铺开的形状分辨出街道巷衢来。盛放的海棠也不愿与赏花者示弱，两列海棠一字排开，树冠张得极大，好似擎不起满树的海棠花，一树压着一树；树上的海棠也开得极密，一簇压着一簇，全开的压着半开的，半开的压着未开的，组成了繁盛的花群。人群中的人伸着脖子望花群，花群中的花也伸着叶梗看人群。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8899?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8899?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 游人熙熙攘攘地往五大道走去&lt;/p&gt;
&lt;p&gt;可花枝柔弱，阵风吹过，花瓣就零落而被阵风裹至空中，引起人群中阵阵讶叹声&amp;mdash;&amp;mdash;看来这场对决还是人群占了上风。摇落的花瓣混入风中，风顿时有了形状，它们翻腾、盘旋，又扶摇而上，在高处击散而化作花雨，洒向一个个抬头瞪大着眼、惊异得还不及合拢嘴的游人身边。风顿时也有了色彩，大片的雪白与点点嫣红在空中交织，它不像颜料盘中两种颜色相混时那样柔和得出现游丝，而是随着花瓣的翻飞，白红两种颜色相互变换，像极了文人所用的花草笺纸。在阳光的照耀下，空中的花瓣闪出熠熠的光芒，看来这笺纸还多了一道洒金的工艺。风顿时还有了气味，风摘落的花瓣被送至每个人的鼻前，不似桃花那样甜腻，芬芳中更多了几分清爽，虽说海棠将所有芳华都留给了春季，但这种气味总能让我想起初夏的时节。据佛经上说，维摩诘讲经讲到妙处时，有天女散花，赞叹其智慧。这花决计不是海棠，这样色、香、味齐全的海棠岂不正犯佛家所说的「五贼」。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8901?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8901?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 来不及拍下风吹下的海棠花雨，捕捉到几点如霰般翻飞的花瓣&lt;/p&gt;
&lt;p&gt;花雨过后，地上自然也铺满了海棠。明人张岱曾描述他书屋中的海棠「花时积三尺香雪」，诚不诬也，果然处处都积满了如雪般的海棠花瓣。有些店家自然不愿意放过这样的景色，径在店门外摆出桌子招徕顾客。更有好事者撤去桌上的阳伞，恣意在花雨之中，任那些海棠落在盘中的糕点之上，落入茶水之中。虽说道上行人熙熙攘攘，但啜茶人亦可闹中取静，其风雅也如此，丝毫不下于古人。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8909?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8909?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 道上也有卖各种物件的小摊，也落满了海棠花&lt;/p&gt;
&lt;p&gt;我被裹挟在人群中，亦步亦趋地穿过海棠花海，流连的景色如万花筒般绚丽，而正所谓兴尽悲来，又深令人感慨春光易逝。天津人自小便生活在海棠之中，求学于海棠花下，他们是惯看了秋月春风而变得更为冷峻，还是年年见证着花朝花暮，相比他人更多了几分细心肠呢？传闻天津人大多缱绻于故乡，而他们的确是如浮萍一般的移民，终于定居在此，与繁茂的海棠杂处而居，其念兹在兹的心情亦可想而知。我不禁想象，奔走于异乡的游子是否会种下一株海棠，欣赏着它与津城的海棠同样开谢，正与记忆中同。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8908?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8908?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 俨然是粉红色的世界&lt;/p&gt;
&lt;p&gt;归来搦管撰写此文时，已经将近立夏，海棠或许早已落尽。清少纳言曾盛赞落花之后赏花为风雅之事，我虽未有此意，奈何已蹈古人之迹。寓居天津多年，我同样遥望着津城海棠的开谢，正与我的记忆同！&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8907?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8907?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="摄影"></category></entry><entry><title>文献总结｜使用上下文增强的分子表示提升少样本药物发现的效果</title><link href="https://leonis.cc/sui-sui-nian/2023-04-22-summary-openreview.html" rel="alternate"></link><published>2023-04-22T00:00:00+08:00</published><updated>2023-04-22T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-22:/sui-sui-nian/2023-04-22-summary-openreview.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年发表在 ICLR 2023 上的一篇文章，文章原标题为 Context-enriched molecule representations improve few-shot drug discovery，文章介绍了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 通过 Hopfield 网络用上下文数据集少样本的强化分子表示，提升了分子性质预测的准确度。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://openreview.net/forum?id=XrMWUuEevr" rel="noopener" target="_blank"&gt;OpenReview&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年发表在 ICLR 2023 上的一篇文章，文章原标题为 Context-enriched molecule representations improve few-shot drug discovery，文章介绍了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 通过 Hopfield 网络用上下文数据集少样本的强化分子表示，提升了分子性质预测的准确度。&lt;/p&gt;
&lt;p&gt;深度学习已经成为了药物发现中的重要工具，但目前大部分深度学习方法都是通过大训练集获得分子信息。药物发现中的深度学习方法通常需要大量的生物试验数据，这在实际的药物研发过程中很难获取。少样本学习解决了药物发现中有效数据较少的问题，少样本学习主要有 3 种方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于数据增强的方法（Data-augmentation based approaches）：变换已有数据达到增加数据量的目的。&lt;/li&gt;
&lt;li&gt;基于词嵌入与最近邻的方法（Embedding-based and nearest neighbour approaches learn approaches）：学习词嵌入的空间，从已有数据邻近位置取得新数据（相似分子）。&lt;/li&gt;
&lt;li&gt;基于优化和微调的方法（Optimization-based or fine-tuning methods）：将大规模的预训练模型放在已有数据上微调，使其迁移到新的化学空间。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文章提出了一种新的 MHNfs 模型用于少样本的药物发现，模型使用联想记忆来提取原始数据中的共现和协变结构从而强化其分子表示，在少样本数据集 FS-Mol 上达到了最佳效果。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="yuan-li"&gt;原理&lt;/h3&gt;
&lt;p&gt;药物发现中所使用的模型 &lt;span class="math"&gt;\(g(\boldsymbol{m})\)&lt;/span&gt; 用于在给定分子表示 &lt;span class="math"&gt;\(\boldsymbol{m}\in\mathcal{M}\)&lt;/span&gt; 的情况下预测分子性质或活性 &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;。深度学习模型中的分子编码器将分子的一些低级表示（如 SMILES、分子图等）映射为模型空间的表示 &lt;span class="math"&gt;\(f^\mathrm{ME}:\mathcal{M}\rightarrow\mathbb{R}^d\)&lt;/span&gt;，再通过后续计算给出分子性质。&lt;/p&gt;
&lt;p&gt;在少样本的情况下，只有分子的小数据集 &lt;span class="math"&gt;\(\{\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N\}\)&lt;/span&gt; 与对应分子是否具有活性的数据 &lt;span class="math"&gt;\(\boldsymbol{y}=\{y_1,\cdots,y_N\}\)&lt;/span&gt;。这里将数据集 &lt;span class="math"&gt;\(\{(\boldsymbol{x}_n,y_n)\}_{n=1}^N\)&lt;/span&gt; 称为 support set，少样本学习就是要正确预测不在 support set 中 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 所对应的 &lt;span class="math"&gt;\(y\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;文章中的模型分为 3 个模块：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    \text{context module: }&amp;amp;\quad&amp;amp;\boldsymbol{m}'&amp;amp;=f^\mathrm{CM}(\boldsymbol{m},\boldsymbol{C})\\
    &amp;amp;\quad&amp;amp;\boldsymbol{X}'&amp;amp;=f^\mathrm{CM}(\boldsymbol{X},\boldsymbol{C})\\
    \text{cross-attention module: }&amp;amp;\quad&amp;amp;[\boldsymbol{m}'',\boldsymbol{X}'']&amp;amp;=f^\mathrm{CAM}([\boldsymbol{m}',\boldsymbol{X}'])\\
    \text{similarity module: }&amp;amp;\quad&amp;amp;\hat{y}&amp;amp;=f^\mathrm{SM}(\boldsymbol{m}'',\boldsymbol{X}'',\boldsymbol{y})
\end{align}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{m}\in\mathbb{R}^d\)&lt;/span&gt; 是分子的词嵌入表示，&lt;span class="math"&gt;\(\boldsymbol{X}\in\mathbb{R}^{d\times N}\)&lt;/span&gt; 是 support set 中分子的词嵌入表示，&lt;span class="math"&gt;\(\boldsymbol{C}\in\mathbb{R}^{d\times M}\)&lt;/span&gt; 是另一个更大的分子数据集（context set）中分子的词嵌入表示。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f^\mathrm{CM}\)&lt;/span&gt; 交换 &lt;span class="math"&gt;\((\boldsymbol{m},\boldsymbol{C})\)&lt;/span&gt; 间与 &lt;span class="math"&gt;\((\boldsymbol{X},\boldsymbol{C})\)&lt;/span&gt; 间的上下文信息，得到强化的表示 &lt;span class="math"&gt;\(\boldsymbol{m}'\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}'\)&lt;/span&gt;。拼合两个增强的表示，&lt;span class="math"&gt;\(f^\mathrm{CAM}\)&lt;/span&gt; 计算两者间注意力，得到进一步增强的 &lt;span class="math"&gt;\(\boldsymbol{m}''\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}''\)&lt;/span&gt;，最后结合二者的信息进行预测。上面的过程可以描述成&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    &amp;amp;\underset{\textsf{symbolic or}\atop\textsf{low-level repr.}}{m}\overset{f^\mathrm{ME}}{\longrightarrow}\underset{\textsf{molecule}\atop\textsf{embedding}}{\boldsymbol{m}}\overset{f^\mathrm{CM}}{\longrightarrow}\underset{\textsf{context}\atop\textsf{repr.}}{\boldsymbol{m}'}\overset{f^\mathrm{CAM}}{\longrightarrow}\underset{\textsf{similarity}\atop\textsf{repr.}}{\boldsymbol{m}''}\\
    &amp;amp;\underset{\textsf{symbolic or}\atop\textsf{low-level repr.}}{x_n}\overset{f^\mathrm{ME}}{\longrightarrow}\underset{\textsf{molecule}\atop\textsf{embedding}}{\boldsymbol{x}_n}\overset{f^\mathrm{CM}}{\longrightarrow}\underset{\textsf{context}\atop\textsf{repr.}}{\boldsymbol{x}_n'}\overset{f^\mathrm{CAM}}{\longrightarrow}\underset{\textsf{similarity}\atop\textsf{repr.}}{\boldsymbol{x}_n''}
\end{align}
$$&lt;/div&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8889?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8889?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MHNfs 由 Transformer 中的 encoder 部分构建，具有与 Transformer 类似的结构与工作方式。&lt;/p&gt;
&lt;p&gt;模型中的上下文模块由现代 Hopfield 网络（Modern Hopfield Network, MHN）实现：&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{Hopfield}(\boldsymbol{\Xi},\boldsymbol{C}):=(\boldsymbol{W}_E\boldsymbol{C})\mathrm{Softmax}\left(\beta(\boldsymbol{W}_C\boldsymbol{C})^\top(\boldsymbol{W}_\Xi\boldsymbol{\Xi})\right)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{m}'=\mathrm{Hopfield(\boldsymbol{m},\boldsymbol{C})},\quad\boldsymbol{X}'=\mathrm{Hopfield}(\boldsymbol{X},\boldsymbol{C})
$$&lt;/div&gt;
&lt;p&gt;MHN 能够计算两个输入间的注意力，最后更新得到的分子表示就具有参考分子集 &lt;span class="math"&gt;\(\boldsymbol{C}\)&lt;/span&gt; 中的联想记忆。&lt;/p&gt;
&lt;p&gt;交叉注意力模块替换了原来 Transformer 中的多头注意力机制，但功能仍然类似，用于记算输入分子 &lt;span class="math"&gt;\(\boldsymbol{m}'\)&lt;/span&gt; 与 support set &lt;span class="math"&gt;\(\boldsymbol{X}'\)&lt;/span&gt; 之间的注意力，再次更新分子表示：&lt;/p&gt;
&lt;div class="math"&gt;$$[\boldsymbol{m}'',\boldsymbol{X}'']=\mathrm{Hopfield}([\boldsymbol{m}',\boldsymbol{X}'],[\boldsymbol{m}',\boldsymbol{X}'])$$&lt;/div&gt;
&lt;p&gt;在最后的相似性模块中，模型计算输入分子 &lt;span class="math"&gt;\(\boldsymbol{m}''\)&lt;/span&gt; 与 support set &lt;span class="math"&gt;\(\boldsymbol{X}''\)&lt;/span&gt; 中每个分子 &lt;span class="math"&gt;\(\boldsymbol{x}_n''\)&lt;/span&gt; 之间的相似性 &lt;span class="math"&gt;\(k(\boldsymbol{m}'',\boldsymbol{x}_n'')\)&lt;/span&gt;，并使用所有相似性的加权平均表示输入分子，用该表示计算输入分子的性质：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}=\mathrm{Sigmoid}\left(\tau^{-1}\frac 1N\sum_{n=1}^Ny_n'k(\boldsymbol{m}'',\boldsymbol{x}_n'')\right)$$&lt;/div&gt;
&lt;p&gt;文章这么做的理由是，考虑现实中的情况，当药物化学家对某系列化合物只有有限的活性数据（support set）而要预测（同一靶点或类似结构的）一化合物（query molecule）的活性时，化学家会将该化合物与手头已有数据的化合物对比，再在化合物库（context set）中对比，综合考虑各项因素后得出判断。模型所做的 MHN 计算以及平均相似性，就是简化了的上述过程，文章认为这样的设计有助于模型模仿化学家的思考方式。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用用于少样本药物发现的标准数据集 FS-Mol 作为模型的数据集。该数据集中的分子来自于 ChEMBL 27，其中定义了 4938 个训练任务，40 个验证任务与 157 个测试任务，平均每个任务下只有 94 个数据点。&lt;/p&gt;
&lt;p&gt;文章使用 ECFPs 分子指纹与 RDKit 描述符来作为初始的分子表示。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8890?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8890?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;实验结果如上表所示，文章对比了各个模型在 FS-Mol 测试集上的 &amp;Delta;AUC-PR，除了 ADKF-IFT 在 Hydol. 与 Oxid. 小部分任务上优于 MHNfs，其他模型的结果都不如 MHNfs，而且 MHNfs 在全部任务的整体结果上优于其他全部模型，所以文章认为 MHNfs 在 FS-Mol 测试集实现了目前药物发现少样本学习的最优性能。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 参考了现实中化学家面对少样本数据时的策略，通过设想的一种上下文增强的方式更新了输入模型的分子表示，使其具有更多大数据集中的背景信息。在实验中，测试结果表示这种增强的分子表示确实提高了模型预测的准确率，MHNfs 也在该任务上达到了最优的性能。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>从零起步的 Transformer 与代码拆解</title><link href="https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html" rel="alternate"></link><published>2023-04-21T00:00:00+08:00</published><updated>2023-04-21T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-21:/sui-sui-nian/2023-04-21-transformer-from-scratch.html</id><summary type="html">&lt;p&gt;自 Google 的论文 &lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Attention Is All You Need&lt;/a&gt; 发布后，几年内涌现了大量基于 Transformer 的模型，俨然形成了 Transformer 横扫人工智能领域的态势。&lt;/p&gt;
&lt;p&gt;网络上也出现了大量解读论文或是讲解 Transformer 的文章，其中也不乏许多高水平人工智能从业者的解读。虽然有些可以称得上是高屋建瓴，但 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;自 Google 的论文 &lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Attention Is All You Need&lt;/a&gt; 发布后，几年内涌现了大量基于 Transformer 的模型，俨然形成了 Transformer 横扫人工智能领域的态势。&lt;/p&gt;
&lt;p&gt;网络上也出现了大量解读论文或是讲解 Transformer 的文章，其中也不乏许多高水平人工智能从业者的解读。虽然有些可以称得上是高屋建瓴，但相当大部分难以避免地落入了知识的诅咒（curse of knowledge），起码在我初开始了解 Transformer 时难以读懂这些文章。&lt;/p&gt;
&lt;p&gt;随着 Transformer 广泛应用到各领域，学习 Transformer 也成了一门「显学」。尽管我已经能读懂一些更深层次的 Transformer 剖析，但我还是未找见一篇合我心意的入门文章，所以我希望能撰写一篇小文章，以初学者的角度来讲解 Transformer，是为序。&lt;/p&gt;
&lt;h2 id="xie-zi"&gt;楔子&lt;/h2&gt;
&lt;p&gt;Transformer 是设计用于 NLP 的一种模型，尽管目前 Transformer 所能完成的任务已经大大扩展，但这里还是以最原始的翻译任务为例。&lt;/p&gt;
&lt;p&gt;在翻译任务中，所需要的数据包括原始语句与目标语句，也就是 Transformer 原论文中所指的「input」和「output」，因为名字太容易混淆，还是将其原始语句与目标语句或是「source」与「target」。&lt;/p&gt;
&lt;p&gt;假设 source 为 &lt;code&gt;你好，世界！&lt;/code&gt;，target 为 &lt;code&gt;Hello, world!&lt;/code&gt;，完成这个中译英任务首先要将文本转化为利于模型处理的数值，这一步称为词嵌入（embedding）。&lt;/p&gt;
&lt;p&gt;常见的词嵌入方法有 word2vec 等等，在这里不做介绍。词嵌入步骤大致的流程是先将 &lt;code&gt;你好，世界！&lt;/code&gt; 转化为 &lt;code&gt;&amp;lt;start&amp;gt; 你好 ， 世界 ！ &amp;lt;end&amp;gt;&lt;/code&gt;，每个「词」都用空格划分开，其中 &lt;code&gt;&amp;lt;start&amp;gt;&lt;/code&gt; 与 &lt;code&gt;&amp;lt;end&amp;gt;&lt;/code&gt; 分别表示文本的起讫，这些「词」在 NLP 通常称为「token」。接着再为每个 token 分配索引，例如 &lt;code&gt;&amp;lt;start&amp;gt;&lt;/code&gt; 为 &lt;code&gt;1&lt;/code&gt;，&lt;code&gt;&amp;lt;end&amp;gt;&lt;/code&gt;为 &lt;code&gt;0&lt;/code&gt;，照这个思路，文本就可以转换为 &lt;code&gt;[1 2 3 4 5 0]&lt;/code&gt; 的表示。当然这是很简单的做法，实际上，每个 token 都会被转化为指定维度的向量，用这一连串向量就可以表示文本。&lt;/p&gt;
&lt;p&gt;将上述过程抽象出来，在词嵌入后，可以得到 source 的表示 &lt;span class="math"&gt;\(\boldsymbol{X}=(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_t)\)&lt;/span&gt; 与 target 的表示 &lt;span class="math"&gt;\(\boldsymbol{Y}=(\boldsymbol{y}_1,\boldsymbol{y}_2,\cdots,\boldsymbol{y}_t)\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\boldsymbol{x}_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{y}_i\)&lt;/span&gt; 都是指定维度 &lt;span class="math"&gt;\(d\)&lt;/span&gt; 的向量。&lt;/p&gt;
&lt;p&gt;那么如何使用 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 完成翻译任务呢？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一种&lt;/strong&gt;是使用 RNN 方法，使用当前的 source token &lt;span class="math"&gt;\(\boldsymbol{x}_t\)&lt;/span&gt; 与前一步中生成的 token &lt;span class="math"&gt;\(\hat{\boldsymbol{y}}_{t-1}\)&lt;/span&gt; 生成下一个 token，逐个生成直至句子末尾：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\hat{\boldsymbol{y}}_{t-1},\boldsymbol{x}_t)$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;第二种&lt;/strong&gt;是使用卷积的方法，定义一个窗口长度再通过小范围中的几个 &lt;span class="math"&gt;\(\boldsymbol{x}_i\)&lt;/span&gt; 计算输出：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_{t-1},\boldsymbol{x}_t,\boldsymbol{x}_{t+1})$$&lt;/div&gt;
&lt;p&gt;可以看出，&lt;dot&gt;RNN 很难学习到全局的信息&lt;/dot&gt;，而&lt;dot&gt;卷积方法只能学习到小范围的局部信息&lt;/dot&gt;。&lt;/p&gt;
&lt;p&gt;所以 Transformer 给出了&lt;strong&gt;第三种&lt;/strong&gt;方法，也就是自注意力方法。自注意力机制让模型就当前的 source token &lt;span class="math"&gt;\(\boldsymbol{x}_t\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 中其他 token 的关系给出输出 &lt;span class="math"&gt;\(\hat{\boldsymbol{y}}_t\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_t, \boldsymbol{X})$$&lt;/div&gt;
&lt;h2 id="transformer-jie-gou"&gt;Transformer 结构&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;标准 Transformer 的结构如上图所示，大致分为左侧的 Encoder 与右侧的 Decoder 两个部分。Inputs 与 Outputs 分别是上文所说的 source 与 target，Output Probabilities 是模型输出的各 token 概率，取其中最大概率的 token 就能组织成模型输出结果。&lt;/p&gt;
&lt;h3 id="wei-zhi-bian-ma"&gt;位置编码&lt;/h3&gt;
&lt;p&gt;Transformer 并没有采用 RNN 与卷积方法所使用的序列处理 token 的方法，因而能够实现并行计算并且很大程度上缓解了长期依赖问题（顺序处理长序列容易丢失多个步骤前的信息）。文本中多个 token 间显然有前后的顺序关系，Transformer 使用位置编码的方式来处理顺序信息。&lt;/p&gt;
&lt;p&gt;source 与 target 送入模型，经过常规的词嵌入过程后，还需要在得到的矩阵上加上位置编码，论文将位置编码定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{PE}_{(\mathrm{pos},2i)}=\sin(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathrm{PE}_{(\mathrm{pos},2i+1)}=\cos(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$&lt;/div&gt;
&lt;p&gt;Transformer 将 &lt;span class="math"&gt;\(\mathrm{pos}\)&lt;/span&gt; 位置映射为 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 维的向量，向量中的第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个元素即按上式计算。位置编码的计算公式是构造出的经验公式，不必深究，当然也有许多文章分析了如此构造的原因，这里从略。&lt;/p&gt;
&lt;h3 id="encoder-yu-decoder"&gt;Encoder 与 Decoder&lt;/h3&gt;
&lt;p&gt;许多完成 seq2seq 任务的模型都采用了 encoder-decoder 模式，Transformer 也不例外。简单来说，encoder 将输入编码得到一个中间变量，decoder 解码该中间变量得到输出。&lt;/p&gt;
&lt;p&gt;在 Transformer 中，source 与 target 分别送入 encoder 与 decoder，encoder 计算得到的中间结果再送入 decoder 中与 target 输入进行计算，得到最后的结果，这就是所谓「编码-解码」的工作方式。&lt;/p&gt;
&lt;p&gt;从 Transformer 的结构图中可以看出，模型具有 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 层 encoder 与 decoder 层。其中，encoder 与 decoder 都具有相同的多头注意力层（Multi-Head Attention）、前馈层（Feed Forward）。encoder 与 decoder 的不同在于 decoder 多了一个多头注意力层，在这一层中，encoder 的输出与 decoder 的输入计算注意力。&lt;/p&gt;
&lt;p&gt;还可以注意到，在 encoder 与 decoder 中，每一层后都有一个 Add &amp;amp; Norm 层，用于归一化计算结果。Add &amp;amp; Norm 层的计算方式是将前一层的输入与前一层的输出相加，然后归一化，可以表示为 &lt;span class="math"&gt;\(\mathrm{LayerNorm}(\boldsymbol{x}+\mathrm{Sublayer}(\boldsymbol{x}))\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4&gt;Attention 机制&lt;/h4&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;数据进入 encoder 与 decoder 的内部，首先要通过注意力机制进行计算，这也是 Transformer 的核心。&lt;/p&gt;
&lt;p&gt;文章中将所使用的注意力称为缩放点积注意力（scaled dot-product attention），定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{Q}_{n\times d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{K}_{m\times d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{V}_{m\times d_v}\)&lt;/span&gt; 分别是若干向量 &lt;span class="math"&gt;\(\boldsymbol{q}\in\mathbb{R}^{d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{k}\in\mathbb{R}^{d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{v}\in\mathbb{R}^{d_v}\)&lt;/span&gt; 组成的矩阵。&lt;/p&gt;
&lt;p&gt;单看矩阵的乘法稍显复杂，不妨先用向量说明计算步骤。通过以下方式可以从输入 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 得到向量 &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{q}=\boldsymbol{x}\boldsymbol{W}^Q,\,\boldsymbol{k}=\boldsymbol{x}\boldsymbol{W}^K,\,\boldsymbol{v}=\boldsymbol{x}\boldsymbol{W}^V$$&lt;/div&gt;
&lt;p&gt;其中，&lt;span class="math"&gt;\(\boldsymbol{W}^Q\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{W}^K\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{W}^V\)&lt;/span&gt; 分别表示相应的权重矩阵。&lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; 代表 query，&lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt; 代表 key，&lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt; 代表 value，目的是&lt;dot&gt;用 query 去寻找更匹配的 key-value 对&lt;/dot&gt;。&lt;/p&gt;
&lt;p&gt;因为数量积可以表示两向量的相似程度，一种简单的做法是使用 &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; 与若干个 &lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt; 计算数量积，将其作为匹配分数：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{score}=\boldsymbol{q}\cdot \boldsymbol{k}_i=\boldsymbol{q}\boldsymbol{k}^\top_i$$&lt;/div&gt;
&lt;p&gt;但这样的「注意力」太过于简单，Google 从上述的数量积出发，设计了更为可靠的注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{q},\boldsymbol{k}_i,\boldsymbol{v}_i)=\frac 1 Z\sum_i\exp\left(\frac{\boldsymbol{q}\boldsymbol{k}^\top_i}{\sqrt{d_k}}\right)\boldsymbol{v}_i$$&lt;/div&gt;
&lt;p&gt;首先，式中 &lt;span class="math"&gt;\(1/Z\sum_i x_i\)&lt;/span&gt; 形式的部分是 Softmax 函数的简写，Softmax 函数由下式定义：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Softmax}(x_i)=\frac{\exp(x_i)}{\sum_j\exp(x_j)}$$&lt;/div&gt;
&lt;p&gt;Softmax 函数的作用是将若干数值 &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 归一化，得到的 &lt;span class="math"&gt;\(\mathrm{Softmax}(x_i)\)&lt;/span&gt; 具有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\sum_i\mathrm{Softmax}(x_i)=1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{Softmax}(x_i)\in[0, 1]\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两点性质，所以与概率具有相似的特征，可以用作概率处理。&lt;/p&gt;
&lt;p&gt;其次，式中新增的 &lt;span class="math"&gt;\(\sqrt{d_k}\)&lt;/span&gt; 用于调节内积 &lt;span class="math"&gt;\(\boldsymbol{q}\boldsymbol{k}^\top_i\)&lt;/span&gt; 的大小。当若干内积的大小过于悬殊时，Softmax 函数很容易将其推向 &lt;span class="math"&gt;\(0\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(1\)&lt;/span&gt; 的边界值，这样的数值处理起来没什么意义。&lt;/p&gt;
&lt;p&gt;最后，再次回忆 Transformer 的注意力机制是用 query 去寻找更匹配的 key-value 对。那么上式的意义就很了然了，就是将 query 与各个 key 的匹配分数转化为各个概率，再按各个概率取各个 key 所对应的 value，组合各 value 分量即得到注意力。&lt;/p&gt;
&lt;p&gt;以具有两个 value 的情况为例，需要得到的中间量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;（理解为注意力亦可）可以通过下式计算：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \boldsymbol{z}_1=\theta_{11}\boldsymbol{v}_1+\theta_{12}\boldsymbol{v}_2\\
    \boldsymbol{z}_2=\theta_{21}\boldsymbol{v}_1+\theta_{22}\boldsymbol{v}_2
\end{align}$$&lt;/div&gt;
&lt;p&gt;权值 &lt;span class="math"&gt;\(\theta_{ij}\)&lt;/span&gt;（即上文所说概率）通过下式得到：&lt;/p&gt;
&lt;div class="math"&gt;$$\theta_{ij}=\mathrm{Softmax}\left(\frac{\boldsymbol{q}_i\boldsymbol{k}^\top_j}{\sqrt{d_k}}\right)$$&lt;/div&gt;
&lt;p&gt;将上述运算转为矩阵形式会简洁许多：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{pmatrix}
    \boldsymbol{z}_1 \\
    \boldsymbol{z}_2
\end{pmatrix}=
\begin{pmatrix}
    \theta_{11} &amp;amp; \theta_{12} \\
    \theta_{21} &amp;amp; \theta_{22}
\end{pmatrix}
\begin{pmatrix}
    \boldsymbol{v}_1 \\
    \boldsymbol{v}_2
\end{pmatrix}\\
$$&lt;/div&gt;
&lt;p&gt;可以记作 &lt;span class="math"&gt;\(\boldsymbol{Z}=\boldsymbol{\theta}\boldsymbol{V}\)&lt;/span&gt;，也就是&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$&lt;/div&gt;
&lt;h4&gt;Multi-Head Attention&lt;/h4&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;前一节中解释了 Transformer 中的缩放点积注意力，但在模型中实际并非通过上述方式直接计算，而是通过多头注意力的方式计算注意力。&lt;/p&gt;
&lt;p&gt;如上图所示，多头注意力同样是在计算缩放点积注意力，但与纯粹缩放点积注意力的不同之处在于多头注意力将多个注意力计算步骤叠加了起来。&lt;/p&gt;
&lt;p&gt;叠加的次数为 &lt;span class="math"&gt;\(h\)&lt;/span&gt;，即代表 head，多少个 head 表示需要进行多少次叠加计算。矩阵 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{K}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{V}\)&lt;/span&gt; 进入多头注意力计算步骤后，首先要分别在第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个 head 中进行线性变换并计算注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{head}_i=\mathrm{Attention}(\boldsymbol{Q}\boldsymbol{W}^Q_i,\boldsymbol{K}\boldsymbol{W}^K_i,\boldsymbol{V}\boldsymbol{W}^V_i)$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}^Q_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^K_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^V_i\in\mathbb{R}^{d_\mathrm{model}\times d_v}\)&lt;/span&gt;，注意不同 head 中的线性变换并不同，输出也不同。然后将所有输出 &lt;span class="math"&gt;\(\mathrm{head}_i\)&lt;/span&gt; 拼合在一起，经线性变换后作为注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{MultiHead}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\mathrm{Concat}(\mathrm{head}_1,\mathrm{head}_2,\cdots,\mathrm{head}_h)\boldsymbol{W}^O$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}^O\in\mathbb{R}^{hd_v\times d_\mathrm{model}}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;注意这个过程中数据维数的变化 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 为单头注意力中模型所处理的维数，&lt;span class="math"&gt;\(\boldsymbol{W}^Q_i\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^K_i\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^V_i\)&lt;/span&gt; 的线性变换将 query、key 的维数从 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 提升到 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;，将 value 的维数从 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 提升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;。最后的 &lt;span class="math"&gt;\(\boldsymbol{W}^O\)&lt;/span&gt; 又将拼合起来维数为 &lt;span class="math"&gt;\(hd_v\)&lt;/span&gt; 的注意力转换为模型所处理的维数 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt;。这些线性变换矩阵 &lt;span class="math"&gt;\(\boldsymbol{W}_i\)&lt;/span&gt; 实际上就是模型训练过程中需要学习的一部分参数。&lt;/p&gt;
&lt;p&gt;至于为什么要用多头的方式计算注意力，这就是个很复杂的问题了。就我的理解而言，由于每个 head 中的线性变换矩阵 &lt;span class="math"&gt;\(\boldsymbol{W}_i\)&lt;/span&gt;，多头注意力实际上是将 query、key、value 映射到不同的子空间中，在多个不同的子空间中寻找与 query 最匹配的 key-value。由于不同子空间中具有不同方面的信息，最后将其拼接起来作为结果，这样可以更多地从多个方面捕获数据中的信息。&lt;/p&gt;
&lt;h4&gt;Feed-Forward 层&lt;/h4&gt;
&lt;p&gt;在多头注意力层之后，就是前馈层，前馈层只在位置方向上计算，所以原文描述其为 position-wise。进入前馈层的数据在该层中先做 1 次线性变换，维度升高，再经过 RELU 激活函数，最后再做 1 次线性变换，维度降低，输入与输出前馈层的维度相同。上述过程可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{FFN}(\boldsymbol{x})=\max(0,\boldsymbol{x}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2$$&lt;/div&gt;
&lt;p&gt;RELU 激活函数定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{ReLU}(x)=x^+=\max(0,x)$$&lt;/div&gt;
&lt;p&gt;即式中的 &lt;span class="math"&gt;\(\max\)&lt;/span&gt;，按原文中的例子，&lt;span class="math"&gt;\(\boldsymbol{W}_1\)&lt;/span&gt; 使 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 由 512 维升高到 2048 维，&lt;span class="math"&gt;\(\boldsymbol{W}_2\)&lt;/span&gt; 使 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 计算由 2048 维再降至 512 维，升维与降维的过程也是为了更好地获得数据中的信息。&lt;/p&gt;
&lt;h3 id="transformer-ji-suan-bu-zou"&gt;Transformer 计算步骤&lt;/h3&gt;
&lt;p&gt;Transformer 模型大致就由上述的几个层连接在一起构成，但或许还是觉得朦朦胧胧，比如究竟什么才是 query、key、value 等等。不妨再来看看 Transformer 的结构图，这一次已熟知大部分模块的工作原理了，所以只看数据流入与流出各模块的路线。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;作为 source 的 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与作为 target 的 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 分别从下方的左右两侧进入模型。&lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 都要经过词嵌入并加上位置编码，按以下方式更新：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    \boldsymbol{X}&amp;amp;\leftarrow\mathrm{Embedding}(\boldsymbol{X})+\mathrm{PE}(\boldsymbol{X})\\
    \boldsymbol{Y}&amp;amp;\leftarrow\mathrm{Embedding}(\boldsymbol{Y})+\mathrm{PE}(\boldsymbol{Y})
\end{align}
$$&lt;/div&gt;
&lt;p&gt;接着 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 分别进入 encoder 与 decoder，可以注意到数据分作 4 条路线，这意味着将数据复制 4 次。先看进入多头注意力层的 3 条数据，以 encoder 为例，在这一层中就是在计算&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$&lt;/div&gt;
&lt;p&gt;不言自明，在这里的 query、key、value 三者都是 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt;，是在 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 内部计算注意力，因此称其为&lt;strong&gt;自注意力&lt;/strong&gt;（self-attention）。&lt;/p&gt;
&lt;p&gt;在后续的 Add &amp;amp; Norm 层中，计算&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X}))$$&lt;/div&gt;
&lt;p&gt;在前馈层与后续的 Add &amp;amp; Norm 层输的输出结果也可想而知：&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\max(0,\boldsymbol{X}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2)$$&lt;/div&gt;
&lt;p&gt;这里的 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 分作两路进入到 decoder 中，在 decoder 的该多头注意力层中，query 与 key 为 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt;，而 value 为类似步骤得到的 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt;，该层的输出为&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{Z}=\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{Y})$$&lt;/div&gt;
&lt;p&gt;这也是 decoder 与 encoder 的关键不同。输出结果 &lt;span class="math"&gt;\(\boldsymbol{Z}\)&lt;/span&gt; 完成后续的计算过程后，就得到各 token 的概率，用各 token 替换即可得到模型输出的文本结果。&lt;/p&gt;
&lt;p&gt;{note begin}有兴趣的读者不妨根据各矩阵的形状尝试计算一下各个变量的维度在 Transformer 在各步骤中是如何变化的，一定会对 Transformer 的计算过程收获更深的了解。{note end}&lt;/p&gt;
&lt;h2 id="dai-ma-chai-jie_1"&gt;代码拆解&lt;/h2&gt;
&lt;p&gt;有了对 Transformer 原理的基本认识，就可以动手实现一个 Transformer 了，通过代码更深入了解 Transformer 的一些细节。这里使用 PyTorch 搭建一个标准的 Transformer，参考代码见 &lt;a href="https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; aladdinpersson / Machine-Learning-Collection &lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;代码中的各模块如下图所示，接下来对各模块逐个拆解。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="positionembedding"&gt;PositionEmbedding&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;import math
import torch
import torch.nn as nn


class PositionEmbedding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        # d_model 为模型处理数据的维数，即公式中 d_k
        # max_len 表示模型处理的最大 token 数量
        super(PositionEmbedding, self).__init__()

        # 生成大小为 max_len * d_model 的零矩阵
        pe = torch.zeros(max_len, d_model)
        # 生成大小为 max_len * 1 的位置矩阵
        position = torch.arange(max_len).unsqueeze(1)
        # 计算位置编码
        div_term = torch.exp(torch.arange(0, d_model, 2) * - (math.log(10000.0) / d_model))
        x = position * div_term
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = self.pe[:, :x.size(1)]
        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;首先实现位置编码模块。在 PyTorch 中，用于搭建神经网络的模块都要继承 &lt;code&gt;nn.Module&lt;/code&gt;，PyTorch 会通过 &lt;code&gt;__call__()&lt;/code&gt; 调用模块的 &lt;code&gt;forward()&lt;/code&gt; 的方法进行前向传播。简单来讲就是，&lt;code&gt;PositionEmbedding(x)&lt;/code&gt; 的功能等同于 &lt;code&gt;PositionEmbedding.forward(x)&lt;/code&gt;，但不能使用 &lt;code&gt;PositionEmbedding.forward(x)&lt;/code&gt;，因为 PyTorch 做了许多条件的判定和优化。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;torch.arange(num)&lt;/code&gt; 的功能类似于 Python 中的 &lt;code&gt;range(num)&lt;/code&gt;，用于生成文本各 token 的顺序位置索引。&lt;code&gt;unsqueeze(dim)&lt;/code&gt; 会令 Tensor 在指定的维度 &lt;code&gt;dim&lt;/code&gt; 上扩张 1 维，这里是为了使 &lt;code&gt;pe&lt;/code&gt; 与 &lt;code&gt;position&lt;/code&gt; 两个矩阵的维度对齐，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; torch.arange(5)
tensor([0, 1, 2, 3, 4])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(0)
tensor([[0, 1, 2, 3, 4]])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(1)
tensor([[0],
        [1],
        [2],
        [3],
        [4]])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).size()
torch.Size([5])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(0).size()
torch.Size([1, 5])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(1).size()
torch.Size([5, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;代码中的位置编码并不是直接按公式计算的，而是做了一些变换，先计算一个中间量 &lt;code&gt;div_term&lt;/code&gt;，其中 &lt;code&gt;torch.arange(0, d_model, 2)&lt;/code&gt; 即为 &lt;span class="math"&gt;\(2i\)&lt;/span&gt;，可以整理出&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \mathrm{div\_term}_i&amp;amp;=\exp\left[2i\times(-\frac{\ln10000}{d_k})\right]\\
    &amp;amp;=\left[\exp(-\frac{\ln10000}{d_k})\right]^{2i}\\
    &amp;amp;=\left[10000^{-\frac{1}{d_k}}\right]^{2i}\\
    &amp;amp;=10000^{-2i/d_k}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;所以 &lt;code&gt;position * div_term&lt;/code&gt; 就可以得到&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{position}\times \mathrm{div\_term}_i=\mathrm{pos}/10000^{2i/d_k}$$&lt;/div&gt;
&lt;p&gt;就是位置编码中的一项。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pe[:, 0::2]&lt;/code&gt; 与 &lt;code&gt;pe[:, 1::2]&lt;/code&gt; 是 Pytorch 中的高级索引操作。索引中用 &lt;code&gt;,&lt;/code&gt; 分隔不同维度，例中以 &lt;code&gt;,&lt;/code&gt; 为分界，前面是对第 1 维的索引，后面是对第 2 维的索引。索引操作也遵守 Python 的规则，即 &lt;code&gt;a:b:c&lt;/code&gt; 中 &lt;code&gt;a&lt;/code&gt; 为起始，&lt;code&gt;b&lt;/code&gt; 为末尾，&lt;code&gt;c&lt;/code&gt; 为步长。&lt;/p&gt;
&lt;p&gt;所以 &lt;code&gt;pe[:, 0::2]&lt;/code&gt; 与 &lt;code&gt;pe[:, 1::2]&lt;/code&gt; 取出全部第 1 维中的元素，即行方向上不操作，再在第 2 维中分别从 &lt;code&gt;0&lt;/code&gt; 或 &lt;code&gt;1&lt;/code&gt; 开始以步长 &lt;code&gt;2&lt;/code&gt; 取出元素，即取出第 &lt;span class="math"&gt;\(2i\)&lt;/span&gt; 或第 &lt;span class="math"&gt;\(2i+1\)&lt;/span&gt; 列。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;forward()&lt;/code&gt; 部分，输出的位置编码为 &lt;code&gt;pe[:, :x.size(1)]&lt;/code&gt;，这主要是为了确保矩阵形状在加法过程中不会因非法输入的广播而改变。其实在输入合法的情况下，&lt;code&gt;x.size(1)&lt;/code&gt; 就是 &lt;code&gt;d_model&lt;/code&gt;，等价于 &lt;code&gt;pe[:, :]&lt;/code&gt;，也等价于 &lt;code&gt;pe&lt;/code&gt;。&lt;/p&gt;
&lt;!-- 指定 `requires_grad_(False)` 是因为 PyTorch 会自动保存 Tensor 的来源，用于更快地计算梯度，而这里的加法计算并不是训练过程，取消保存能节省一部分资源。 --&gt;
&lt;h3 id="selfattention"&gt;SelfAttention&lt;/h3&gt;
&lt;p&gt;在进入 Transformer 核心部分之前，我们需要再次明确一下输入模型的数据格式。上文中仅以输入模型一条数据（由若干 token 组成的一条句子）为例，在实际操作中，为了提高训练效率，会同时输入若干条数据，在构建模型时也要考虑到这一点。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，一次输入模型的数据条数就称为 batch size，所以模型所处理的其实是一个 &lt;span class="math"&gt;\(\mathrm{batch\_size}\times\mathrm{max\_len}\times\mathrm{d\_model}\)&lt;/span&gt; 的高维矩阵。也就是说，&lt;code&gt;x.size()&lt;/code&gt; 的结果是 &lt;code&gt;[batch_size, max_len, d_model]&lt;/code&gt;，务必注意三者顺序。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        # 确保 embed_size 能被 heads 整除
        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(embed_size, embed_size)
        self.keys = nn.Linear(embed_size, embed_size)
        self.queries = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;先看 &lt;code&gt;SelfAttention&lt;/code&gt; 的初始化部分，明白了注意力机制的计算过程就不难理解上面的各个属性了。&lt;code&gt;head_dim&lt;/code&gt; 是每一个 head 中注意力的维度，&lt;code&gt;embeds_size&lt;/code&gt; 必须能被 &lt;code&gt;heads&lt;/code&gt; 整除，否则将多头注意力拼接在一起的维数不等于模型处理的维数就会出现问题。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;values&lt;/code&gt;、&lt;code&gt;keys&lt;/code&gt;、&lt;code&gt;queries&lt;/code&gt; 都是计算多头注意力前的线性变换，&lt;code&gt;fc_out&lt;/code&gt; 是拼接多头注意力后的线性变换。线性变换可以直接调用 &lt;code&gt;nn.Linear(in_dim, out_dim)&lt;/code&gt;，只需要指定线性变换前后的维数即可，这里线性变换前后维数没有变化。&lt;/p&gt;
&lt;p&gt;可能会有读者疑惑为什么这里所设定的线性变换不改变维数，原文中所描述的步骤不是应该将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt; 再计算注意力吗？这是正确的，原文中的计算流程确实如此。如下图所示，在线性变换后复制 &lt;code&gt;h&lt;/code&gt; 份（例中为 2） &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt;，用若干份 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 分别计算注意力再拼合起来，得到注意力的维数自然就是 &lt;code&gt;h * d_v&lt;/code&gt; （例中为 2 * 6），再用一个线性变换将其转化回模型所处理的维数 &lt;code&gt;d_model&lt;/code&gt;（例中为 5）。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;但代码中优化了一部分比较繁琐的操作，也有其他版本的代码使用了更接近原文的实现方式，如  &lt;a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; jadore801120 / attention-is-all-you-need-pytorch &lt;/a&gt;，流程就如下图所示，勉强称之为「单头注意力变多头注意力」的一种代码实现吧。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;例中 &lt;code&gt;d_model&lt;/code&gt; 也就是词嵌入的维数还是 5，&lt;code&gt;heads&lt;/code&gt; 仍为 2，&lt;code&gt;d_value&lt;/code&gt; 仍为 6，但模型不再是将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;，而是将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 直接升至 &lt;span class="math"&gt;\(hd_v\)&lt;/span&gt;，然后将 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 分成 &lt;code&gt;h&lt;/code&gt; 份，每份分别用于计算并拼接为注意力。与上例相比，本质上其实并无区别，区别仅仅是上例先复制多个矩阵再分别做线性变换，而该例只使用了一个更大的矩阵乘法就完成了上述操作，效率上更优。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;多头注意力还有一种实现方法，也是这里展示代码所使用的方法。如上图所示，这种方法对词嵌入的维数有要求，在词嵌入的步骤中就将 token 表示为 &lt;code&gt;d_v * h&lt;/code&gt; 维，这也是前文代码在初始化中使用 &lt;code&gt;assert&lt;/code&gt; 语句缘由。后续的线性变换不改变维数，计算多头注意力时直接将 &lt;code&gt;d_v * h&lt;/code&gt; 维切分为 &lt;code&gt;h&lt;/code&gt; 份作为每个 head 计算的对象。拼接各 head 的注意力后，最后的线性变换也不改变维数。&lt;/p&gt;
&lt;p&gt;在我看来，这种方法应该是对前两种方法的简化，三个例子中用于计算多头注意力的 &lt;code&gt;d_value&lt;/code&gt; 都为 6，计算量相同。第 3 种方法需要更大的 &lt;code&gt;d_model&lt;/code&gt;，而且计算多头注意力时没有使用到全部的 embedding，虽说效果类似，但总觉有些奇怪。这或许是为了计算上的方便，不用做过多的矩阵变换 🤔&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;# class SelfAttention(nn.Module):
    def forward(self, values, keys, query, mask):
        # 获取 batch_size
        N = query.shape[0]
        # d_v, d_k, d_q
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # 对 query, key, value 做线性变换
        values = self.values(values)    # (N, value_len, embed_size)
        keys = self.keys(keys)          # (N, key_len, embed_size)
        queries = self.queries(query)   # (N, query_len, embed_size)

        # 将 token 的词嵌入划分为 heads 份
        # d_model = embed_size = d_v * heads
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = queries.reshape(N, query_len, self.heads, self.head_dim)

        # queries: (N, query_len, heads, heads_dim),
        # keys: (N, key_len, heads, heads_dim)
        # energy: (N, heads, query_len, key_len)
        energy = torch.einsum("nqhd,nkhd-&amp;gt;nhqk", [queries, keys])

        # 将掩码矩阵中为 0 的对应项设为 -inf，不参与计算
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        # 得到的点积除以 sqrt(d_k) 并用 Softmax 归一化
        # attention: (N, heads, query_len, key_len)
        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)

        # attention: (N, heads, query_len, key_len)
        # values: (N, value_len, heads, heads_dim)
        # out after matrix multiply: (N, query_len, heads, head_dim), then
        # we reshape and flatten the last two dimensions.
        out = torch.einsum("nhql,nlhd-&amp;gt;nqhd", [attention, values]).reshape(
            N, query_len, self.heads * self.head_dim
        )

        # 拼接多头注意力后的线性变换
        # out: (N, query_len, embed_size)
        out = self.fc_out(out)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 部分描述了上述计算多头重意力的过程。线性变换后，使用 &lt;code&gt;reshape()&lt;/code&gt; 方法将 Tensor 转化化为指定维度，也就是将词嵌入划分为 &lt;code&gt;heads&lt;/code&gt; 份的操作，Tensor 的形状由 &lt;code&gt;[N, query_len, embed_size]&lt;/code&gt; 变为 &lt;code&gt;[N, query_len, self.heads, self.head_dim]&lt;/code&gt;，把 &lt;code&gt;embed_size&lt;/code&gt; 拆成 &lt;code&gt;heads * head_dim&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;接着使用 &lt;code&gt;torch.einsum()&lt;/code&gt; 得到注意力计算的一个中间量 &lt;code&gt;energy&lt;/code&gt;。&lt;code&gt;torch.einsum()&lt;/code&gt; 称为爱因斯坦求和约定，可以非常简洁地进行矩阵乘法、转置待操作，但会有些难以理解。&lt;/p&gt;
&lt;p&gt;例如矩阵乘法 &lt;span class="math"&gt;\(\boldsymbol{A}_{i\times j}\boldsymbol{B}_{j\times k}=\boldsymbol{C}_{i\times k}\)&lt;/span&gt;，可以表示为 &lt;code&gt;"ij,jk-&amp;gt;ik"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; B = torch.randn(4, 5)
&amp;gt;&amp;gt;&amp;gt; C = torch.einsum("ij,jk-&amp;gt;ik", [A, B])
&amp;gt;&amp;gt;&amp;gt; C.size()
torch.Size([3, 5])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;例如矩阵转置 &lt;span class="math"&gt;\((\boldsymbol{A}_{i\times j})^\top=\boldsymbol{B}_{j\times i}\)&lt;/span&gt;，可以表示为 &lt;code&gt;"ij-&amp;gt;ji"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; B = torch.einsum("ij-&amp;gt;ji", [A])
&amp;gt;&amp;gt;&amp;gt; B.size()
torch.Size([4, 3])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;定义了矩阵乘法的表示后，相应的数量积与向量积就也能表示了，不再赘述。求和操作将矩阵转化为数值，行与列都会消失，所以 &lt;span class="math"&gt;\(\sum a_{ij}\in\boldsymbol{A}_{i\times j}\)&lt;/span&gt; 可以记作 &lt;code&gt;"ij-&amp;gt;"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; torch.einsum("ij-&amp;gt;", [A])
tensor(0.5634)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此外，爱因斯坦求和约定还可以表示在指定维度上求和、做数量积等一系列的复杂操作，读者可以自行试验。&lt;/p&gt;
&lt;p&gt;代码中 &lt;code&gt;queries&lt;/code&gt; 的形状为 &lt;code&gt;[N, query_len, heads, heads_dim]&lt;/code&gt;，记作 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times q\times h \times d}\)&lt;/span&gt;，&lt;code&gt;keys&lt;/code&gt; 的形状为 &lt;code&gt;[N, key_len, heads, heads_dim]&lt;/code&gt;，记作 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times k\times h \times d}\)&lt;/span&gt;，那么 &lt;code&gt;torch.einsum("nqhd,nkhd-&amp;gt;nhqk", [queries, keys])&lt;/code&gt; 所做的操作就是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times q\times h \times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times h \times q\times d}\)&lt;/span&gt;，将 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times k\times h \times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h\times k \times d}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;两个矩阵中的 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 是 &lt;code&gt;batch_size&lt;/code&gt; 与 &lt;code&gt;heads&lt;/code&gt; 的乘积，仅仅是表示数量，所以 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times k\times d}\)&lt;/span&gt; 可以视作由 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 个 &lt;span class="math"&gt;\((\boldsymbol{K}_i)_{\ k\times d}\)&lt;/span&gt; 子矩阵构成的大矩阵。那么固定前两维不变，转置后两维，相当于&lt;strong&gt;转置&lt;/strong&gt;所有子矩阵，得到 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times d\times k}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;固定前两维，令 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times h \times q\times d}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times d\times k}\)&lt;/span&gt; 在后两维上做乘法，得到 &lt;span class="math"&gt;\((\boldsymbol{QK})_{N\times h \times q \times k}\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;仔细思考上述的转置和乘法过程，实际上就是在做多头注意力中的 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;掩码部分的操作先略过。接着 &lt;code&gt;torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)&lt;/code&gt; 先将前一步中得到 &lt;code&gt;energy&lt;/code&gt; 除以 &lt;span class="math"&gt;\(\sqrt{d_k}\)&lt;/span&gt; 再用 Softmax 归一化。指定的 &lt;code&gt;dim=3&lt;/code&gt; 与 &lt;code&gt;dim=-1&lt;/code&gt; 等价，其目的是在最后一维的方向上归一化。&lt;/p&gt;
&lt;p&gt;以一个简单的 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt; 乘法为例，如下图所示，&lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{K}\)&lt;/span&gt; 的每一行都是一个 token 的词嵌入表示。计算得到 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt; 后需要归一化，&lt;code&gt;softmax(dim=0)&lt;/code&gt; 是在行方向上归一化，在得到的结果中，全部行加起来，各元素为 1；&lt;code&gt;softmax(dim=1)&lt;/code&gt; 是在列方向上归一化，结果中的全部列加起来，各元素为 1。&lt;/p&gt;
&lt;p&gt;计算注意力还是为了得到更准确的 token 表示，所以归一化的方向应该与原始的 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 方向相同，即 &lt;code&gt;softmax(dim=1)&lt;/code&gt;。代码中也是一样，&lt;span class="math"&gt;\((\boldsymbol{QK})_{N\times h \times q \times k}\)&lt;/span&gt; 是 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 个 &lt;span class="math"&gt;\((\boldsymbol{Q}\boldsymbol{K}_i)_{q\times k}\)&lt;/span&gt; 子矩阵，要在所有子矩阵的列方向上做归一化，那么就是在第 4 个维度上做 Softmax，即 &lt;code&gt;softmax(dim=3)&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此时，上述过程已经完成了多头注意力中的 &lt;span class="math"&gt;\(\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\)&lt;/span&gt;，将结果记作 &lt;span class="math"&gt;\(\boldsymbol{A}_{N\times h\times q\times k}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;在下一步中，用 &lt;code&gt;"nhql,nlhd-&amp;gt;nqhd"&lt;/code&gt; 表示了 &lt;span class="math"&gt;\(\boldsymbol{A}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{V}\)&lt;/span&gt; 的乘法，具体操作是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times v\times h\times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times h\times v\times d}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;固定前两维，令 &lt;span class="math"&gt;\(\boldsymbol{A}_{N\times h\times q\times k}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times h\times v\times d}\)&lt;/span&gt; 在后两维上做乘法，这里有 &lt;span class="math"&gt;\(q=k=v\)&lt;/span&gt;，所以结果为 &lt;span class="math"&gt;\((AV)_{N\times h \times q\times d}\)&lt;/span&gt;，到这一步已经计算了 &lt;span class="math"&gt;\(\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\boldsymbol{V}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;将结果转置为 &lt;span class="math"&gt;\((AV)_{N\times q \times h\times d}\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后代码使用 &lt;code&gt;reshape()&lt;/code&gt; 合并后两维，将结果转化为 &lt;span class="math"&gt;\((AV)_{N\times q \times hd}\)&lt;/span&gt;，很巧妙地拼接了多个 head 的注意力，最后通过线性层再输出结果。&lt;/p&gt;
&lt;p&gt;至此，Transformer 中的 &lt;code&gt;SelfAttention&lt;/code&gt; 部分已经结束，读者或许会觉得头昏脑胀。不必担心，最为艰涩的一部分已经过去，接下来是一路下坡 🚩&lt;/p&gt;
&lt;h3 id="transformerblock"&gt;TransformerBlock&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class TransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TransformerBlock, self).__init__()
        # 前一层的多头注意力
        self.attention = SelfAttention(embed_size, heads)
        # Add &amp;amp; Norm 层
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        # 前馈层
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, value, key, query, mask):
        attention = self.attention(value, key, query, mask)

        x = self.dropout(self.norm1(attention + query))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;TransformerBlock&lt;/code&gt; 模块包括多头注意力与后接的 Add &amp;amp; Norm、Feed Forward、Add &amp;amp; Norm 三层。&lt;/p&gt;
&lt;p&gt;初始化部分使用 &lt;code&gt;nn.Sequential()&lt;/code&gt; 将 &lt;code&gt;nn.Linear()&lt;/code&gt;、&lt;code&gt;nn.ReLU()&lt;/code&gt;、&lt;code&gt;nn.Linear&lt;/code&gt; 依次连接起来形成前馈层，正如前文所说的，数据进入前馈层先升维再激活，最后再降回原来维度，&lt;code&gt;forward_expansion&lt;/code&gt; 决定升维的倍数。&lt;code&gt;dropout&lt;/code&gt; 用于随机弃用一部分数据防止过拟合，直接调用 &lt;code&gt;nn.Dropout()&lt;/code&gt; 类，接收的数值决定了弃用数据的比例。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 部分也很简单，计算的多头注意力依次做 Add &amp;amp; Norm、Feed Forward、Add &amp;amp; Norm 三层后输出数据。&lt;/p&gt;
&lt;h3 id="encoder"&gt;Encoder&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Encoder(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        embed_size,
        num_layers,
        heads,
        device,
        forward_expansion,
        dropout,
        max_length,
    ):

        super(Encoder, self).__init__()
        self.embed_size = embed_size
        # CPU or GPU
        self.device = device
        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)
        self.position_embedding = PositionalEncoding(embed_size, max_length)

        self.layers = nn.ModuleList(
            [
                TransformerBlock(
                    embed_size,
                    heads,
                    dropout=dropout,
                    forward_expansion=forward_expansion,
                )
                for _ in range(num_layers)
            ]
        )

        self.dropout = nn.Dropout(dropout)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Encoder 是 Transformer 中的左边部分，Transformer 中有 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个 &lt;code&gt;TransformerBlock&lt;/code&gt; 顺序叠放在一起组成 encoder。所以在初始化部分，使用列表推导式在 &lt;code&gt;layers&lt;/code&gt; 中放置了 &lt;code&gt;num_layers&lt;/code&gt; 层 &lt;code&gt;TransformerBlock&lt;/code&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;# class Encoder(nn.Module):
    def forward(self, x, mask):
        # 输入数据的 batch_size 与长度
        N, seq_length = x.shape
        # 从输入数据计算位置索引
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        # 由位置索引得到位置编码，并 dropout 一部分数据
        out = self.dropout(
            (self.word_embedding(x) + self.position_embedding(positions))
        )

        # 让数据逐层经过 encoder，计算自注意力
        for layer in self.layers:
            out = layer(out, out, out, mask)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 &lt;code&gt;forward()&lt;/code&gt; 部分中，使用 &lt;code&gt;torch.arange()&lt;/code&gt; 得到位置索引，再用 &lt;code&gt;expand()&lt;/code&gt; 方法将位置索引矩阵的形状变为与输入数据相同，&lt;code&gt;expand()&lt;/code&gt; 方法的主要作用是复制，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; torch.arange(0, 5)
tensor([0, 1, 2, 3, 4])
&amp;gt;&amp;gt;&amp;gt; torch.arange(0, 5).expand(2, 5)
tensor([[0, 1, 2, 3, 4],
        [0, 1, 2, 3, 4]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;to()&lt;/code&gt; 方法用于指定 Tensor 存储的设备，例如 &lt;code&gt;"CPU"&lt;/code&gt; 或 &lt;code&gt;"GPU"&lt;/code&gt;。将词嵌入加上位置编码得到 &lt;code&gt;out&lt;/code&gt;，再将 &lt;code&gt;out&lt;/code&gt; 送入 encoder 中计算结果。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;layer(out, out, out)&lt;/code&gt; 看起来或许有些奇怪，请留意，前文已经讨论过，在 encoder 中计算的是&lt;strong&gt;自注意力&lt;/strong&gt;，所以此时的 query、key、value 都是相同的，而在 decoder 中就会有所不同了。&lt;/p&gt;
&lt;h3 id="decoderblock"&gt;DecoderBlock&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class DecoderBlock(nn.Module):
    def __init__(self, embed_size, heads, forward_expansion, dropout, device):
        super(DecoderBlock, self).__init__()
        self.norm = nn.LayerNorm(embed_size)
        self.attention = SelfAttention(embed_size, heads=heads)
        self.transformer_block = TransformerBlock(
            embed_size, heads, dropout, forward_expansion
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, value, key, src_mask, trg_mask):
        attention = self.attention(x, x, x, trg_mask)
        query = self.dropout(self.norm(attention + x))
        out = self.transformer_block(value, key, query, src_mask)
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;类似地，Decoder 是 Transformer 结构图中的右侧部分，也是由 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 层 &lt;code&gt;DecoderBlock&lt;/code&gt; 组成。decoder 只比 encoder 多了一个掩码注意力层，其他结构相同，所以 &lt;code&gt;DecoderBlock&lt;/code&gt; 的初始化中直接调用了先前定义的 &lt;code&gt;TransformerBlock&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 中，target 进入 decoder 后，先计算&lt;strong&gt;自注意力&lt;/strong&gt;（&lt;code&gt;attention(x, x, x)&lt;/code&gt;），再经过 Add &amp;amp; Norm 层得到 &lt;code&gt;query&lt;/code&gt;，再与 encoder 中的结果做多头注意力（&lt;code&gt;attention(value, key, query)&lt;/code&gt;），输出结果。留意两种注意力计算的不同，参考 Transformer 结构图理解一下就会很明确。&lt;/p&gt;
&lt;h3 id="decoder"&gt;Decoder&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Decoder(nn.Module):
    def __init__(
        self,
        trg_vocab_size,
        embed_size,
        num_layers,
        heads,
        forward_expansion,
        dropout,
        device,
        max_length,
    ):
        super(Decoder, self).__init__()
        self.device = device
        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)
        self.position_embedding = PositionEmbedding(embed_size,max_length)

        self.layers = nn.ModuleList(
            [
                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)
                for _ in range(num_layers)
            ]
        )
        self.fc_out = nn.Linear(embed_size, trg_vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, src_mask, trg_mask):
        N, seq_length = x.shape
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))

        for layer in self.layers:
            x = layer(x, enc_out, enc_out, src_mask, trg_mask)

        out = self.fc_out(x)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;实现了 &lt;code&gt;DecoderBlock&lt;/code&gt; 后，&lt;code&gt;Decoder&lt;/code&gt; 就没有什么内容了，与 encoder 类似，就是将多个 &lt;code&gt;DecoderBlock&lt;/code&gt; 组装起来，按接口传入数据进行计算。&lt;/p&gt;
&lt;h3 id="transformer"&gt;Transformer&lt;/h3&gt;
&lt;p&gt;最后的 &lt;code&gt;Transformer&lt;/code&gt; 将各个模块都组合起来：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        trg_vocab_size,
        src_pad_idx,
        trg_pad_idx,
        embed_size=512,
        num_layers=6,
        forward_expansion=4,
        heads=8,
        dropout=0,
        device="cpu",
        max_length=100,
    ):

        super(Transformer, self).__init__()

        self.encoder = Encoder(
            src_vocab_size,
            embed_size,
            num_layers,
            heads,
            device,
            forward_expansion,
            dropout,
            max_length,
        )

        self.decoder = Decoder(
            trg_vocab_size,
            embed_size,
            num_layers,
            heads,
            forward_expansion,
            dropout,
            device,
            max_length,
        )

        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        self.device = device

    def make_src_mask(self, src):
        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)
        # (N, 1, 1, src_len)
        return src_mask.to(self.device)

    def make_trg_mask(self, trg):
        N, trg_len = trg.shape
        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(
            N, 1, trg_len, trg_len
        )

        return trg_mask.to(self.device)

    def forward(self, src, trg):
        src_mask = self.make_src_mask(src)
        trg_mask = self.make_trg_mask(trg)
        enc_src = self.encoder(src, src_mask)
        out = self.decoder(trg, enc_src, src_mask, trg_mask)
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;初如化部分主要是设定了默认的参数，并引入前面定义好的 &lt;code&gt;Encoder&lt;/code&gt; 与 &lt;code&gt;Decoder&lt;/code&gt; 模块。&lt;code&gt;Transformer&lt;/code&gt; 中还多了 &lt;code&gt;make_src_mask()&lt;/code&gt; 与 &lt;code&gt;make_trg_mask()&lt;/code&gt; 两个函数，这就不得不谈谈 Transformer 中的掩码机制了。&lt;/p&gt;
&lt;p&gt;考虑一个情境，需要使用 Transformer 翻译一批（若干条）句子，各句子的长度自然是不同的，那么输入模型的数据的形状也是不同的，这在后续步骤中就会出现很多问题。在实际中，通常会找到文本中最长的句子（&lt;code&gt;max_len&lt;/code&gt;），再将所有句子都变为该长度，这种操作称为 padding。&lt;/p&gt;
&lt;p&gt;具体做法如下图所示，分别用 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 与 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 标记句子的起讫，用 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 填充 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 后的空位，各数据的长度就会一致。然后根据设定的词典，将 token 转化为索引，接着再做词嵌入。&lt;code&gt;make_src_mask()&lt;/code&gt; 就是根据 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 的索引，将 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 所在位置都标记为 &lt;code&gt;False&lt;/code&gt;，其他位置标记为 &lt;code&gt;True&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;后续 &lt;code&gt;unsqueeze()&lt;/code&gt; 的操作比较费解，其实它是利用了 PyTorch 的广播机制，用于自动匹配矩阵的形状。图中的例子可以看作是将矩阵翻转再在第 3 个方向上拉长。因为代码中的掩码要用于掩盖形状为 &lt;code&gt;[N, heads, query_len, key_len]&lt;/code&gt; 具有 4 个方向的 &lt;code&gt;energy&lt;/code&gt;，所以要额外再做一次 &lt;code&gt;unsqueeze()&lt;/code&gt;。最后将掩码用于掩盖词嵌入数据，掩码就像一个罩子盖在词嵌入数据上，模型只计算 &lt;code&gt;True&lt;/code&gt; 位置上的数据。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;使用掩码可以让模型灵活地处理不同长度的数据，数据的长度由掩码决定，改变掩码就相当于改变处理的数据，而不去改变存储在硬件中的数据，这对于计算更有利。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;make_trg_mask()&lt;/code&gt; 函数产生用于 target 数据的掩码，在 target 上使用掩码的原因与 source 不同。在 decoder 中，模型要根据输入数据的计算结果给出新 token，而生成文本的过程是顺序的，依赖于前一步生成的结果。具体来说就是，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;序列以 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 标记起始；&lt;/li&gt;
&lt;li&gt;根据已有的 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 生成 &lt;code&gt;A&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;根据生成的 &lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt; 生成 &lt;code&gt;B&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;根据生成的 &lt;code&gt;&amp;lt;s&amp;gt; A B&lt;/code&gt; 生成 &lt;code&gt;C&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;以此类推，直至模型生成 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt;，句子结束。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;前文已经讨论过，这种方法有很多局限性，而 Transformer 的巧妙之处就在于能够并行完成这个过程。&lt;/p&gt;
&lt;p&gt;我们可以考虑训练过程，实际上与生成过程类似，训练过程就是要根据已经生成的 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 建立与下一个 token &lt;code&gt;A&lt;/code&gt; 的关系，而不能是与后续 &lt;code&gt;B&lt;/code&gt; 或 &lt;code&gt;C&lt;/code&gt; 的关系，将这种关系以参数的形式存储到模型中，推理阶段就能顺利地根据 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 生成 &lt;code&gt;A&lt;/code&gt;。这样的训练过程可以表示为一个下三角矩阵，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Transformer 不需要逐个 token 生成再建立关系，可以通过下三角矩阵一次直接取出 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt;、&lt;code&gt;&amp;lt;s&amp;gt; A B&lt;/code&gt; 等 token 序列，并行地训练模型与对应的下一个 token 建立关系。最后将 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 与每一步骤中新生成 token &lt;code&gt;A&lt;/code&gt;、&lt;code&gt;B&lt;/code&gt;、&lt;code&gt;C&lt;/code&gt;、&lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 拼合起来，即得到生成的文本。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;make_trg_mask()&lt;/code&gt; 就是在构建这个下三角的掩码。&lt;code&gt;torch.ones()&lt;/code&gt; 用于生成指定大小元素全为 &lt;code&gt;1&lt;/code&gt; 的矩阵，然后用 &lt;code&gt;torch.tril()&lt;/code&gt; 取该矩阵的下三角，再用 &lt;code&gt;expand()&lt;/code&gt; 方法将该矩阵复制到与 &lt;code&gt;batch_size&lt;/code&gt; 匹配。&lt;/p&gt;
&lt;h3 id="train"&gt;Train&lt;/h3&gt;
&lt;p&gt;从前面讨论的模型生成过程还可以知道的一点是，模型永远不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，所以 target 中没有 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，而 source 则必须由 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 起始。在实际中，一种做法是，用预处理的脚本在原始训练数据（例如 &lt;code&gt;.csv&lt;/code&gt;、&lt;code&gt;.txt&lt;/code&gt; 文件）中标上标记；另一种方法是，在训练代码中加入预处理的功能，读取数据时分别为数据做上相应标记。为了方便起见，本文就不实现这一部分功能，使用 Transformer 可以直接处理的数据。&lt;/p&gt;
&lt;p&gt;生成训练数据的函数为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def generate_random_batch(batch_size, max_length=16):
    src = []
    for i in range(batch_size):
        # 随机指定有效数据的长度
        random_len = random.randint(1, max_length - 2)
        # 在数据起讫处加上标记，"&amp;lt;s&amp;gt;": 0, "&amp;lt;e&amp;gt;": 1
        random_nums = [0] + [random.randint(3, 9) for _ in range(random_len)] + [1]
        # padding 填满数据长度，"&amp;lt;p&amp;gt;": [2]
        random_nums = random_nums + [2] * (max_length - random_len - 2)
        src.append(random_nums)

    src = torch.LongTensor(src)
    # tgt 去除末尾的 token
    tgt = src[:, :-1]
    # tgt_y 去除首个 &amp;lt;s&amp;gt;，即模型需要预测的 token，用于计算损失
    tgt_y = src[:, 1:]
    # 模型需要预测的 token 数量（不计 &amp;lt;p&amp;gt;），用于计算损失函数
    n_tokens = (tgt_y != 2).sum()

    return src, tgt, tgt_y, n_tokens
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;generate_random_batch()&lt;/code&gt; 能够生成 Transformer 可以直接计算的相同的 source 与 target，该模型的任务目标就是生成与输入相同的序列。模型不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，所以&lt;code&gt;tgt_y&lt;/code&gt; 去除 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 用于与生成的序列对比计算损失，这很容易理解。但为什么 &lt;code&gt;tgt&lt;/code&gt; 需要去除最后一个 token 呢？这一点我将在后文生成序列的 Predict 一节讨论。训练与测试模型的代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# &amp;lt;p&amp;gt; 索引
src_pad_idx = 2
trg_pad_idx = 2
# 词表大小，即全部 token 数量，包括 &amp;lt;s&amp;gt; &amp;lt;e&amp;gt; &amp;lt;p&amp;gt; 等标记
src_vocab_size = 10
trg_vocab_size = 10
# 文本最大长度
max_len = 16

model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,
                    embed_size=128, num_layers=2, dropout=0.1, max_length=max_len,
                    device=device).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
criteria = nn.CrossEntropyLoss()
total_loss = 0

for step in range(2000):
    src, tgt, tgt_y, n_tokens = generate_random_batch(batch_size=2, max_length=max_len)
    optimizer.zero_grad()
    out = model(src, tgt)

    # contiguous() 与 view() 将矩阵在各行首尾相连为一行（即向量）
    # 在两向量间计算损失函数
    # tgt_y 中元素的值是索引，除以 n_tokens 将其缩放到 [0, 1]
    loss = criteria(out.contiguous().view(-1, out.size(-1)),
                    tgt_y.contiguous().view(-1)) / n_tokens
    loss.backward()
    optimizer.step()

    total_loss += loss

    if step != 0 and step % 40 == 0:
        print(f"Step {step}, total_loss: {total_loss}")
        total_loss = 0

# Predict
copy_test(model, max_len)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PyTorch 使用 &lt;code&gt;torch.optim&lt;/code&gt; 定义模型的训练过程，其中可以选择非常多种的优化过程，这里选择了 &lt;code&gt;Adam()&lt;/code&gt;，&lt;code&gt;lr=3e-4&lt;/code&gt; 指定了训练步骤的学习率。&lt;code&gt;nn.CrossEntropyLoss()&lt;/code&gt; 用于计算两个向量的交叉熵损失，作为训练过程的损失函数。&lt;/p&gt;
&lt;p&gt;在训练循环中，每一个循环处理 1 个 batch 的数据，在同一个 batch 中 PyTorch 自动计算梯度的反向传播并更新参数。但在新的 batch 中，因为已经更新到参数中了，我们不希望保留上一个 batch 的梯度，所以用 &lt;code&gt;optimizer.zero_grad()&lt;/code&gt; 将梯度清空。&lt;/p&gt;
&lt;p&gt;将 &lt;code&gt;src&lt;/code&gt; 与 &lt;code&gt;tgt&lt;/code&gt; 传入模型，&lt;code&gt;out&lt;/code&gt; 就是 Transformer 的计算结果。&lt;code&gt;loss.backward()&lt;/code&gt; 与 &lt;code&gt;optimizer.step()&lt;/code&gt; 两行代码就是前面所说的让 PyTorch 自动计算梯度的反向传播并更新参数。&lt;/p&gt;
&lt;h3 id="predict"&gt;Predict&lt;/h3&gt;
&lt;p&gt;训练结束后，我用 &lt;code&gt;copy_test()&lt;/code&gt; 函数测试模型的效果，这个测试函数定义为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def copy_test(model, max_len):
    model = model.eval()
    src = torch.LongTensor([[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]])
    # 模型从 &amp;lt;s&amp;gt; 开始生成序列，但不会生成 &amp;lt;s&amp;gt;，所以指定起始的 &amp;lt;s&amp;gt;
    tgt = torch.LongTensor([[0]])

    for i in range(max_len):
        # out： (1, i + 1, 10)
        # i + 1 模型输出的 token 数量
        # 10 为 vocab_size，是词表中 token 数量，out 是词表中各 token 在此处出现的概率
        out = model(src, tgt)
        # 取输出的 i + 1 个 token 中的最后一个
        # predict: (1, 10)
        predict = out[:, -1]
        # 取得概率最大的 token 索引
        # y: (1, )
        y = torch.argmax(predict, dim=1)
        # 逐个拼合 token 索引
        # y.unsqueeze(0): (1, 1)
        # tgt: (1, i + 1 )
        tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)
        # 若生成 token &amp;lt;e&amp;gt;，表示句子结束，退出循环
        if y == 1:
            break
    print(tgt)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;eval()&lt;/code&gt; 方法令模型退出训练模式，会关闭 dropout 等训练过程中才需要的功能。在循环中逐个拼合生成的 token，就能得到生成的句子。循环中的操作如下图所示，在第 1 次循环中，&lt;code&gt;tgt&lt;/code&gt; 为 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，通过与 &lt;code&gt;src&lt;/code&gt; 的注意力与下三角矩阵得到计算结果 &lt;code&gt;out&lt;/code&gt; 为 &lt;code&gt;A&lt;/code&gt;，然后将 &lt;code&gt;tgt&lt;/code&gt; 更新为 &lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt;，在第 2 次循环中，得到的 &lt;code&gt;out&lt;/code&gt; 为 &lt;code&gt;A B&lt;/code&gt;，所以在每次循环中都只取新生成的 &lt;code&gt;out[-1]&lt;/code&gt; 更新 &lt;code&gt;tgt&lt;/code&gt;，最后将结果拼接起来得到完整的输出结果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;或许读者会有疑惑，既然使用下三角矩阵并行计算是 Transformer 的优势，为什么这里却是用循环顺序地生成呢？为什么计算上图中最后一个矩阵的 &lt;code&gt;out&lt;/code&gt;，而是要用一个个的 &lt;code&gt;out[-1]&lt;/code&gt; 呢？&lt;/p&gt;
&lt;p&gt;要注意的是，训练与生成有重要的一个不同，就是生成中的 &lt;code&gt;tgt&lt;/code&gt; 是空白的、模型不可知的，而训练中的 &lt;code&gt;tgt&lt;/code&gt; 是完整的、模型可知的。如上图中，&lt;code&gt;tgt&lt;/code&gt; 在每个循环中都在变长，只有 &lt;code&gt;tgt&lt;/code&gt; 变成了 &lt;code&gt;&amp;lt;s&amp;gt; A B C &amp;hellip;&lt;/code&gt; 才会有最后一个矩阵中的 &lt;code&gt;out&lt;/code&gt;。如果说只要最后一个矩阵中的 &lt;code&gt;out&lt;/code&gt; 而不要前面的步骤，就变成了「吃两个馒头吃饱，所以只吃后一个能吃得饱的馒头」的笑话。&lt;/p&gt;
&lt;p&gt;所以&lt;dot&gt;生成过程并不是并行的，Transformer 的并行指的是训练过程&lt;/dot&gt;。如下图所示，在训练过程中 Transformer 只需要做一次下三角矩阵的运算就可以建立多个 token 间的关系。这张图还解释了模型永远不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 但 &lt;code&gt;tgt&lt;/code&gt; 必须以 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 起始的原因。图中还可以很明白的看出为什么先前的训练代码要去除 &lt;code&gt;tgt&lt;/code&gt; 末尾的 token，因为 Transformer 的输出 &lt;code&gt;out&lt;/code&gt; 计算的是 &lt;code&gt;tgt&lt;/code&gt; 下一个 token（及此前）的计算结果，若不去除末位就超出范围了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后训练与测试的结果为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;cpu
Step 40, total_loss: 4.021485328674316
Step 80, total_loss: 2.8817126750946045
&amp;hellip;&amp;hellip;
Step 1920, total_loss: 0.9760974049568176
Step 1960, total_loss: 0.8644390106201172
tensor([[0, 6, 3, 4, 5, 7, 6, 4, 3, 1]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出的结果没有输出 source &lt;code&gt;[[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]]&lt;/code&gt; 中末尾代表 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 的 &lt;code&gt;2&lt;/code&gt;，前面的 token 索引也与 source 相差无几，说明模型正确复制了输入序列，训练是成功的。&lt;/p&gt;
&lt;h2 id="hou-ji_1"&gt;后记&lt;/h2&gt;
&lt;p&gt;至此，这篇 Transformer 的介绍终于告一段落了。从起草、绘图再到最后的代码梳理，前后花了一周多的时间。虽名为介绍，其实还是为自己在做梳理，边写边想、边想边查，终于把 Transformer 中的一些细节弄明白了，这篇笔记也能为读者勾勒出一个大致的图景。&lt;/p&gt;
&lt;p&gt;当然，限于篇幅，限于「从零起步」的初衷，也限于笔力，还有许多更深层次问题都没有探讨，但我相信，在看懂了这篇笔记之后，再去阅读那些文章已经不成问题了，这也符合我的初心。&lt;/p&gt;
&lt;p&gt;或许读者还很困惑，疑惑为什么数学推导上并不那么严谨的模型居然能有效，甚至具有极好的表现，那就说明需要钻入研究 Transformer 的底层了，不可不再读些更专业的文章。我也把写这篇文章时所参考以及较好的相关资料罗列于后，以飨读者。&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Vaswani, A. et al. Attention Is All You Need (2017) - arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spaces.ac.cn/archives/4765" rel="noopener" target="_blank"&gt;《Attention is All You Need》浅读（简介+代码）- 科学空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spaces.ac.cn/archives/6933" rel="noopener" target="_blank"&gt;从语言模型到 Seq2Seq：Transformer 如戏，全靠 Mask - 科学空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" rel="noopener" target="_blank"&gt;Language Modeling with nn.Transformer and torchtext - PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener" target="_blank"&gt;The Illustrated Transformer - Jay Alammar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/wevolf/p/12484972.html" rel="noopener" target="_blank"&gt;Transformer 源码中 Mask 机制的实现 - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/434232512" rel="noopener" target="_blank"&gt;torch.einsum 详解 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.csdn.net/zhaohongfei_358/article/details/126019181" rel="noopener" target="_blank"&gt;Pytorch 中 nn.Transformer 的使用详解与 Transformer 的黑盒讲解 - CSDN 博客&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Python"></category><category term="PyTorch"></category><category term="Transformer"></category></entry><entry><title>文献总结｜药物发现中的匹配分子对分析：方法与当前应用</title><link href="https://leonis.cc/sui-sui-nian/2023-04-15-summary-doi.org/10.1021/acs.jmedchem.2c01787.html" rel="alternate"></link><published>2023-04-15T00:00:00+08:00</published><updated>2023-04-15T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-15:/sui-sui-nian/2023-04-15-summary-doi.org/10.1021/acs.jmedchem.2c01787.html</id><summary type="html">&lt;p&gt;本文介绍 2023 年由曹东升与侯廷军研究团队发表在 &lt;em&gt;Journal of Medicinal Chemistry&lt;/em&gt; 上的一篇展望，文章原标题为 Matched Molecular Pair Analysis in Drug Discovery: Methods and Recent Applications，文章介绍了主要介绍了匹配分子对分析的理论与目前基于匹配分子对分析的实际应用。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/acs.jmedchem.2c01787" rel="noopener" target="_blank"&gt;doi.org/10.1021/acs.jmedchem.2c01787&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍 2023 年由曹东升与侯廷军研究团队发表在 &lt;em&gt;Journal of Medicinal Chemistry&lt;/em&gt; 上的一篇展望，文章原标题为 Matched Molecular Pair Analysis in Drug Discovery: Methods and Recent Applications，文章介绍了主要介绍了匹配分子对分析的理论与目前基于匹配分子对分析的实际应用。&lt;/p&gt;
&lt;p&gt;匹配分子对（matched molecular pair, MMP）的概念自提出以来，已成为了从化合物中提取药物化学知识并用于指导先导化合物优化的标准方法，MMP 的定义是只在局部具有较小的结构差异的一对化合物。合成化学家、药物化学家借助匹配分子对分析（molecular matched pair analysis, MMPA）的手段，可以从人类研究过的海量化合物中总结出化学改造的方法、化学改造对于化合物性质的影响等重要经验知识。&lt;/p&gt;
&lt;h2 id="mmpa-li-lun"&gt;MMPA 理论&lt;/h2&gt;
&lt;h3 id="mmp-sou-suo-suan-fa"&gt;MMP 搜索算法&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8784?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8784?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在需要对大量分子数据做 MMPA 时，首要任务就是提取出其中的 MMP，MMP 搜索算法可以分为 3 类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;预设的变换规则：使用人为设计的切分规则分割分子，寻找分子数据中的 MMP，常用规则如 retrosynthetic combinatorial analysis procedure（RECAP）和 breaking of retrosynthetically interesting chemical substructures（BRICS）。这种方法的局限性也很明显，例如忽略了预设规则以外的 MMP 并且只能处理单点的化学结构变换。&lt;/li&gt;
&lt;li&gt;基于最大公共子结构（maximum common substructure, MCS）的方法：先寻找指定分子的的公共结构，将其设定为固定部分，只有具有公共结构的分子才能构成 MMP，分子中除去公共结构所剩余的结构就是改变部分，所以该方法通常用用于表示化学变换的 SMIRKS 存储 MMP。这种方法的问题在于计算 MCS 的计算开销很大。&lt;/li&gt;
&lt;li&gt;片段与索引（fragmentation and indexing, F+I）方法：该方法是目前寻找 MMP 最通用的方法，主要方法是在两非氢原子间的非环单键处切断，构建 key 与 value 片段的对应索引，通过键值对间的匹配寻找 MMP，具体方法可以&lt;a href="https://leonis.cc/sui-sui-nian/2023-02-25-summary-doi.org/10.1021/ci900450m.html" rel="noopener" target="_blank"&gt;参看前文&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="ying-xiang-mmpa-de-guan-jian-yin-su"&gt;影响 MMPA 的关键因素&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8785?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8785?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MMPA 的基本假设是，分子结构中一些小的结构改变将引起特定物理性质或是生物活性的改变。然而现实中化合物性质改变的原因更为复杂，表现出更为偶然的现象，例如分子改造中的活性悬崖（对分子仅做微小的改造而生物活性变化巨大）等，所以在 MMPA 中也要考虑到许多因素的影响。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分子表示：2D 与 3D 分子结构都被用于 MMPA 研究中，2D 分子描述的主要优点是处理简单，但许多实践表明 3D 分子表示方法表示了分子的空间信息，使其对于微小的结构差异更为敏感，这对 MMPA 十分重要。&lt;/li&gt;
&lt;li&gt;环境特征：在早期的研究中，人们认为只有 MMP 中的化学转换改变了分子的性质，因此只针对化学转换进行研究，而没有考虑具体分子。如今人们已经意识到，在 MMPA 还需要考虑具体分子的结构以及改造位点等环境特征，不能只研究 MMP 中的化学转化规则。目前，大部分研究使用分子图或 SMILES 来表示 MMP 中的完整分子，用于 MMPA 研究。除了分子信息以外，也有研究将蛋白口袋的信息也融入 MMPA，这有助于更深入研究 MMP 转化对受体与配体间结合作用影响。&lt;/li&gt;
&lt;li&gt;统计显著性：MMPA 的统计分析对于研究 MMP 间性质的变化十分重要，因为一种化学转换可以引起多种性质的改变，多种化学转换也可能使分子的某些性质不发生改变。MMPA 的统计学研究发现，在同一化合物上所做的两个结构改造所产生的影响远不同于单一结构改造影响的加和，这也称为「不可加和性」效应，这意味着简单的单一结构改造间存在着相互作用。不可加和性同样影响了分子的溶解度等性质，在 MMPA 中对可加和性进行统计分析，可以更好地识别药物分子的构效关系与分子中潜在的相互作用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="mmpa-shi-ji-ying-yong_1"&gt;MMPA 实际应用&lt;/h2&gt;
&lt;p&gt;MMPA 已经广泛应用在寻找得到目标性质分子所需的化学改造中（ADMET 优化），除了应用在先导化合物优化，MMPA 也用于靶点预测、生物电子等排体替换、构效关系确定、全新药物设计等任务中，这里主要介绍 MMPA 在分子结构改造和全新药物设计中的应用。&lt;/p&gt;
&lt;h3 id="pi-pei-fen-zi-xu-lie"&gt;匹配分子序列&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8786?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8786?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;将多个仅具有一个子结构区别的分子组织起来，就得到了匹配分子序列（matching molecular series, MMS），该方法最早被用于药物分子构效关系的分析，将不同 MMS 组织起来还得到形成匹配分子序列图，用于决策分子改造的路线。称为 SAR 转移的方法通过对比两个 MMS 间化合物性质的变化，可以判断替换结构的效果与。&lt;/p&gt;
&lt;h3 id="ji-yu-mmpa-de-quan-xin-yao-wu-she-ji"&gt;基于 MMPA 的全新药物设计&lt;/h3&gt;
&lt;p&gt;将 MMP 化学变换规则用于分子生成是全新药物设计中的重要步骤，输入的分子首先被分割为片段，然后通过 MMP 数据库搜索找到相应的化学转换，将这些化学转换用于输入分子就得到了新分子。也有研究提出了基于片段的 MMP 分子生成方法，主要步骤是收集 MMP 片段信息，通过遗传算法等方法合理地相互组合 MMP 片段，得到新分子。&lt;/p&gt;
&lt;p&gt;也有研究使用分子骨架和分子骨架以外的子结构来构建分子生成模型，模型是使用 SMILES 的 RNN 模型，第一步是生成正确的分子骨架，第二步在分子骨架上添加结构改造得到正确的分子。此外，MMS 方法可以很容易地将分子分为若干类的类似物，也可以很方便地用于全新药物设计。DeepSARM 模型的目标是寻找生物作用类似而化学结构新颖的类似物，就使用了 MMS 方法，模型同时还考虑了靶点信息，扩大的 MMS 方法的应用范围。&lt;/p&gt;
&lt;h2 id="zhan-wang_1"&gt;展望&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8787?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8787?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;分子设计所面临的一个重要难题是如何基于有限的实验数据决定下一步的分子改造，MMPA 有助于人们从已有的分子改造数据中得到化学转换的信息。为了能更好地利用 MMPA，文章提出了以下几点展望：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 QSAR 与 MMPA 相结合。QSAR 模型着重于整体的结构特征，MMPA 主要用于确定局部子结构的改变，在一定程度上二者是互补的，在未来 MMPA 也可能对 QSAR 模型的预测有帮助。&lt;/li&gt;
&lt;li&gt;将 MMPA 的概念用于蛋白质等大分子。&lt;/li&gt;
&lt;li&gt;融合 MMPA 相关的分子优化方法，构建自动化的分子优化流程。尽管目前 MMP 已经应用于分子生成，但 MMP 数据的提取等步骤还需要人工处理。文章提出了上图所示的预期 MMPA 工作流程，希望能够实现 MMP 的自动提取、组织、应用和评估。&lt;/li&gt;
&lt;/ol&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Review"></category></entry><entry><title>我的 2023 年春播计划</title><link href="https://leonis.cc/zai-lu-shang/2023-04-11-my-gardening-plan.html" rel="alternate"></link><published>2023-04-11T00:00:00+08:00</published><updated>2023-04-11T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-11:/zai-lu-shang/2023-04-11-my-gardening-plan.html</id><summary type="html">&lt;p&gt;过了惊蛰之后，万物萌动，看到园林工人正在整饬路旁的花圃，我的心里也跟着躁动起来，想着非要在阳台上种点什么才好，于是在一番调查与纠结之后确定了这篇春播计划。&lt;/p&gt;</summary><content type="html">&lt;p&gt;过了惊蛰之后，万物萌动，看到园林工人正在整饬路旁的花圃，我的心里也跟着躁动起来，想着非要在阳台上种点什么才好，于是在一番调查与纠结之后确定了这篇春播计划。&lt;/p&gt;
&lt;h2 id="pin-chong"&gt;品种&lt;/h2&gt;
&lt;p&gt;我的阳台正朝南方，一整天都可以受到阳光，非常适合种些花草，之所以一直闲置，还是因为漂泊在外，总担心有时无暇顾及这些无言的小小生灵。所以在我下定决心后，首先要考虑的就是草木的品种。由于春节会回家，到时就无人照看这些花草了，我优先考虑一年生的植物，并且植株的越冬所需要的操作一定要越简单越好。&lt;/p&gt;
&lt;p&gt;结合以上客观因素和个人喜好等主观因素下，我敲定了这几个大类别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;大丽花&lt;/strong&gt;：大丽花喜欢光照，光照越充足花开得越盛，正好符合阳台的光照条件，而在冬天枯萎后可以挖出种球保存，也不需要专门照看，所以非常合适；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;甘菊等一年生的切花&lt;/strong&gt;：我个人特别喜欢切花，不仅盛开时美，还可以剪下做成干花，某种意义上也算是经冬不凋，而天津的秋冬季十分干燥，制作干花实在太合适了；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;番茄等一年生的蔬果&lt;/strong&gt;：许多人调侃中国人的天赋是种菜，喜欢种蔬果胜过于种花草，其实蔬果也有许多园艺品种，除了食用以外也有很好的观赏价值，蔬果的生长周期短，播种时间可以很灵活，而且谁不想一尝丰收的喜悦呢？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我的春播计划包括 5 个具体的品种，用店家的图做个参考吧：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="朵拉大丽花" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8760?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="朵拉大丽花" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8760?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 朵拉大丽花 Dahlia 'Melody Dora'&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="小甘菊" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8761?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="小甘菊" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8761?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 小甘菊&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="文森特向日葵" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8762?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="文森特向日葵" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8762?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 文森特向日葵 Helianthus annuus 'Vincent's Fresh'&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="橙色番茄" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8763?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="橙色番茄" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8763?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 橙色番茄&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="墨西哥小番茄" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8764?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="墨西哥小番茄" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8764?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 墨西哥小番茄 Solanum lycopersicum 'Mexico Midget'&lt;/p&gt;
&lt;p&gt;{warn begin}品种学名来自于植物数据库 &lt;a href="https://garden.org/plants/" rel="noopener" target="_blank"&gt;Plants Database&lt;/a&gt;，主要靠对照图片推测，不一定准确。{warn end}&lt;/p&gt;
&lt;h2 id="gong-ju"&gt;工具&lt;/h2&gt;
&lt;p&gt;接着是一些园艺用具，包括花盆、花土、花肥等等。&lt;/p&gt;
&lt;p&gt;包括我在内的很多人都没有地栽的条件，只能购置花盆选择盆栽。花盆的上选当然是红陶盆，不仅透水透气还美观耐看，红陶盆的一个问题是太重，所以运费会导致价格比较高，也不适合漂泊在外没有自己固定居所的人；再一个问题是在植株换盆时，为了避免伤根，通常会把盆给敲碎，像我这样拮据的人很难容忍这种浪费。基于这些考虑，我选择了更便捷一些的塑料盆。&lt;/p&gt;
&lt;p&gt;盆的大小要依种植的植株品种来定，比较常用的尺寸是直径 15 cm 左右的花盆，容量大约 2 L。大丽花需要较大的花盆，可以按大丽花的株高选盆：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;株高 50 cm 以下，盆径 18 cm&lt;/li&gt;
&lt;li&gt;株高 50-90 cm，盆径 25 cm&lt;/li&gt;
&lt;li&gt;株高 90 cm 以上，盆径 30 cm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于番茄需要注意的是，番茄可以按生长模式分为两类&amp;mdash;&amp;mdash;有限生长型与无限生长型。不要把生长模式与植物的寿命混淆，番茄大部分都是一年生的，有限生长是指番茄的果实都在同一时期成熟，收完一茬就结束了；而无限生长型番茄的果实会在一段较长的时间内次序成熟，犹如永远收获不完，因而得名。&lt;dot&gt;有限生长型番茄大多是矮株，无限生长型番茄大多是高株&lt;/dot&gt;，所以要选择匹配的盆径。有限生长型番茄用 20 cm 的盆尚可，无限生长型番茄就需要用 30 cm 左右的花盆了，因为它们甚至能长到 2 米多高。&lt;/p&gt;
&lt;p&gt;在番茄品种的选择上出现了一些失误，把小番茄误当作了矮株番茄，收到种子后才得知我购买的两个品种都是无限生长型的番茄，于是慌忙之下又购买了几个大盆。&lt;/p&gt;
&lt;p&gt;那么我所种植的品种与花盆的搭配就是，矮型大丽花朵拉和两种番茄使用 25 cm 的大盆，向日葵搭配 20 cm 的花盆，小甘菊可以使用 17 cm 的盆。&lt;/p&gt;
&lt;p&gt;我购买的花土也简单，就是普通的泥炭土，先后买了大约 18 L，还不太确定用量，后续不够可以再买，这样也能先试试土的质量。&lt;/p&gt;
&lt;p&gt;至于肥料，我只购买了一些非常便宜的有机肥，没有购买无机肥，主要是因为花肥的品牌琳琅满目，花肥又不像花土一般适用，购买了不适配的就容易浪费，并且我对花肥的品牌不太了解也难以选择。再者作为学化学的，有什么化学试剂是我难以获得的呢，我相信我的专业水平更多于那些名不见经传的品牌。&lt;/p&gt;
&lt;h2 id="bo-chong"&gt;播种&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;朵拉大丽花&lt;/strong&gt;：大丽花在 3 月中旬左右就可以种下了，大丽花的花期很长，从夏季持续到秋季，所以在天气暖和了之后早些种下也有利于大丽花蓄积养份，为盛开做好准备。收到大丽花的球根后，我提前泡了一天的水，剪去一些枯干的残根，然后确认茎的方向，茎向上埋入土中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;小甘菊&lt;/strong&gt;：播种温度 15～20℃，我在 4 月 10 日种下。使用育苗块播种，种子用手指小心按进湿润的泥土中，等待发芽长大后再移栽到盆中，一个花盆中种植 3 棵左右。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文森特向日葵&lt;/strong&gt;：播种温度 15～30℃，向日葵大约 2 个月就能开花，我打算在 4 月 20 日左右种下，希望恰好在夏至时能看到黄灿灿的向日葵。据店家的说明，向日葵的发芽率很高，不需要育苗，直接在盆中挖 5 mm 左右的小坑，将葵花籽横放埋入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;橙色番茄&lt;/strong&gt;：播种温度 15～30℃，可以把播种时间向后推一些，在 4 月至 5 月种下，也有人喜欢秋播，但不太适合我。我打算在 4 月 15 日左右种下，同样使用育苗块播种，长大后再移栽到盆中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;墨西哥小番茄&lt;/strong&gt;：与橙色番茄相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在春天也忙碌起来了，快快发芽吧 🌱&lt;/p&gt;
&lt;h2 id="zhang-dan"&gt;账单&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;价格（CNY）&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;球根与种子&lt;/td&gt;
&lt;td&gt;38.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;花盆（5 个）&lt;/td&gt;
&lt;td&gt;54.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;泥炭土（18 L）&lt;/td&gt;
&lt;td&gt;21.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;鸡粪有机肥&lt;/td&gt;
&lt;td&gt;3.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;育苗块 + 育苗盒&lt;/td&gt;
&lt;td&gt;11.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;总计（计运费）&lt;/td&gt;
&lt;td&gt;142.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="在路上"></category><category term="园艺"></category></entry></feed>