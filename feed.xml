<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Leo's blog</title><link href="https://leonis.cc/" rel="alternate"></link><link href="https://leonis.cc/feed.xml" rel="self"></link><id>https://leonis.cc/</id><updated>2023-07-28T00:00:00+08:00</updated><subtitle>A nook to hoard my manuscripts.</subtitle><entry><title>如何在 X86 设备上使用 Docker 构建 ARM 镜像</title><link href="https://leonis.cc/sui-sui-nian/2023-07-28-ru-he-zai-x86-she-bei-shang-shi-yong-docker-gou-jian-arm-jing-xiang.html" rel="alternate"></link><published>2023-07-28T00:00:00+08:00</published><updated>2023-07-28T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-07-28:/sui-sui-nian/2023-07-28-ru-he-zai-x86-she-bei-shang-shi-yong-docker-gou-jian-arm-jing-xiang.html</id><summary type="html">&lt;p&gt;最近一直在使用华为 ModelArts 的计算平台，使用这类计算平台的一般流程是先在本地用 Docker 构建镜像，再上传至云端，然后就可以在该环境下部署具体的计算作业了。使用 Docker 构建环境非常方便，基于官方或其他用户提供的基础镜像安装上自己所需要 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;最近一直在使用华为 ModelArts 的计算平台，使用这类计算平台的一般流程是先在本地用 Docker 构建镜像，再上传至云端，然后就可以在该环境下部署具体的计算作业了。使用 Docker 构建环境非常方便，基于官方或其他用户提供的基础镜像安装上自己所需要的依赖就可以直接上传使用了，完全不用跟驱动安装等等令人头疼又心累的事情打交道。&lt;/p&gt;
&lt;p&gt;但在使用 Docker 构建镜像时，有一个挺棘手的问题。计算平台或是服务器所使用的设备一般是 ARM 架构，个人电脑使用基本上是 X86 架构。由于二者 CPU 指令集不同，尽管可以在 X86 设备上用 &lt;code&gt;docker pull --platform=linux/arm64&lt;/code&gt; 拉取用于 ARM 设备的镜像，但无法使用 &lt;code&gt;docker run&lt;/code&gt; 或 &lt;code&gt;docker build&lt;/code&gt; 运行或是通过构建的方法修改该镜像。&lt;/p&gt;
&lt;h2 id="qemu-user-static"&gt;qemu-user-static&lt;/h2&gt;
&lt;p&gt;去寻找 ARM 设备再使用 Docker 构建镜像就太麻烦了，幸好找到了一个工具 &lt;a href="https://github.com/multiarch/qemu-user-static" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; qemu-user-static&lt;/a&gt;，专门用于解决这个问题。先来看看仓库中给出的示例：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;$ uname -m
x86_64

$ docker run --rm -t arm64v8/ubuntu uname -m
standard_init_linux.go:211: exec user process caused "exec format error"

$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes

$ docker run --rm -t arm64v8/ubuntu uname -m
aarch64
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;第一行的 &lt;code&gt;uname -m&lt;/code&gt; 用于检测宿主机的架构，终端给出的信息表明这是一台 X86 设备。&lt;/li&gt;
&lt;li&gt;第二行命令用 Docker 运行 &lt;code&gt;arm64v8/ubuntu&lt;/code&gt; 镜像，并运行同样的 &lt;code&gt;uname -m&lt;/code&gt;，当然由于架构不同，无法运行该镜像，给出了 &lt;code&gt;standard_init_linux.go:211: exec user process caused "exec format error"&lt;/code&gt; 错误。在使用 Dockerfile 构建镜像时，遇到类似的 &lt;code&gt;exec /bin/bash: exec format error&lt;/code&gt; 错误也需要考虑是不是架构的问题。&lt;/li&gt;
&lt;li&gt;运行 &lt;code&gt;qemu-user-static&lt;/code&gt; 镜像后，&lt;code&gt;arm64v8/ubuntu&lt;/code&gt; 就可以成功运行了，终端给出的信息表明 &lt;code&gt;arm64v8/ubuntu&lt;/code&gt; 是一个用于 ARM 设备的镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简单来说，qemu-user-static 通过 QEMU 模拟器模拟出了 ARM 设备，从而实现在 X86 设备上运行或是构建 ARM 镜像。当然，qemu-user-static 能模拟的硬件不仅限于 ARM，对于支持的硬件，官网上有更详细的介绍。&lt;/p&gt;
&lt;p&gt;qemu-user-static 的安装和使用都可以通过以下命令完成，若本地不存在该镜像，Docker 会自动从云端拉取：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;也有人会让 qemu-user-static 在后台一直运行，我嫌维护起来麻烦，就直接使用上面的命令，如果后台挂掉了，再运行一次就好。&lt;/p&gt;
&lt;h2 id="docker-chang-yong-ming-ling"&gt;Docker 常用命令&lt;/h2&gt;
&lt;p&gt;最后再记录几个创建环境时常用的 Docker 命令：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;# 检查镜像的架构
$ docker inspect {image_name}:{tag} | grep "Architecture"

# 用终端交互模式进入镜像的 /bin/bash
$ docker run -it {image_name}:{tag} /bin/bash

# 使用当前文件夹中的 Dockerfile 构建镜像，不使用缓存并输出详细信息
$ docker build -t {image_name}:{tag} . --progress=plain --no-cache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dockerfile 中记录了配置镜像的所有步骤，其他人也可以通过分享出去的 Dockerfile 构建相同的环境。而在撰写 Dockerfile 时，由于不熟悉基本镜像，一般都需要参考着终端给出的反馈来修改 Dockerfile 中的命令。这时候使用 &lt;code&gt;docker run -it&lt;/code&gt; 就很方便，特别是运行 qemu-user-static 后，可以直接进入 ARM 镜像的交互终端中，一步步安装依赖后再保存命令。&lt;/p&gt;
&lt;p&gt;上面的方法在简单的镜像中尚可，有的基本镜像做了特别复杂的操作，就算使用 qemu-user-static 也无法执行 &lt;code&gt;docker run&lt;/code&gt;，这种情况下就必须根据 &lt;code&gt;docker build&lt;/code&gt; 给出的错误信息修改 Dockerfile 了。在对 Dockerfile Debug 时，指定 &lt;code&gt;--progress=plain --no-cache&lt;/code&gt; 两个参数能输出更为完整的错误。&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/chen2ha/p/17180287.html" rel="noopener" target="_blank"&gt;x86 平台利用 qemu-user-static 实现 arm64 平台 docker 镜像的运行和构建 - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="Docker"></category><category term="Linux"></category></entry><entry><title>文献总结｜结构诱导的预训练</title><link href="https://leonis.cc/sui-sui-nian/2023-06-23-summary-doi.org/10.1038/s42256-023-00647-z.html" rel="alternate"></link><published>2023-06-23T00:00:00+08:00</published><updated>2023-06-23T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-06-23:/sui-sui-nian/2023-06-23-summary-doi.org/10.1038/s42256-023-00647-z.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年 MIT 研究团队在 Nature Machine Intelligence 发表上的一篇文章，文章原标题为 Structure-inducing pre-training，文章调查了目前广泛应用的多种预训练模型，设计了一种通过图结构在预训练过程中引入显式且深层结构约束的方法。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1038/s42256-023-00647-z" rel="noopener" target="_blank"&gt;doi.org/10.1038/s42256-023-00647-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年 MIT 研究团队在 Nature Machine Intelligence 发表上的一篇文章，文章原标题为 Structure-inducing pre-training，文章调查了目前广泛应用的多种预训练模型，设计了一种通过图结构在预训练过程中引入显式且深层结构约束的方法。&lt;/p&gt;
&lt;p&gt;预训练-微调的学习模式在自然语言处理及其他相关领域都已经得到广泛的应用，预训练通过在隐空间中提取样本的特征，从而提升模型在下游任务上的表现。但目前的预训练模型都没能在潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 上添加结构约束，从而获得既显式又深层的特征，这是目前预训练模型的一大缺陷。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;p&gt;对于数据集 &lt;span class="math"&gt;\(\boldsymbol{X}_\mathrm{PT}\in\mathcal{X}^{N_\mathrm{PT}}\)&lt;/span&gt;，预训练的目标就是从学习过程中得到编码器 &lt;span class="math"&gt;\(f_\theta:\mathcal{X}\rightarrow\mathcal{Z}\)&lt;/span&gt;，然后将 &lt;span class="math"&gt;\(f_\theta\)&lt;/span&gt; 用于各种各样的下游任务。&lt;/p&gt;
&lt;h3 id="xian-shi-he-shen-ceng-jie-gou-yue-shu"&gt;显式和深层结构约束&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;显示结构约束&lt;/strong&gt;：如果能从隐空间 &lt;span class="math"&gt;\(\mathcal{Z}\)&lt;/span&gt; 中的两个样本 &lt;span class="math"&gt;\(\boldsymbol{z}_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{z}_j\)&lt;/span&gt; 直接推导出两者间的关系（如距离），那么该预训练过程就有显示的结构约束。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深层结构约束&lt;/strong&gt;：预训练过程中所使用的信息越多（如维数），那么预训练过程所使用的结构约束越深。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前大部分的预训练模型都无法同时保证显式与深层的结构约束，调查目前超过 90 种的预训模型，其方法可以分为以下几类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;完全不使用样本间的关系，例如 prompt 训练，主要用于文本生成。&lt;/li&gt;
&lt;li&gt;使用显式，但浅层的监督预训练目标，例如 BERT 的 Next Sentence Prediction 训练模式。&lt;/li&gt;
&lt;li&gt;使用深层，但隐式的无监督或自监督预训练目标，例如通过添加噪声的数据强化方法。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9284?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9284?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;因此文章设计了一种同时使用显式与深层的结构约束的预训练框架，称这种方法为结构诱导的预训练。&lt;/p&gt;
&lt;p&gt;首先将预训练问题表示为图 &lt;span class="math"&gt;\(G_\mathrm{PT}=(V,E)\)&lt;/span&gt;，其中结点 &lt;span class="math"&gt;\(V\)&lt;/span&gt; 表示 &lt;span class="math"&gt;\(\boldsymbol{X}_\mathrm{PT}\)&lt;/span&gt; 中的预训练样本，&lt;span class="math"&gt;\(E\)&lt;/span&gt; 表示预先定义的样本间关系。&lt;/p&gt;
&lt;p&gt;接着预训练的损失函数就定义为&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathcal{L}_\mathrm{PT}=(1-\lambda_\mathrm{SI})\mathcal{L}_\mathrm{M}+\lambda_\mathrm{SI}\mathcal{L}_{SI}
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\mathcal{L}_\mathrm{M}\)&lt;/span&gt; 为传统预训练模型所使用的损失函数，&lt;span class="math"&gt;\(\mathcal{L}_\mathrm{SI}\)&lt;/span&gt; 是定义用于实现结构诱导目标的损失函数，使隐空间的各潜变量满足 &lt;span class="math"&gt;\(G_\mathrm{PT}\)&lt;/span&gt; 中的边（样本间关系）。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用了 3 类数据用于预训练：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proteins：来自 Stanford tree-of-life 数据集约 150 万条蛋白序列&lt;/li&gt;
&lt;li&gt;Abstracts：来自 Microsoft Academic Graph 数据集约 650,000 篇的生物医学相关的文本摘要&lt;/li&gt;
&lt;li&gt;Networks：来自文献的 70,000 条蛋白-蛋白相互作用网络的子图&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Proteins 与 Abstracts 预训练的编码器是 Transformer 架构，Networks 预训练所使用的模型是具有图同构网络（Graph Isomorphism Network, GIN）编码器的图卷积神经网络（graph convolutional neural network, GNN）。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9285?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9285?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;预训练模型在下游任务上的测试结果如上图所示，&amp;Delta; 一列中以 &amp;uarr; 表示相对传统预训练模型性质的提升，可以看出不管是相对于 per-token 还是 per-sample 的传统预训练策略，文中提出的结构诱导的预训练方法（structure-inducing pre-training, SIPT）在各下游任务上具有更好的表现。&lt;/p&gt;
&lt;p&gt;分析 Networks 任务得到的各种预训练模型在下游任务中微调的过程，SIPT 方法相比其他预训练方法得到的特征能够更快收敛，且在最后得到更好的效果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9286?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9286?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章调查了多种预训练模型，分析其训练目标发现大多数都没有引入显式且深层的结构约束，文章设计了一种预训练策略 SIPT，通过预训练图 &lt;span class="math"&gt;\(G_\mathrm{PT}\)&lt;/span&gt; 在隐空间中加入了显式且深层的结构约束，相比于传统的预训练方法，这种策略在下游任务的层次上提升上模型表现。&lt;/p&gt;
&lt;p&gt;文章借鉴了图结构来对样本与样本间的关系建模，但文中并未对得到「显式且深层」的特征做详尽的研究，只能推测这种方法更适用于蛋白-蛋白相互作用等更关注于样本间关系的任务，还不能证明 SIPT 得到的例如分子表示比传统预训练方法得到的分子表示更好。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category><category term="Transformer"></category></entry><entry><title>文献总结｜MTGL-ADMET：一种通过地位理论与最大流增强并用于 ADMET 预测的多任务图学习框架</title><link href="https://leonis.cc/sui-sui-nian/2023-06-09-summary-doi.org/10.1007/978-3-031-29119-7_6.html" rel="alternate"></link><published>2023-06-09T00:00:00+08:00</published><updated>2023-06-09T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-06-09:/sui-sui-nian/2023-06-09-summary-doi.org/10.1007/978-3-031-29119-7_6.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年 西北工业大学发表在 RECOMB 2023 上的一篇文章，文章原标题为 MTGL-ADMET: A Novel Multi-task Graph Learning Framework for ADMET Prediction Enhanced by Status-Theory and Maximum Flow，文章通过地位理论与最大流构造了由主要任务与辅助任务构成的多任务模型，相比单任务模型在预测准确性上有很大提高。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1007/978-3-031-29119-7_6" rel="noopener" target="_blank"&gt;doi.org/10.1007/978-3-031-29119-7_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年 西北工业大学发表在 RECOMB 2023 上的一篇文章，文章原标题为 MTGL-ADMET: A Novel Multi-task Graph Learning Framework for ADMET Prediction Enhanced by Status-Theory and Maximum Flow，文章通过地位理论与最大流构造了由主要任务与辅助任务构成的多任务模型，相比单任务模型在预测准确性上有很大提高。&lt;/p&gt;
&lt;p&gt;对于 ADMET 多种性质的预测，一般的方法是单任务学习，也就是一个模型只完成一种任务（预测一种性质），这种方法不仅繁琐，而且在缺少真实数据的情况下效果不佳。近年来出现的一种新范式是先通过预训练得到分子的通用表示，再将其用于多任务学习，使用一个模型完成所有预测任务，预训练的步骤弥补了缺少真实数据的问题。&lt;/p&gt;
&lt;p&gt;文章认为，现有基于多任务的 ADMET 模型都是通过一个模型完成所有预测任务，这样的共同学习很难保证模型能够共同学习到多种性质的信息，导致效果甚至不如单任务学习。文章设想以一个任务为主要任务，多个其他任务作为辅助任务，并通过地位理论找到最佳的任务搭配，改善模型效果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9255?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9255?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9256?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9256?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在文章设计的「一个主要任务，多个辅助任务」模式下，需要通过 3 个步骤找到这最佳的任务搭配，如上图中 &lt;strong&gt;a&lt;/strong&gt; 所示。&lt;/p&gt;
&lt;p&gt;首先先以各任务为单任务建立模型，例如对任务 &lt;span class="math"&gt;\(t_w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(t_k\)&lt;/span&gt; 分别建立单任务模型 &lt;span class="math"&gt;\(\mathcal{S}_w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\mathcal{S}_k\)&lt;/span&gt;，再为其建立多任务模型 &lt;span class="math"&gt;\(\mathcal{D}_{w,k}\)&lt;/span&gt;，那么 &lt;span class="math"&gt;\(t_w\)&lt;/span&gt; 对 &lt;span class="math"&gt;\(t_k\)&lt;/span&gt; 的影响就可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{Z}_{w\rightarrow k}=Z^{(d)}_{k|w}-Z^{(s)}_k
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(Z^{(s)}_k\)&lt;/span&gt; 就是 &lt;span class="math"&gt;\(\mathcal{S}_k\)&lt;/span&gt; 模型的表现，&lt;span class="math"&gt;\(Z^{(d)}_{k|w}\)&lt;/span&gt; 就是 &lt;span class="math"&gt;\(\mathcal{D}_{w,k}\)&lt;/span&gt; 模型的表现，从而可以得到类似下图中的结果：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9257?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9257?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;接着根据以上结果将相互增强的任务作为同一组的多任务，再通过地位理论决定各组任务中的主要任务，其他任务作为辅助任务。简单来说，地位理论就是将对模型表现提升最多的任务视为主要任务。&lt;/p&gt;
&lt;p&gt;最后通过最大流优化所选择的辅助任务。经过以上步骤，就可以将许多 ADMET 性质的预测任务分组，分别建立多任务模型。&lt;/p&gt;
&lt;p&gt;多任务模型的预测过程如上图 &lt;strong&gt;b&lt;/strong&gt; 所示，输入的分子通过两层 GCN 提取分子的信息，得到分子 embedding 表示，再在 Task-specific molecular embedding module 中得到适用于特定任务的分子表示。对于辅助任务，分子表示直接通过全连接层得到相应任务的预测结果。对于主要任务，除了针对于本任务的分子表示，还通过 Gating Network 通过可学习的权重融合来自于辅助任务的分子表示（图 &lt;strong&gt;c&lt;/strong&gt;），最后得到预测结果。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;模型所使用的 ADMET 数据来源于各文献中收集到的 24 种性质（18 个分类任务，6 个回归任务），共包含 43291 个类药的化合物。&lt;/p&gt;
&lt;p&gt;输入模型的分子以图的形式表示，分子图除了原子信息外，还添加了手性、电荷、芳香性、杂化等信息。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9258?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9258?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MTGL-ADMET 在 24 种性质上的预测结果如上图所示，括号中的数字代表辅助任务的数量。与其他图模型相比，MTGL-ADMET 在 20 个任务上表现最优，另外 4 个任务上表现仅次于最优。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9259?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9259?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在消融实验中，文章验证了「主要任务+辅助任务」策略的效果，测试结果如上图所示。与单任务（Single）、随机挑选 5 个辅助任务（Ran-5）和不使用地位理论与最大流而仅挑选对模型提升最大的 5 个辅助任务（Top-5）相比，MTGL-ADMET 在所有性质的预测上表现都是最佳的，说明了文章所设计多任务策略的优势。&lt;/p&gt;
&lt;p&gt;最后，文章展示了模型的可解释性，下图的案例展示了化合物结构片段与相应性质的相关性。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9260?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9260?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种用于构建 ADMET 多任务的策略，该策略主要使用地位理论与最大流分析了对主要任务具有增强作用的辅助任务，将主要任务与辅助任务一起构建多任务模型，使模型最后的预测效果好过很多完成类似任务的图模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;局限&lt;/strong&gt;：文章只评估了多任务模型中主要任务的预测结果，而没有全面评估模型包括辅助任务在内的多个预测结果，文章中的策略可以找到辅助提升主要任务结果的辅助任务，但这样的多任务模型不一定在多个任务上都表现得很好。文章中所测试的 ADMET 数据较少，在 ADMET 性质种类很多时，在两两任务间寻找是否具有性能提升的步骤就会变得繁琐。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category></entry><entry><title>文献总结｜探测图表示</title><link href="https://leonis.cc/sui-sui-nian/2023-06-02-summary-doi.org/10.48550/arXiv.2303.03951.html" rel="alternate"></link><published>2023-06-02T00:00:00+08:00</published><updated>2023-06-02T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-06-02:/sui-sui-nian/2023-06-02-summary-doi.org/10.48550/arXiv.2303.03951.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年德国亥姆霍兹信息安全中心研究团队发表在 AISTATS 2023 上的一篇文章，文章原标题为 Probing Graph Representations，文章设计了多种分子表示的探测模型，并通过探测模型研究了图模型在预训练后所编码分子信息。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2303.03951" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2303.03951&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年 德国亥姆霍兹信息安全中心研究团队发表在 AISTATS 2023 上的一篇文章，文章原标题为 Probing Graph Representations，文章设计了多种分子表示的探测模型，并通过探测模型研究了图模型在预训练后所编码分子信息。&lt;/p&gt;
&lt;p&gt;随着基于图的深度学习模型不断出现，亟需回答的一个问题是「图模型将什么信息编码进了表示中？」为了研究这一问题，文章构建了探测模型测试预训练图模型得到的分子表示。&lt;/p&gt;
&lt;p&gt;探测图表示的思路很简单，如果能从图模型输出的分子表示中提取出分子性质，那么就可以认为该性质被编码进分子表示中，所以文章的工作流程是「预训练-预测」（略不同于「预训练-微调」）。通过该流程，文章测试了传统 GNN 与基于 Transformer 的图模型等不同架构、不同数据集、不同优化算法等因素对于模型编码得到的潜变量的影响。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;p&gt;在分子性质预测中，对于分子 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 与其性质 &lt;span class="math"&gt;\(y\)&lt;/span&gt;，完成该任务的模型就是映射 &lt;span class="math"&gt;\(f:\boldsymbol{x}\mapsto y\)&lt;/span&gt;。取出 GNN 或图 Transformer 模型中 &lt;span class="math"&gt;\(d\)&lt;/span&gt; 维的 &lt;span class="math"&gt;\(l\)&lt;/span&gt; 层输出 &lt;span class="math"&gt;\(f_l(\boldsymbol{x})=\boldsymbol{z}\)&lt;/span&gt;，该潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 可以作为输入 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 的一种表示，进一步得到 &lt;span class="math"&gt;\(y\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;文章使用不同的图模型得到分子表示，再通过另一模型测试分子表示 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 预测分子性质 &lt;span class="math"&gt;\(y\)&lt;/span&gt; 的性能，从而对比不同图模型提取特征信息的能力。&lt;/p&gt;
&lt;p&gt;所构建的预测分子性质任务包括较为基础的判断是否具有某些官能团、更高层次的毒性、血脑屏障渗透性等。&lt;/p&gt;
&lt;h3 id="tan-ce-ce-lue"&gt;探测策略&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;线性探测（Linear Probing）：使用最简单的线性层，将分子表示映射为分子性质。&lt;/li&gt;
&lt;li&gt;贝叶斯探测（Bayesian Probing）：互信息可以用于 &lt;span class="math"&gt;\(Z\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(P\)&lt;/span&gt; 两个随机变量之间的依赖程度，文中通过计算潜变量与分子性质间的贝叶斯互信息进行评估。&lt;/li&gt;
&lt;li&gt;成对探测（Pairwise Probing）：将结构相近而性质差异大的分子构成一对 &lt;span class="math"&gt;\((\boldsymbol{x}_i,\boldsymbol{x}'_i)\)&lt;/span&gt;，通过主成分分析等方法分子潜变量与分子性质之间的关系。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9217?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9217?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先使用线性模型用 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 预测了分子中是否具有某种子结构，结果如上图所示，基于 Transformer 的一类图模型显然具有比 GCN 和 GIN 具有更好的表现，同时 GCN 模型得到的表示又比以 Morgan 指纹作为分子表示更好。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9218?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9218?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在更高层次的分子性质数据集上测试各种分子表示，结果如上图所示，以 Morgan 指纹作为分子表示的任务效果比部分图模型更好，Morgan 指纹作为一种可以简单获得的分子表示，仍然适合用于许多机器学习模型中完成预测任务。&lt;/p&gt;
&lt;p&gt;基于 Transformer 的图模型在更高层次的分子性质数据集上同样具有更好的表现，是具有潜力的新一代分子表示方式。这一点也可以从下图中看出，在左图中，基于 Transformer 图模型的结果都位于右上角，既能表示低层次的子结构信息，也能有效编码高层次的分子性质信息，而其他分子表示则位于左下角。右图使用贝叶斯互信息评估了样本数量与 &lt;span class="math"&gt;\(Z\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; 之间的依赖程度的关系，就整体趋势而言，仍然是基于 Transformer 图模型效果更好。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9219?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9219?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章通过主成分分析评估了相似分子间不同的分子表示，两个相似分子仅在官能团上有所不同，文中选择的官能团为硝基。结果如下图左侧一列所示，with FG 表示含硝基分子，w/o FG 表示去除该官能团的分子，可以明显看出，相比于 GCN，GraphGPS 这一基于 Transformer 的图模型所产生的特征中，两种结构相似的分子也具有较大的区分子，是更好的分子表示。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9220?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9220?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章设计探测模型研究了图模型在预训练后编码的分子信息，最终发现相比于使用消息传递聚合信息的传统 GNN 模型，基于 Transformer 的图模型能够学习到更多与化学相关的化学信息，得到更好的分子表示。文章中提出的分析方法为预训练模型的测试以及分子表示的评估提供了指导。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category></entry><entry><title>四月十二奉新纸一试</title><link href="https://leonis.cc/zai-lu-shang/2023-05-30-new-calligraphy-paper.html" rel="alternate"></link><published>2023-05-30T00:00:00+08:00</published><updated>2023-05-30T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-30:/zai-lu-shang/2023-05-30-new-calligraphy-paper.html</id><summary type="html">&lt;p&gt;常常划拉大字，练字的毛边纸用得很快，加之想多试试不同品种的纸，于是日前购买了一批新纸。从古至今，宣纸的价格都不算便宜，也少有人负担得起用宣纸练字，我所购买的纸也大多是毛边纸。新纸亦属于毛边纸，但与以前所买的毛边纸大不相 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;常常划拉大字，练字的毛边纸用得很快，加之想多试试不同品种的纸，于是日前购买了一批新纸。从古至今，宣纸的价格都不算便宜，也少有人负担得起用宣纸练字，我所购买的纸也大多是毛边纸。新纸亦属于毛边纸，但与以前所买的毛边纸大不相同，欣然提笔一试，果然令人惊喜。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="浓墨试纸" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9191?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="浓墨试纸" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9191?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 左为旧竹纸，单面粗糙且无帘纹，右为新纸&lt;/p&gt;
&lt;p&gt;新购之纸明显更为厚实，且颜色偏白，不像竹纸那样黄。取纸在灯下观之，帘纹新晰，料不是机器所制，人工捞纸才有这样的痕迹。向者识别毛边纸的方法是「若纸单面糙，则为机制纸；若双面糙，则为手工纸」，我也尝试用手指轻捻，发现竟两面粗糙。可我购买的的确是价廉的机制纸，取发货单审阅，上面也分明写着「机制」二字。疑惑这余，仔细摩挲再三，才发觉的确一面更滑，两面仅差毫厘。从这几点上看，虽说买的是毛边纸，却有下宣纸一等的做工了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="淡墨试纸" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9192?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="淡墨试纸" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9192?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 淡墨在新纸上晕开的痕迹，纸面上的帘纹清晰可见&lt;/p&gt;
&lt;p&gt;竹纸不吸墨，就算使用较稀的墨我也喜欢再加些水。在新纸上一试，墨水骤然晕开，分出浓淡的墨色，竹纸决没有这样的表现力，这种水汽氤氲之感特别适合用来写邓石如的篆书。我又兑上浓墨，虽然纸面抚摸着似乎并没有竹纸粗糙，行笔却要用更大的力气，也就是所谓「吃」得住笔。从浓淡墨的线条来看，新纸干湿两宜，行笔之间的迟滞顺滑又全然不同于竹纸，这种妙趣是在竹纸上完全找不到的。最重要的是这新纸仅比以前用的竹纸贵少许，但仍比宣纸便宜得多，很适合用来日常练字。&lt;/p&gt;
&lt;p&gt;在发现好物的欣喜之余，我不由地又惊异于科技的发展。旧时认为宣纸必须借由人工制作，制作过程还需要制纸师傅具有高超的捞纸技术，这些观点似乎正在被改写。费孝通先生在《乡土中国》一书中提到，在人们生于斯而长于斯的「乡土社会」中，一切都是那么的自然，一切生活中行之有效的法则都可以由口耳相传的经验得到，而当进入到原子化的「现代社会」后，无数人在世界范围发生着巨大规模的迁徙，在面对新事物时，那些法则就失效了。我自诩为年轻一代，在科技昌明的环境中成长，对那些流传下来的陋习也是弃如敝履，毫不惋惜，并自矜于终于能与老大帝国的积习切割。当我摩挲纸背发现我的经验失效时，内心竟也划过了一丝惶恐，原来我曾以为的法则也正在失效，我正迷惘地处在乡土社会与现代社会的间隙。一切都在改变，一切又都似也没变，似距离跨出乡土社会还甚遥远。这种种又何尝不是对我趋于「保守」的一种警醒？&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="书法"></category></entry><entry><title>文献总结｜可以同时完成分子语言序列回归和生成的 Regression Transformer</title><link href="https://leonis.cc/sui-sui-nian/2023-05-27-summary-doi.org/10.1038/s42256-023-00639-z.html" rel="alternate"></link><published>2023-05-27T00:00:00+08:00</published><updated>2023-05-27T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-27:/sui-sui-nian/2023-05-27-summary-doi.org/10.1038/s42256-023-00639-z.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年 IBM 研究团队发表在 &lt;em&gt;Nature Machine Intelligence&lt;/em&gt; 上的一篇文章，文章原标题为 Regression Transformer enables concurrent sequence regression and generation for molecular language modelling，文章提出了一种可以同时处理序列中的数值与文本并完成回归与生成的多任务的 Transformer 模型。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1038/s42256-023-00639-z" rel="noopener" target="_blank"&gt;doi.org/10.1038/s42256-023-00639-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年 IBM 研究团队发表在 &lt;em&gt;Nature Machine Intelligence&lt;/em&gt; 上的一篇文章，文章原标题为 Regression Transformer enables concurrent sequence regression and generation for molecular language modelling，文章提出了一种可以同时处理序列中的数值与文本并完成回归与生成的多任务的 Transformer 模型。&lt;/p&gt;
&lt;p&gt;基于 Transformer 的模型是化学任务中常用的模型，但由于 Transformer 最早是用于自然语言处理的模型，难以处理回归任务，这些模型只能完成性质预测或条件分子生成，无法同时完成指定结构的生成和性质预测。若要实现有约束的分子生成，即根据指定的性质生成分子，则不得不通过在多个模型间传递参数再得到反馈的方法不断调节并得到目标的分子，如下图中 &lt;strong&gt;a&lt;/strong&gt; 所示。&lt;/p&gt;
&lt;p&gt;文章尝试将回归任务融入到文本序列建模的过程中，提出了一种可以同时处理序列中的数值与文本并完成回归与生成的多任务模型，称为 回归 Transformer（Regression Transformer, RT）。在实验部分，文章使用化学领域中常见的分子生成、性质预测、化学反应预测、生物领域中蛋白质性质预测以及自然语言处理中的文本生成等多种任务测试了模型效果，证明 RT 是一种可以通用于多种任务且可以同时完成序列回归和生成的模型。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9166?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9166?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;Transformer 原为由左至右逐次由前一个 token 预测下一个 token 的自回归模型，而在分子语言，如 SMILES 中，序列中各原子的顺序是没有特定意义的，序列中的原子也并非由前一个原子决定，因此文章选择使用非自回归模型。BERT、XLNet 都是 Transformer 的变种，BERT 使用掩码的方式随机掩盖序列中的 token，并根据周围的 token 预测被掩盖的 token，因为这个过程使用周围信息编码掩盖的 token，这类模型称为自编码模型。&lt;/p&gt;
&lt;p&gt;XLNet 结合了自回归模型与自编码模型的优势，尽管 XLNet 还是由左至右预测 token，但它使用排列置换的方法将随机选择的待预测 token 放至序列末端，与 BERT 的掩码机制实际上相同，称为排列语言模型（Permutation language modeling, PLM）。文章使用 XLNet 作为主要的模型。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9167?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9167?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h4&gt;数值编码器&lt;/h4&gt;
&lt;p&gt;如上图所示，输入的数据格式为 &lt;code&gt;&amp;lt;ESOL&amp;gt;-2.92|SMILES&lt;/code&gt;，&lt;code&gt;&amp;lt;ESOL&amp;gt;&lt;/code&gt; 标识了预测的性质，&lt;code&gt;-2.92&lt;/code&gt; 为该性质的数值。由于 Transformer 无法识别数值，会将其识别为数字字符，文章设计了数值编码器（numeric encoder, NE）获取数值信息。&lt;/p&gt;
&lt;p&gt;先将 &lt;code&gt;-2.92&lt;/code&gt; 分为 &lt;code&gt;_-_&lt;/code&gt; &lt;code&gt;_2_0_&lt;/code&gt; &lt;code&gt;_._&lt;/code&gt; &lt;code&gt;_9_-1_&lt;/code&gt; &lt;code&gt;_2_-2_&lt;/code&gt; 几个 token，其中的 &lt;code&gt;_-_&lt;/code&gt; 与 &lt;code&gt;_._&lt;/code&gt; 分别表示负号与小数点，数字 &lt;code&gt;9&lt;/code&gt; 就以 &lt;code&gt;_9_-1_&lt;/code&gt; 表示，其中 &lt;code&gt;9&lt;/code&gt; 表示数值为 9，&lt;code&gt;-1&lt;/code&gt; 表示该值位于十分位（10&lt;sup&gt;-1&lt;/sup&gt;）。&lt;/p&gt;
&lt;p&gt;对于数值 token &lt;span class="math"&gt;\(t_{v,p}\)&lt;/span&gt;，&lt;span class="math"&gt;\(v\)&lt;/span&gt; 表示该 token 的数值，&lt;span class="math"&gt;\(p\)&lt;/span&gt; 表示该 token 数值的位置，词嵌入的第 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 维按下式计算：&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{NE_{Float}}(v,p,j)=(-1)^j\cdot\frac{v\cdot 10^p}{j+1}
$$&lt;/div&gt;
&lt;p&gt;然后与 SMILES 的常规词嵌入一起加上位置编码进入 XLNet 中进行计算。&lt;/p&gt;
&lt;h4&gt;XLNet&lt;/h4&gt;
&lt;p&gt;输入 RT 的 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 是由 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 个性质 token &lt;span class="math"&gt;\([\boldsymbol{x}^p]_k\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(l\)&lt;/span&gt; 个文本 token &lt;span class="math"&gt;\([\boldsymbol{x}^t]_l\)&lt;/span&gt; 拼接而成，即&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{x}=[\boldsymbol{x}^p,\boldsymbol{x}^t]_T=[x^p_1,\cdots,x^p_k,x^t_1,\cdots,x^t_l]
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(T=k+l\)&lt;/span&gt;，为整个序列的 token 数量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PLM objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{PLM}\)&lt;/span&gt;）：在原始的 XLNet 中，输入的序列就要做 &lt;span class="math"&gt;\(T!\)&lt;/span&gt; 次的排列，将掩盖的 token 放置到序列末端，训练目标是使模型能够预测出掩盖的 token。如上图中 PLM objective 所示，由于这种训练方法是随机选取，打断了整体的 &lt;span class="math"&gt;\(\boldsymbol{x}^p\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(\boldsymbol{x}^t\)&lt;/span&gt;，因而不适合该任务，仅用于预训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Property prediction objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{P}\)&lt;/span&gt;）：对于分子性质预测的回归任务，将表示分子性质的 &lt;span class="math"&gt;\(\boldsymbol{x}^p\)&lt;/span&gt; 全部掩盖并排列置换位置，使用分子的文本 &lt;span class="math"&gt;\(\boldsymbol{x}^t\)&lt;/span&gt; 预测被掩盖的分子性质。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conditional text generation objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{G}\)&lt;/span&gt;）：对于分子生成任务，正与上述过程相反，将表示分子的 &lt;span class="math"&gt;\(\boldsymbol{x}^t\)&lt;/span&gt; 全部掩盖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Self-consistency (SC) objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{SC}\)&lt;/span&gt;）：为了使 RT 能够同时完成回归和生成任务，文章设计了该训练目标：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{J}_\mathrm{SC}=\mathcal{J}_\mathrm{G}(\boldsymbol{x})+\alpha\cdot\mathcal{J}_\mathrm{P}(\hat{\boldsymbol{x}})$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; 为权重，&lt;span class="math"&gt;\(\hat{\boldsymbol{x}}=[\boldsymbol{x}^p,\hat{\boldsymbol{x}}^t]\)&lt;/span&gt; 为生成的样本。该训练任务就是先使用分子性质生成分子，再用生成的分子预测其性质。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;使用 SELFIES 作为分子表示，许多研究表明，相比 SMILES，SELFIES 在分子生成任务上更具有优势。&lt;/p&gt;
&lt;p&gt;Synthetic QED dataset：由 ChEMBL 得到的约 160 万个分子，约 140 万用于训练，1000 条数据用于验证，10000 条数据用于测试。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;p&gt;文章中使用 RT 在化学反应、蛋白质性质预测等任务上测试了模型性能，这里仅以分子生成与分子性质预测的任务为例。&lt;/p&gt;
&lt;p&gt;在 QED 数据集上，先使用 &lt;span class="math"&gt;\(\mathcal{J}_\mathrm{PLM}\)&lt;/span&gt; 训练模型，至验证集数据的指标收敛后，再每 50 轮用 &lt;span class="math"&gt;\(\mathcal{J}_\mathrm{P}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{G}\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(\mathcal{J}_\mathrm{SC}\)&lt;/span&gt; 轮流微调（Alternate），不同模型设定的结果如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9168?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9168?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;从实验结果中可以看出，（1）SELFIES 在生成任务上更有优势，但在回归任务上稍逊于 SMILES；（2）不论是回归还是生成任务，预训练使模型的表现提升；（3）设计的数值编码器有利于模型识别数值信息，提升模型表现；（4）在微调阶段轮流使用不同的训练任务，使模型在回归和生成两种任务上的泛化能力更好，在回归和生成单个任务上都具有与单任务模型接近甚至更优的表现。&lt;/p&gt;
&lt;p&gt;能够处理回归与生成两种任务的模型也可以用于实现分子的性质优化，具体过程是设定一个 seed 分子以及目标的性质（primer），模型随机掩盖分子中 token 再通过 primer 将 token 预测出来，得到优化后的新分子，再通过新分子计算其性质的预测值，下图展示了在两种不同的数据集上微调得到的模型实现分子性质优化的样例。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9169?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9169?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了回归 Transformer（RT）模型，该模型以 XLNet 为主要的结构，文章增加了数值编码器用于获取数值信息，并设计了不同的训练模式使模型在预训练-微调后能够完成数值回归与序列生成两种不同的任务。RT 设计用于数值回归与序列生成，因此也可以用于蛋白性质预测、反应预测等。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>旧书市场淘书记</title><link href="https://leonis.cc/zai-lu-shang/2023-05-22-wander-in-old-book-shop.html" rel="alternate"></link><published>2023-05-22T00:00:00+08:00</published><updated>2023-05-22T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-22:/zai-lu-shang/2023-05-22-wander-in-old-book-shop.html</id><summary type="html">&lt;p&gt;不得不说，在北方诸多城市中，天津的二手旧物市场可以说是相当火热的。我猜测的原因有二，一则是天津的老龄人口占比多，古玩旧物收藏有很大的受众；二则是得益于近代天津经济、文化的繁荣，许多官商士绅定居在此，天津民间仍流通有十分 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;不得不说，在北方诸多城市中，天津的二手旧物市场可以说是相当火热的。我猜测的原因有二，一则是天津的老龄人口占比多，古玩旧物收藏有很大的受众；二则是得益于近代天津经济、文化的繁荣，许多官商士绅定居在此，天津民间仍流通有十分具有价值的骨董。&lt;/p&gt;
&lt;p&gt;我对古玩是一窍不通，再加之俚谚「多看少买」的教育，更有许多低劣到我都能看出的赝品，常引得我在心中暗笑，所以我对那些地摊上的古玩也一点不感兴趣。但旧物中有一门类却是我的心头好，那就是旧书。&lt;/p&gt;
&lt;p&gt;我一向认为书应当是用来读的，次之才是历史等其他价值。由于许多好书由于各种原因不再出版了，或是更改了原来的版本，没有旧版本更好读了，于是有了「藏书」的群体去搜罗这些旧书。所以「藏书」的「藏」不该是像对待金银珠宝那样的「秘藏」，而是作「保存」解。这是我的藏书主张，也是我搜集旧书的信条。&lt;/p&gt;
&lt;p&gt;之所以提及以上原则，还是因为天津旧货市场上的旧书实在太多了，近乎可以同逛新书店一般，不带任何想法去，抱回好几摞的书，为了避免这种无谓的金钱开销，必须要有筛选的准绳。前些天在反复告诉自己&lt;dot&gt;买书是为了读书&lt;/dot&gt;后，终于敢大胆淘了几本书，对其中几本实在喜欢得紧，也算小有收获。&lt;/p&gt;
&lt;h2 id="qie-jie-ting-za-wen-qie-jie-ting-za-wen-mo-bian"&gt;《且介亭杂文》《且介亭杂文末编》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="且介亭杂文" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9071?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="且介亭杂文" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9071?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;人文社 1973 年出版的鲁迅作品集应该是最优良的鲁迅作品版本，因为印量大，价格也不贵，但有几本很少见，凑齐全套并不容易。所以我一般是遇见了品相较好且为手中所无才购买，一切都随缘，并不特意搜集。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="且介亭杂文内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9070?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="且介亭杂文内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9070?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《且介亭杂文末编》，是我在一堆未经整理的书堆中翻找出来的，售 5 元。人民文学出版社 1973 年 4 月北京 1 版 1 印，扉枼钤「天津市第一机械工业学校图书舘藏书」，内枼整洁，纸张坚韧泛黄。唯一不美的是封面钤「不外借」圆印且封面有墨渍污损。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="且介亭杂文扉枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9072?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="且介亭杂文扉枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9072?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《且介亭杂文》，由我在另一书摊上访得，要价 4 元。人民文学出版社 1973 年 6 月山西 1 版 1 印，扉枼有 82 年的购书识记，内枼整洁，纸张洁白，可惜曾遭水浸，整册书都有湿后的压痕。鲁迅冠以「且介亭杂文」为名的集子共有 3 册，那么我还差一册《且介亭杂文二集》就可成一小帙了。&lt;/p&gt;
&lt;h2 id="tang-shi-xuan-yi-wei-liu-zhao-shi-xuan"&gt;《唐诗选》《汉魏六朝诗选》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐诗选与汉魏六朝诗选" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9078?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐诗选与汉魏六朝诗选" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9078?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;人文社的文学类古籍也具有口碑，可这一套《中国古典文学读本丛书》让我颇为困惑。这一套丛书中的书籍都具有类似的封面和题签，古雅简洁，装帧精美，而且编者与注者都是各领域的权威，内容也很精良，我十分喜欢。可是这一套丛书中兼有简体横排本和繁体竖排本，例如《唐诗选》和《汉魏六朝诗选》就都是简体横排，对简体横排介怀者在挑选这一套书时务必留意。能翻看时一看便知，但有时书商将书用塑料纸包装起来，不允翻看内枼，这时可以根据书口方向分辨，书口向右者为简体横排，书口向左者为繁体竖排本。在读古典文学时，我当然更喜欢用繁体竖排，这套丛书夹杂的两种版本让我困惑又纠结。&lt;/p&gt;
&lt;p&gt;此一册《唐诗选（上）》，中国社科院文学研究所编，全套为上下两册，因此仅售我 5 元，待有机会再访下册，这套《唐诗选》印量很大，可货比三家，寻找品相好，更适合翻阅的版本。人民文学出版社 1978 年 4 月北京 1 版 1 印，内枼整洁，纸张泛黄。许多人认为这套集子选的诗并不好，但我只以其简体横排为遗憾。唐诗存世量极大，唐朝诗人又如群星璀璨，无论怎么选都有顾此失彼之嫌，这套集子已经尽可能选出唐朝代表性诗人的作品，注释详略得当，限于篇幅可能未选许多代表作，但对于业余的爱好者概览唐诗完全是足够的了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="汉魏六朝诗选内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9079?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="汉魏六朝诗选内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9079?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《汉魏六朝诗选》，余冠英选注，可能由于印量少些，再加上这家书商的眼光比其他家更利，竟要价 10 元，不过余冠英的注本，加之品相不错，这个价格并不亏。人民文学出版社 1979 年 3 月北京 1 版 2 印，内枼整洁，纸张微黄。&lt;/p&gt;
&lt;h2 id="du-fu-shi-xuan-song-shi-xuan-zhu"&gt;《杜甫诗选》《宋诗选注》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="杜甫诗选与宋诗选注" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9080?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="杜甫诗选与宋诗选注" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9080?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;同样是《中国古典文学读本丛书》，《杜甫诗选》和《宋诗选注》都是繁体竖排，版式相同，老铅字实在赏心悦目，我十分钟爱这两本。&lt;/p&gt;
&lt;p&gt;此一册《杜甫诗选》，冯至选，要价 10 元，还价不允，无奈购下。人民文学出版社 1987 年北京 1 版 11 印，内枼整洁，纸张洁白，摩挲纸面铅字凹痕明显，字画如新，真令人边不释手。《杜甫诗选》并不是最好的杜甫诗集，但又是读杜诗难以绕开的选本。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="杜甫诗选内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9081?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="杜甫诗选内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9081?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《宋诗选注》，钱锺书选注，售 2 元，可以说是捡到的最大漏。人民文学出版社 1982 年 7 月北京 1 版重庆 1 印，内枼整洁，纸张洁白柔韧，字画清晰，惜其封面有折痕。这本集子是由钱锺书选、钱锺书选的宋诗，读宋诗的人可能不多，但是这本集子是宋诗最好的选注本。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="宋诗选注内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9083?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="宋诗选注内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9083?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="tang-song-ci-xuan-shi"&gt;《唐宋詞選釋》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐宋詞選釋" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9082?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐宋詞選釋" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9082?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《唐宋詞選釋》，俞平伯编，售 10 元，叹无人识此宝又恐有人抢购，立马购入。人民文学出版社 1979 年北京 1 版 1 印，扉枼钤「天津自行车二厂图书舘」，内枼整洁，纸张洁白柔韧，铅字的字画虽不如前面两本清晰，但同样令人赏玩不忍释手。俞平伯的《唐宋詞選釋》是读宋词的入门，选、释皆精良，说是读宋词必读并不为过。其实家中已有一本《唐宋詞選釋》，但已经快要脱胶散枼，遇到品相如此好的一本，真令我欣喜！&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐宋詞選釋内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9084?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐宋詞選釋内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9084?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="tang-shi-san-bai-shou-xin-zhu"&gt;《唐詩三百首新注》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐詩三百首新注" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9094?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐詩三百首新注" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9094?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《唐詩三百首新注》，金性尧注，售 5 元。上海古籍出版社 1980 年上海 1 版 1 印，扉枼有 00 年购书识记，竟购于内蒙而流入我手。内枼如新，纸张洁白柔韧，适合翻阅。看多了总感觉上古的铅字整体比人文的更好，字画更清晰，字形也更优美，但若是真让我哪些细节上有差异则有些困难。《唐诗三百首》是家喻户晓的唐诗集子，中华书局前几年覆刻的《唐诗三百首》是更好的版本，版式古雅且价格低廉，现在也很容易买到，但肯定是激光排印而不是铅印了。这本《唐詩三百首新注》静静躺在一角，封面的金字闪耀动人，立马吸引了我的注意，展卷翻阅，心甚悦之，遂购入。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐詩三百首新注扉枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9093?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐詩三百首新注扉枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9093?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐詩三百首新注内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9098?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐詩三百首新注内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9098?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后以全部收获的合影作结吧，共计约 50 元，从堆积成山的书堆里挑出这几本，真可谓是如大浪淘沙一般的「淘」书。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="淘书收获" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9099?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="淘书收获" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9099?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="藏书"></category></entry><entry><title>文献总结｜一种用于基于结构药物设计的 3D 生成模型</title><link href="https://leonis.cc/sui-sui-nian/2023-05-19-summary-doi.org/10.48550/arXiv.2203.10446.html" rel="alternate"></link><published>2023-05-19T00:00:00+08:00</published><updated>2023-05-19T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-19:/sui-sui-nian/2023-05-19-summary-doi.org/10.48550/arXiv.2203.10446.html</id><summary type="html">&lt;p&gt;本文介绍于 2021 年彭健课题组发表在 NeurIPS 2021 上的一篇文章，文章原标题为 A 3D Generative Model for Structure-Based Drug Design，文章提出了一种能够针对指定的蛋白生成药物分子的 3D 生成模型，在利用蛋白空间信息的情况下生成分子，实现基于结构的药物设计。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2203.10446" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2203.10446&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2021 年彭健课题组发表在 NeurIPS 2021 上的一篇文章，文章原标题为 A 3D Generative Model for Structure-Based Drug Design。&lt;/p&gt;
&lt;p&gt;基于结构药物设计中的一个基本问题是针对指定的蛋白结合位点生成分子，目前解决这一问题的深度学习方法可以分为两类：基于字符序列与基于图的方法。但不论是基于字符序列的 1 维模型，还是基于图的 2 维模型，其本质上缺少蛋白质 3 维空间中的信息。为了获取空间信息，目前也出现了在 3D 空间中实现分子生成的模型，但这些模型只能生成较小的分子，无法有效生成类药的更大分子。&lt;/p&gt;
&lt;p&gt;因此，文章提出了一种能够针对指定的蛋白生成药物分子的 3D 生成模型，在利用蛋白空间信息的情况下生成分子，实现基于结构的药物设计。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;蛋白的结合位点可以定义为原子的集合 &lt;span class="math"&gt;\(\mathcal{C}=\{(\boldsymbol{a}_i,\boldsymbol{r}_i)\}^{N_b}_{i=1}\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(N_b\)&lt;/span&gt; 是结合位点原子的数量，&lt;span class="math"&gt;\(\boldsymbol{a}_i\)&lt;/span&gt; 是第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个原子的特征，&lt;span class="math"&gt;\(\boldsymbol{r}_i\)&lt;/span&gt; 是其空间坐标。可以将在结合位点生成原子的任务视作为模拟结合位点中各位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 上出现原子的概率，也就是模拟原子在结合位点上出现的概率密度 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(e\in\mathcal{E}=\{\mathrm{H},\mathrm{C},\mathrm{O},\cdots\}\)&lt;/span&gt; 代表生成分子中的原子。&lt;/p&gt;
&lt;p&gt;为了对 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt; 建模，文章设计了两个模块：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;上下文编码器（Context Encoder）：使用图神经网络（graph neural networks, GNN）学习环境 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 下各原子的表示；&lt;/li&gt;
&lt;li&gt;空间分类器（Spatial Classifier）：输入任意位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt;，集合该位置附近所有上下文原子的表示，输出预测结果 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;上下文编码器&lt;/h4&gt;
&lt;p&gt;上下文编码器用于提取特征，获得各原子的表示，在该任务中，对原子表示有两个要求：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;原子表示不应只具有本身的信息，还应具有环境中的信息；&lt;/li&gt;
&lt;li&gt;在旋转和平移变换后，原子性质的性质不会发生改变，原子表示应具有旋转和平移不变性。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基于以上两点要求，文章使用了旋转平移不变的图神经网络。&lt;/p&gt;
&lt;p&gt;首先，针对蛋白结合位点构建 k-近邻图，基于结合位点 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 中各原子的距离得到图 &lt;span class="math"&gt;\(\mathcal{G}=\langle\mathcal{C},\boldsymbol{A}\rangle\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\boldsymbol{A}\)&lt;/span&gt; 为邻接矩阵，将 k-近邻中的第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个原子记作 &lt;span class="math"&gt;\(N_k(\boldsymbol{r}_i)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;接着，编码器将 &lt;span class="math"&gt;\(\mathcal{G}\)&lt;/span&gt; 中所有结点原子的特征 &lt;span class="math"&gt;\(\{\boldsymbol{a}_i\}\)&lt;/span&gt; 转化为嵌入表示 &lt;span class="math"&gt;\(\{\boldsymbol{h}^{(0)}_i\}\)&lt;/span&gt;，然后进入消息传递层。&lt;/p&gt;
&lt;p&gt;一般的 GNN 消息传递过程定义为&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{h}^{(\ell+1)}_i=\sigma\left(\boldsymbol{W}^\ell_\mathrm{self}\boldsymbol{h}^{(\ell)}_i+\boldsymbol{W}^\ell_\mathrm{nergh}\sum_{j\in\mathcal{N}}\boldsymbol{h}^{(\ell)}_j\right)
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}\)&lt;/span&gt; 为模型需要训练的参数，&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; 为激活函数。从上式中可以看出，GNN 的消息传递是在将 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 结点周围临近的 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 结点的信息按权重聚集起来。&lt;/p&gt;
&lt;p&gt;在文章中所使用的消息传递过程为&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{h}^{(\ell+1)}_i=\sigma\left(\boldsymbol{W}^\ell_0\boldsymbol{h}^{(\ell)}_i+\sum_{j\in N_k(\boldsymbol{r}_i)}\boldsymbol{W}^\ell_\mathrm{1}\boldsymbol{w}(d_{ij})\odot\boldsymbol{W}^\ell_2\boldsymbol{h}^{(\ell)}_j\right)
$$&lt;/div&gt;
&lt;p&gt;相比原式，文章在第 2 项中做了一些改动，&lt;span class="math"&gt;\(\boldsymbol{w}(\cdot)\)&lt;/span&gt; 是一个权重网络，&lt;span class="math"&gt;\(d_{ij}\)&lt;/span&gt; 为 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 两个结点间的距离。上述过程就是在聚集信息时，根据距离的远近分配权重，逐个原子计算后得到 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 中所有原子的嵌入表示集合 &lt;span class="math"&gt;\(\{\boldsymbol{h}^{(L)}_i\}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4&gt;空间分类器&lt;/h4&gt;
&lt;p&gt;在空间中的任意位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 上，空间分类器聚集由上下文编码器得到的原子的嵌入表示：&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{v}=\sum_{j\in N_k(\boldsymbol{r})}\boldsymbol{W}_0\boldsymbol{w}_\mathrm{aggr}(||\boldsymbol{r}-\boldsymbol{r}_j||)\odot\boldsymbol{W}_i\boldsymbol{h}^{(L)}_j
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{w}_\mathrm{aggr}(\cdot)\)&lt;/span&gt; 同样是一个权重网络。在这一步中，类似地根据任意位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 与周围结点间的距离 &lt;span class="math"&gt;\(||\boldsymbol{r}-\boldsymbol{r}_j||\)&lt;/span&gt; 分配权重，聚集该位置附近出现过原子的信息，得到特征 &lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;最后通过多层感知机、归一化后得到所求概率分布：&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{c}=\mathrm{MLP}(\boldsymbol{v})\\
p(e|\boldsymbol{r},\mathcal{C})=\frac{\exp(\boldsymbol{c}[e])}{1+\sum_{e'\in\mathcal{E}}\exp(\boldsymbol{c}[e'])}
$$&lt;/div&gt;
&lt;h4&gt;取样&lt;/h4&gt;
&lt;p&gt;因为 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt; 需要指定结合位点 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 和位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 得到预测的原子 &lt;span class="math"&gt;\(e\)&lt;/span&gt;，而分子生成需要根据 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 自动分配各原子的位置，所以由 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt; 导出&lt;/p&gt;
&lt;div class="math"&gt;$$p(e,\boldsymbol{r}|\mathcal{C})=\frac{\exp(\boldsymbol{c}[e])}{Z}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(Z\)&lt;/span&gt; 为未定的归一化常数。&lt;/p&gt;
&lt;p&gt;分子生成的过程为，在 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 步骤，使用结合位点（环境） &lt;span class="math"&gt;\(\mathcal{C}_t\)&lt;/span&gt; 由 &lt;span class="math"&gt;\(p(e,\boldsymbol{r}|\mathcal{C}_t)\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\((e_{t+1},\boldsymbol{r}_{t+1})\)&lt;/span&gt;，将 &lt;span class="math"&gt;\((e_{t+1},\boldsymbol{r}_{t+1})\)&lt;/span&gt; 加入到环境 &lt;span class="math"&gt;\(\mathcal{C}_t\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\(\mathcal{C_{t+1}}\)&lt;/span&gt;，再用于预测下一个原子的种类和位置，即&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    &amp;amp;(e_{t+1},\boldsymbol{r}_{t+1})\sim p(e,\boldsymbol{r}|\mathcal{C}_t)\\
    &amp;amp;\mathcal{C}_{t+1}\leftarrow\mathcal{C}_t\cup\{(e_{t+1},\boldsymbol{r}_{t+1})\}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;再增加一个辅助的分类网络用于判断生成原子是否为末端原子，若为末端原子则结束分子生成过程。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9062?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9062?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;CrossDocked 数据集中有 2.25 千万条对接得到的蛋白-配体对数据，经数据清洗后，使用其中的 100000 条数据训练模型，100 条数据作为测试集。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9063?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9063?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章首先测试了模型根据蛋白结合位点生成分子的整体效果。结果如上图所示，模型生成分子的对接打分略差于参考分子，但要更好于同类模型 liGAN，生成分子的 QED 与 SA 甚至好过于参考分子，生成分子的所有指标均好于 liGAN。在各分子性质分布的对比中，相比另两个数据集，生成分子的 QED 向右偏移，具有更好的类药性。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9064?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9064?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;以上结果也可以从生成分子的样例中看出，上图展示了模型针对两个蛋白生成的多个分子，生成分子的对接打分、QED 都要好于参考分子，同时许多生成分子还具有参考分子的类似结构。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9066?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9066?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章测试了模型在 linker 预测上的应用。模型不需要经过重新训练或都微调，只需将结合位点与片段作为初始的环境 &lt;span class="math"&gt;\(\mathcal{C}_0\)&lt;/span&gt;，模型就会根据环境补足片段间的 linker。测试结果如上图所示，与设计用于 linker 预测任务的 DeLinker 相比，文章中的模型在各方面都具有优势。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9065?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9065?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章也列举了 linker 预测结果的样例，虽然模型不一定能预测并找回参考分子，但预测生成的分子中都包含了指定的片段，同时模型是根据蛋白的 3D 信息生成 linker，这在基于结构的药物设计上可以作为应用工具。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章使用 GNN 构建了一种用于基于结构药物设计的分子生成模型，该模型使用 GNN 通过设计用于蛋白 3D 信息的消息传递过程提取结合位点中的空间信息，根据配体各原子在结合位点中各位置出现的概率建立模型，从该概率中取样实现分子生成。模型生成的分子具有蛋白的空间信息，在各方面指标上都具有较好的表现。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category></entry><entry><title>文献总结｜对编码器-解码器模型学习过程中化学结构识别的研究</title><link href="https://leonis.cc/sui-sui-nian/2023-05-13-summary-doi.org/10.1186/s13321-023-00713-z.html" rel="alternate"></link><published>2023-05-13T00:00:00+08:00</published><updated>2023-05-13T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-13:/sui-sui-nian/2023-05-13-summary-doi.org/10.1186/s13321-023-00713-z.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年东京大学发表在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Investigation of chemical structure recognition by encoder–decoder models in learning progress，文章，文章研究了编码器-解码器模型训练过程中对化学结构识别的过程以及将其潜变量作为分子表示用于下游任务的效果。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1186/s13321-023-00713-z" rel="noopener" target="_blank"&gt;doi.org/10.1186/s13321-023-00713-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年东京大学发表在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Investigation of chemical structure recognition by encoder&amp;ndash;decoder models in learning progress，文章，文章研究了编码器-解码器模型训练过程中对化学结构识别的过程以及将其潜变量作为分子表示用于下游任务的效果。&lt;/p&gt;
&lt;p&gt;基于结构的分子表示又被称为描述符，如何获得更好的描述符是化学信息学中很重要的问题。在近年兴起的深度学习领域，编码器-解码器（encoder-decoder, ED）类模型广受关注，以分子的字符序列 SMILES 作为输入，编码器模型会将其转化为一连串蕴含化学学信息的描述符，解码器模型通过该中间变量还原出原来的分子，这种由 SMILES（或其他）分子表示编码至隐空间中的潜变量就可以用作为数字形式的分子表示，用于各种下游任务，这也是自然语言处理中「预训练-微调」的范式。&lt;/p&gt;
&lt;p&gt;在传统方法中，也有例如 ECFP、NFP 一类的分子指纹，但他们只描述了分子中所具有的特定结构，无法由描述符再还原出分子结构。只有更好的表示才能在下游任务得到更好的结果，所以文章研究了在 ED 模型中化学结构的识别过程，ED 模型对化学结构的识别是指模型获得反映化学结构的数字信息和将该信息还原为化学结构的能力。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9036?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9036?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;模型的编码器部分是 3 层的 GRU，后面的全连接层将 GRU 的输出映射到 256 维的潜空间，解码器部分以该潜变量为输入，进入全连接层后同样是 3 层 GRU，输出还原的 SMILES。模型以输出 SMILES 与目标 SMILES 的交叉熵损失作为损失函数，训练模型的过程是使其能通过输入的随机化 SMILES 输出标准 SMILES。&lt;/p&gt;
&lt;p&gt;为了评估将 ED 模型中潜变量作为分子表示的效果，文章使用 ToxCast 中 113 个任务分别训练了 XGBoost，使用其预测结果（即下游任务结果）判断分子表示的优劣。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;模型所使用的训练数据来源于 ZINC15，从中随机获取了 3 千万个分子，后续通过去除非有机物常见原子、去除重原子等方式清洗数据。&lt;/p&gt;
&lt;h3 id="zhi-biao"&gt;指标&lt;/h3&gt;
&lt;p&gt;文章定义了两个指标用于评估 ED 模型的准确率，完全准确率定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{perfect\ accuracy}=\frac 1n\sum^n_iI(t=p)$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 表示正确的 SMILES，&lt;span class="math"&gt;\(p\)&lt;/span&gt; 表示预测的 SMILES，即计算与标签值相同的输出所占比例。&lt;/p&gt;
&lt;p&gt;另一个指标部分准确率定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{partial\ accuracy}=\frac 1n\sum^n_i\left\{\frac{1}{\max(l(t),l(p))}\sum^{\min(l(t),l(p))}_jI(t_i=p_i)\right\}$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(t_i\)&lt;/span&gt; 表示 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 位置上正确的 SMILES 字符，&lt;span class="math"&gt;\(p_i\)&lt;/span&gt; 表示 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 位置上预测的 SMILES 字符，该式即计算所有 SMILES 字符中，预测结果与标签值相同位置上相同的字符所占比例。&lt;/p&gt;
&lt;p&gt;在 XGBoost 模型中，使用 ROC 曲线下面积（AUROC）与 Matthews 相关系数（MCC）评估模型准确率。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9035?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9035?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章首先分别使用 10 k、100 k、1 M 的测诫集测试了训练过程中的 ED 模型，结果如上图所示。随着训练轮次的增加，模型还原出化学结构的准确率也在上升，对比部分准确率与完全准确率，可以发现在相同时刻下，部分准确率要高于完全准确率，这说明模型先学习到了还原出分子中若干个字符组成的小片段，然后才将其拼合还原出整个分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9038?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9038?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;以训练后的还原准确率命名模型，例如 94% 的模型记为「Model_94」，测试各模型的分子表示在 ToxCast 任务上的效果，结果如上图（a）所示。可以看出，除了未经训练完全无法还原出分子的 Model_0，其他模型都能较好地完成分类任务，而且分类效果比较接近。接着文章选定了三类结构的分子，使用 UMAP 降维的方法绘制出了其分子表示在化学空间中的位置，如上图（b）所示，在 Model_0 中，三种结构混杂在一起，很难完成分类，而 ED 模型只需要经过训练，三种结构就区别开来，与前一个实验的结果也吻合，这说明 ED 模型生成的潜变量很适用于分子分类的任务。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9037?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9037?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章比较了训练过程中还原出分子的分子量与 SMILES 长度的准确关系，在上图中只有在黄线上的样本表明还原分子与标签值一致，可以看出虽然 DE 模型在训练的靠前阶段无法还原出分子的特定性质，但其潜变量在化学空间中已经有了区分度，这种区分度足可以完成分类任务，但其所蕴含的化学信息还不能使模型还原出结构。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章研究了 DE 模型训练过程中对化学结构的识别以及将其中的潜变量作为分子表示的效果，文章的实验结果展示了模型训练过程中分子表示的变化，证明了将其作为下游任务的分子表示的可行性。&lt;/p&gt;
&lt;p&gt;在目前，预训练-微调是广泛使用的模型训练范式，而在化学信息学领域，其中的关键步骤，也就是将潜变量作为分子表示尚缺乏研究。文章研究的是较早的 GRU（RNN）所构成的 DE 模型，还应该对 Transformer、GPT 等目前更广泛使用的模型进行潜变量的深入研究。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="RNN"></category></entry><entry><title>文献总结｜我们能用 Transformer 模型快速学会「翻译」活性分子吗？</title><link href="https://leonis.cc/sui-sui-nian/2023-04-28-summary-doi.org/10.1021/acs.jcim.2c01618.html" rel="alternate"></link><published>2023-04-28T00:00:00+08:00</published><updated>2023-04-28T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-28:/sui-sui-nian/2023-04-28-summary-doi.org/10.1021/acs.jcim.2c01618.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Can We Quickly Learn to “Translate” Bioactive Molecules with Transformer Models? 文章使用 MMP 数据训练 Transformer，使其生成具有活性的分子，文章结果表明 Transformer 对于未知靶点也能生成活性分子。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/acs.jcim.2c01618" rel="noopener" target="_blank"&gt;doi.org/10.1021/acs.jcim.2c01618&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Can We Quickly Learn to &amp;ldquo;Translate&amp;rdquo; Bioactive Molecules with Transformer Models? 文章使用 MMP 数据训练 Transformer，使其生成具有活性的分子，文章结果表明 Transformer 对于未知靶点也能生成活性分子。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章所使用的分子数据来源于 ChEMBL 29，包含有 950640 个分子。数据集中的分子都由 SMILES 表示，将其输入 MMP 软件匹配其中的相似分子。输出的数据中，每对数据都由两个 SMILES 构成，形成一个 MMP 对，用 SMIRK 表示两个 SMILES 间的化学转化，最后得到了约 5700 万条 MMP 对。&lt;/p&gt;
&lt;p&gt;接着文章对得到的 MMP 对数据进一步清洗，主要包括两个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;排除 SMIRK 出现次数少于 &lt;span class="math"&gt;\(N_1\)&lt;/span&gt; 的 MMP 对；&lt;/li&gt;
&lt;li&gt;在剩余的 MMP 对中，随机保留 &lt;span class="math"&gt;\(N_2\)&lt;/span&gt; 条数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第 1 步的清洗是为了除去数据中出现频率过少的化学转化，因为它们过于特殊，并不能普适地用于所有分子；第 2 步是为了避免数据中的极端偏向影响模型，因为在数据集中，简单的转化（如 -H &amp;rarr; -CH&lt;sub&gt;3&lt;/sub&gt;）出现频率极高，这会导致模型无法学习到那些复杂的转化。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8914?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8914?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;文章使用 OpenNMT 构建 Transformer 模型，在 SMILES 数据输入模型前，都将其转化为 SELFIES 形式。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8917?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8917?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在训练过程中，使用 OpenNMT 中默认的损失函数，使用困惑度（perplexity）评估模型训练效果。困惑度是自然语言处理中所使用的评估指标，它定义为 &lt;span class="math"&gt;\(ppl=\exp(L/N)\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(L\)&lt;/span&gt; 为损失函数，&lt;span class="math"&gt;\(N\)&lt;/span&gt; 为全部备选的 token 数量。在自然语言处理中，模型根据前一个 token 预测下一个 token，并将其连成句子，困惑度的意义就是模型在获取前一个 token 的情况下，概率较高的下一个 token 的数量，所以困惑度越小时，表明模型能在目前分布下生成更合理的句子。同样，将其应用于分子生成模型，困惑度就可以表示分枝原子上可以备选的连接原子。&lt;/p&gt;
&lt;p&gt;此外，文章测试了模型对未知靶点生成分子的效果。在上一步得到的数据中，分别除去对 COX2、DRD2 或 HERG 有活性的分子，分别用三种数据训练 Transformer，最后得到的各模型对指定靶点「不可知」。文章又将去除掉的活性分子根据活性大小分为前 5% 与后 95% 的子数据集，用后 95% 作为模型输入，测试模型是否能输出前 5% 的分子。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;h4&gt;Transformer 可以为未知靶点生成活性分子&lt;/h4&gt;
&lt;p&gt;分别从训练数据中除去 COX2、DRD2 或 HERG 的活性分子，训练了 3 个 Transformer 模型，模型无法得到关于特点靶点的信息。将对相应靶点具有活性的后 95% 分子作为输入，模型生成的分子不仅满足相应的化学规则，而且相当数量的活性分子，说明模型具有相当好的泛化能力。生成分子的结果如下图所示，横轴表示分子与前 5% 高活性分子的相似性，竖轴表示分子活性，其中蓝色圆点表示生成的分子，红色菱形表示输入模型的分子。图中只展示了一部分输入-输出的变换，用箭头表示，可以看出许多生成的分子都是往右上方向移动，也就是生成活性更高、与高活性分子更相似的结构。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8916?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8916?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h4&gt;Transformer 可以为苗头化合物发现生成新颖分子&lt;/h4&gt;
&lt;p&gt;文章统计了训练集与输入-输出分子中的 SMIRKS 化学转化，在模型生成的结果中有 1086 种化转化并未出现在训练集中，所以文章认为模型可以生成新颖的化学结构。一部分化学转化如下图所示，训练集中的化学转化都是 MMP 转化，而图中的化学换化显然并不符合规则，而是模型根据训练集中的信息所新造的化学转化。&lt;/p&gt;
&lt;p&gt;文章认为使用深度学习实现分子生成并不是将所有可能的 SMIRKS 规则放到输入分子上，这种排列组合的模式势必会大大增加生成的数据量，将有价值的信息淹没。Transformer 能够上下文相关地获取分子信息，并根据信息通过合适的 SMIRKS 规则构建新分子，这种分子生成的手段更有帮助，同时文章发现 Transformer 所使用的 SMIRKS 并不是完全照搬训练集中的数据，而是根据已有信息新构造出的转化规则，这一点可能也是提升 Transformer 分子生成效果的方向。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8915?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8915?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章还认为，相比于更常用的 SMILES，SELFIES 更有利于 Transformer 学习其中的化学信息，因为 SMILES 无法保证分子合法，必须先训练模型使其学习生成合乎规则的分子。相反，使用 SELFIES 的分子生成模型不需要先让模型学会表示分子的语法，极少出现分子不合法的情况，可能使用 SELFIES 表示分子也是能实现文章实验中效果的重要原因。&lt;/p&gt;
&lt;p&gt;文章也指出这只是一个尝试性的工作，还有很多的问题没有解决。首要的一点就是文章使用与活性分子的相似性来评价生成分子的效果，尽管两个分子十分相似，它们也可能具有十分悬殊的活性，这是使用深度学习手段进行药物发现尤需解决的问题。所以文章中的分子生成也只能起到「启发」的作用，并不能真接指导药物化学家找到活性更高的分子。另一点就是文章中并没有对模型做全面的评估与超参数的选择，只是验证的方法的可行性，并没有对比与其他模型的优势。文章中还推测 SELFIES 相比 SMILES 更具有优势，但也未对比两种模型的效果。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章将 Transformer 用于苗头化合物的发现，并且发现 Transformer 对于训练集中不存在的未知靶点也能生成相当数量的活性分子，其中一部分分子与高活性的配体具有很高的相似性。文章还发现，Transformer 生成的结果中，其化学变化并不局限于用于训练的 MMP 数据，这表明 Transformer 具有上下文相关的信息识别能力，利用好这一特性有利于实现活性分子的生成。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>津城海棠</title><link href="https://leonis.cc/zai-lu-shang/2023-04-27-crabapple-of-tianjin.html" rel="alternate"></link><published>2023-04-27T00:00:00+08:00</published><updated>2023-04-27T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-27:/zai-lu-shang/2023-04-27-crabapple-of-tianjin.html</id><summary type="html">&lt;p&gt;五大道的海棠花开了，天津又到了最美的时候。&lt;/p&gt;
&lt;p&gt;几年前初到天津时，给我印象最深刻的就是城中随处可见的海棠花。天津遍植海棠，却不是密密地栽种为若干排。而是漫步在街头时，走过几个街道，转过几个巷口，蓦地发现几枝洁白而间杂洋红的海 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;五大道的海棠花开了，天津又到了最美的时候。&lt;/p&gt;
&lt;p&gt;几年前初到天津时，给我印象最深刻的就是城中随处可见的海棠花。天津遍植海棠，却不是密密地栽种为若干排。而是漫步在街头时，走过几个街道，转过几个巷口，蓦地发现几枝洁白而间杂洋红的海棠花在那静静地开放，而无意间的拜访者也成了唯一的赏花者，独享这春光。至于如庆王府的几处，大概是因为人们喜爱海棠，几代经营之下竟植海棠如林。阳春三月，津城就被海棠花环抱，绝不是一句虚言。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8898?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8898?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;我在故乡不曾见过这样的景象，倒不是没有海棠，却没有天津的海棠这般而引人重视。我一度误把海棠当作天津的市花，我愿意把这一切都推作是天津与海棠有缘。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8906?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8906?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 向着窗口盛开的海棠&lt;/p&gt;
&lt;p&gt;我与朋友晃悠悠地骑着单车往五大道拜访海棠，过桂林路至大理道&amp;mdash;&amp;mdash;好一趟西南之行。天津与上海相似，在收回各国租界后便以国内诸大小城市为道路名，因而也成了一大特色。想必不少来天津的游人都玩过寻找家乡路牌的游戏，冥冥之中建立起了一种远跨大半个中国的联系。看着大理道旁盛开的海棠，万里之遥的大理也为这春景增色不少吧。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8900?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8900?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 大理道上的海棠花&lt;/p&gt;
&lt;p&gt;不及步入五大道，已经能望见一批批游人往前走去。天津市也乐于营造海棠的氛围，工作人员用围挡临时封闭了路段，仅允游人入内。但这并不意味着五大道内有多少宽敞余裕的空间，也休想从容地漫步海棠花下。汹涌的人潮占满了可及之处，我甚至难以看清路面，却能依人群铺开的形状分辨出街道巷衢来。盛放的海棠也不愿与赏花者示弱，两列海棠一字排开，树冠张得极大，好似擎不起满树的海棠花，一树压着一树；树上的海棠也开得极密，一簇压着一簇，全开的压着半开的，半开的压着未开的，组成了繁盛的花群。人群中的人伸着脖子望花群，花群中的花也伸着叶梗看人群。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8899?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8899?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 游人熙熙攘攘地往五大道走去&lt;/p&gt;
&lt;p&gt;可花枝柔弱，阵风吹过，花瓣就零落而被阵风裹至空中，引起人群中阵阵讶叹声&amp;mdash;&amp;mdash;看来这场对决还是人群占了上风。摇落的花瓣混入风中，风顿时有了形状，它们翻腾、盘旋，又扶摇而上，在高处击散而化作花雨，洒向一个个抬头瞪大着眼、惊异得还不及合拢嘴的游人身边。风顿时也有了色彩，大片的雪白与点点嫣红在空中交织，它不像颜料盘中两种颜色相混时那样柔和得出现游丝，而是随着花瓣的翻飞，白红两种颜色相互变换，像极了文人所用的花草笺纸。在阳光的照耀下，空中的花瓣闪出熠熠的光芒，看来这笺纸还多了一道洒金的工艺。风顿时还有了气味，风摘落的花瓣被送至每个人的鼻前，不似桃花那样甜腻，芬芳中更多了几分清爽，虽说海棠将所有芳华都留给了春季，但这种气味总能让我想起初夏的时节。据佛经上说，维摩诘讲经讲到妙处时，有天女散花，赞叹其智慧。这花决计不是海棠，这样色、香、味齐全的海棠岂不正犯佛家所说的「五贼」。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8901?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8901?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 来不及拍下风吹下的海棠花雨，捕捉到几点如霰般翻飞的花瓣&lt;/p&gt;
&lt;p&gt;花雨过后，地上自然也铺满了海棠。明人张岱曾描述他书屋中的海棠「花时积三尺香雪」，诚不诬也，果然处处都积满了如雪般的海棠花瓣。有些店家自然不愿意放过这样的景色，径在店门外摆出桌子招徕顾客。更有好事者撤去桌上的阳伞，恣意在花雨之中，任那些海棠落在盘中的糕点之上，落入茶水之中。虽说道上行人熙熙攘攘，但啜茶人亦可闹中取静，其风雅也如此，丝毫不下于古人。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8909?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8909?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 道上也有卖各种物件的小摊，也落满了海棠花&lt;/p&gt;
&lt;p&gt;我被裹挟在人群中，亦步亦趋地穿过海棠花海，流连的景色如万花筒般绚丽，而正所谓兴尽悲来，又深令人感慨春光易逝。天津人自小便生活在海棠之中，求学于海棠花下，他们是惯看了秋月春风而变得更为冷峻，还是年年见证着花朝花暮，相比他人更多了几分细心肠呢？传闻天津人大多缱绻于故乡，而他们的确是如浮萍一般的移民，终于定居在此，与繁茂的海棠杂处而居，其念兹在兹的心情亦可想而知。我不禁想象，奔走于异乡的游子是否会种下一株海棠，欣赏着它与津城的海棠同样开谢，正与记忆中同。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8908?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8908?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 俨然是粉红色的世界&lt;/p&gt;
&lt;p&gt;归来搦管撰写此文时，已经将近立夏，海棠或许早已落尽。清少纳言曾盛赞落花之后赏花为风雅之事，我虽未有此意，奈何已蹈古人之迹。寓居天津多年，我同样遥望着津城海棠的开谢，正与我的记忆同！&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8907?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8907?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="摄影"></category></entry><entry><title>文献总结｜使用上下文增强的分子表示提升少样本药物发现的效果</title><link href="https://leonis.cc/sui-sui-nian/2023-04-22-summary-openreview.html" rel="alternate"></link><published>2023-04-22T00:00:00+08:00</published><updated>2023-04-22T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-22:/sui-sui-nian/2023-04-22-summary-openreview.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年发表在 ICLR 2023 上的一篇文章，文章原标题为 Context-enriched molecule representations improve few-shot drug discovery，文章介绍了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 通过 Hopfield 网络用上下文数据集少样本的强化分子表示，提升了分子性质预测的准确度。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://openreview.net/forum?id=XrMWUuEevr" rel="noopener" target="_blank"&gt;OpenReview&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年发表在 ICLR 2023 上的一篇文章，文章原标题为 Context-enriched molecule representations improve few-shot drug discovery，文章介绍了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 通过 Hopfield 网络用上下文数据集少样本的强化分子表示，提升了分子性质预测的准确度。&lt;/p&gt;
&lt;p&gt;深度学习已经成为了药物发现中的重要工具，但目前大部分深度学习方法都是通过大训练集获得分子信息。药物发现中的深度学习方法通常需要大量的生物试验数据，这在实际的药物研发过程中很难获取。少样本学习解决了药物发现中有效数据较少的问题，少样本学习主要有 3 种方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于数据增强的方法（Data-augmentation based approaches）：变换已有数据达到增加数据量的目的。&lt;/li&gt;
&lt;li&gt;基于词嵌入与最近邻的方法（Embedding-based and nearest neighbour approaches learn approaches）：学习词嵌入的空间，从已有数据邻近位置取得新数据（相似分子）。&lt;/li&gt;
&lt;li&gt;基于优化和微调的方法（Optimization-based or fine-tuning methods）：将大规模的预训练模型放在已有数据上微调，使其迁移到新的化学空间。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文章提出了一种新的 MHNfs 模型用于少样本的药物发现，模型使用联想记忆来提取原始数据中的共现和协变结构从而强化其分子表示，在少样本数据集 FS-Mol 上达到了最佳效果。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="yuan-li"&gt;原理&lt;/h3&gt;
&lt;p&gt;药物发现中所使用的模型 &lt;span class="math"&gt;\(g(\boldsymbol{m})\)&lt;/span&gt; 用于在给定分子表示 &lt;span class="math"&gt;\(\boldsymbol{m}\in\mathcal{M}\)&lt;/span&gt; 的情况下预测分子性质或活性 &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;。深度学习模型中的分子编码器将分子的一些低级表示（如 SMILES、分子图等）映射为模型空间的表示 &lt;span class="math"&gt;\(f^\mathrm{ME}:\mathcal{M}\rightarrow\mathbb{R}^d\)&lt;/span&gt;，再通过后续计算给出分子性质。&lt;/p&gt;
&lt;p&gt;在少样本的情况下，只有分子的小数据集 &lt;span class="math"&gt;\(\{\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N\}\)&lt;/span&gt; 与对应分子是否具有活性的数据 &lt;span class="math"&gt;\(\boldsymbol{y}=\{y_1,\cdots,y_N\}\)&lt;/span&gt;。这里将数据集 &lt;span class="math"&gt;\(\{(\boldsymbol{x}_n,y_n)\}_{n=1}^N\)&lt;/span&gt; 称为 support set，少样本学习就是要正确预测不在 support set 中 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 所对应的 &lt;span class="math"&gt;\(y\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;文章中的模型分为 3 个模块：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    \text{context module: }&amp;amp;\quad&amp;amp;\boldsymbol{m}'&amp;amp;=f^\mathrm{CM}(\boldsymbol{m},\boldsymbol{C})\\
    &amp;amp;\quad&amp;amp;\boldsymbol{X}'&amp;amp;=f^\mathrm{CM}(\boldsymbol{X},\boldsymbol{C})\\
    \text{cross-attention module: }&amp;amp;\quad&amp;amp;[\boldsymbol{m}'',\boldsymbol{X}'']&amp;amp;=f^\mathrm{CAM}([\boldsymbol{m}',\boldsymbol{X}'])\\
    \text{similarity module: }&amp;amp;\quad&amp;amp;\hat{y}&amp;amp;=f^\mathrm{SM}(\boldsymbol{m}'',\boldsymbol{X}'',\boldsymbol{y})
\end{align}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{m}\in\mathbb{R}^d\)&lt;/span&gt; 是分子的词嵌入表示，&lt;span class="math"&gt;\(\boldsymbol{X}\in\mathbb{R}^{d\times N}\)&lt;/span&gt; 是 support set 中分子的词嵌入表示，&lt;span class="math"&gt;\(\boldsymbol{C}\in\mathbb{R}^{d\times M}\)&lt;/span&gt; 是另一个更大的分子数据集（context set）中分子的词嵌入表示。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f^\mathrm{CM}\)&lt;/span&gt; 交换 &lt;span class="math"&gt;\((\boldsymbol{m},\boldsymbol{C})\)&lt;/span&gt; 间与 &lt;span class="math"&gt;\((\boldsymbol{X},\boldsymbol{C})\)&lt;/span&gt; 间的上下文信息，得到强化的表示 &lt;span class="math"&gt;\(\boldsymbol{m}'\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}'\)&lt;/span&gt;。拼合两个增强的表示，&lt;span class="math"&gt;\(f^\mathrm{CAM}\)&lt;/span&gt; 计算两者间注意力，得到进一步增强的 &lt;span class="math"&gt;\(\boldsymbol{m}''\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}''\)&lt;/span&gt;，最后结合二者的信息进行预测。上面的过程可以描述成&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    &amp;amp;\underset{\textsf{symbolic or}\atop\textsf{low-level repr.}}{m}\overset{f^\mathrm{ME}}{\longrightarrow}\underset{\textsf{molecule}\atop\textsf{embedding}}{\boldsymbol{m}}\overset{f^\mathrm{CM}}{\longrightarrow}\underset{\textsf{context}\atop\textsf{repr.}}{\boldsymbol{m}'}\overset{f^\mathrm{CAM}}{\longrightarrow}\underset{\textsf{similarity}\atop\textsf{repr.}}{\boldsymbol{m}''}\\
    &amp;amp;\underset{\textsf{symbolic or}\atop\textsf{low-level repr.}}{x_n}\overset{f^\mathrm{ME}}{\longrightarrow}\underset{\textsf{molecule}\atop\textsf{embedding}}{\boldsymbol{x}_n}\overset{f^\mathrm{CM}}{\longrightarrow}\underset{\textsf{context}\atop\textsf{repr.}}{\boldsymbol{x}_n'}\overset{f^\mathrm{CAM}}{\longrightarrow}\underset{\textsf{similarity}\atop\textsf{repr.}}{\boldsymbol{x}_n''}
\end{align}
$$&lt;/div&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8889?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8889?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MHNfs 由 Transformer 中的 encoder 部分构建，具有与 Transformer 类似的结构与工作方式。&lt;/p&gt;
&lt;p&gt;模型中的上下文模块由现代 Hopfield 网络（Modern Hopfield Network, MHN）实现：&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{Hopfield}(\boldsymbol{\Xi},\boldsymbol{C}):=(\boldsymbol{W}_E\boldsymbol{C})\mathrm{Softmax}\left(\beta(\boldsymbol{W}_C\boldsymbol{C})^\top(\boldsymbol{W}_\Xi\boldsymbol{\Xi})\right)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{m}'=\mathrm{Hopfield(\boldsymbol{m},\boldsymbol{C})},\quad\boldsymbol{X}'=\mathrm{Hopfield}(\boldsymbol{X},\boldsymbol{C})
$$&lt;/div&gt;
&lt;p&gt;MHN 能够计算两个输入间的注意力，最后更新得到的分子表示就具有参考分子集 &lt;span class="math"&gt;\(\boldsymbol{C}\)&lt;/span&gt; 中的联想记忆。&lt;/p&gt;
&lt;p&gt;交叉注意力模块替换了原来 Transformer 中的多头注意力机制，但功能仍然类似，用于记算输入分子 &lt;span class="math"&gt;\(\boldsymbol{m}'\)&lt;/span&gt; 与 support set &lt;span class="math"&gt;\(\boldsymbol{X}'\)&lt;/span&gt; 之间的注意力，再次更新分子表示：&lt;/p&gt;
&lt;div class="math"&gt;$$[\boldsymbol{m}'',\boldsymbol{X}'']=\mathrm{Hopfield}([\boldsymbol{m}',\boldsymbol{X}'],[\boldsymbol{m}',\boldsymbol{X}'])$$&lt;/div&gt;
&lt;p&gt;在最后的相似性模块中，模型计算输入分子 &lt;span class="math"&gt;\(\boldsymbol{m}''\)&lt;/span&gt; 与 support set &lt;span class="math"&gt;\(\boldsymbol{X}''\)&lt;/span&gt; 中每个分子 &lt;span class="math"&gt;\(\boldsymbol{x}_n''\)&lt;/span&gt; 之间的相似性 &lt;span class="math"&gt;\(k(\boldsymbol{m}'',\boldsymbol{x}_n'')\)&lt;/span&gt;，并使用所有相似性的加权平均表示输入分子，用该表示计算输入分子的性质：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}=\mathrm{Sigmoid}\left(\tau^{-1}\frac 1N\sum_{n=1}^Ny_n'k(\boldsymbol{m}'',\boldsymbol{x}_n'')\right)$$&lt;/div&gt;
&lt;p&gt;文章这么做的理由是，考虑现实中的情况，当药物化学家对某系列化合物只有有限的活性数据（support set）而要预测（同一靶点或类似结构的）一化合物（query molecule）的活性时，化学家会将该化合物与手头已有数据的化合物对比，再在化合物库（context set）中对比，综合考虑各项因素后得出判断。模型所做的 MHN 计算以及平均相似性，就是简化了的上述过程，文章认为这样的设计有助于模型模仿化学家的思考方式。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用用于少样本药物发现的标准数据集 FS-Mol 作为模型的数据集。该数据集中的分子来自于 ChEMBL 27，其中定义了 4938 个训练任务，40 个验证任务与 157 个测试任务，平均每个任务下只有 94 个数据点。&lt;/p&gt;
&lt;p&gt;文章使用 ECFPs 分子指纹与 RDKit 描述符来作为初始的分子表示。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8890?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8890?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;实验结果如上表所示，文章对比了各个模型在 FS-Mol 测试集上的 &amp;Delta;AUC-PR，除了 ADKF-IFT 在 Hydol. 与 Oxid. 小部分任务上优于 MHNfs，其他模型的结果都不如 MHNfs，而且 MHNfs 在全部任务的整体结果上优于其他全部模型，所以文章认为 MHNfs 在 FS-Mol 测试集实现了目前药物发现少样本学习的最优性能。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 参考了现实中化学家面对少样本数据时的策略，通过设想的一种上下文增强的方式更新了输入模型的分子表示，使其具有更多大数据集中的背景信息。在实验中，测试结果表示这种增强的分子表示确实提高了模型预测的准确率，MHNfs 也在该任务上达到了最优的性能。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>从零起步的 Transformer 与代码拆解</title><link href="https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html" rel="alternate"></link><published>2023-04-21T00:00:00+08:00</published><updated>2023-04-21T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-21:/sui-sui-nian/2023-04-21-transformer-from-scratch.html</id><summary type="html">&lt;p&gt;自 Google 的论文 &lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Attention Is All You Need&lt;/a&gt; 发布后，几年内涌现了大量基于 Transformer 的模型，俨然形成了 Transformer 横扫人工智能领域的态势。&lt;/p&gt;
&lt;p&gt;网络上也出现了大量解读论文或是讲解 Transformer 的文章，其中也不乏许多高水平人工智能从业者的解读。虽然有些可以称得上是高屋建瓴，但 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;自 Google 的论文 &lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Attention Is All You Need&lt;/a&gt; 发布后，几年内涌现了大量基于 Transformer 的模型，俨然形成了 Transformer 横扫人工智能领域的态势。&lt;/p&gt;
&lt;p&gt;网络上也出现了大量解读论文或是讲解 Transformer 的文章，其中也不乏许多高水平人工智能从业者的解读。虽然有些可以称得上是高屋建瓴，但相当大部分难以避免地落入了知识的诅咒（curse of knowledge），起码在我初开始了解 Transformer 时难以读懂这些文章。&lt;/p&gt;
&lt;p&gt;随着 Transformer 广泛应用到各领域，学习 Transformer 也成了一门「显学」。尽管我已经能读懂一些更深层次的 Transformer 剖析，但我还是未找见一篇合我心意的入门文章，所以我希望能撰写一篇小文章，以初学者的角度来讲解 Transformer，是为序。&lt;/p&gt;
&lt;h2 id="xie-zi"&gt;楔子&lt;/h2&gt;
&lt;p&gt;Transformer 是设计用于 NLP 的一种模型，尽管目前 Transformer 所能完成的任务已经大大扩展，但这里还是以最原始的翻译任务为例。&lt;/p&gt;
&lt;p&gt;在翻译任务中，所需要的数据包括原始语句与目标语句，也就是 Transformer 原论文中所指的「input」和「output」，因为名字太容易混淆，还是将其原始语句与目标语句或是「source」与「target」。&lt;/p&gt;
&lt;p&gt;假设 source 为 &lt;code&gt;你好，世界！&lt;/code&gt;，target 为 &lt;code&gt;Hello, world!&lt;/code&gt;，完成这个中译英任务首先要将文本转化为利于模型处理的数值，这一步称为词嵌入（embedding）。&lt;/p&gt;
&lt;p&gt;常见的词嵌入方法有 word2vec 等等，在这里不做介绍。词嵌入步骤大致的流程是先将 &lt;code&gt;你好，世界！&lt;/code&gt; 转化为 &lt;code&gt;&amp;lt;start&amp;gt; 你好 ， 世界 ！ &amp;lt;end&amp;gt;&lt;/code&gt;，每个「词」都用空格划分开，其中 &lt;code&gt;&amp;lt;start&amp;gt;&lt;/code&gt; 与 &lt;code&gt;&amp;lt;end&amp;gt;&lt;/code&gt; 分别表示文本的起讫，这些「词」在 NLP 通常称为「token」。接着再为每个 token 分配索引，例如 &lt;code&gt;&amp;lt;start&amp;gt;&lt;/code&gt; 为 &lt;code&gt;1&lt;/code&gt;，&lt;code&gt;&amp;lt;end&amp;gt;&lt;/code&gt;为 &lt;code&gt;0&lt;/code&gt;，照这个思路，文本就可以转换为 &lt;code&gt;[1 2 3 4 5 0]&lt;/code&gt; 的表示。当然这是很简单的做法，实际上，每个 token 都会被转化为指定维度的向量，用这一连串向量就可以表示文本。&lt;/p&gt;
&lt;p&gt;将上述过程抽象出来，在词嵌入后，可以得到 source 的表示 &lt;span class="math"&gt;\(\boldsymbol{X}=(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_t)\)&lt;/span&gt; 与 target 的表示 &lt;span class="math"&gt;\(\boldsymbol{Y}=(\boldsymbol{y}_1,\boldsymbol{y}_2,\cdots,\boldsymbol{y}_t)\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\boldsymbol{x}_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{y}_i\)&lt;/span&gt; 都是指定维度 &lt;span class="math"&gt;\(d\)&lt;/span&gt; 的向量。&lt;/p&gt;
&lt;p&gt;那么如何使用 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 完成翻译任务呢？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一种&lt;/strong&gt;是使用 RNN 方法，使用当前的 source token &lt;span class="math"&gt;\(\boldsymbol{x}_t\)&lt;/span&gt; 与前一步中生成的 token &lt;span class="math"&gt;\(\hat{\boldsymbol{y}}_{t-1}\)&lt;/span&gt; 生成下一个 token，逐个生成直至句子末尾：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\hat{\boldsymbol{y}}_{t-1},\boldsymbol{x}_t)$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;第二种&lt;/strong&gt;是使用卷积的方法，定义一个窗口长度再通过小范围中的几个 &lt;span class="math"&gt;\(\boldsymbol{x}_i\)&lt;/span&gt; 计算输出：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_{t-1},\boldsymbol{x}_t,\boldsymbol{x}_{t+1})$$&lt;/div&gt;
&lt;p&gt;可以看出，&lt;dot&gt;RNN 很难学习到全局的信息&lt;/dot&gt;，而&lt;dot&gt;卷积方法只能学习到小范围的局部信息&lt;/dot&gt;。&lt;/p&gt;
&lt;p&gt;所以 Transformer 给出了&lt;strong&gt;第三种&lt;/strong&gt;方法，也就是自注意力方法。自注意力机制让模型就当前的 source token &lt;span class="math"&gt;\(\boldsymbol{x}_t\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 中其他 token 的关系给出输出 &lt;span class="math"&gt;\(\hat{\boldsymbol{y}}_t\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_t, \boldsymbol{X})$$&lt;/div&gt;
&lt;h2 id="transformer-jie-gou"&gt;Transformer 结构&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;标准 Transformer 的结构如上图所示，大致分为左侧的 Encoder 与右侧的 Decoder 两个部分。Inputs 与 Outputs 分别是上文所说的 source 与 target，Output Probabilities 是模型输出的各 token 概率，取其中最大概率的 token 就能组织成模型输出结果。&lt;/p&gt;
&lt;h3 id="wei-zhi-bian-ma"&gt;位置编码&lt;/h3&gt;
&lt;p&gt;Transformer 并没有采用 RNN 与卷积方法所使用的序列处理 token 的方法，因而能够实现并行计算并且很大程度上缓解了长期依赖问题（顺序处理长序列容易丢失多个步骤前的信息）。文本中多个 token 间显然有前后的顺序关系，Transformer 使用位置编码的方式来处理顺序信息。&lt;/p&gt;
&lt;p&gt;source 与 target 送入模型，经过常规的词嵌入过程后，还需要在得到的矩阵上加上位置编码，论文将位置编码定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{PE}_{(\mathrm{pos},2i)}=\sin(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathrm{PE}_{(\mathrm{pos},2i+1)}=\cos(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$&lt;/div&gt;
&lt;p&gt;Transformer 将 &lt;span class="math"&gt;\(\mathrm{pos}\)&lt;/span&gt; 位置映射为 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 维的向量，向量中的第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个元素即按上式计算。位置编码的计算公式是构造出的经验公式，不必深究，当然也有许多文章分析了如此构造的原因，这里从略。&lt;/p&gt;
&lt;h3 id="encoder-yu-decoder"&gt;Encoder 与 Decoder&lt;/h3&gt;
&lt;p&gt;许多完成 seq2seq 任务的模型都采用了 encoder-decoder 模式，Transformer 也不例外。简单来说，encoder 将输入编码得到一个中间变量，decoder 解码该中间变量得到输出。&lt;/p&gt;
&lt;p&gt;在 Transformer 中，source 与 target 分别送入 encoder 与 decoder，encoder 计算得到的中间结果再送入 decoder 中与 target 输入进行计算，得到最后的结果，这就是所谓「编码-解码」的工作方式。&lt;/p&gt;
&lt;p&gt;从 Transformer 的结构图中可以看出，模型具有 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 层 encoder 与 decoder 层。其中，encoder 与 decoder 都具有相同的多头注意力层（Multi-Head Attention）、前馈层（Feed Forward）。encoder 与 decoder 的不同在于 decoder 多了一个多头注意力层，在这一层中，encoder 的输出与 decoder 的输入计算注意力。&lt;/p&gt;
&lt;p&gt;还可以注意到，在 encoder 与 decoder 中，每一层后都有一个 Add &amp;amp; Norm 层，用于归一化计算结果。Add &amp;amp; Norm 层的计算方式是将前一层的输入与前一层的输出相加，然后归一化，可以表示为 &lt;span class="math"&gt;\(\mathrm{LayerNorm}(\boldsymbol{x}+\mathrm{Sublayer}(\boldsymbol{x}))\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4&gt;Attention 机制&lt;/h4&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;数据进入 encoder 与 decoder 的内部，首先要通过注意力机制进行计算，这也是 Transformer 的核心。&lt;/p&gt;
&lt;p&gt;文章中将所使用的注意力称为缩放点积注意力（scaled dot-product attention），定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{Q}_{n\times d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{K}_{m\times d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{V}_{m\times d_v}\)&lt;/span&gt; 分别是若干向量 &lt;span class="math"&gt;\(\boldsymbol{q}\in\mathbb{R}^{d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{k}\in\mathbb{R}^{d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{v}\in\mathbb{R}^{d_v}\)&lt;/span&gt; 组成的矩阵。&lt;/p&gt;
&lt;p&gt;单看矩阵的乘法稍显复杂，不妨先用向量说明计算步骤。通过以下方式可以从输入 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 得到向量 &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{q}=\boldsymbol{x}\boldsymbol{W}^Q,\,\boldsymbol{k}=\boldsymbol{x}\boldsymbol{W}^K,\,\boldsymbol{v}=\boldsymbol{x}\boldsymbol{W}^V$$&lt;/div&gt;
&lt;p&gt;其中，&lt;span class="math"&gt;\(\boldsymbol{W}^Q\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{W}^K\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{W}^V\)&lt;/span&gt; 分别表示相应的权重矩阵。&lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; 代表 query，&lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt; 代表 key，&lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt; 代表 value，目的是&lt;dot&gt;用 query 去寻找更匹配的 key-value 对&lt;/dot&gt;。&lt;/p&gt;
&lt;p&gt;因为数量积可以表示两向量的相似程度，一种简单的做法是使用 &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; 与若干个 &lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt; 计算数量积，将其作为匹配分数：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{score}=\boldsymbol{q}\cdot \boldsymbol{k}_i=\boldsymbol{q}\boldsymbol{k}^\top_i$$&lt;/div&gt;
&lt;p&gt;但这样的「注意力」太过于简单，Google 从上述的数量积出发，设计了更为可靠的注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{q},\boldsymbol{k}_i,\boldsymbol{v}_i)=\frac 1 Z\sum_i\exp\left(\frac{\boldsymbol{q}\boldsymbol{k}^\top_i}{\sqrt{d_k}}\right)\boldsymbol{v}_i$$&lt;/div&gt;
&lt;p&gt;首先，式中 &lt;span class="math"&gt;\(1/Z\sum_i x_i\)&lt;/span&gt; 形式的部分是 Softmax 函数的简写，Softmax 函数由下式定义：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Softmax}(x_i)=\frac{\exp(x_i)}{\sum_j\exp(x_j)}$$&lt;/div&gt;
&lt;p&gt;Softmax 函数的作用是将若干数值 &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 归一化，得到的 &lt;span class="math"&gt;\(\mathrm{Softmax}(x_i)\)&lt;/span&gt; 具有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\sum_i\mathrm{Softmax}(x_i)=1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{Softmax}(x_i)\in[0, 1]\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两点性质，所以与概率具有相似的特征，可以用作概率处理。&lt;/p&gt;
&lt;p&gt;其次，式中新增的 &lt;span class="math"&gt;\(\sqrt{d_k}\)&lt;/span&gt; 用于调节内积 &lt;span class="math"&gt;\(\boldsymbol{q}\boldsymbol{k}^\top_i\)&lt;/span&gt; 的大小。当若干内积的大小过于悬殊时，Softmax 函数很容易将其推向 &lt;span class="math"&gt;\(0\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(1\)&lt;/span&gt; 的边界值，这样的数值处理起来没什么意义。&lt;/p&gt;
&lt;p&gt;最后，再次回忆 Transformer 的注意力机制是用 query 去寻找更匹配的 key-value 对。那么上式的意义就很了然了，就是将 query 与各个 key 的匹配分数转化为各个概率，再按各个概率取各个 key 所对应的 value，组合各 value 分量即得到注意力。&lt;/p&gt;
&lt;p&gt;以具有两个 value 的情况为例，需要得到的中间量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;（理解为注意力亦可）可以通过下式计算：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \boldsymbol{z}_1=\theta_{11}\boldsymbol{v}_1+\theta_{12}\boldsymbol{v}_2\\
    \boldsymbol{z}_2=\theta_{21}\boldsymbol{v}_1+\theta_{22}\boldsymbol{v}_2
\end{align}$$&lt;/div&gt;
&lt;p&gt;权值 &lt;span class="math"&gt;\(\theta_{ij}\)&lt;/span&gt;（即上文所说概率）通过下式得到：&lt;/p&gt;
&lt;div class="math"&gt;$$\theta_{ij}=\mathrm{Softmax}\left(\frac{\boldsymbol{q}_i\boldsymbol{k}^\top_j}{\sqrt{d_k}}\right)$$&lt;/div&gt;
&lt;p&gt;将上述运算转为矩阵形式会简洁许多：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{pmatrix}
    \boldsymbol{z}_1 \\
    \boldsymbol{z}_2
\end{pmatrix}=
\begin{pmatrix}
    \theta_{11} &amp;amp; \theta_{12} \\
    \theta_{21} &amp;amp; \theta_{22}
\end{pmatrix}
\begin{pmatrix}
    \boldsymbol{v}_1 \\
    \boldsymbol{v}_2
\end{pmatrix}\\
$$&lt;/div&gt;
&lt;p&gt;可以记作 &lt;span class="math"&gt;\(\boldsymbol{Z}=\boldsymbol{\theta}\boldsymbol{V}\)&lt;/span&gt;，也就是&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$&lt;/div&gt;
&lt;h4&gt;Multi-Head Attention&lt;/h4&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;前一节中解释了 Transformer 中的缩放点积注意力，但在模型中实际并非通过上述方式直接计算，而是通过多头注意力的方式计算注意力。&lt;/p&gt;
&lt;p&gt;如上图所示，多头注意力同样是在计算缩放点积注意力，但与纯粹缩放点积注意力的不同之处在于多头注意力将多个注意力计算步骤叠加了起来。&lt;/p&gt;
&lt;p&gt;叠加的次数为 &lt;span class="math"&gt;\(h\)&lt;/span&gt;，即代表 head，多少个 head 表示需要进行多少次叠加计算。矩阵 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{K}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{V}\)&lt;/span&gt; 进入多头注意力计算步骤后，首先要分别在第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个 head 中进行线性变换并计算注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{head}_i=\mathrm{Attention}(\boldsymbol{Q}\boldsymbol{W}^Q_i,\boldsymbol{K}\boldsymbol{W}^K_i,\boldsymbol{V}\boldsymbol{W}^V_i)$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}^Q_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^K_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^V_i\in\mathbb{R}^{d_\mathrm{model}\times d_v}\)&lt;/span&gt;，注意不同 head 中的线性变换并不同，输出也不同。然后将所有输出 &lt;span class="math"&gt;\(\mathrm{head}_i\)&lt;/span&gt; 拼合在一起，经线性变换后作为注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{MultiHead}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\mathrm{Concat}(\mathrm{head}_1,\mathrm{head}_2,\cdots,\mathrm{head}_h)\boldsymbol{W}^O$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}^O\in\mathbb{R}^{hd_v\times d_\mathrm{model}}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;注意这个过程中数据维数的变化 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 为单头注意力中模型所处理的维数，&lt;span class="math"&gt;\(\boldsymbol{W}^Q_i\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^K_i\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^V_i\)&lt;/span&gt; 的线性变换将 query、key 的维数从 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 提升到 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;，将 value 的维数从 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 提升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;。最后的 &lt;span class="math"&gt;\(\boldsymbol{W}^O\)&lt;/span&gt; 又将拼合起来维数为 &lt;span class="math"&gt;\(hd_v\)&lt;/span&gt; 的注意力转换为模型所处理的维数 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt;。这些线性变换矩阵 &lt;span class="math"&gt;\(\boldsymbol{W}_i\)&lt;/span&gt; 实际上就是模型训练过程中需要学习的一部分参数。&lt;/p&gt;
&lt;p&gt;至于为什么要用多头的方式计算注意力，这就是个很复杂的问题了。就我的理解而言，由于每个 head 中的线性变换矩阵 &lt;span class="math"&gt;\(\boldsymbol{W}_i\)&lt;/span&gt;，多头注意力实际上是将 query、key、value 映射到不同的子空间中，在多个不同的子空间中寻找与 query 最匹配的 key-value。由于不同子空间中具有不同方面的信息，最后将其拼接起来作为结果，这样可以更多地从多个方面捕获数据中的信息。&lt;/p&gt;
&lt;h4&gt;Feed-Forward 层&lt;/h4&gt;
&lt;p&gt;在多头注意力层之后，就是前馈层，前馈层只在位置方向上计算，所以原文描述其为 position-wise。进入前馈层的数据在该层中先做 1 次线性变换，维度升高，再经过 RELU 激活函数，最后再做 1 次线性变换，维度降低，输入与输出前馈层的维度相同。上述过程可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{FFN}(\boldsymbol{x})=\max(0,\boldsymbol{x}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2$$&lt;/div&gt;
&lt;p&gt;RELU 激活函数定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{ReLU}(x)=x^+=\max(0,x)$$&lt;/div&gt;
&lt;p&gt;即式中的 &lt;span class="math"&gt;\(\max\)&lt;/span&gt;，按原文中的例子，&lt;span class="math"&gt;\(\boldsymbol{W}_1\)&lt;/span&gt; 使 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 由 512 维升高到 2048 维，&lt;span class="math"&gt;\(\boldsymbol{W}_2\)&lt;/span&gt; 使 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 计算由 2048 维再降至 512 维，升维与降维的过程也是为了更好地获得数据中的信息。&lt;/p&gt;
&lt;h3 id="transformer-ji-suan-bu-zou"&gt;Transformer 计算步骤&lt;/h3&gt;
&lt;p&gt;Transformer 模型大致就由上述的几个层连接在一起构成，但或许还是觉得朦朦胧胧，比如究竟什么才是 query、key、value 等等。不妨再来看看 Transformer 的结构图，这一次已熟知大部分模块的工作原理了，所以只看数据流入与流出各模块的路线。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;作为 source 的 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与作为 target 的 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 分别从下方的左右两侧进入模型。&lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 都要经过词嵌入并加上位置编码，按以下方式更新：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    \boldsymbol{X}&amp;amp;\leftarrow\mathrm{Embedding}(\boldsymbol{X})+\mathrm{PE}(\boldsymbol{X})\\
    \boldsymbol{Y}&amp;amp;\leftarrow\mathrm{Embedding}(\boldsymbol{Y})+\mathrm{PE}(\boldsymbol{Y})
\end{align}
$$&lt;/div&gt;
&lt;p&gt;接着 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 分别进入 encoder 与 decoder，可以注意到数据分作 4 条路线，这意味着将数据复制 4 次。先看进入多头注意力层的 3 条数据，以 encoder 为例，在这一层中就是在计算&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$&lt;/div&gt;
&lt;p&gt;不言自明，在这里的 query、key、value 三者都是 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt;，是在 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 内部计算注意力，因此称其为&lt;strong&gt;自注意力&lt;/strong&gt;（self-attention）。&lt;/p&gt;
&lt;p&gt;在后续的 Add &amp;amp; Norm 层中，计算&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X}))$$&lt;/div&gt;
&lt;p&gt;在前馈层与后续的 Add &amp;amp; Norm 层输的输出结果也可想而知：&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\max(0,\boldsymbol{X}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2)$$&lt;/div&gt;
&lt;p&gt;这里的 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 分作两路进入到 decoder 中，在 decoder 的该多头注意力层中，query 与 key 为 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt;，而 value 为类似步骤得到的 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt;，该层的输出为&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{Z}=\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{Y})$$&lt;/div&gt;
&lt;p&gt;这也是 decoder 与 encoder 的关键不同。输出结果 &lt;span class="math"&gt;\(\boldsymbol{Z}\)&lt;/span&gt; 完成后续的计算过程后，就得到各 token 的概率，用各 token 替换即可得到模型输出的文本结果。&lt;/p&gt;
&lt;p&gt;{note begin}有兴趣的读者不妨根据各矩阵的形状尝试计算一下各个变量的维度在 Transformer 在各步骤中是如何变化的，一定会对 Transformer 的计算过程收获更深的了解。{note end}&lt;/p&gt;
&lt;h2 id="dai-ma-chai-jie_1"&gt;代码拆解&lt;/h2&gt;
&lt;p&gt;有了对 Transformer 原理的基本认识，就可以动手实现一个 Transformer 了，通过代码更深入了解 Transformer 的一些细节。这里使用 PyTorch 搭建一个标准的 Transformer，参考代码见 &lt;a href="https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; aladdinpersson / Machine-Learning-Collection &lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;代码中的各模块如下图所示，接下来对各模块逐个拆解。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="positionembedding"&gt;PositionEmbedding&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;import math
import torch
import torch.nn as nn


class PositionEmbedding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        # d_model 为模型处理数据的维数，即公式中 d_k
        # max_len 表示模型处理的最大 token 数量
        super(PositionEmbedding, self).__init__()

        # 生成大小为 max_len * d_model 的零矩阵
        pe = torch.zeros(max_len, d_model)
        # 生成大小为 max_len * 1 的位置矩阵
        position = torch.arange(max_len).unsqueeze(1)
        # 计算位置编码
        div_term = torch.exp(torch.arange(0, d_model, 2) * - (math.log(10000.0) / d_model))
        x = position * div_term
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = self.pe[:, :x.size(1)]
        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;首先实现位置编码模块。在 PyTorch 中，用于搭建神经网络的模块都要继承 &lt;code&gt;nn.Module&lt;/code&gt;，PyTorch 会通过 &lt;code&gt;__call__()&lt;/code&gt; 调用模块的 &lt;code&gt;forward()&lt;/code&gt; 的方法进行前向传播。简单来讲就是，&lt;code&gt;PositionEmbedding(x)&lt;/code&gt; 的功能等同于 &lt;code&gt;PositionEmbedding.forward(x)&lt;/code&gt;，但不能使用 &lt;code&gt;PositionEmbedding.forward(x)&lt;/code&gt;，因为 PyTorch 做了许多条件的判定和优化。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;torch.arange(num)&lt;/code&gt; 的功能类似于 Python 中的 &lt;code&gt;range(num)&lt;/code&gt;，用于生成文本各 token 的顺序位置索引。&lt;code&gt;unsqueeze(dim)&lt;/code&gt; 会令 Tensor 在指定的维度 &lt;code&gt;dim&lt;/code&gt; 上扩张 1 维，这里是为了使 &lt;code&gt;pe&lt;/code&gt; 与 &lt;code&gt;position&lt;/code&gt; 两个矩阵的维度对齐，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; torch.arange(5)
tensor([0, 1, 2, 3, 4])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(0)
tensor([[0, 1, 2, 3, 4]])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(1)
tensor([[0],
        [1],
        [2],
        [3],
        [4]])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).size()
torch.Size([5])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(0).size()
torch.Size([1, 5])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(1).size()
torch.Size([5, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;代码中的位置编码并不是直接按公式计算的，而是做了一些变换，先计算一个中间量 &lt;code&gt;div_term&lt;/code&gt;，其中 &lt;code&gt;torch.arange(0, d_model, 2)&lt;/code&gt; 即为 &lt;span class="math"&gt;\(2i\)&lt;/span&gt;，可以整理出&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \mathrm{div\_term}_i&amp;amp;=\exp\left[2i\times(-\frac{\ln10000}{d_k})\right]\\
    &amp;amp;=\left[\exp(-\frac{\ln10000}{d_k})\right]^{2i}\\
    &amp;amp;=\left[10000^{-\frac{1}{d_k}}\right]^{2i}\\
    &amp;amp;=10000^{-2i/d_k}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;所以 &lt;code&gt;position * div_term&lt;/code&gt; 就可以得到&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{position}\times \mathrm{div\_term}_i=\mathrm{pos}/10000^{2i/d_k}$$&lt;/div&gt;
&lt;p&gt;就是位置编码中的一项。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pe[:, 0::2]&lt;/code&gt; 与 &lt;code&gt;pe[:, 1::2]&lt;/code&gt; 是 Pytorch 中的高级索引操作。索引中用 &lt;code&gt;,&lt;/code&gt; 分隔不同维度，例中以 &lt;code&gt;,&lt;/code&gt; 为分界，前面是对第 1 维的索引，后面是对第 2 维的索引。索引操作也遵守 Python 的规则，即 &lt;code&gt;a:b:c&lt;/code&gt; 中 &lt;code&gt;a&lt;/code&gt; 为起始，&lt;code&gt;b&lt;/code&gt; 为末尾，&lt;code&gt;c&lt;/code&gt; 为步长。&lt;/p&gt;
&lt;p&gt;所以 &lt;code&gt;pe[:, 0::2]&lt;/code&gt; 与 &lt;code&gt;pe[:, 1::2]&lt;/code&gt; 取出全部第 1 维中的元素，即行方向上不操作，再在第 2 维中分别从 &lt;code&gt;0&lt;/code&gt; 或 &lt;code&gt;1&lt;/code&gt; 开始以步长 &lt;code&gt;2&lt;/code&gt; 取出元素，即取出第 &lt;span class="math"&gt;\(2i\)&lt;/span&gt; 或第 &lt;span class="math"&gt;\(2i+1\)&lt;/span&gt; 列。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;forward()&lt;/code&gt; 部分，输出的位置编码为 &lt;code&gt;pe[:, :x.size(1)]&lt;/code&gt;，这主要是为了确保矩阵形状在加法过程中不会因非法输入的广播而改变。其实在输入合法的情况下，&lt;code&gt;x.size(1)&lt;/code&gt; 就是 &lt;code&gt;d_model&lt;/code&gt;，等价于 &lt;code&gt;pe[:, :]&lt;/code&gt;，也等价于 &lt;code&gt;pe&lt;/code&gt;。&lt;/p&gt;
&lt;!-- 指定 `requires_grad_(False)` 是因为 PyTorch 会自动保存 Tensor 的来源，用于更快地计算梯度，而这里的加法计算并不是训练过程，取消保存能节省一部分资源。 --&gt;
&lt;h3 id="selfattention"&gt;SelfAttention&lt;/h3&gt;
&lt;p&gt;在进入 Transformer 核心部分之前，我们需要再次明确一下输入模型的数据格式。上文中仅以输入模型一条数据（由若干 token 组成的一条句子）为例，在实际操作中，为了提高训练效率，会同时输入若干条数据，在构建模型时也要考虑到这一点。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，一次输入模型的数据条数就称为 batch size，所以模型所处理的其实是一个 &lt;span class="math"&gt;\(\mathrm{batch\_size}\times\mathrm{max\_len}\times\mathrm{d\_model}\)&lt;/span&gt; 的高维矩阵。也就是说，&lt;code&gt;x.size()&lt;/code&gt; 的结果是 &lt;code&gt;[batch_size, max_len, d_model]&lt;/code&gt;，务必注意三者顺序。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        # 确保 embed_size 能被 heads 整除
        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(embed_size, embed_size)
        self.keys = nn.Linear(embed_size, embed_size)
        self.queries = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;先看 &lt;code&gt;SelfAttention&lt;/code&gt; 的初始化部分，明白了注意力机制的计算过程就不难理解上面的各个属性了。&lt;code&gt;head_dim&lt;/code&gt; 是每一个 head 中注意力的维度，&lt;code&gt;embeds_size&lt;/code&gt; 必须能被 &lt;code&gt;heads&lt;/code&gt; 整除，否则将多头注意力拼接在一起的维数不等于模型处理的维数就会出现问题。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;values&lt;/code&gt;、&lt;code&gt;keys&lt;/code&gt;、&lt;code&gt;queries&lt;/code&gt; 都是计算多头注意力前的线性变换，&lt;code&gt;fc_out&lt;/code&gt; 是拼接多头注意力后的线性变换。线性变换可以直接调用 &lt;code&gt;nn.Linear(in_dim, out_dim)&lt;/code&gt;，只需要指定线性变换前后的维数即可，这里线性变换前后维数没有变化。&lt;/p&gt;
&lt;p&gt;可能会有读者疑惑为什么这里所设定的线性变换不改变维数，原文中所描述的步骤不是应该将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt; 再计算注意力吗？这是正确的，原文中的计算流程确实如此。如下图所示，在线性变换后复制 &lt;code&gt;h&lt;/code&gt; 份（例中为 2） &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt;，用若干份 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 分别计算注意力再拼合起来，得到注意力的维数自然就是 &lt;code&gt;h * d_v&lt;/code&gt; （例中为 2 * 6），再用一个线性变换将其转化回模型所处理的维数 &lt;code&gt;d_model&lt;/code&gt;（例中为 5）。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;但代码中优化了一部分比较繁琐的操作，也有其他版本的代码使用了更接近原文的实现方式，如  &lt;a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; jadore801120 / attention-is-all-you-need-pytorch &lt;/a&gt;，流程就如下图所示，勉强称之为「单头注意力变多头注意力」的一种代码实现吧。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;例中 &lt;code&gt;d_model&lt;/code&gt; 也就是词嵌入的维数还是 5，&lt;code&gt;heads&lt;/code&gt; 仍为 2，&lt;code&gt;d_value&lt;/code&gt; 仍为 6，但模型不再是将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;，而是将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 直接升至 &lt;span class="math"&gt;\(hd_v\)&lt;/span&gt;，然后将 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 分成 &lt;code&gt;h&lt;/code&gt; 份，每份分别用于计算并拼接为注意力。与上例相比，本质上其实并无区别，区别仅仅是上例先复制多个矩阵再分别做线性变换，而该例只使用了一个更大的矩阵乘法就完成了上述操作，效率上更优。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;多头注意力还有一种实现方法，也是这里展示代码所使用的方法。如上图所示，这种方法对词嵌入的维数有要求，在词嵌入的步骤中就将 token 表示为 &lt;code&gt;d_v * h&lt;/code&gt; 维，这也是前文代码在初始化中使用 &lt;code&gt;assert&lt;/code&gt; 语句缘由。后续的线性变换不改变维数，计算多头注意力时直接将 &lt;code&gt;d_v * h&lt;/code&gt; 维切分为 &lt;code&gt;h&lt;/code&gt; 份作为每个 head 计算的对象。拼接各 head 的注意力后，最后的线性变换也不改变维数。&lt;/p&gt;
&lt;p&gt;在我看来，这种方法应该是对前两种方法的简化，三个例子中用于计算多头注意力的 &lt;code&gt;d_value&lt;/code&gt; 都为 6，计算量相同。第 3 种方法需要更大的 &lt;code&gt;d_model&lt;/code&gt;，而且计算多头注意力时没有使用到全部的 embedding，虽说效果类似，但总觉有些奇怪。这或许是为了计算上的方便，不用做过多的矩阵变换 🤔&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;# class SelfAttention(nn.Module):
    def forward(self, values, keys, query, mask):
        # 获取 batch_size
        N = query.shape[0]
        # d_v, d_k, d_q
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # 对 query, key, value 做线性变换
        values = self.values(values)    # (N, value_len, embed_size)
        keys = self.keys(keys)          # (N, key_len, embed_size)
        queries = self.queries(query)   # (N, query_len, embed_size)

        # 将 token 的词嵌入划分为 heads 份
        # d_model = embed_size = d_v * heads
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = queries.reshape(N, query_len, self.heads, self.head_dim)

        # queries: (N, query_len, heads, heads_dim),
        # keys: (N, key_len, heads, heads_dim)
        # energy: (N, heads, query_len, key_len)
        energy = torch.einsum("nqhd,nkhd-&amp;gt;nhqk", [queries, keys])

        # 将掩码矩阵中为 0 的对应项设为 -inf，不参与计算
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        # 得到的点积除以 sqrt(d_k) 并用 Softmax 归一化
        # attention: (N, heads, query_len, key_len)
        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)

        # attention: (N, heads, query_len, key_len)
        # values: (N, value_len, heads, heads_dim)
        # out after matrix multiply: (N, query_len, heads, head_dim), then
        # we reshape and flatten the last two dimensions.
        out = torch.einsum("nhql,nlhd-&amp;gt;nqhd", [attention, values]).reshape(
            N, query_len, self.heads * self.head_dim
        )

        # 拼接多头注意力后的线性变换
        # out: (N, query_len, embed_size)
        out = self.fc_out(out)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 部分描述了上述计算多头重意力的过程。线性变换后，使用 &lt;code&gt;reshape()&lt;/code&gt; 方法将 Tensor 转化化为指定维度，也就是将词嵌入划分为 &lt;code&gt;heads&lt;/code&gt; 份的操作，Tensor 的形状由 &lt;code&gt;[N, query_len, embed_size]&lt;/code&gt; 变为 &lt;code&gt;[N, query_len, self.heads, self.head_dim]&lt;/code&gt;，把 &lt;code&gt;embed_size&lt;/code&gt; 拆成 &lt;code&gt;heads * head_dim&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;接着使用 &lt;code&gt;torch.einsum()&lt;/code&gt; 得到注意力计算的一个中间量 &lt;code&gt;energy&lt;/code&gt;。&lt;code&gt;torch.einsum()&lt;/code&gt; 称为爱因斯坦求和约定，可以非常简洁地进行矩阵乘法、转置待操作，但会有些难以理解。&lt;/p&gt;
&lt;p&gt;例如矩阵乘法 &lt;span class="math"&gt;\(\boldsymbol{A}_{i\times j}\boldsymbol{B}_{j\times k}=\boldsymbol{C}_{i\times k}\)&lt;/span&gt;，可以表示为 &lt;code&gt;"ij,jk-&amp;gt;ik"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; B = torch.randn(4, 5)
&amp;gt;&amp;gt;&amp;gt; C = torch.einsum("ij,jk-&amp;gt;ik", [A, B])
&amp;gt;&amp;gt;&amp;gt; C.size()
torch.Size([3, 5])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;例如矩阵转置 &lt;span class="math"&gt;\((\boldsymbol{A}_{i\times j})^\top=\boldsymbol{B}_{j\times i}\)&lt;/span&gt;，可以表示为 &lt;code&gt;"ij-&amp;gt;ji"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; B = torch.einsum("ij-&amp;gt;ji", [A])
&amp;gt;&amp;gt;&amp;gt; B.size()
torch.Size([4, 3])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;定义了矩阵乘法的表示后，相应的数量积与向量积就也能表示了，不再赘述。求和操作将矩阵转化为数值，行与列都会消失，所以 &lt;span class="math"&gt;\(\sum a_{ij}\in\boldsymbol{A}_{i\times j}\)&lt;/span&gt; 可以记作 &lt;code&gt;"ij-&amp;gt;"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; torch.einsum("ij-&amp;gt;", [A])
tensor(0.5634)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此外，爱因斯坦求和约定还可以表示在指定维度上求和、做数量积等一系列的复杂操作，读者可以自行试验。&lt;/p&gt;
&lt;p&gt;代码中 &lt;code&gt;queries&lt;/code&gt; 的形状为 &lt;code&gt;[N, query_len, heads, heads_dim]&lt;/code&gt;，记作 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times q\times h \times d}\)&lt;/span&gt;，&lt;code&gt;keys&lt;/code&gt; 的形状为 &lt;code&gt;[N, key_len, heads, heads_dim]&lt;/code&gt;，记作 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times k\times h \times d}\)&lt;/span&gt;，那么 &lt;code&gt;torch.einsum("nqhd,nkhd-&amp;gt;nhqk", [queries, keys])&lt;/code&gt; 所做的操作就是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times q\times h \times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times h \times q\times d}\)&lt;/span&gt;，将 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times k\times h \times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h\times k \times d}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;两个矩阵中的 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 是 &lt;code&gt;batch_size&lt;/code&gt; 与 &lt;code&gt;heads&lt;/code&gt; 的乘积，仅仅是表示数量，所以 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times k\times d}\)&lt;/span&gt; 可以视作由 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 个 &lt;span class="math"&gt;\((\boldsymbol{K}_i)_{\ k\times d}\)&lt;/span&gt; 子矩阵构成的大矩阵。那么固定前两维不变，转置后两维，相当于&lt;strong&gt;转置&lt;/strong&gt;所有子矩阵，得到 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times d\times k}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;固定前两维，令 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times h \times q\times d}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times d\times k}\)&lt;/span&gt; 在后两维上做乘法，得到 &lt;span class="math"&gt;\((\boldsymbol{QK})_{N\times h \times q \times k}\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;仔细思考上述的转置和乘法过程，实际上就是在做多头注意力中的 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;掩码部分的操作先略过。接着 &lt;code&gt;torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)&lt;/code&gt; 先将前一步中得到 &lt;code&gt;energy&lt;/code&gt; 除以 &lt;span class="math"&gt;\(\sqrt{d_k}\)&lt;/span&gt; 再用 Softmax 归一化。指定的 &lt;code&gt;dim=3&lt;/code&gt; 与 &lt;code&gt;dim=-1&lt;/code&gt; 等价，其目的是在最后一维的方向上归一化。&lt;/p&gt;
&lt;p&gt;以一个简单的 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt; 乘法为例，如下图所示，&lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{K}\)&lt;/span&gt; 的每一行都是一个 token 的词嵌入表示。计算得到 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt; 后需要归一化，&lt;code&gt;softmax(dim=0)&lt;/code&gt; 是在行方向上归一化，在得到的结果中，全部行加起来，各元素为 1；&lt;code&gt;softmax(dim=1)&lt;/code&gt; 是在列方向上归一化，结果中的全部列加起来，各元素为 1。&lt;/p&gt;
&lt;p&gt;计算注意力还是为了得到更准确的 token 表示，所以归一化的方向应该与原始的 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 方向相同，即 &lt;code&gt;softmax(dim=1)&lt;/code&gt;。代码中也是一样，&lt;span class="math"&gt;\((\boldsymbol{QK})_{N\times h \times q \times k}\)&lt;/span&gt; 是 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 个 &lt;span class="math"&gt;\((\boldsymbol{Q}\boldsymbol{K}_i)_{q\times k}\)&lt;/span&gt; 子矩阵，要在所有子矩阵的列方向上做归一化，那么就是在第 4 个维度上做 Softmax，即 &lt;code&gt;softmax(dim=3)&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此时，上述过程已经完成了多头注意力中的 &lt;span class="math"&gt;\(\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\)&lt;/span&gt;，将结果记作 &lt;span class="math"&gt;\(\boldsymbol{A}_{N\times h\times q\times k}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;在下一步中，用 &lt;code&gt;"nhql,nlhd-&amp;gt;nqhd"&lt;/code&gt; 表示了 &lt;span class="math"&gt;\(\boldsymbol{A}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{V}\)&lt;/span&gt; 的乘法，具体操作是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times v\times h\times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times h\times v\times d}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;固定前两维，令 &lt;span class="math"&gt;\(\boldsymbol{A}_{N\times h\times q\times k}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times h\times v\times d}\)&lt;/span&gt; 在后两维上做乘法，这里有 &lt;span class="math"&gt;\(q=k=v\)&lt;/span&gt;，所以结果为 &lt;span class="math"&gt;\((AV)_{N\times h \times q\times d}\)&lt;/span&gt;，到这一步已经计算了 &lt;span class="math"&gt;\(\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\boldsymbol{V}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;将结果转置为 &lt;span class="math"&gt;\((AV)_{N\times q \times h\times d}\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后代码使用 &lt;code&gt;reshape()&lt;/code&gt; 合并后两维，将结果转化为 &lt;span class="math"&gt;\((AV)_{N\times q \times hd}\)&lt;/span&gt;，很巧妙地拼接了多个 head 的注意力，最后通过线性层再输出结果。&lt;/p&gt;
&lt;p&gt;至此，Transformer 中的 &lt;code&gt;SelfAttention&lt;/code&gt; 部分已经结束，读者或许会觉得头昏脑胀。不必担心，最为艰涩的一部分已经过去，接下来是一路下坡 🚩&lt;/p&gt;
&lt;h3 id="transformerblock"&gt;TransformerBlock&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class TransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TransformerBlock, self).__init__()
        # 前一层的多头注意力
        self.attention = SelfAttention(embed_size, heads)
        # Add &amp;amp; Norm 层
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        # 前馈层
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, value, key, query, mask):
        attention = self.attention(value, key, query, mask)

        x = self.dropout(self.norm1(attention + query))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;TransformerBlock&lt;/code&gt; 模块包括多头注意力与后接的 Add &amp;amp; Norm、Feed Forward、Add &amp;amp; Norm 三层。&lt;/p&gt;
&lt;p&gt;初始化部分使用 &lt;code&gt;nn.Sequential()&lt;/code&gt; 将 &lt;code&gt;nn.Linear()&lt;/code&gt;、&lt;code&gt;nn.ReLU()&lt;/code&gt;、&lt;code&gt;nn.Linear&lt;/code&gt; 依次连接起来形成前馈层，正如前文所说的，数据进入前馈层先升维再激活，最后再降回原来维度，&lt;code&gt;forward_expansion&lt;/code&gt; 决定升维的倍数。&lt;code&gt;dropout&lt;/code&gt; 用于随机弃用一部分数据防止过拟合，直接调用 &lt;code&gt;nn.Dropout()&lt;/code&gt; 类，接收的数值决定了弃用数据的比例。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 部分也很简单，计算的多头注意力依次做 Add &amp;amp; Norm、Feed Forward、Add &amp;amp; Norm 三层后输出数据。&lt;/p&gt;
&lt;h3 id="encoder"&gt;Encoder&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Encoder(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        embed_size,
        num_layers,
        heads,
        device,
        forward_expansion,
        dropout,
        max_length,
    ):

        super(Encoder, self).__init__()
        self.embed_size = embed_size
        # CPU or GPU
        self.device = device
        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)
        self.position_embedding = PositionalEncoding(embed_size, max_length)

        self.layers = nn.ModuleList(
            [
                TransformerBlock(
                    embed_size,
                    heads,
                    dropout=dropout,
                    forward_expansion=forward_expansion,
                )
                for _ in range(num_layers)
            ]
        )

        self.dropout = nn.Dropout(dropout)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Encoder 是 Transformer 中的左边部分，Transformer 中有 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个 &lt;code&gt;TransformerBlock&lt;/code&gt; 顺序叠放在一起组成 encoder。所以在初始化部分，使用列表推导式在 &lt;code&gt;layers&lt;/code&gt; 中放置了 &lt;code&gt;num_layers&lt;/code&gt; 层 &lt;code&gt;TransformerBlock&lt;/code&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;# class Encoder(nn.Module):
    def forward(self, x, mask):
        # 输入数据的 batch_size 与长度
        N, seq_length = x.shape
        # 从输入数据计算位置索引
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        # 由位置索引得到位置编码，并 dropout 一部分数据
        out = self.dropout(
            (self.word_embedding(x) + self.position_embedding(positions))
        )

        # 让数据逐层经过 encoder，计算自注意力
        for layer in self.layers:
            out = layer(out, out, out, mask)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 &lt;code&gt;forward()&lt;/code&gt; 部分中，使用 &lt;code&gt;torch.arange()&lt;/code&gt; 得到位置索引，再用 &lt;code&gt;expand()&lt;/code&gt; 方法将位置索引矩阵的形状变为与输入数据相同，&lt;code&gt;expand()&lt;/code&gt; 方法的主要作用是复制，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; torch.arange(0, 5)
tensor([0, 1, 2, 3, 4])
&amp;gt;&amp;gt;&amp;gt; torch.arange(0, 5).expand(2, 5)
tensor([[0, 1, 2, 3, 4],
        [0, 1, 2, 3, 4]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;to()&lt;/code&gt; 方法用于指定 Tensor 存储的设备，例如 &lt;code&gt;"CPU"&lt;/code&gt; 或 &lt;code&gt;"GPU"&lt;/code&gt;。将词嵌入加上位置编码得到 &lt;code&gt;out&lt;/code&gt;，再将 &lt;code&gt;out&lt;/code&gt; 送入 encoder 中计算结果。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;layer(out, out, out)&lt;/code&gt; 看起来或许有些奇怪，请留意，前文已经讨论过，在 encoder 中计算的是&lt;strong&gt;自注意力&lt;/strong&gt;，所以此时的 query、key、value 都是相同的，而在 decoder 中就会有所不同了。&lt;/p&gt;
&lt;h3 id="decoderblock"&gt;DecoderBlock&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class DecoderBlock(nn.Module):
    def __init__(self, embed_size, heads, forward_expansion, dropout, device):
        super(DecoderBlock, self).__init__()
        self.norm = nn.LayerNorm(embed_size)
        self.attention = SelfAttention(embed_size, heads=heads)
        self.transformer_block = TransformerBlock(
            embed_size, heads, dropout, forward_expansion
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, value, key, src_mask, trg_mask):
        attention = self.attention(x, x, x, trg_mask)
        query = self.dropout(self.norm(attention + x))
        out = self.transformer_block(value, key, query, src_mask)
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;类似地，Decoder 是 Transformer 结构图中的右侧部分，也是由 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 层 &lt;code&gt;DecoderBlock&lt;/code&gt; 组成。decoder 只比 encoder 多了一个掩码注意力层，其他结构相同，所以 &lt;code&gt;DecoderBlock&lt;/code&gt; 的初始化中直接调用了先前定义的 &lt;code&gt;TransformerBlock&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 中，target 进入 decoder 后，先计算&lt;strong&gt;自注意力&lt;/strong&gt;（&lt;code&gt;attention(x, x, x)&lt;/code&gt;），再经过 Add &amp;amp; Norm 层得到 &lt;code&gt;query&lt;/code&gt;，再与 encoder 中的结果做多头注意力（&lt;code&gt;attention(value, key, query)&lt;/code&gt;），输出结果。留意两种注意力计算的不同，参考 Transformer 结构图理解一下就会很明确。&lt;/p&gt;
&lt;h3 id="decoder"&gt;Decoder&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Decoder(nn.Module):
    def __init__(
        self,
        trg_vocab_size,
        embed_size,
        num_layers,
        heads,
        forward_expansion,
        dropout,
        device,
        max_length,
    ):
        super(Decoder, self).__init__()
        self.device = device
        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)
        self.position_embedding = PositionEmbedding(embed_size,max_length)

        self.layers = nn.ModuleList(
            [
                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)
                for _ in range(num_layers)
            ]
        )
        self.fc_out = nn.Linear(embed_size, trg_vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, src_mask, trg_mask):
        N, seq_length = x.shape
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))

        for layer in self.layers:
            x = layer(x, enc_out, enc_out, src_mask, trg_mask)

        out = self.fc_out(x)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;实现了 &lt;code&gt;DecoderBlock&lt;/code&gt; 后，&lt;code&gt;Decoder&lt;/code&gt; 就没有什么内容了，与 encoder 类似，就是将多个 &lt;code&gt;DecoderBlock&lt;/code&gt; 组装起来，按接口传入数据进行计算。&lt;/p&gt;
&lt;h3 id="transformer"&gt;Transformer&lt;/h3&gt;
&lt;p&gt;最后的 &lt;code&gt;Transformer&lt;/code&gt; 将各个模块都组合起来：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        trg_vocab_size,
        src_pad_idx,
        trg_pad_idx,
        embed_size=512,
        num_layers=6,
        forward_expansion=4,
        heads=8,
        dropout=0,
        device="cpu",
        max_length=100,
    ):

        super(Transformer, self).__init__()

        self.encoder = Encoder(
            src_vocab_size,
            embed_size,
            num_layers,
            heads,
            device,
            forward_expansion,
            dropout,
            max_length,
        )

        self.decoder = Decoder(
            trg_vocab_size,
            embed_size,
            num_layers,
            heads,
            forward_expansion,
            dropout,
            device,
            max_length,
        )

        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        self.device = device

    def make_src_mask(self, src):
        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)
        # (N, 1, 1, src_len)
        return src_mask.to(self.device)

    def make_trg_mask(self, trg):
        N, trg_len = trg.shape
        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(
            N, 1, trg_len, trg_len
        )

        return trg_mask.to(self.device)

    def forward(self, src, trg):
        src_mask = self.make_src_mask(src)
        trg_mask = self.make_trg_mask(trg)
        enc_src = self.encoder(src, src_mask)
        out = self.decoder(trg, enc_src, src_mask, trg_mask)
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;初如化部分主要是设定了默认的参数，并引入前面定义好的 &lt;code&gt;Encoder&lt;/code&gt; 与 &lt;code&gt;Decoder&lt;/code&gt; 模块。&lt;code&gt;Transformer&lt;/code&gt; 中还多了 &lt;code&gt;make_src_mask()&lt;/code&gt; 与 &lt;code&gt;make_trg_mask()&lt;/code&gt; 两个函数，这就不得不谈谈 Transformer 中的掩码机制了。&lt;/p&gt;
&lt;p&gt;考虑一个情境，需要使用 Transformer 翻译一批（若干条）句子，各句子的长度自然是不同的，那么输入模型的数据的形状也是不同的，这在后续步骤中就会出现很多问题。在实际中，通常会找到文本中最长的句子（&lt;code&gt;max_len&lt;/code&gt;），再将所有句子都变为该长度，这种操作称为 padding。&lt;/p&gt;
&lt;p&gt;具体做法如下图所示，分别用 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 与 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 标记句子的起讫，用 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 填充 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 后的空位，各数据的长度就会一致。然后根据设定的词典，将 token 转化为索引，接着再做词嵌入。&lt;code&gt;make_src_mask()&lt;/code&gt; 就是根据 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 的索引，将 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 所在位置都标记为 &lt;code&gt;False&lt;/code&gt;，其他位置标记为 &lt;code&gt;True&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;后续 &lt;code&gt;unsqueeze()&lt;/code&gt; 的操作比较费解，其实它是利用了 PyTorch 的广播机制，用于自动匹配矩阵的形状。图中的例子可以看作是将矩阵翻转再在第 3 个方向上拉长。因为代码中的掩码要用于掩盖形状为 &lt;code&gt;[N, heads, query_len, key_len]&lt;/code&gt; 具有 4 个方向的 &lt;code&gt;energy&lt;/code&gt;，所以要额外再做一次 &lt;code&gt;unsqueeze()&lt;/code&gt;。最后将掩码用于掩盖词嵌入数据，掩码就像一个罩子盖在词嵌入数据上，模型只计算 &lt;code&gt;True&lt;/code&gt; 位置上的数据。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;使用掩码可以让模型灵活地处理不同长度的数据，数据的长度由掩码决定，改变掩码就相当于改变处理的数据，而不去改变存储在硬件中的数据，这对于计算更有利。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;make_trg_mask()&lt;/code&gt; 函数产生用于 target 数据的掩码，在 target 上使用掩码的原因与 source 不同。在 decoder 中，模型要根据输入数据的计算结果给出新 token，而生成文本的过程是顺序的，依赖于前一步生成的结果。具体来说就是，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;序列以 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 标记起始；&lt;/li&gt;
&lt;li&gt;根据已有的 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 生成 &lt;code&gt;A&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;根据生成的 &lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt; 生成 &lt;code&gt;B&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;根据生成的 &lt;code&gt;&amp;lt;s&amp;gt; A B&lt;/code&gt; 生成 &lt;code&gt;C&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;以此类推，直至模型生成 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt;，句子结束。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;前文已经讨论过，这种方法有很多局限性，而 Transformer 的巧妙之处就在于能够并行完成这个过程。&lt;/p&gt;
&lt;p&gt;我们可以考虑训练过程，实际上与生成过程类似，训练过程就是要根据已经生成的 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 建立与下一个 token &lt;code&gt;A&lt;/code&gt; 的关系，而不能是与后续 &lt;code&gt;B&lt;/code&gt; 或 &lt;code&gt;C&lt;/code&gt; 的关系，将这种关系以参数的形式存储到模型中，推理阶段就能顺利地根据 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 生成 &lt;code&gt;A&lt;/code&gt;。这样的训练过程可以表示为一个下三角矩阵，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Transformer 不需要逐个 token 生成再建立关系，可以通过下三角矩阵一次直接取出 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt;、&lt;code&gt;&amp;lt;s&amp;gt; A B&lt;/code&gt; 等 token 序列，并行地训练模型与对应的下一个 token 建立关系。最后将 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 与每一步骤中新生成 token &lt;code&gt;A&lt;/code&gt;、&lt;code&gt;B&lt;/code&gt;、&lt;code&gt;C&lt;/code&gt;、&lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 拼合起来，即得到生成的文本。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;make_trg_mask()&lt;/code&gt; 就是在构建这个下三角的掩码。&lt;code&gt;torch.ones()&lt;/code&gt; 用于生成指定大小元素全为 &lt;code&gt;1&lt;/code&gt; 的矩阵，然后用 &lt;code&gt;torch.tril()&lt;/code&gt; 取该矩阵的下三角，再用 &lt;code&gt;expand()&lt;/code&gt; 方法将该矩阵复制到与 &lt;code&gt;batch_size&lt;/code&gt; 匹配。&lt;/p&gt;
&lt;h3 id="train"&gt;Train&lt;/h3&gt;
&lt;p&gt;从前面讨论的模型生成过程还可以知道的一点是，模型永远不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，所以 target 中没有 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，而 source 则必须由 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 起始。在实际中，一种做法是，用预处理的脚本在原始训练数据（例如 &lt;code&gt;.csv&lt;/code&gt;、&lt;code&gt;.txt&lt;/code&gt; 文件）中标上标记；另一种方法是，在训练代码中加入预处理的功能，读取数据时分别为数据做上相应标记。为了方便起见，本文就不实现这一部分功能，使用 Transformer 可以直接处理的数据。&lt;/p&gt;
&lt;p&gt;生成训练数据的函数为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def generate_random_batch(batch_size, max_length=16):
    src = []
    for i in range(batch_size):
        # 随机指定有效数据的长度
        random_len = random.randint(1, max_length - 2)
        # 在数据起讫处加上标记，"&amp;lt;s&amp;gt;": 0, "&amp;lt;e&amp;gt;": 1
        random_nums = [0] + [random.randint(3, 9) for _ in range(random_len)] + [1]
        # padding 填满数据长度，"&amp;lt;p&amp;gt;": [2]
        random_nums = random_nums + [2] * (max_length - random_len - 2)
        src.append(random_nums)

    src = torch.LongTensor(src)
    # tgt 去除末尾的 token
    tgt = src[:, :-1]
    # tgt_y 去除首个 &amp;lt;s&amp;gt;，即模型需要预测的 token，用于计算损失
    tgt_y = src[:, 1:]
    # 模型需要预测的 token 数量（不计 &amp;lt;p&amp;gt;），用于计算损失函数
    n_tokens = (tgt_y != 2).sum()

    return src, tgt, tgt_y, n_tokens
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;generate_random_batch()&lt;/code&gt; 能够生成 Transformer 可以直接计算的相同的 source 与 target，该模型的任务目标就是生成与输入相同的序列。模型不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，所以&lt;code&gt;tgt_y&lt;/code&gt; 去除 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 用于与生成的序列对比计算损失，这很容易理解。但为什么 &lt;code&gt;tgt&lt;/code&gt; 需要去除最后一个 token 呢？这一点我将在后文生成序列的 Predict 一节讨论。训练与测试模型的代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# &amp;lt;p&amp;gt; 索引
src_pad_idx = 2
trg_pad_idx = 2
# 词表大小，即全部 token 数量，包括 &amp;lt;s&amp;gt; &amp;lt;e&amp;gt; &amp;lt;p&amp;gt; 等标记
src_vocab_size = 10
trg_vocab_size = 10
# 文本最大长度
max_len = 16

model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,
                    embed_size=128, num_layers=2, dropout=0.1, max_length=max_len,
                    device=device).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
criteria = nn.CrossEntropyLoss()
total_loss = 0

for step in range(2000):
    src, tgt, tgt_y, n_tokens = generate_random_batch(batch_size=2, max_length=max_len)
    optimizer.zero_grad()
    out = model(src, tgt)

    # contiguous() 与 view() 将矩阵在各行首尾相连为一行（即向量）
    # 在两向量间计算损失函数
    # tgt_y 中元素的值是索引，除以 n_tokens 将其缩放到 [0, 1]
    loss = criteria(out.contiguous().view(-1, out.size(-1)),
                    tgt_y.contiguous().view(-1)) / n_tokens
    loss.backward()
    optimizer.step()

    total_loss += loss

    if step != 0 and step % 40 == 0:
        print(f"Step {step}, total_loss: {total_loss}")
        total_loss = 0

# Predict
copy_test(model, max_len)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PyTorch 使用 &lt;code&gt;torch.optim&lt;/code&gt; 定义模型的训练过程，其中可以选择非常多种的优化过程，这里选择了 &lt;code&gt;Adam()&lt;/code&gt;，&lt;code&gt;lr=3e-4&lt;/code&gt; 指定了训练步骤的学习率。&lt;code&gt;nn.CrossEntropyLoss()&lt;/code&gt; 用于计算两个向量的交叉熵损失，作为训练过程的损失函数。&lt;/p&gt;
&lt;p&gt;在训练循环中，每一个循环处理 1 个 batch 的数据，在同一个 batch 中 PyTorch 自动计算梯度的反向传播并更新参数。但在新的 batch 中，因为已经更新到参数中了，我们不希望保留上一个 batch 的梯度，所以用 &lt;code&gt;optimizer.zero_grad()&lt;/code&gt; 将梯度清空。&lt;/p&gt;
&lt;p&gt;将 &lt;code&gt;src&lt;/code&gt; 与 &lt;code&gt;tgt&lt;/code&gt; 传入模型，&lt;code&gt;out&lt;/code&gt; 就是 Transformer 的计算结果。&lt;code&gt;loss.backward()&lt;/code&gt; 与 &lt;code&gt;optimizer.step()&lt;/code&gt; 两行代码就是前面所说的让 PyTorch 自动计算梯度的反向传播并更新参数。&lt;/p&gt;
&lt;h3 id="predict"&gt;Predict&lt;/h3&gt;
&lt;p&gt;训练结束后，我用 &lt;code&gt;copy_test()&lt;/code&gt; 函数测试模型的效果，这个测试函数定义为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def copy_test(model, max_len):
    model = model.eval()
    src = torch.LongTensor([[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]])
    # 模型从 &amp;lt;s&amp;gt; 开始生成序列，但不会生成 &amp;lt;s&amp;gt;，所以指定起始的 &amp;lt;s&amp;gt;
    tgt = torch.LongTensor([[0]])

    for i in range(max_len):
        # out： (1, i + 1, 10)
        # i + 1 模型输出的 token 数量
        # 10 为 vocab_size，是词表中 token 数量，out 是词表中各 token 在此处出现的概率
        out = model(src, tgt)
        # 取输出的 i + 1 个 token 中的最后一个
        # predict: (1, 10)
        predict = out[:, -1]
        # 取得概率最大的 token 索引
        # y: (1, )
        y = torch.argmax(predict, dim=1)
        # 逐个拼合 token 索引
        # y.unsqueeze(0): (1, 1)
        # tgt: (1, i + 1 )
        tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)
        # 若生成 token &amp;lt;e&amp;gt;，表示句子结束，退出循环
        if y == 1:
            break
    print(tgt)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;eval()&lt;/code&gt; 方法令模型退出训练模式，会关闭 dropout 等训练过程中才需要的功能。在循环中逐个拼合生成的 token，就能得到生成的句子。循环中的操作如下图所示，在第 1 次循环中，&lt;code&gt;tgt&lt;/code&gt; 为 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，通过与 &lt;code&gt;src&lt;/code&gt; 的注意力与下三角矩阵得到计算结果 &lt;code&gt;out&lt;/code&gt; 为 &lt;code&gt;A&lt;/code&gt;，然后将 &lt;code&gt;tgt&lt;/code&gt; 更新为 &lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt;，在第 2 次循环中，得到的 &lt;code&gt;out&lt;/code&gt; 为 &lt;code&gt;A B&lt;/code&gt;，所以在每次循环中都只取新生成的 &lt;code&gt;out[-1]&lt;/code&gt; 更新 &lt;code&gt;tgt&lt;/code&gt;，最后将结果拼接起来得到完整的输出结果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;或许读者会有疑惑，既然使用下三角矩阵并行计算是 Transformer 的优势，为什么这里却是用循环顺序地生成呢？为什么计算上图中最后一个矩阵的 &lt;code&gt;out&lt;/code&gt;，而是要用一个个的 &lt;code&gt;out[-1]&lt;/code&gt; 呢？&lt;/p&gt;
&lt;p&gt;要注意的是，训练与生成有重要的一个不同，就是生成中的 &lt;code&gt;tgt&lt;/code&gt; 是空白的、模型不可知的，而训练中的 &lt;code&gt;tgt&lt;/code&gt; 是完整的、模型可知的。如上图中，&lt;code&gt;tgt&lt;/code&gt; 在每个循环中都在变长，只有 &lt;code&gt;tgt&lt;/code&gt; 变成了 &lt;code&gt;&amp;lt;s&amp;gt; A B C &amp;hellip;&lt;/code&gt; 才会有最后一个矩阵中的 &lt;code&gt;out&lt;/code&gt;。如果说只要最后一个矩阵中的 &lt;code&gt;out&lt;/code&gt; 而不要前面的步骤，就变成了「吃两个馒头吃饱，所以只吃后一个能吃得饱的馒头」的笑话。&lt;/p&gt;
&lt;p&gt;所以&lt;dot&gt;生成过程并不是并行的，Transformer 的并行指的是训练过程&lt;/dot&gt;。如下图所示，在训练过程中 Transformer 只需要做一次下三角矩阵的运算就可以建立多个 token 间的关系。这张图还解释了模型永远不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 但 &lt;code&gt;tgt&lt;/code&gt; 必须以 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 起始的原因。图中还可以很明白的看出为什么先前的训练代码要去除 &lt;code&gt;tgt&lt;/code&gt; 末尾的 token，因为 Transformer 的输出 &lt;code&gt;out&lt;/code&gt; 计算的是 &lt;code&gt;tgt&lt;/code&gt; 下一个 token（及此前）的计算结果，若不去除末位就超出范围了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后训练与测试的结果为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;cpu
Step 40, total_loss: 4.021485328674316
Step 80, total_loss: 2.8817126750946045
&amp;hellip;&amp;hellip;
Step 1920, total_loss: 0.9760974049568176
Step 1960, total_loss: 0.8644390106201172
tensor([[0, 6, 3, 4, 5, 7, 6, 4, 3, 1]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出的结果没有输出 source &lt;code&gt;[[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]]&lt;/code&gt; 中末尾代表 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 的 &lt;code&gt;2&lt;/code&gt;，前面的 token 索引也与 source 相差无几，说明模型正确复制了输入序列，训练是成功的。&lt;/p&gt;
&lt;h2 id="hou-ji_1"&gt;后记&lt;/h2&gt;
&lt;p&gt;至此，这篇 Transformer 的介绍终于告一段落了。从起草、绘图再到最后的代码梳理，前后花了一周多的时间。虽名为介绍，其实还是为自己在做梳理，边写边想、边想边查，终于把 Transformer 中的一些细节弄明白了，这篇笔记也能为读者勾勒出一个大致的图景。&lt;/p&gt;
&lt;p&gt;当然，限于篇幅，限于「从零起步」的初衷，也限于笔力，还有许多更深层次问题都没有探讨，但我相信，在看懂了这篇笔记之后，再去阅读那些文章已经不成问题了，这也符合我的初心。&lt;/p&gt;
&lt;p&gt;或许读者还很困惑，疑惑为什么数学推导上并不那么严谨的模型居然能有效，甚至具有极好的表现，那就说明需要钻入研究 Transformer 的底层了，不可不再读些更专业的文章。我也把写这篇文章时所参考以及较好的相关资料罗列于后，以飨读者。&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Vaswani, A. et al. Attention Is All You Need (2017) - arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spaces.ac.cn/archives/4765" rel="noopener" target="_blank"&gt;《Attention is All You Need》浅读（简介+代码）- 科学空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spaces.ac.cn/archives/6933" rel="noopener" target="_blank"&gt;从语言模型到 Seq2Seq：Transformer 如戏，全靠 Mask - 科学空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" rel="noopener" target="_blank"&gt;Language Modeling with nn.Transformer and torchtext - PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener" target="_blank"&gt;The Illustrated Transformer - Jay Alammar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/wevolf/p/12484972.html" rel="noopener" target="_blank"&gt;Transformer 源码中 Mask 机制的实现 - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/434232512" rel="noopener" target="_blank"&gt;torch.einsum 详解 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.csdn.net/zhaohongfei_358/article/details/126019181" rel="noopener" target="_blank"&gt;Pytorch 中 nn.Transformer 的使用详解与 Transformer 的黑盒讲解 - CSDN 博客&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Python"></category><category term="PyTorch"></category><category term="Transformer"></category></entry><entry><title>文献总结｜药物发现中的匹配分子对分析：方法与当前应用</title><link href="https://leonis.cc/sui-sui-nian/2023-04-15-summary-doi.org/10.1021/acs.jmedchem.2c01787.html" rel="alternate"></link><published>2023-04-15T00:00:00+08:00</published><updated>2023-04-15T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-15:/sui-sui-nian/2023-04-15-summary-doi.org/10.1021/acs.jmedchem.2c01787.html</id><summary type="html">&lt;p&gt;本文介绍 2023 年由曹东升与侯廷军研究团队发表在 &lt;em&gt;Journal of Medicinal Chemistry&lt;/em&gt; 上的一篇展望，文章原标题为 Matched Molecular Pair Analysis in Drug Discovery: Methods and Recent Applications，文章介绍了主要介绍了匹配分子对分析的理论与目前基于匹配分子对分析的实际应用。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/acs.jmedchem.2c01787" rel="noopener" target="_blank"&gt;doi.org/10.1021/acs.jmedchem.2c01787&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍 2023 年由曹东升与侯廷军研究团队发表在 &lt;em&gt;Journal of Medicinal Chemistry&lt;/em&gt; 上的一篇展望，文章原标题为 Matched Molecular Pair Analysis in Drug Discovery: Methods and Recent Applications，文章介绍了主要介绍了匹配分子对分析的理论与目前基于匹配分子对分析的实际应用。&lt;/p&gt;
&lt;p&gt;匹配分子对（matched molecular pair, MMP）的概念自提出以来，已成为了从化合物中提取药物化学知识并用于指导先导化合物优化的标准方法，MMP 的定义是只在局部具有较小的结构差异的一对化合物。合成化学家、药物化学家借助匹配分子对分析（molecular matched pair analysis, MMPA）的手段，可以从人类研究过的海量化合物中总结出化学改造的方法、化学改造对于化合物性质的影响等重要经验知识。&lt;/p&gt;
&lt;h2 id="mmpa-li-lun"&gt;MMPA 理论&lt;/h2&gt;
&lt;h3 id="mmp-sou-suo-suan-fa"&gt;MMP 搜索算法&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8784?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8784?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在需要对大量分子数据做 MMPA 时，首要任务就是提取出其中的 MMP，MMP 搜索算法可以分为 3 类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;预设的变换规则：使用人为设计的切分规则分割分子，寻找分子数据中的 MMP，常用规则如 retrosynthetic combinatorial analysis procedure（RECAP）和 breaking of retrosynthetically interesting chemical substructures（BRICS）。这种方法的局限性也很明显，例如忽略了预设规则以外的 MMP 并且只能处理单点的化学结构变换。&lt;/li&gt;
&lt;li&gt;基于最大公共子结构（maximum common substructure, MCS）的方法：先寻找指定分子的的公共结构，将其设定为固定部分，只有具有公共结构的分子才能构成 MMP，分子中除去公共结构所剩余的结构就是改变部分，所以该方法通常用用于表示化学变换的 SMIRKS 存储 MMP。这种方法的问题在于计算 MCS 的计算开销很大。&lt;/li&gt;
&lt;li&gt;片段与索引（fragmentation and indexing, F+I）方法：该方法是目前寻找 MMP 最通用的方法，主要方法是在两非氢原子间的非环单键处切断，构建 key 与 value 片段的对应索引，通过键值对间的匹配寻找 MMP，具体方法可以&lt;a href="https://leonis.cc/sui-sui-nian/2023-02-25-summary-doi.org/10.1021/ci900450m.html" rel="noopener" target="_blank"&gt;参看前文&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="ying-xiang-mmpa-de-guan-jian-yin-su"&gt;影响 MMPA 的关键因素&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8785?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8785?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MMPA 的基本假设是，分子结构中一些小的结构改变将引起特定物理性质或是生物活性的改变。然而现实中化合物性质改变的原因更为复杂，表现出更为偶然的现象，例如分子改造中的活性悬崖（对分子仅做微小的改造而生物活性变化巨大）等，所以在 MMPA 中也要考虑到许多因素的影响。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分子表示：2D 与 3D 分子结构都被用于 MMPA 研究中，2D 分子描述的主要优点是处理简单，但许多实践表明 3D 分子表示方法表示了分子的空间信息，使其对于微小的结构差异更为敏感，这对 MMPA 十分重要。&lt;/li&gt;
&lt;li&gt;环境特征：在早期的研究中，人们认为只有 MMP 中的化学转换改变了分子的性质，因此只针对化学转换进行研究，而没有考虑具体分子。如今人们已经意识到，在 MMPA 还需要考虑具体分子的结构以及改造位点等环境特征，不能只研究 MMP 中的化学转化规则。目前，大部分研究使用分子图或 SMILES 来表示 MMP 中的完整分子，用于 MMPA 研究。除了分子信息以外，也有研究将蛋白口袋的信息也融入 MMPA，这有助于更深入研究 MMP 转化对受体与配体间结合作用影响。&lt;/li&gt;
&lt;li&gt;统计显著性：MMPA 的统计分析对于研究 MMP 间性质的变化十分重要，因为一种化学转换可以引起多种性质的改变，多种化学转换也可能使分子的某些性质不发生改变。MMPA 的统计学研究发现，在同一化合物上所做的两个结构改造所产生的影响远不同于单一结构改造影响的加和，这也称为「不可加和性」效应，这意味着简单的单一结构改造间存在着相互作用。不可加和性同样影响了分子的溶解度等性质，在 MMPA 中对可加和性进行统计分析，可以更好地识别药物分子的构效关系与分子中潜在的相互作用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="mmpa-shi-ji-ying-yong_1"&gt;MMPA 实际应用&lt;/h2&gt;
&lt;p&gt;MMPA 已经广泛应用在寻找得到目标性质分子所需的化学改造中（ADMET 优化），除了应用在先导化合物优化，MMPA 也用于靶点预测、生物电子等排体替换、构效关系确定、全新药物设计等任务中，这里主要介绍 MMPA 在分子结构改造和全新药物设计中的应用。&lt;/p&gt;
&lt;h3 id="pi-pei-fen-zi-xu-lie"&gt;匹配分子序列&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8786?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8786?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;将多个仅具有一个子结构区别的分子组织起来，就得到了匹配分子序列（matching molecular series, MMS），该方法最早被用于药物分子构效关系的分析，将不同 MMS 组织起来还得到形成匹配分子序列图，用于决策分子改造的路线。称为 SAR 转移的方法通过对比两个 MMS 间化合物性质的变化，可以判断替换结构的效果与。&lt;/p&gt;
&lt;h3 id="ji-yu-mmpa-de-quan-xin-yao-wu-she-ji"&gt;基于 MMPA 的全新药物设计&lt;/h3&gt;
&lt;p&gt;将 MMP 化学变换规则用于分子生成是全新药物设计中的重要步骤，输入的分子首先被分割为片段，然后通过 MMP 数据库搜索找到相应的化学转换，将这些化学转换用于输入分子就得到了新分子。也有研究提出了基于片段的 MMP 分子生成方法，主要步骤是收集 MMP 片段信息，通过遗传算法等方法合理地相互组合 MMP 片段，得到新分子。&lt;/p&gt;
&lt;p&gt;也有研究使用分子骨架和分子骨架以外的子结构来构建分子生成模型，模型是使用 SMILES 的 RNN 模型，第一步是生成正确的分子骨架，第二步在分子骨架上添加结构改造得到正确的分子。此外，MMS 方法可以很容易地将分子分为若干类的类似物，也可以很方便地用于全新药物设计。DeepSARM 模型的目标是寻找生物作用类似而化学结构新颖的类似物，就使用了 MMS 方法，模型同时还考虑了靶点信息，扩大的 MMS 方法的应用范围。&lt;/p&gt;
&lt;h2 id="zhan-wang_1"&gt;展望&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8787?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8787?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;分子设计所面临的一个重要难题是如何基于有限的实验数据决定下一步的分子改造，MMPA 有助于人们从已有的分子改造数据中得到化学转换的信息。为了能更好地利用 MMPA，文章提出了以下几点展望：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 QSAR 与 MMPA 相结合。QSAR 模型着重于整体的结构特征，MMPA 主要用于确定局部子结构的改变，在一定程度上二者是互补的，在未来 MMPA 也可能对 QSAR 模型的预测有帮助。&lt;/li&gt;
&lt;li&gt;将 MMPA 的概念用于蛋白质等大分子。&lt;/li&gt;
&lt;li&gt;融合 MMPA 相关的分子优化方法，构建自动化的分子优化流程。尽管目前 MMP 已经应用于分子生成，但 MMP 数据的提取等步骤还需要人工处理。文章提出了上图所示的预期 MMPA 工作流程，希望能够实现 MMP 的自动提取、组织、应用和评估。&lt;/li&gt;
&lt;/ol&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Review"></category></entry><entry><title>我的 2023 年春播计划</title><link href="https://leonis.cc/zai-lu-shang/2023-04-11-my-gardening-plan.html" rel="alternate"></link><published>2023-04-11T00:00:00+08:00</published><updated>2023-04-11T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-11:/zai-lu-shang/2023-04-11-my-gardening-plan.html</id><summary type="html">&lt;p&gt;过了惊蛰之后，万物萌动，看到园林工人正在整饬路旁的花圃，我的心里也跟着躁动起来，想着非要在阳台上种点什么才好，于是在一番调查与纠结之后确定了这篇春播计划。&lt;/p&gt;</summary><content type="html">&lt;p&gt;过了惊蛰之后，万物萌动，看到园林工人正在整饬路旁的花圃，我的心里也跟着躁动起来，想着非要在阳台上种点什么才好，于是在一番调查与纠结之后确定了这篇春播计划。&lt;/p&gt;
&lt;h2 id="pin-chong"&gt;品种&lt;/h2&gt;
&lt;p&gt;我的阳台正朝南方，一整天都可以受到阳光，非常适合种些花草，之所以一直闲置，还是因为漂泊在外，总担心有时无暇顾及这些无言的小小生灵。所以在我下定决心后，首先要考虑的就是草木的品种。由于春节会回家，到时就无人照看这些花草了，我优先考虑一年生的植物，并且植株的越冬所需要的操作一定要越简单越好。&lt;/p&gt;
&lt;p&gt;结合以上客观因素和个人喜好等主观因素下，我敲定了这几个大类别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;大丽花&lt;/strong&gt;：大丽花喜欢光照，光照越充足花开得越盛，正好符合阳台的光照条件，而在冬天枯萎后可以挖出种球保存，也不需要专门照看，所以非常合适；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;甘菊等一年生的切花&lt;/strong&gt;：我个人特别喜欢切花，不仅盛开时美，还可以剪下做成干花，某种意义上也算是经冬不凋，而天津的秋冬季十分干燥，制作干花实在太合适了；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;番茄等一年生的蔬果&lt;/strong&gt;：许多人调侃中国人的天赋是种菜，喜欢种蔬果胜过于种花草，其实蔬果也有许多园艺品种，除了食用以外也有很好的观赏价值，蔬果的生长周期短，播种时间可以很灵活，而且谁不想一尝丰收的喜悦呢？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我的春播计划包括 5 个具体的品种，用店家的图做个参考吧：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="朵拉大丽花" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8760?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="朵拉大丽花" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8760?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 朵拉大丽花 Dahlia 'Melody Dora'&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="小甘菊" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8761?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="小甘菊" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8761?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 小甘菊&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="文森特向日葵" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8762?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="文森特向日葵" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8762?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 文森特向日葵 Helianthus annuus 'Vincent's Fresh'&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="橙色番茄" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8763?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="橙色番茄" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8763?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 橙色番茄&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="墨西哥小番茄" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8764?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="墨西哥小番茄" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8764?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 墨西哥小番茄 Solanum lycopersicum 'Mexico Midget'&lt;/p&gt;
&lt;p&gt;{warn begin}品种学名来自于植物数据库 &lt;a href="https://garden.org/plants/" rel="noopener" target="_blank"&gt;Plants Database&lt;/a&gt;，主要靠对照图片推测，不一定准确。{warn end}&lt;/p&gt;
&lt;h2 id="gong-ju"&gt;工具&lt;/h2&gt;
&lt;p&gt;接着是一些园艺用具，包括花盆、花土、花肥等等。&lt;/p&gt;
&lt;p&gt;包括我在内的很多人都没有地栽的条件，只能购置花盆选择盆栽。花盆的上选当然是红陶盆，不仅透水透气还美观耐看，红陶盆的一个问题是太重，所以运费会导致价格比较高，也不适合漂泊在外没有自己固定居所的人；再一个问题是在植株换盆时，为了避免伤根，通常会把盆给敲碎，像我这样拮据的人很难容忍这种浪费。基于这些考虑，我选择了更便捷一些的塑料盆。&lt;/p&gt;
&lt;p&gt;盆的大小要依种植的植株品种来定，比较常用的尺寸是直径 15 cm 左右的花盆，容量大约 2 L。大丽花需要较大的花盆，可以按大丽花的株高选盆：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;株高 50 cm 以下，盆径 18 cm&lt;/li&gt;
&lt;li&gt;株高 50-90 cm，盆径 25 cm&lt;/li&gt;
&lt;li&gt;株高 90 cm 以上，盆径 30 cm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于番茄需要注意的是，番茄可以按生长模式分为两类&amp;mdash;&amp;mdash;有限生长型与无限生长型。不要把生长模式与植物的寿命混淆，番茄大部分都是一年生的，有限生长是指番茄的果实都在同一时期成熟，收完一茬就结束了；而无限生长型番茄的果实会在一段较长的时间内次序成熟，犹如永远收获不完，因而得名。&lt;dot&gt;有限生长型番茄大多是矮株，无限生长型番茄大多是高株&lt;/dot&gt;，所以要选择匹配的盆径。有限生长型番茄用 20 cm 的盆尚可，无限生长型番茄就需要用 30 cm 左右的花盆了，因为它们甚至能长到 2 米多高。&lt;/p&gt;
&lt;p&gt;在番茄品种的选择上出现了一些失误，把小番茄误当作了矮株番茄，收到种子后才得知我购买的两个品种都是无限生长型的番茄，于是慌忙之下又购买了几个大盆。&lt;/p&gt;
&lt;p&gt;那么我所种植的品种与花盆的搭配就是，矮型大丽花朵拉和两种番茄使用 25 cm 的大盆，向日葵搭配 20 cm 的花盆，小甘菊可以使用 17 cm 的盆。&lt;/p&gt;
&lt;p&gt;我购买的花土也简单，就是普通的泥炭土，先后买了大约 18 L，还不太确定用量，后续不够可以再买，这样也能先试试土的质量。&lt;/p&gt;
&lt;p&gt;至于肥料，我只购买了一些非常便宜的有机肥，没有购买无机肥，主要是因为花肥的品牌琳琅满目，花肥又不像花土一般适用，购买了不适配的就容易浪费，并且我对花肥的品牌不太了解也难以选择。再者作为学化学的，有什么化学试剂是我难以获得的呢，我相信我的专业水平更多于那些名不见经传的品牌。&lt;/p&gt;
&lt;h2 id="bo-chong"&gt;播种&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;朵拉大丽花&lt;/strong&gt;：大丽花在 3 月中旬左右就可以种下了，大丽花的花期很长，从夏季持续到秋季，所以在天气暖和了之后早些种下也有利于大丽花蓄积养份，为盛开做好准备。收到大丽花的球根后，我提前泡了一天的水，剪去一些枯干的残根，然后确认茎的方向，茎向上埋入土中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;小甘菊&lt;/strong&gt;：播种温度 15～20℃，我在 4 月 10 日种下。使用育苗块播种，种子用手指小心按进湿润的泥土中，等待发芽长大后再移栽到盆中，一个花盆中种植 3 棵左右。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文森特向日葵&lt;/strong&gt;：播种温度 15～30℃，向日葵大约 2 个月就能开花，我打算在 4 月 20 日左右种下，希望恰好在夏至时能看到黄灿灿的向日葵。据店家的说明，向日葵的发芽率很高，不需要育苗，直接在盆中挖 5 mm 左右的小坑，将葵花籽横放埋入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;橙色番茄&lt;/strong&gt;：播种温度 15～30℃，可以把播种时间向后推一些，在 4 月至 5 月种下，也有人喜欢秋播，但不太适合我。我打算在 4 月 15 日左右种下，同样使用育苗块播种，长大后再移栽到盆中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;墨西哥小番茄&lt;/strong&gt;：与橙色番茄相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在春天也忙碌起来了，快快发芽吧 🌱&lt;/p&gt;
&lt;h2 id="zhang-dan"&gt;账单&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;价格（CNY）&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;球根与种子&lt;/td&gt;
&lt;td&gt;38.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;花盆（5 个）&lt;/td&gt;
&lt;td&gt;54.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;泥炭土（18 L）&lt;/td&gt;
&lt;td&gt;21.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;鸡粪有机肥&lt;/td&gt;
&lt;td&gt;3.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;育苗块 + 育苗盒&lt;/td&gt;
&lt;td&gt;11.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;总计（计运费）&lt;/td&gt;
&lt;td&gt;142.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="在路上"></category><category term="园艺"></category></entry><entry><title>文献总结｜DrugEx v3：使用基于图 Transformer 的强化学习进行以分子骨架为约束的药物设计</title><link href="https://leonis.cc/sui-sui-nian/2023-04-06-summary-doi.org/10.1186/s13321-023-00694-z.html" rel="alternate"></link><published>2023-04-06T00:00:00+08:00</published><updated>2023-04-06T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-06:/sui-sui-nian/2023-04-06-summary-doi.org/10.1186/s13321-023-00694-z.html</id><summary type="html">&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 DrugEx v3: scaffold‑constrained drug design with graph transformer‑based reinforcement learning，文章介绍了使用包括 Transformer 和 LSTM 模型实现以分子骨架为约束的药物设计的方法并对比了使用 SMILES 与图两种方式的分子表示在分子生成中的区别。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1186/s13321-023-00694-z" rel="noopener" target="_blank"&gt;doi.org/10.1186/s13321-023-00694-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 DrugEx v3: scaffold‑constrained drug design
with graph transformer‑based reinforcement learning，文章介绍了使用包括 Transformer 和 LSTM 模型实现以分子骨架为约束的药物设计的方法，并且对比了使用 SMILES 与图两种分子表示方式在分子生成中的区别。&lt;/p&gt;
&lt;p&gt;在先前的工作中，作者设计了名为 DrugEx 的 RNN 模型，它能够通过基于分布的方式探索化学空间并通过强化学习的策略实现基于目标的分子生成，但它无法接受用户的输入，无法基于先验知识给出结果，只能在已有的化学空间中给出结果，当任务改变后又需要重新训练模型，这些方面的问题使其在具体应用上具有很大的局限性。&lt;/p&gt;
&lt;p&gt;因此这篇文章使用多种深度学习模型重构了 DrugEx，DrugEx 可以接受用户指定的分子骨架生成具有目标结构的分子，并且在模型中引入强化学习策略，更加有效地控制生成分子的目标性质，此外文章对比了不同深度学习模型和以 SMILES 或图两种不同编码方式实现以分子骨架为约束的药物设计的效果。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8738?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8738?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;由来源于 ChEMBL 的约 170 万条分子数据构成 ChEMBL 数据集，用于预训练模型，由对人腺苷受体具有活性的 10828 条分子数据构成 LIGAND 数据集，用于微调生成模型。&lt;/p&gt;
&lt;p&gt;所有数据都构建为「输入- 输出」对的形式，使用 BRICS 规则将每个分子分割为最多 4 个的一系列片段，片段的排列组合就作为输入部分，被分割的分子就作为输出部分。分割完成后，用于预训练生成模型的分子对数据有 9335410 万条。&lt;/p&gt;
&lt;p&gt;若以 SMILES 作为分子表示，则是使用词表将每条 SMILES 序列分为若干 token，就可以将 SMILES 表示为 token 的索引序列，用于模型计算。&lt;/p&gt;
&lt;p&gt;若以图作为分子表示，首先需要计算分子的临接矩阵，接着每个分子都会被表示为具有 5 行的矩阵，前两行分别代表原子类型和化学键类型，第三行表示连接原子的索引，第四行表示目前原子的索引，第五行表示片段索引。按列连接 start、fragment、growing、end 和 linking 五个部分的上述五种信息，start 与 end 两部分分别具有一列，只有标记分隔的作用，fragment 部分中组织了各分子骨架中的原子信息，growing 部分组织了分子中去除分子骨架后剩余原子的信息，linking 部分组织了 fragment 与 growing 部分间相互连接的化学键信息。&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8739?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8739?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章中设计了 4 种模型用于完成分子生成成任务：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;图 Transformer&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;LSTM + 注意力机制&lt;/li&gt;
&lt;li&gt;序列 Transformer&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中分子的图表示只用于图 Transformer 模型中，其他三种模型都使用 SMILES 表示分子。&lt;/p&gt;
&lt;p&gt;由于图 Transformer 无法同时处理原子与化学键的信息，因此按下式组合原子和化学键索引：&lt;/p&gt;
&lt;div class="math"&gt;$$W=T_\mathrm{atom}\times 4+T_\mathrm{bond}$$&lt;/div&gt;
&lt;p&gt;通过将原子类型与化学键类型的总数相乘再加上化学键类型得到结果 &lt;span class="math"&gt;\(W\)&lt;/span&gt;，用于计算词向量。&lt;/p&gt;
&lt;p&gt;由于图 Transformer 处理的不是序列信息，原有的位置编码计算方式同样无法使用，文章设计了以下位置编码：&lt;/p&gt;
&lt;div class="math"&gt;$$P=I_\mathrm{atom}\times L_\mathrm{max}+I_\mathrm{connected}$$&lt;/div&gt;
&lt;p&gt;式中将当前原子索引 &lt;span class="math"&gt;\(I_\mathrm{atom}\)&lt;/span&gt; 与最大长度 &lt;span class="math"&gt;\(L_\mathrm{max}\)&lt;/span&gt; 相乘，然后再加上连接原子的索引 &lt;span class="math"&gt;\(I_\mathrm{connected}\)&lt;/span&gt; 得到位置编码。&lt;/p&gt;
&lt;h3 id="ping-gu-zhi-biao"&gt;评估指标&lt;/h3&gt;
&lt;p&gt;为了更好评估生成分子的多样性，除了常见的分子指标外，文章中还使用了 Solow Polasky measurement，由下式给出：&lt;/p&gt;
&lt;div class="math"&gt;$$I(A)=\frac{1}{|A|}\boldsymbol{e}^\mathrm{T}F(\boldsymbol{s})^{-1}\boldsymbol{e}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 表示分子数据集，&lt;span class="math"&gt;\(|A|\)&lt;/span&gt; 为数据集大小，&lt;span class="math"&gt;\(\boldsymbol{e}\)&lt;/span&gt; 为 &lt;span class="math"&gt;\(|A|\)&lt;/span&gt; 维元素全为 &lt;span class="math"&gt;\(1\)&lt;/span&gt; 的向量，&lt;span class="math"&gt;\(F(s)=[f(d_{ij})]\)&lt;/span&gt;，&lt;span class="math"&gt;\(f(d_{ij})\)&lt;/span&gt; 是表示每对分子间距离的函数：&lt;/p&gt;
&lt;div class="math"&gt;$$f(d)=\mathrm{e}^{-\theta d_{ij}}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 为常数，取 &lt;span class="math"&gt;\(\theta=0.5\)&lt;/span&gt;，&lt;span class="math"&gt;\(d_{ij}\)&lt;/span&gt; 为分子 &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(s_j\)&lt;/span&gt; 的分子指纹间的谷本距离。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun_1"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8751?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8751?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先分别使用 ChEMBL 数据集预训练四种模型，再用 LIGAND 数据集微调预训练的生成模型，使用测试集生成分子的结果如上表所示。&lt;/p&gt;
&lt;p&gt;同样使用 SMILES 表示分子，相比于 LSTM 模型，训练 Transformer 模型需要的计算资源更多，但训练时间更短而且效果更好。使用 SMILES 的模型在微调后表现有所上升，但还是差于使用图的模型，这主要是由于用图表示分子更容易获得原子的几何关系信息。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8752?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8752?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章绘制了图 Transformer 生成分子的降维结果，可以看出图 Transformer 生成的分子很好地覆盖了 ChEMBL 和 LIGAND 两个数据集的化学空间。在对生成分子进一步评估中发现，图 Transformer 生成分子的可合成性低于使用 SMILES 的模型，作者认为这是因为基于图的模型能够生成更复杂的结构，导致可合成性降低。&lt;/p&gt;
&lt;p&gt;文章总结了图 Transformer 的 4 点优势：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;局部尺度上的不变性：图 Transformer 能够很好地识别输入的分子骨架，并使输出的生成分子中具有相同的结构；&lt;/li&gt;
&lt;li&gt;全局尺度上的可扩展性：图 Transformer 在生成分子的过程中，可以将生成部分直接插入到表示图的矩阵中，具有很大灵活性；&lt;/li&gt;
&lt;li&gt;无语法约束：图 Transformer 不需要关注 SMILES 语法要求，模型不需要额外学习分子中的语法特征；&lt;/li&gt;
&lt;li&gt;可引入化学规则：可以在图 Transformer 中引入化学规则，例如价键匹配规则，提高生成分子的准确性。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后文章还在图 Transformer 中引入了强化学习的策略，模型能够生成对 A&lt;sub&gt;2A&lt;/sub&gt;AR 的亲合力和 QED 分数更高的分子。文章中输入模型的分子骨架与生成分子如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8753?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8753?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章重构了以前的 DrugEx 的模型，在对多种深度模型的试验中，图 Transformer 具有最好的分子生成效果。相比于 SMILES，分子的图表示在分子生成任务中可以更好地识别输入的分子结构，并且可以很容易地改造分子结构生成分子，这一点在发现先导化合物以及先导化合物的优化上都能发挥很大的作用。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>文献总结｜用于从蛋白序列进行药物设计的深度生成模型</title><link href="https://leonis.cc/sui-sui-nian/2023-04-01-summary-doi.org/10.1186/s13321-023-00702-2.html" rel="alternate"></link><published>2023-04-01T00:00:00+08:00</published><updated>2023-04-01T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-01:/sui-sui-nian/2023-04-01-summary-doi.org/10.1186/s13321-023-00702-2.html</id><summary type="html">&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Deep generative model for drug design from protein target sequence，文章设计了一种基于 GAN 的蛋白配体分子生成模型，该模型只需要获取氨基酸序列的信息就可以生成相应蛋白口袋的配体。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1186/s13321-023-00702-2" rel="noopener" target="_blank"&gt;doi.org/10.1186/s13321-023-00702-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Deep generative model for drug design from protein target sequence，文章设计了一种基于 GAN 的蛋白配体分子生成模型，该模型只需要获取氨基酸序列的信息就可以生成相应蛋白口袋的配体。&lt;/p&gt;
&lt;p&gt;目前的分子生成方法可以分为两类，其中一种是基于配体的分子生成（ligand-based molecule generation, LBMG），另一种是基于口袋的分子生成（pocketbased
molecule generation, PBMG）。LBMG 方法难以跳出目前的化空间，因而难以生成具有新颖结构的分子；PBMG 方法需要获取更多蛋白口袋的信息，但计算蛋白 3D 构象通常开销巨大。&lt;/p&gt;
&lt;p&gt;文章提出了一种输入蛋白序列即可获得配体的分子生成模型，称为 DeepTarget。DeepTarget 既不需要考虑蛋白口袋的构象信息，也不需要在特定的分子库上微调，有效避免了上述两种方法的局限。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用了来自于 ChEMBL 的分子-蛋白对数据，经数据清洗后，共得到 551223 个分子-蛋白对，涉及 1970 种蛋白质与 333399 种分子的 SMILES 序列。&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8733?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8733?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;DeepTarget 由 3 个部分构成，分别是氨基酸序列嵌入（Amino Acid Sequence Embedding, AASE）、结构特征推理（Structural Feature Inference, SFI）和分子生成（Molecule Generation, MG）。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;氨基酸序列嵌入：AASE 是模型的嵌入层，使用了 Transformer 的架构，主要用于将序列数据转化为模型计算并处理的特征向量。&lt;/li&gt;
&lt;li&gt;结构特征推理：SFI 部分采用了 GAN 的结构，其主要任务是在 AASE 中得到蛋白特征表示上加上一定的噪声，再通过多层感知机得到潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;分子生成：MG 部分使用了 LSTM 结构，是一个预训练的解码器，它将潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 解码为目标分子。该解码器是一个在 ChEMBL 大数据集上训练好的模型，能将分子的潜变量转换为相似的分子。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;SFI 中的 GAN 模型是 DeepTarget 生成蛋白配体的关键，GAN 由生成器与分别器两个模块构成，生成器从潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 生成分子，而分别器则识别生成分子与该蛋白口袋之间的关系，训练 GAN 就是让生成器不断生成分子，直至生成的分子可以「欺骗」分别器，也就是此时生成的分子满足该蛋白口袋的配体分布特征。在推理阶段，则将此生成分子的表示送入 MG 中得到分子的 SMILES 编码。&lt;/p&gt;
&lt;p&gt;传统的生成模型关注于生成器与生成结果之间的关系，而在文章所设计的任务中，不同的蛋白口袋与配体分子在化学空间中有着不同的分布，这也是影响分子生成的因素。因此文章引入了对比学习（Contrastive Learning, CL）的手段，将不同的蛋白作为标签而使分子分簇，在相应的化学空间中生成分子。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun_1"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8734?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8734?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章首先针对 DRD2 和 PARP1 两个蛋白的活性口袋生成分子。图 a 展示了对应的真实配体分子与生成分子的对接打分，生成分子相对于真实分子向打分更低方向偏移，说明具有更高的亲合力。&lt;/p&gt;
&lt;p&gt;图 b 挑出了生成分子中的代表分子，与训练集分子计算相似性，两个分子与训练集的相似性都在 0.2-0.6 左右，说明这两个分子与训练集分子存在一定差异，DeepTarget 能生成新颖的分子。&lt;/p&gt;
&lt;p&gt;图 c 展示了是否在模型中引入对比学习的了生成结果，使用对比学习策略模型生成的分子明显向打分更低处偏移，具有更好的效果。&lt;/p&gt;
&lt;p&gt;图 d 测试了模型的泛化能力，先在训练集中删去 DRD2 和 PARP1 两个蛋白的数据，将生成分子与先前生成的分子对比，从测试结果中可以看出，删除相应训练数据后，生成分子的对接打分上升，但还是生成了相当数量打分低于 -6 的分子，作者认为这可以说明 DeepTarget 能针对训练集中不存在的活性口袋生成配体分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8735?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8735?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章对 DeepTarget 生成的分子与其他模型针对这两个口袋生成的分子做了评估，结果如上表所示，DeepTarget 生成分子的 Valid 高于两个 GAN 模型，其他指标与其他模型相当。&lt;/p&gt;
&lt;p&gt;作者认为这些指标只能做为模型的参考，因为模型并没有针对生成分子的 Valid 和 Unique 等指标进行优化，DeepTarget 的目标更着重于生成与指定口袋真正具有相互作用的配体分子。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章设计了一种基于 GAN 的蛋白配体分子生成模型，模型只需要获取氨基酸序列的信息就可以生成相应蛋白口袋的配体，文章验证了生成分子具有较好的对接打分，并且模型表现出了一定的泛化能力，可用于针对训练集以外的蛋白生成配体。但文章中设计的模型评估实验有限，只针对两个靶点生成了分子，从文章中的数据来看，与其他模型相比该模型没有特别明显的优势。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GAN"></category></entry><entry><title>在明清小说中索隐：读《中国叙事学》</title><link href="https://leonis.cc/gu-zhi-dui/2023-03-30-book-chinese-narratology.html" rel="alternate"></link><published>2023-03-30T00:00:00+08:00</published><updated>2023-03-30T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-30:/gu-zhi-dui/2023-03-30-book-chinese-narratology.html</id><summary type="html">&lt;p&gt;浦安迪的《中国叙事学》是一本讨论中国叙事传统和明清小说的小书，页数并不多，不消四五天即可翻完。虽然这是一本学术著作，但语言流畅、分析丝丝入扣，读起来时并不觉得枯燥乏味，反而觉得酣畅淋漓。书题虽为《中国叙事学》，但全书中专门论述 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;浦安迪的《中国叙事学》是一本讨论中国叙事传统和明清小说的小书，页数并不多，不消四五天即可翻完。虽然这是一本学术著作，但语言流畅、分析丝丝入扣，读起来时并不觉得枯燥乏味，反而觉得酣畅淋漓。书题虽为《中国叙事学》，但全书中专门论述中国叙事的篇幅并不多，只是在全书的前面部分章节立起「中国叙事」的概念与分析方法，在后文中则全是基于这些概念与方法来分析中国的奇书文体，也就是范围更窄的明清小说，我在后文中即以「明清小说」这一更为通俗的文学类别指代书中所说的「奇书文体」。&lt;/p&gt;
&lt;p&gt;全书只有前一部分中才真正讨论了「中国叙事学」，但这一部分中不乏许多精要切当的论述，让人觉得豁然开朗；作者将其作为分析方法，一以贯之用于剖析明清小说，又深让人惊异于明清小说原来还能这么读，读完这本书后再去重读明清小说，相信又会有另一番收获。基于以上的原因，本文另起了一个标题&amp;mdash;&amp;mdash;「在明清小说中索隐」，我认为能够更准确地概括全书的主题。&lt;/p&gt;
&lt;div class="bookshelf"&gt;
&lt;div class="book"&gt;
&lt;img referrerpolicy="no-referrer" src="https://img2.doubanio.com/view/subject/s/public/s33747413.jpg"/&gt;
&lt;div class="infos"&gt;
&lt;a class="title" href="https://book.douban.com/subject/30244064/"&gt;中国叙事学&lt;/a&gt;
&lt;div class="作者"&gt;作者：[美] 浦安迪 (Andrew H. Plaks)&lt;/div&gt;
&lt;div class="出版社"&gt;出版社：北京大学出版社&lt;/div&gt;
&lt;div class="出版年"&gt;出版年：2018-8&lt;/div&gt;
&lt;div class="页数"&gt;页数：292&lt;/div&gt;
&lt;div class="定价"&gt;定价：45.00元&lt;/div&gt;
&lt;div class="ISBN"&gt;ISBN：9787301295960&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="he-wei-xu-shi"&gt;何为叙事&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;任何时代，任何地方，任何社会，都少不了叙述。它从远古时代就开始存在，古往今来，哪里有人，哪里就有叙述。&lt;/p&gt;
&amp;mdash;&amp;mdash;法国当代文论家 罗兰・巴特&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;书中认为文学作品是在传递某种人生本质，那么文学中的三大体式就也可以按传递人生本质的方式区别，即&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;抒情诗：直接描绘绘静态的人生本质，有叙述人但没有故事&lt;/li&gt;
&lt;li&gt;戏剧：关注人生矛盾，通过舞台传达人生本质，有场面、故事但没有叙述人&lt;/li&gt;
&lt;li&gt;叙事文：不直接描绘人生本质，而以传事为主要目标，从而展示延绵不断的经验流中的人生本质&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们不妨回忆我们曾经接触过的文学作品，一定是可以将大体的内容归置到上述的 3 个类别中去的。之所以说「大体的内容」，就是因为三种体式间不是完全孤立的，实际上三者相互交融渗透，抒情中存在着叙事，叙事中同样存在着抒情。这里用这种机械的分类有助于我们厘清全书讨论的对象&amp;mdash;&amp;mdash;那也就是叙事文，在三者对比之下，相信我们对「何为叙事」已经有了朦胧的答案。&lt;/p&gt;
&lt;h2 id="xu-shi-wen-xue-de-yuan-liu"&gt;叙事文学的源流&lt;/h2&gt;
&lt;p&gt;西方叙事文学的历史可以概括为「&lt;dot&gt;史诗（epic）&amp;rarr; 罗曼史（romance）&amp;rarr; 小说（novel）&lt;/dot&gt;」。&lt;/p&gt;
&lt;p&gt;而中国的古代传统文学则是一条「三百篇 - 骚 - 赋 - 乐府 - 律诗 - 词曲 - 小说」的发展脉络，也就是说，中国传统文学的重心是抒情。&lt;/p&gt;
&lt;p&gt;中国的叙事文学源头应当是《尚书》以及《左传》，作者认为二者深刻地影响了后世的叙事文学，并画下了一定的定式。二者都属于史文，而后世的虚构文学则是从六朝志怪发源，分化为「文言小说」和「白话小说」两大类别。&lt;/p&gt;
&lt;p&gt;作者强调，文言小说和白话小说泾渭分明，「杂录」「志怪」等文言小说又被称为「史余」，与史文的关系更为密切，书中提到纪昀将《山海经》等书从史部抽出，纳入小说，可为一证。而白话小说则可能是由民间说书的「通俗文学」发展而来（鲁迅等持此说），也可能是由当时文人所创作的「才子书」（作者持此说）。&lt;/p&gt;
&lt;p&gt;书中将中国的叙事文学历史总结为「&lt;dot&gt;神话 &amp;rarr; 史文 &amp;rarr; 明清奇书&lt;/dot&gt;」，正如在对 novel 的批评不得不在 epic 的传统中一样，在批评中国的明清奇书时也脱不开神话与史文对其的影响。&lt;/p&gt;
&lt;h2 id="shi-wen-de-ying-xiang"&gt;史文的影响&lt;/h2&gt;
&lt;p&gt;在介绍书中分析史文与神话的内容之前，我想先引用顾颉刚先生的观点奠定一个基调，那就是应当审慎地对待传世文献，未经检验的文献是不可信的。经常有人被民族主义裹挟，把「疑古」简单地想象为「否认一切历史」并言之凿凿地抨击顾颉刚先生，我只能对此深表遗憾。在面对浩如烟海的史料时，「疑古」才是去伪存真、还原历史的科学方法。&lt;/p&gt;
&lt;p&gt;书中的想法与顾颉刚先生的观点不谋而合，我不了解作者是否参考了顾颉刚先生的著作，如若不然，就是顾颉刚先生从历史研究的经验中总结出了「疑古」的观点，而作者从文学分析的角度告诉读者对史文的应当审慎，可以称得上合作。原书中的文字足够精彩，无需我的赘言，兹引录原文如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;中国的史书虽然力图给我们造成一种客观记载的感觉，但实际上不外乎一种美学上的幻觉，是用各种人为的方法和手段造成的「拟客观」效果。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;由于中国历代长期形成的对史近乎宗教的狂热崇拜，也由于在清亡以前史料永远只对史官开放的历史事实，中国正史叙述者总是摆出一副「全知全能者」的姿态；然而，这种全知全能却只是局限在冠冕堂皇的庙堂里。它的触角甚至伸不进皇家的后院，当然更难看见「处江湖之远」的草民百姓的众生相。一种纯客观的叙事幻觉由此产生，并且成为一种经久不坏的模式，从史官实录到虚构文本，横贯中国叙事的各种文体。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;西方的史诗原则上是虚构的艺术，只与历史传说有些微弱的关联；而中国的史文对于「虚构」和「实事」却从来就没有严格的分界线。西方文学理论家一般认为，历史讲实事，小说讲虚构。中国古代批评家则强调，「历史中有小说，小说中有历史」。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;西人重「模仿」，等于假定所讲述的一切都是出于虚构，中国人尚「传述」，等于宣称所述的一切都出于实事。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="shen-hua-de-ying-xiang"&gt;神话的影响&lt;/h2&gt;
&lt;p&gt;由史文上溯至远古神话，可以发现中国神话具有「人本位」倾向，也就是通常借神话来表人事，因而常常把历史与神话混作一团，譬如说我们都知道宙斯是虚构的，而禹究竟是神是人仍无定论，再例如我们都知道十个太阳是断无可能的，但后羿是远古的君主还是人格化的神呢？或许我们习于这些神话，在面对这样的疑难时会将其推作中国的神话过于琐碎繁杂而不成体系，但希腊神话何尝不零碎，却没有这种明显的「人本位」特征。&lt;/p&gt;
&lt;p&gt;再来考查中神话的叙事，在中国古代典籍中，对神话的具体情节都是话焉不详的，这可以说是&lt;dot&gt;在中国的美学原动力中缺乏一种要求「头身尾」连贯的结构原型&lt;/dot&gt;，这种「非叙述性」美学原型导致了中西叙事传统的分流。这两种不同的叙事传统在神话中表现为希腊神话以时间为轴心，故重过程而善于讲述故事；中国神话以空间为宗旨，故重本体而善于画图案。&lt;/p&gt;
&lt;p&gt;这里要注意，神话是诞生于远古时代的故事，但其叙述未并产生于远古时代，例如《淮南子》《庄子》中的神话显然不能用于作神话叙述的分析，所以作者更花精力着眼于例如《尚书》中的神话。&lt;/p&gt;
&lt;h2 id="zhong-guo-de-xu-shi"&gt;中国的叙事&lt;/h2&gt;
&lt;p&gt;在神话与史文的孳乳中，中国的叙事形成了不同于西方的叙事传统，这种叙事传统的影响绵延至明清小说乃至其后。中国的叙事传统中，普遍将重点放在「事隙」上，与西方恰恰相反，也就是真正的具有动作的「事」，都被诸如宴会等「无事之事」包围。&lt;/p&gt;
&lt;p&gt;至于为什么中国的叙事传统如此，没有形成或是拋弃了「叙述性」，甚至在远古神话中已经接受了了这一范式，作者认为这可能是因为中国人重礼的传统，因而中国人把诸如阴阳交替、四时更换等仪礼形式作为了一种基本原则，从而将其「空间化」，抹杀了其中的「时间性」。&lt;/p&gt;
&lt;h2 id="ming-qing-xiao-shuo-zhong-de-xu-shi"&gt;明清小说中的叙事&lt;/h2&gt;
&lt;p&gt;今人在读明清小说时，相当大部分人会因为叙事中的时间性或因果性不强感觉琐碎或是乏味，想必很多人在中学时代读四大名著时，除了《西游记》与《三国演义》以外都觉得昏昏欲睡，这很大程度上就是因为《西游记》具有「取经」这条真正的具有动作的「事」贯穿始终，《三国演义》的时间性则明显更强，就算书中没有明显地以时间为线索，但汉末三分天下终而三家归晋的历史在开卷之前已经为读者熟知。&lt;/p&gt;
&lt;p&gt;西人在读明清小说时亦有这种感受，他们用「缀段性」批评明清小说的叙事方法。缀段性来源于西方对诗歌的文学批评，亚里士多德曾说「缀段性的情节是所有情节中最坏的一种。我所谓的缀段性情节，是指前后毫无因果关系而串接成的情节。」&lt;/p&gt;
&lt;p&gt;但是我们要注意，这是批评西方诗歌的方法，将这一术语用于评价明清小说时已经陷入了西方视角。西方之所以贬斥缀段性，是因为西方文学中有重视「头身尾」结构完整性的传统，这种叙述完整性的要求下，缀段性显然是一种低格。但这种结构完整性的要求并不是金科玉律，我们将眼光放回到「叙事」这一行为上，就会发现叙事天然就带有一定缀段性的特征，因为叙事就是在处理人类经验的一个个片段单元，例如中国的史文，就是由史官将一长段的时间线的人类经验分割为一个个事件片段而创作出的巨制。&lt;/p&gt;
&lt;p&gt;明清小说在结构上的一个明显特征就是「百回」，古人刻意将一部小说分为百回或是百廿回本身就反映了一种中式的美感，一种中国人追求平衡的美学倾向。再者，明清小说的百回完帙在出版时又惯被分作十卷，这是今人难以注意到的细节，同时这也不是出于偶然。作者发现，每十回就能组成一个小故事，百回中的十个小故事又现出一种特殊的韵律感。&lt;/p&gt;
&lt;p&gt;若以《水浒传》为为例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;武（松）十回&lt;/li&gt;
&lt;li&gt;林（冲）十回&lt;/li&gt;
&lt;li&gt;宋（江）十回&lt;/li&gt;
&lt;li&gt;第七十二至八十二回：受招安&lt;/li&gt;
&lt;li&gt;第八十三至九十回：平辽&lt;/li&gt;
&lt;li&gt;第九十一至一百回：平田虎&lt;/li&gt;
&lt;li&gt;第一百一至一百十回：平王庆&lt;/li&gt;
&lt;li&gt;第一百十一至一百二十回：平方腊&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若以《三国演义》为例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一至九回：董卓传，至董卓身死&lt;/li&gt;
&lt;li&gt;第十至十九回：吕布传，至吕布战死白门楼&lt;/li&gt;
&lt;li&gt;第二十至三十一回：曹操传，至大破袁绍&lt;/li&gt;
&lt;li&gt;第三十二至三十八回：刘备传，至隆中对&lt;/li&gt;
&lt;li&gt;第三十九至五十回：诸葛亮传，至华容道&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在这些十回的小故事中，又有相应的重心，这个重心大约是在三四回左右，在全书之中丝毫不乱，作者也基于这些结构上的巧妙安排而认定明清小说属于独运匠心的「才子书」，绝不可能脱胎于民间说书人口中。&lt;/p&gt;
&lt;p&gt;再将全书的十回目故事缀合起来，对于全书而言还有一个结构上的经典范式，那就是「二十-六十-二十」的叙述程式。《金瓶梅》就是具有这种对称美的典型，在前二十回与后二十回中，书中都是在描写西门庆院墙外的故事，在中间六十回转入展开这间深宅大院中离合，同时前二十回叙述家中新添金、瓶、梅三小妾，奠定全书格局，后二十回则是西门庆死去，树倒猢孙散，这种对称的安排不能说不是作者的心思。&lt;/p&gt;
&lt;p&gt;作者还发现，明清小说中的另一个叙事特点就是情节的高潮在终点前就已经发生了，大约在全书四分之三位置处，同时全书又可以分成上下两截相互照应。以《三国演义》为例子最为简单，第五十回赤壁之战确定了天下三分的格局，在此全书分为两截，而在第七十八回曹操死去，全书自此高潮突然收束，一路走向下坡。《金瓶梅》要更为规整，以第四十九回为上下截的分水岭，上截描述西门庆升官发财、步步高升的经历，下截则是其春风得意而加速自毁的过程，同样在第七十九回死去，达到故事的最高潮。&lt;/p&gt;
&lt;p&gt;作者认为这种绵延不绝的故事布局是中国叙事独特的「转轮式」布局，故事的发展正像一个不断旋转的法轮，暗给人天道循环的感受。正如《三国演义》所要强调的「合久并分，分久并合」，故事开始于东汉末年的纷乱，结束于一统于晋，故事从刘、关、张等主人公手中交递给他们的后辈，正像长江后浪推前浪，呈现出一种无了局的形式，作者想表达的思想也暗含其中了。&lt;/p&gt;
&lt;p&gt;作者在书的最后部分上述方法分析了四大奇书中的中心思想，对于只接触教科书式解读的我，其分析方法与结果都可谓是别开生面，例如《西游记》不能简单理解为明末政治的黑暗，在一定程度上它还与明末的思想主流「心学」有关，书中「心猿」等等用语肯定另有所指，作者将其解读为告诫人们「诚意正心」。但部分观点有些冒进而难免令人觉得略显穿凿，但我认为作者只是提出了他的观点，真正怎样理解和解读明清小说还是依赖于读者。我相信在看过作者的一系列剖析后，再去重新读一读四大奇书，一定会有新的理解，这些新收获才是作者浦安迪撰写此书想要传达给我们的东西。&lt;/p&gt;</content><category term="故纸堆"></category><category term="阅读"></category><category term="文学"></category></entry><entry><title>文献总结｜通过连接感知模版挖掘实现从头生成分子</title><link href="https://leonis.cc/sui-sui-nian/2023-03-25-summary-doi.org/10.48550/arXiv.2302.01129.html" rel="alternate"></link><published>2023-03-25T00:00:00+08:00</published><updated>2023-03-25T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-25:/sui-sui-nian/2023-03-25-summary-doi.org/10.48550/arXiv.2302.01129.html</id><summary type="html">&lt;p&gt;本文介绍由中科大于 2023 年发布在 ICLR 2023 上的一篇文章，文章原标题为 De Novo Molecular Generation via Connection-aware Motif Mining，文章提出了一种从分子数据集中挖掘模版结构的算法，同时设计了一种通过组合模版结构实现分子生成的模型。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2302.01129" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2302.01129&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍由中科大于 2023 年发布在 ICLR 2023 上的一篇文章，文章原标题为 De Novo Molecular Generation via Connection-aware Motif Mining，文章提出了一种从分子数据集中挖掘模版结构的算法，同时设计了一种通过组合模版结构实现分子生成的模型 MiCaM（Mined Connection-aware Motifs）。&lt;/p&gt;
&lt;h2 id="suan-fa"&gt;算法&lt;/h2&gt;
&lt;p&gt;文章中提出的连接感知模版挖掘能够从数据集中构建模版词汇，用于后续的分子生成，该算法包括两个主要步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;合并操作（Merging-operation Learning Phase）&lt;/li&gt;
&lt;li&gt;构建模版词汇（Motif-vocabulary Construction Phase）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="算法" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8702?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="算法" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8702?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="he-bing-cao-zuo"&gt;合并操作&lt;/h3&gt;
&lt;p&gt;数据集 &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 中的每个分子都可以表示为图 &lt;span class="math"&gt;\(\mathcal{G(V,E)}\)&lt;/span&gt;，结点 &lt;span class="math"&gt;\(\mathcal{V}\)&lt;/span&gt; 表示原子，边 &lt;span class="math"&gt;\(\mathcal{E}\)&lt;/span&gt; 表示化学键。接着定义与之类似的 &lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M(\mathcal{V}^{(k)}_M,\mathcal{E}^{(k)}_M)\)&lt;/span&gt; 表示第 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 次合并后的分子，其中 &lt;span class="math"&gt;\(\mathcal{F}\in\mathcal{V}_M\)&lt;/span&gt; 表示分子的子结构，可以由一个原子构成，也可以由多个原子构成，那么边 &lt;span class="math"&gt;\(\mathcal{E}_M\)&lt;/span&gt; 相应就表示子结构之间的连接方式。显然，&lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M\)&lt;/span&gt; 由 &lt;span class="math"&gt;\(\mathcal{G}\)&lt;/span&gt; 初始化得到，也就是 &lt;span class="math"&gt;\(\mathcal{G}^{(0)}_M(\mathcal{V}^{(0)}_M,\mathcal{E}^{(0)}_M)=\mathcal{G(V,E)}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;接下来介绍由 &lt;span class="math"&gt;\(\mathcal{G}^{(0)}_M\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M\)&lt;/span&gt; 的合并操作，文章将操作「&lt;span class="math"&gt;\(\oplus\)&lt;/span&gt;」定义将子结构合并为新的子结构，即 &lt;span class="math"&gt;\(\mathcal{F}_{ij}=\mathcal{F}_i\oplus\mathcal{F}_j\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\mathcal{F}_{ij}\)&lt;/span&gt; 就包含了 &lt;span class="math"&gt;\(\mathcal{F}_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\mathcal{F}_j\)&lt;/span&gt; 中所有的结点与边。&lt;span class="math"&gt;\(\mathcal{G}^{(0)}_M\)&lt;/span&gt; 包含了边 &lt;span class="math"&gt;\(\mathcal{E}^{(0)}_M\)&lt;/span&gt;，边又连接了相应的子结构，所以对于通过边连接的子结构 &lt;span class="math"&gt;\((\mathcal{F}^{(0)}_i,\mathcal{F}^{(0)}_j)\in\mathcal{E}^{(0)}_M\)&lt;/span&gt; 计算 &lt;span class="math"&gt;\(\mathcal{F}^{(0)}_{ij}=\mathcal{F}^{(0)}_i\oplus\mathcal{F}^{(0)}_j\)&lt;/span&gt;，其中出现频率最高的 &lt;span class="math"&gt;\(\mathcal{F}^{(0)}_{ij}\)&lt;/span&gt; 就记作 &lt;span class="math"&gt;\(\mathcal{M}^{(0)}\)&lt;/span&gt;。再次遍历子结构 &lt;span class="math"&gt;\((\mathcal{F}^{(0)}_i,\mathcal{F}^{(0)}_j)\in\mathcal{E}^{(0)}_M\)&lt;/span&gt;，只要 &lt;span class="math"&gt;\(\mathcal{F}^{(0)}_i\oplus\mathcal{F}^{(0)}_j==\mathcal{M}^{(0)}\)&lt;/span&gt;，就将这两个子结构合并，完成后得到的所有新子结构就是 &lt;span class="math"&gt;\(\mathcal{V}^{(1)}_M\)&lt;/span&gt;，相应的新连接边就是 &lt;span class="math"&gt;\(\mathcal{E}^{(1)}_M\)&lt;/span&gt;，二者构成了经过 1 次合并的分子 &lt;span class="math"&gt;\(\mathcal{G}^{(1)}_M\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;以上合并操作可以推广，在第 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 次合并中，可以从分子 &lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M(\mathcal{V}^{(k)}_M,\mathcal{E}^{(k)}_M)\)&lt;/span&gt; 得到分子 &lt;span class="math"&gt;\(\mathcal{G}^{(k+1)}_M(\mathcal{V}^{(k+1)}_M,\mathcal{E}^{(k+1)}_M)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id="gou-jian-mo-ban-ci-hui"&gt;构建模版词汇&lt;/h3&gt;
&lt;p&gt;数据集中的所有分子在合并操作后都变为 &lt;span class="math"&gt;\(\mathcal{G}^{(K)}_M(\mathcal{V}^{(K)}_M,\mathcal{E}^{(K)}_M)\)&lt;/span&gt;，此时分子已经大大简化，将分子中的结点分割开来，并添加上连接位置的标记 &lt;code&gt;*&lt;/code&gt;，就得到了分子结构模版。&lt;/p&gt;
&lt;h2 id="mo-xing_1"&gt;模型&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="模型" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8703?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="模型" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8703?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MiCaM 通过多个逐渐组合模版结构的步骤生成分子，这样的组合有两种方式，一种是直接在连接位置上连上另一个模版结构，另一种是分子中的两个连接位置相连（成环）。&lt;/p&gt;
&lt;p&gt;将第 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 步得到的分子记作 &lt;span class="math"&gt;\(\mathcal{G}_t\)&lt;/span&gt;，用 &lt;span class="math"&gt;\(\mathcal{C}_{\mathcal{G}_t}\)&lt;/span&gt; 表示该分子的所有连接位置，使用序列 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt; 记录 &lt;span class="math"&gt;\(\mathcal{C}_{\mathcal{G}_t}\)&lt;/span&gt; 中所有连接位置的顺序。在第 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 步取出 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt; 的首个元素 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt;，也就是当前操作的连接位置，在该处连接或者成环后，就得到了新分子。新分子中可能具有新的连接位置，所以还使用 RDKit 更新序列 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt;。分子生成步骤就是不断重复以上过程，直至 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt; 为空，此时分子中的所有连接位置都被填满，就得到了输出分子。&lt;/p&gt;
&lt;p&gt;因为具有连接和成环两种组合方式，所以在每个生成步骤中还需要确定与当前操作的连接位置 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt; 相连的位置与子结构。&lt;/p&gt;
&lt;p&gt;文章为此设计了以下步骤，首先使用 GNN&lt;sub&gt;pmol&lt;/sub&gt; 编码当前步骤得到的分子 &lt;span class="math"&gt;\(\mathcal{G}_t\)&lt;/span&gt; 和当前操作的连接位置 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt;，分别得到相应的表示 &lt;span class="math"&gt;\(\boldsymbol{h}_{v_t}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{h}_{\mathcal{G}_t}\)&lt;/span&gt;，使用 GNN&lt;sub&gt;motif&lt;/sub&gt; 编码模版库中的所有结构模版，得到所有连接位置 &lt;span class="math"&gt;\(v\in\mathcal{C}_\mathrm{Vocab}\)&lt;/span&gt; 的表示 &lt;span class="math"&gt;\(\boldsymbol{h}_{v}\)&lt;/span&gt;。使用神经网络将相应的表示转换为 key 向量与 query 向量，寻找与 query 向量最相似的 key 向量，也就是根据以下概率选择与当前操作的连接位置 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt; 相连的位置：&lt;/p&gt;
&lt;div class="math"&gt;$$P_v=\mathop{\mathrm{softmax}}_{v\in\mathcal{C}_\mathrm{Vocab}\cup\mathcal{C}_{\mathcal{G}_t}\backslash\{v_t\}}(\mathrm{NN_{query}}([\boldsymbol{z},\boldsymbol{h}_{\mathcal{G}_t},\boldsymbol{h}_{v_t}])\cdot\mathrm{NN_{key}}(\boldsymbol{h}_v))$$&lt;/div&gt;
&lt;p&gt;若候选的连接位置 &lt;span class="math"&gt;\(v\in\mathcal{C}_\mathrm{Vocab}\)&lt;/span&gt;，则在模版库中取得相应的 &lt;span class="math"&gt;\(\mathcal{F}^*\)&lt;/span&gt;，并将其接入分子 &lt;span class="math"&gt;\(\mathcal{G}_t\)&lt;/span&gt;，完成连接；若 &lt;span class="math"&gt;\(v\in\mathcal{C}_{\mathcal{G}_t}\)&lt;/span&gt;，那么就合并 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(v\)&lt;/span&gt;，完成成环。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun"&gt;结果与讨论&lt;/h2&gt;
&lt;h3 id="fen-bu-xue-xi"&gt;分布学习&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8704?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8704?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章在 QM9、ZINC 和 GuacaMol 三个数据集上分别训练了 MiCaM，并使用 MiCaM 生成分子，检查生成的分子集是否接近训练集的分布。实验结果如上表所示，在 QM9 和 GuacaMol 数据集上，MiCaM 生成分子的 Uniqueness 和 Novelty 低于 MoLeR 和 GP-VAE 等模型，但在 KL Div 和 FCD score 上，MiCaM 完全优于其他模型。KL Div 与 FCD score 都衡量了生成分子集与训练集分布的相似程度，也就是说，MiCaM 生成的分子最接近训练集的分布。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8705?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8705?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在算法一节提到的参数 &lt;span class="math"&gt;\(K\)&lt;/span&gt; 决定了合并次数，文章在 QM9 数据集上测试了该参数对生成分子的影响。随着合并次数增加，KL Div 与 FCD score 都随之增加，而生成分子的 Novelty 却在下降。这是因为合并次数越多，分子简化程度越大，最后进入模版库中的模版结构也会更加复杂、更加接近训练集中的分子，最后使用这些模版结构构造的分子就会趋于与训练集分子「雷同」，这也一点程度解释了为什么 MiCaM 生成分子的 Uniqueness 和 Novelty 低于部分模型。&lt;/p&gt;
&lt;h3 id="die-dai-you-hua"&gt;迭代优化&lt;/h3&gt;
&lt;p&gt;文章还使用 MiCaM 进行了迭代目标增强（Iterative Target Augmentation, ITA）的分子生成。首先选出数据集中针对目标要求打分最高的 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个分子，接着使用模型在该训练集上微调并产生新分子，每次迭代过程中，在新分子与训练集中再选出打分最高的 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个分子，用作为下一次迭代的训练集。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8706?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8706?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在文章设计的 5 种任务中，MiCaM 优于其他所有模型。&lt;/p&gt;
&lt;p&gt;最后文章展示了 MiCaM 生成分子的过程，经过 5 个步骤，MiCaM 就能生成相当复杂的分子，同时分子的各性质分数也相应提高。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8707?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8707?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun_1"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种从分子数据中挖掘模版结构的算法，能够用于提取分子数据集中频繁出现的子结构并构建模版结构库，同时文章设计了基于该模版结构库的分子生成模型 MiCaM。MiCaM 通过逐步组合模版结构实现分子生成，很大程度解决了以往启发式分子生成随机性大、难以生成复杂结构的问题，使用模版结构组合生成的分子也更接近于现实中化学家对化合物改造的策略。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category><category term="VAE"></category></entry><entry><title>春日漫步小记</title><link href="https://leonis.cc/zai-lu-shang/2023-03-24-spring-trip-in-tianjin.html" rel="alternate"></link><published>2023-03-24T00:00:00+08:00</published><updated>2023-03-24T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-24:/zai-lu-shang/2023-03-24-spring-trip-in-tianjin.html</id><summary type="html">&lt;p&gt;五六天前刚起了几阵沙尘暴，入眼的一切都是黄蒙蒙的。黄沙中的行人都拉紧着衣服、戴着口罩，可能是为了防沙，也可能是为了防病毒。唯一不避风沙的只有两旁的行道树，经冬之后满是枯枝，更添了几分萧索。就是在这样一片了无生气之中，竟在 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;五六天前刚起了几阵沙尘暴，入眼的一切都是黄蒙蒙的。黄沙中的行人都拉紧着衣服、戴着口罩，可能是为了防沙，也可能是为了防病毒。唯一不避风沙的只有两旁的行道树，经冬之后满是枯枝，更添了几分萧索。就是在这样一片了无生气之中，竟在几天之中气温陡升，绽开了桃花，催绿了柳梢，赶在春分前酝酿好了春天的氛围。我想，也只有中原与华北的气候能如此依时令而行，不误物候，也只有在这时，看着次第开放的春花，才深感只有北方的花如此重诺而能被称为「花信」。&lt;/p&gt;
&lt;p&gt;虽是在尘埃满扑之中，但桃红柳绿的色彩也使春光亮丽了起来，现出了新鲜的气象。自新冠管控放松之后，趁着东风，京津间的通勤也恢复如常了，于是与北京的好友几人约在周末沿海河散散步。&lt;/p&gt;
&lt;p&gt;海河是天津最有风采的景致，虽说不如长江黄河之于中国，但若是失了海河，天津的趣味就要大打折扣。此行的起点就在三岔河口前的丁字沽，北运河、子牙河与海河在此交汇，形成「丁」形，因而得名「丁字沽」。朋友看着路边介绍京杭大运河的牌子，打趣这里该算天津的陆家嘴，运河虽是依旧却再难见到繁忙来往的船舶。&lt;/p&gt;
&lt;p&gt;丁字沽一带的桃花园与西沽公园是赏春的好去处，不用多言，其中最负盛名的自然是桃花。节临三月，占得春花头筹的就是桃花，连通往景点的路上也游人如织，非摩肩接朣不得过。也许是为了不打消游人赏花的兴致，原先告示的「预约入园」也成了君子协定，凡是到场即可入内，园中的人流便可以推测了。&lt;/p&gt;
&lt;p&gt;桃花园是北运河边的一条长堤，沿堤遍植桃树，因此没有「园」之实，称为桃花堤更为贴切。长堤上人头攒动，头顶上两边的桃树相互环合，拱成了一条花廊。游人在花廊下行走，而时不时几阵薰风吹过，就有几枚桃花脱了梢头，落入人流之中。人在花群中，花亦在人群中。&lt;/p&gt;
&lt;p&gt;花廊难以目力穷尽，只得顺人流向前。沿桃花堤前行，一旁的北运河也沿着桃花堤悠悠向前流淌。正当乱花迷眼而觉得乏味之时，桃花堤悄然南折，北运河也在此转而向南一路奔流汇注海河，我也在此转身向南。赫然映入眼中的是一株高三丈许的玉兰，玉兰的枝干上着满了白花，不杂一叶，开得是那样浓烈，温润洁白的玉兰浑如古玉，又是那样祥和。还不及细想缘何没有人驻足树下，才发觉前方悬挂着河北工业大学的匾额，同时提醒谢绝游人的参观。静静伫立在校园中，仅有的喧闹也只是汩汩的运河水与朗朗书声，悄悄花开花落，委心任乎去留，这株洁白的玉兰也变得更加洒脱起来。&lt;/p&gt;
&lt;p&gt;南折不远，便出了桃花堤，前方遥遥与之照应的是西沽公园。西沽公园与桃花堤不同，所占面积要大许多，所以尽管设置了巨大的广告牌招徕游客，园中也不至于像桃花堤那样拥挤。西沽公园中的游人更多是附近的居民，按天津人的闲致，家旁有如此山水湖林，自然免不了来此唱几句京戏，园中也得以檀板丝弦不绝于耳，平添了生活气息。&lt;/p&gt;
&lt;p&gt;午后日光稍晻，沿海河一路步行至旧意租界。随着渐渐临近春分，东风解冻了海河，虽没有鸭子来试试水温，但漂浮在河面上随波逐流的海鸥也提示着春水已暖。海河曾是繁忙的航道，如今常见的船只也只有观光轮渡，安静下来的海河于是成了海鸥休憩觅食的场所。每有渡船经过，稍谨慎的扑翅腾空而上，盘旋数周，待船只曳开的波澜平息后又落入水中，稍慵懒的也不动弹，宁随着艏波漂荡，一派春日融融的景象。&lt;/p&gt;
&lt;p&gt;与海河作别，进入旧意租界，眼见日头已经西颓，于是匆匆穿过条条欧式风情的街道寻觅晚餐，又匆匆送朋友往火车站，而后分别。一天的漫步果然让我收获了腰酸腿痛，但兴致、友人与良辰美景总是难以聚齐，难得能够这样共同游目聘怀，不仅幸甚，也算是不负如此韶光了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="桃花堤" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8688?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="桃花堤" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8688?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 游人如织的桃花堤&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="桃花堤" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8689?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="桃花堤" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8689?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="桃花堤" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8690?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="桃花堤" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8690?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="河北工业大学" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8691?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="河北工业大学" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8691?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 河工大中独自盛开的玉兰&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="海河" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8694?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="海河" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8694?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 海河中戏水的海鸥&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="海河" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8692?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="海河" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8692?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 与骑手相伴而飞&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="海河" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8693?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="海河" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8693?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="但丁广场" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8695?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="但丁广场" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8695?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 但丁握着哪部文集的稿子？&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="意式风情街" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8696?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="意式风情街" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8696?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 某无人居住的故居，窗前桃花已开，主人归未？&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="游记"></category><category term="摄影"></category></entry></feed>