<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Leo's blog</title><link href="https://leonis.cc/" rel="alternate"></link><link href="https://leonis.cc/feed.xml" rel="self"></link><id>https://leonis.cc/</id><updated>2023-05-13T00:00:00+08:00</updated><subtitle>A nook to hoard my manuscripts.</subtitle><entry><title>文献总结｜对编码器-解码器模型学习过程中化学结构识别的研究</title><link href="https://leonis.cc/sui-sui-nian/2023-05-13-summary-doi.org/10.1186/s13321-023-00713-z.html" rel="alternate"></link><published>2023-05-13T00:00:00+08:00</published><updated>2023-05-13T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-13:/sui-sui-nian/2023-05-13-summary-doi.org/10.1186/s13321-023-00713-z.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年东京大学发表在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Investigation of chemical structure recognition by encoder–decoder models in learning progress，文章，文章研究了编码器-解码器模型训练过程中对化学结构识别的过程以及将其潜变量作为分子表示用于下游任务的效果。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1186/s13321-023-00713-z" rel="noopener" target="_blank"&gt;doi.org/10.1186/s13321-023-00713-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年东京大学发表在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Investigation of chemical structure recognition by encoder&amp;ndash;decoder models in learning progress，文章，文章研究了编码器-解码器模型训练过程中对化学结构识别的过程以及将其潜变量作为分子表示用于下游任务的效果。&lt;/p&gt;
&lt;p&gt;基于结构的分子表示又被称为描述符，如何获得更好的描述符是化学信息学中很重要的问题。在近年兴起的深度学习领域，编码器-解码器（encoder-decoder, ED）类模型广受关注，以分子的字符序列 SMILES 作为输入，编码器模型会将其转化为一连串蕴含化学学信息的描述符，解码器模型通过该中间变量还原出原来的分子，这种由 SMILES（或其他）分子表示编码至隐空间中的潜变量就可以用作为数字形式的分子表示，用于各种下游任务，这也是自然语言处理中「预训练-微调」的范式。&lt;/p&gt;
&lt;p&gt;在传统方法中，也有例如 ECFP、NFP 一类的分子指纹，但他们只描述了分子中所具有的特定结构，无法由描述符再还原出分子结构。只有更好的表示才能在下游任务得到更好的结果，所以文章研究了在 ED 模型中化学结构的识别过程，ED 模型对化学结构的识别是指模型获得反映化学结构的数字信息和将该信息还原为化学结构的能力。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9036?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9036?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;模型的编码器部分是 3 层的 GRU，后面的全连接层将 GRU 的输出映射到 256 维的潜空间，解码器部分以该潜变量为输入，进入全连接层后同样是 3 层 GRU，输出还原的 SMILES。模型以输出 SMILES 与目标 SMILES 的交叉熵损失作为损失函数，训练模型的过程是使其能通过输入的随机化 SMILES 输出标准 SMILES。&lt;/p&gt;
&lt;p&gt;为了评估将 ED 模型中潜变量作为分子表示的效果，文章使用 ToxCast 中 113 个任务分别训练了 XGBoost，使用其预测结果（即下游任务结果）判断分子表示的优劣。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;模型所使用的训练数据来源于 ZINC15，从中随机获取了 3 千万个分子，后续通过去除非有机物常见原子、去除重原子等方式清洗数据。&lt;/p&gt;
&lt;h3 id="zhi-biao"&gt;指标&lt;/h3&gt;
&lt;p&gt;文章定义了两个指标用于评估 ED 模型的准确率，完全准确率定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{perfect\ accuracy}=\frac 1n\sum^n_iI(t=p)$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 表示正确的 SMILES，&lt;span class="math"&gt;\(p\)&lt;/span&gt; 表示预测的 SMILES，即计算与标签值相同的输出所占比例。&lt;/p&gt;
&lt;p&gt;另一个指标部分准确率定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{partial\ accuracy}=\frac 1n\sum^n_i\left\{\frac{1}{\max(l(t),l(p))}\sum^{\min(l(t),l(p))}_jI(t_i=p_i)\right\}$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(t_i\)&lt;/span&gt; 表示 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 位置上正确的 SMILES 字符，&lt;span class="math"&gt;\(p_i\)&lt;/span&gt; 表示 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 位置上预测的 SMILES 字符，该式即计算所有 SMILES 字符中，预测结果与标签值相同位置上相同的字符所占比例。&lt;/p&gt;
&lt;p&gt;在 XGBoost 模型中，使用 ROC 曲线下面积（AUROC）与 Matthews 相关系数（MCC）评估模型准确率。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9035?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9035?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章首先分别使用 10 k、100 k、1 M 的测诫集测试了训练过程中的 ED 模型，结果如上图所示。随着训练轮次的增加，模型还原出化学结构的准确率也在上升，对比部分准确率与完全准确率，可以发现在相同时刻下，部分准确率要高于完全准确率，这说明模型先学习到了还原出分子中若干个字符组成的小片段，然后才将其拼合还原出整个分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9038?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9038?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;以训练后的还原准确率命名模型，例如 94% 的模型记为「Model_94」，测试各模型的分子表示在 ToxCast 任务上的效果，结果如上图（a）所示。可以看出，除了未经训练完全无法还原出分子的 Model_0，其他模型都能较好地完成分类任务，而且分类效果比较接近。接着文章选定了三类结构的分子，使用 UMAP 降维的方法绘制出了其分子表示在化学空间中的位置，如上图（b）所示，在 Model_0 中，三种结构混杂在一起，很难完成分类，而 ED 模型只需要经过训练，三种结构就区别开来，与前一个实验的结果也吻合，这说明 ED 模型生成的潜变量很适用于分子分类的任务。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9037?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9037?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章比较了训练过程中还原出分子的分子量与 SMILES 长度的准确关系，在上图中只有在黄线上的样本表明还原分子与标签值一致，可以看出虽然 DE 模型在训练的靠前阶段无法还原出分子的特定性质，但其潜变量在化学空间中已经有了区分度，这种区分度足可以完成分类任务，但其所蕴含的化学信息还不能使模型还原出结构。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章研究了 DE 模型训练过程中对化学结构的识别以及将其中的潜变量作为分子表示的效果，文章的实验结果展示了模型训练过程中分子表示的变化，证明了将其作为下游任务的分子表示的可行性。&lt;/p&gt;
&lt;p&gt;在目前，预训练-微调是广泛使用的模型训练范式，而在化学信息学领域，其中的关键步骤，也就是将潜变量作为分子表示尚缺乏研究。文章研究的是较早的 GRU（RNN）所构成的 DE 模型，还应该对 Transformer、GPT 等目前更广泛使用的模型进行潜变量的深入研究。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="RNN"></category></entry><entry><title>文献总结｜我们能用 Transformer 模型快速学会「翻译」活性分子吗？</title><link href="https://leonis.cc/sui-sui-nian/2023-04-28-summary-doi.org/10.1021/acs.jcim.2c01618.html" rel="alternate"></link><published>2023-04-28T00:00:00+08:00</published><updated>2023-04-28T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-28:/sui-sui-nian/2023-04-28-summary-doi.org/10.1021/acs.jcim.2c01618.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Can We Quickly Learn to “Translate” Bioactive Molecules with Transformer Models? 文章使用 MMP 数据训练 Transformer，使其生成具有活性的分子，文章结果表明 Transformer 对于未知靶点也能生成活性分子。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/acs.jcim.2c01618" rel="noopener" target="_blank"&gt;doi.org/10.1021/acs.jcim.2c01618&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Can We Quickly Learn to &amp;ldquo;Translate&amp;rdquo; Bioactive Molecules with Transformer Models? 文章使用 MMP 数据训练 Transformer，使其生成具有活性的分子，文章结果表明 Transformer 对于未知靶点也能生成活性分子。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章所使用的分子数据来源于 ChEMBL 29，包含有 950640 个分子。数据集中的分子都由 SMILES 表示，将其输入 MMP 软件匹配其中的相似分子。输出的数据中，每对数据都由两个 SMILES 构成，形成一个 MMP 对，用 SMIRK 表示两个 SMILES 间的化学转化，最后得到了约 5700 万条 MMP 对。&lt;/p&gt;
&lt;p&gt;接着文章对得到的 MMP 对数据进一步清洗，主要包括两个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;排除 SMIRK 出现次数少于 &lt;span class="math"&gt;\(N_1\)&lt;/span&gt; 的 MMP 对；&lt;/li&gt;
&lt;li&gt;在剩余的 MMP 对中，随机保留 &lt;span class="math"&gt;\(N_2\)&lt;/span&gt; 条数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第 1 步的清洗是为了除去数据中出现频率过少的化学转化，因为它们过于特殊，并不能普适地用于所有分子；第 2 步是为了避免数据中的极端偏向影响模型，因为在数据集中，简单的转化（如 -H &amp;rarr; -CH&lt;sub&gt;3&lt;/sub&gt;）出现频率极高，这会导致模型无法学习到那些复杂的转化。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8914?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8914?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;文章使用 OpenNMT 构建 Transformer 模型，在 SMILES 数据输入模型前，都将其转化为 SELFIES 形式。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8917?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8917?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在训练过程中，使用 OpenNMT 中默认的损失函数，使用困惑度（perplexity）评估模型训练效果。困惑度是自然语言处理中所使用的评估指标，它定义为 &lt;span class="math"&gt;\(ppl=\exp(L/N)\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(L\)&lt;/span&gt; 为损失函数，&lt;span class="math"&gt;\(N\)&lt;/span&gt; 为全部备选的 token 数量。在自然语言处理中，模型根据前一个 token 预测下一个 token，并将其连成句子，困惑度的意义就是模型在获取前一个 token 的情况下，概率较高的下一个 token 的数量，所以困惑度越小时，表明模型能在目前分布下生成更合理的句子。同样，将其应用于分子生成模型，困惑度就可以表示分枝原子上可以备选的连接原子。&lt;/p&gt;
&lt;p&gt;此外，文章测试了模型对未知靶点生成分子的效果。在上一步得到的数据中，分别除去对 COX2、DRD2 或 HERG 有活性的分子，分别用三种数据训练 Transformer，最后得到的各模型对指定靶点「不可知」。文章又将去除掉的活性分子根据活性大小分为前 5% 与后 95% 的子数据集，用后 95% 作为模型输入，测试模型是否能输出前 5% 的分子。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;h4&gt;Transformer 可以为未知靶点生成活性分子&lt;/h4&gt;
&lt;p&gt;分别从训练数据中除去 COX2、DRD2 或 HERG 的活性分子，训练了 3 个 Transformer 模型，模型无法得到关于特点靶点的信息。将对相应靶点具有活性的后 95% 分子作为输入，模型生成的分子不仅满足相应的化学规则，而且相当数量的活性分子，说明模型具有相当好的泛化能力。生成分子的结果如下图所示，横轴表示分子与前 5% 高活性分子的相似性，竖轴表示分子活性，其中蓝色圆点表示生成的分子，红色菱形表示输入模型的分子。图中只展示了一部分输入-输出的变换，用箭头表示，可以看出许多生成的分子都是往右上方向移动，也就是生成活性更高、与高活性分子更相似的结构。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8916?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8916?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h4&gt;Transformer 可以为苗头化合物发现生成新颖分子&lt;/h4&gt;
&lt;p&gt;文章统计了训练集与输入-输出分子中的 SMIRKS 化学转化，在模型生成的结果中有 1086 种化转化并未出现在训练集中，所以文章认为模型可以生成新颖的化学结构。一部分化学转化如下图所示，训练集中的化学转化都是 MMP 转化，而图中的化学换化显然并不符合规则，而是模型根据训练集中的信息所新造的化学转化。&lt;/p&gt;
&lt;p&gt;文章认为使用深度学习实现分子生成并不是将所有可能的 SMIRKS 规则放到输入分子上，这种排列组合的模式势必会大大增加生成的数据量，将有价值的信息淹没。Transformer 能够上下文相关地获取分子信息，并根据信息通过合适的 SMIRKS 规则构建新分子，这种分子生成的手段更有帮助，同时文章发现 Transformer 所使用的 SMIRKS 并不是完全照搬训练集中的数据，而是根据已有信息新构造出的转化规则，这一点可能也是提升 Transformer 分子生成效果的方向。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8915?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8915?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章还认为，相比于更常用的 SMILES，SELFIES 更有利于 Transformer 学习其中的化学信息，因为 SMILES 无法保证分子合法，必须先训练模型使其学习生成合乎规则的分子。相反，使用 SELFIES 的分子生成模型不需要先让模型学会表示分子的语法，极少出现分子不合法的情况，可能使用 SELFIES 表示分子也是能实现文章实验中效果的重要原因。&lt;/p&gt;
&lt;p&gt;文章也指出这只是一个尝试性的工作，还有很多的问题没有解决。首要的一点就是文章使用与活性分子的相似性来评价生成分子的效果，尽管两个分子十分相似，它们也可能具有十分悬殊的活性，这是使用深度学习手段进行药物发现尤需解决的问题。所以文章中的分子生成也只能起到「启发」的作用，并不能真接指导药物化学家找到活性更高的分子。另一点就是文章中并没有对模型做全面的评估与超参数的选择，只是验证的方法的可行性，并没有对比与其他模型的优势。文章中还推测 SELFIES 相比 SMILES 更具有优势，但也未对比两种模型的效果。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章将 Transformer 用于苗头化合物的发现，并且发现 Transformer 对于训练集中不存在的未知靶点也能生成相当数量的活性分子，其中一部分分子与高活性的配体具有很高的相似性。文章还发现，Transformer 生成的结果中，其化学变化并不局限于用于训练的 MMP 数据，这表明 Transformer 具有上下文相关的信息识别能力，利用好这一特性有利于实现活性分子的生成。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>津城海棠</title><link href="https://leonis.cc/zai-lu-shang/2023-04-27-crabapple-of-tianjin.html" rel="alternate"></link><published>2023-04-27T00:00:00+08:00</published><updated>2023-04-27T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-27:/zai-lu-shang/2023-04-27-crabapple-of-tianjin.html</id><summary type="html">&lt;p&gt;五大道的海棠花开了，天津又到了最美的时候。&lt;/p&gt;
&lt;p&gt;几年前初到天津时，给我印象最深刻的就是城中随处可见的海棠花。天津遍植海棠，却不是密密地栽种为若干排。而是漫步在街头时，走过几个街道，转过几个巷口，蓦地发现几枝洁白而间杂洋红的海 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;五大道的海棠花开了，天津又到了最美的时候。&lt;/p&gt;
&lt;p&gt;几年前初到天津时，给我印象最深刻的就是城中随处可见的海棠花。天津遍植海棠，却不是密密地栽种为若干排。而是漫步在街头时，走过几个街道，转过几个巷口，蓦地发现几枝洁白而间杂洋红的海棠花在那静静地开放，而无意间的拜访者也成了唯一的赏花者，独享这春光。至于如庆王府的几处，大概是因为人们喜爱海棠，几代经营之下竟植海棠如林。阳春三月，津城就被海棠花环抱，绝不是一句虚言。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8898?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8898?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;我在故乡不曾见过这样的景象，倒不是没有海棠，却没有天津的海棠这般而引人重视。我一度误把海棠当作天津的市花，我愿意把这一切都推作是天津与海棠有缘。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8906?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8906?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 向着窗口盛开的海棠&lt;/p&gt;
&lt;p&gt;我与朋友晃悠悠地骑着单车往五大道拜访海棠，过桂林路至大理道&amp;mdash;&amp;mdash;好一趟西南之行。天津与上海相似，在收回各国租界后便以国内诸大小城市为道路名，因而也成了一大特色。想必不少来天津的游人都玩过寻找家乡路牌的游戏，冥冥之中建立起了一种远跨大半个中国的联系。看着大理道旁盛开的海棠，万里之遥的大理也为这春景增色不少吧。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8900?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8900?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 大理道上的海棠花&lt;/p&gt;
&lt;p&gt;不及步入五大道，已经能望见一批批游人往前走去。天津市也乐于营造海棠的氛围，工作人员用围挡临时封闭了路段，仅允游人入内。但这并不意味着五大道内有多少宽敞余裕的空间，也休想从容地漫步海棠花下。汹涌的人潮占满了可及之处，我甚至难以看清路面，却能依人群铺开的形状分辨出街道巷衢来。盛放的海棠也不愿与赏花者示弱，两列海棠一字排开，树冠张得极大，好似擎不起满树的海棠花，一树压着一树；树上的海棠也开得极密，一簇压着一簇，全开的压着半开的，半开的压着未开的，组成了繁盛的花群。人群中的人伸着脖子望花群，花群中的花也伸着叶梗看人群。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8899?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8899?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 游人熙熙攘攘地往五大道走去&lt;/p&gt;
&lt;p&gt;可花枝柔弱，阵风吹过，花瓣就零落而被阵风裹至空中，引起人群中阵阵讶叹声&amp;mdash;&amp;mdash;看来这场对决还是人群占了上风。摇落的花瓣混入风中，风顿时有了形状，它们翻腾、盘旋，又扶摇而上，在高处击散而化作花雨，洒向一个个抬头瞪大着眼、惊异得还不及合拢嘴的游人身边。风顿时也有了色彩，大片的雪白与点点嫣红在空中交织，它不像颜料盘中两种颜色相混时那样柔和得出现游丝，而是随着花瓣的翻飞，白红两种颜色相互变换，像极了文人所用的花草笺纸。在阳光的照耀下，空中的花瓣闪出熠熠的光芒，看来这笺纸还多了一道洒金的工艺。风顿时还有了气味，风摘落的花瓣被送至每个人的鼻前，不似桃花那样甜腻，芬芳中更多了几分清爽，虽说海棠将所有芳华都留给了春季，但这种气味总能让我想起初夏的时节。据佛经上说，维摩诘讲经讲到妙处时，有天女散花，赞叹其智慧。这花决计不是海棠，这样色、香、味齐全的海棠岂不正犯佛家所说的「五贼」。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8901?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8901?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 来不及拍下风吹下的海棠花雨，捕捉到几点如霰般翻飞的花瓣&lt;/p&gt;
&lt;p&gt;花雨过后，地上自然也铺满了海棠。明人张岱曾描述他书屋中的海棠「花时积三尺香雪」，诚不诬也，果然处处都积满了如雪般的海棠花瓣。有些店家自然不愿意放过这样的景色，径在店门外摆出桌子招徕顾客。更有好事者撤去桌上的阳伞，恣意在花雨之中，任那些海棠落在盘中的糕点之上，落入茶水之中。虽说道上行人熙熙攘攘，但啜茶人亦可闹中取静，其风雅也如此，丝毫不下于古人。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8909?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8909?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 道上也有卖各种物件的小摊，也落满了海棠花&lt;/p&gt;
&lt;p&gt;我被裹挟在人群中，亦步亦趋地穿过海棠花海，流连的景色如万花筒般绚丽，而正所谓兴尽悲来，又深令人感慨春光易逝。天津人自小便生活在海棠之中，求学于海棠花下，他们是惯看了秋月春风而变得更为冷峻，还是年年见证着花朝花暮，相比他人更多了几分细心肠呢？传闻天津人大多缱绻于故乡，而他们的确是如浮萍一般的移民，终于定居在此，与繁茂的海棠杂处而居，其念兹在兹的心情亦可想而知。我不禁想象，奔走于异乡的游子是否会种下一株海棠，欣赏着它与津城的海棠同样开谢，正与记忆中同。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8908?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8908?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 俨然是粉红色的世界&lt;/p&gt;
&lt;p&gt;归来搦管撰写此文时，已经将近立夏，海棠或许早已落尽。清少纳言曾盛赞落花之后赏花为风雅之事，我虽未有此意，奈何已蹈古人之迹。寓居天津多年，我同样遥望着津城海棠的开谢，正与我的记忆同！&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8907?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8907?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="摄影"></category></entry><entry><title>文献总结｜使用上下文增强的分子表示提升少样本药物发现的效果</title><link href="https://leonis.cc/sui-sui-nian/2023-04-22-summary-openreview.html" rel="alternate"></link><published>2023-04-22T00:00:00+08:00</published><updated>2023-04-22T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-22:/sui-sui-nian/2023-04-22-summary-openreview.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年发表在 ICLR 2023 上的一篇文章，文章原标题为 Context-enriched molecule representations improve few-shot drug discovery，文章介绍了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 通过 Hopfield 网络用上下文数据集少样本的强化分子表示，提升了分子性质预测的准确度。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://openreview.net/forum?id=XrMWUuEevr" rel="noopener" target="_blank"&gt;OpenReview&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年发表在 ICLR 2023 上的一篇文章，文章原标题为 Context-enriched molecule representations improve few-shot drug discovery，文章介绍了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 通过 Hopfield 网络用上下文数据集少样本的强化分子表示，提升了分子性质预测的准确度。&lt;/p&gt;
&lt;p&gt;深度学习已经成为了药物发现中的重要工具，但目前大部分深度学习方法都是通过大训练集获得分子信息。药物发现中的深度学习方法通常需要大量的生物试验数据，这在实际的药物研发过程中很难获取。少样本学习解决了药物发现中有效数据较少的问题，少样本学习主要有 3 种方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于数据增强的方法（Data-augmentation based approaches）：变换已有数据达到增加数据量的目的。&lt;/li&gt;
&lt;li&gt;基于词嵌入与最近邻的方法（Embedding-based and nearest neighbour approaches learn approaches）：学习词嵌入的空间，从已有数据邻近位置取得新数据（相似分子）。&lt;/li&gt;
&lt;li&gt;基于优化和微调的方法（Optimization-based or fine-tuning methods）：将大规模的预训练模型放在已有数据上微调，使其迁移到新的化学空间。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文章提出了一种新的 MHNfs 模型用于少样本的药物发现，模型使用联想记忆来提取原始数据中的共现和协变结构从而强化其分子表示，在少样本数据集 FS-Mol 上达到了最佳效果。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="yuan-li"&gt;原理&lt;/h3&gt;
&lt;p&gt;药物发现中所使用的模型 &lt;span class="math"&gt;\(g(\boldsymbol{m})\)&lt;/span&gt; 用于在给定分子表示 &lt;span class="math"&gt;\(\boldsymbol{m}\in\mathcal{M}\)&lt;/span&gt; 的情况下预测分子性质或活性 &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;。深度学习模型中的分子编码器将分子的一些低级表示（如 SMILES、分子图等）映射为模型空间的表示 &lt;span class="math"&gt;\(f^\mathrm{ME}:\mathcal{M}\rightarrow\mathbb{R}^d\)&lt;/span&gt;，再通过后续计算给出分子性质。&lt;/p&gt;
&lt;p&gt;在少样本的情况下，只有分子的小数据集 &lt;span class="math"&gt;\(\{\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N\}\)&lt;/span&gt; 与对应分子是否具有活性的数据 &lt;span class="math"&gt;\(\boldsymbol{y}=\{y_1,\cdots,y_N\}\)&lt;/span&gt;。这里将数据集 &lt;span class="math"&gt;\(\{(\boldsymbol{x}_n,y_n)\}_{n=1}^N\)&lt;/span&gt; 称为 support set，少样本学习就是要正确预测不在 support set 中 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 所对应的 &lt;span class="math"&gt;\(y\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;文章中的模型分为 3 个模块：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    \text{context module: }&amp;amp;\quad&amp;amp;\boldsymbol{m}'&amp;amp;=f^\mathrm{CM}(\boldsymbol{m},\boldsymbol{C})\\
    &amp;amp;\quad&amp;amp;\boldsymbol{X}'&amp;amp;=f^\mathrm{CM}(\boldsymbol{X},\boldsymbol{C})\\
    \text{cross-attention module: }&amp;amp;\quad&amp;amp;[\boldsymbol{m}'',\boldsymbol{X}'']&amp;amp;=f^\mathrm{CAM}([\boldsymbol{m}',\boldsymbol{X}'])\\
    \text{similarity module: }&amp;amp;\quad&amp;amp;\hat{y}&amp;amp;=f^\mathrm{SM}(\boldsymbol{m}'',\boldsymbol{X}'',\boldsymbol{y})
\end{align}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{m}\in\mathbb{R}^d\)&lt;/span&gt; 是分子的词嵌入表示，&lt;span class="math"&gt;\(\boldsymbol{X}\in\mathbb{R}^{d\times N}\)&lt;/span&gt; 是 support set 中分子的词嵌入表示，&lt;span class="math"&gt;\(\boldsymbol{C}\in\mathbb{R}^{d\times M}\)&lt;/span&gt; 是另一个更大的分子数据集（context set）中分子的词嵌入表示。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f^\mathrm{CM}\)&lt;/span&gt; 交换 &lt;span class="math"&gt;\((\boldsymbol{m},\boldsymbol{C})\)&lt;/span&gt; 间与 &lt;span class="math"&gt;\((\boldsymbol{X},\boldsymbol{C})\)&lt;/span&gt; 间的上下文信息，得到强化的表示 &lt;span class="math"&gt;\(\boldsymbol{m}'\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}'\)&lt;/span&gt;。拼合两个增强的表示，&lt;span class="math"&gt;\(f^\mathrm{CAM}\)&lt;/span&gt; 计算两者间注意力，得到进一步增强的 &lt;span class="math"&gt;\(\boldsymbol{m}''\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}''\)&lt;/span&gt;，最后结合二者的信息进行预测。上面的过程可以描述成&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    &amp;amp;\underset{\textsf{symbolic or}\atop\textsf{low-level repr.}}{m}\overset{f^\mathrm{ME}}{\longrightarrow}\underset{\textsf{molecule}\atop\textsf{embedding}}{\boldsymbol{m}}\overset{f^\mathrm{CM}}{\longrightarrow}\underset{\textsf{context}\atop\textsf{repr.}}{\boldsymbol{m}'}\overset{f^\mathrm{CAM}}{\longrightarrow}\underset{\textsf{similarity}\atop\textsf{repr.}}{\boldsymbol{m}''}\\
    &amp;amp;\underset{\textsf{symbolic or}\atop\textsf{low-level repr.}}{x_n}\overset{f^\mathrm{ME}}{\longrightarrow}\underset{\textsf{molecule}\atop\textsf{embedding}}{\boldsymbol{x}_n}\overset{f^\mathrm{CM}}{\longrightarrow}\underset{\textsf{context}\atop\textsf{repr.}}{\boldsymbol{x}_n'}\overset{f^\mathrm{CAM}}{\longrightarrow}\underset{\textsf{similarity}\atop\textsf{repr.}}{\boldsymbol{x}_n''}
\end{align}
$$&lt;/div&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8889?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8889?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MHNfs 由 Transformer 中的 encoder 部分构建，具有与 Transformer 类似的结构与工作方式。&lt;/p&gt;
&lt;p&gt;模型中的上下文模块由现代 Hopfield 网络（Modern Hopfield Network, MHN）实现：&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{Hopfield}(\boldsymbol{\Xi},\boldsymbol{C}):=(\boldsymbol{W}_E\boldsymbol{C})\mathrm{Softmax}\left(\beta(\boldsymbol{W}_C\boldsymbol{C})^\top(\boldsymbol{W}_\Xi\boldsymbol{\Xi})\right)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{m}'=\mathrm{Hopfield(\boldsymbol{m},\boldsymbol{C})},\quad\boldsymbol{X}'=\mathrm{Hopfield}(\boldsymbol{X},\boldsymbol{C})
$$&lt;/div&gt;
&lt;p&gt;MHN 能够计算两个输入间的注意力，最后更新得到的分子表示就具有参考分子集 &lt;span class="math"&gt;\(\boldsymbol{C}\)&lt;/span&gt; 中的联想记忆。&lt;/p&gt;
&lt;p&gt;交叉注意力模块替换了原来 Transformer 中的多头注意力机制，但功能仍然类似，用于记算输入分子 &lt;span class="math"&gt;\(\boldsymbol{m}'\)&lt;/span&gt; 与 support set &lt;span class="math"&gt;\(\boldsymbol{X}'\)&lt;/span&gt; 之间的注意力，再次更新分子表示：&lt;/p&gt;
&lt;div class="math"&gt;$$[\boldsymbol{m}'',\boldsymbol{X}'']=\mathrm{Hopfield}([\boldsymbol{m}',\boldsymbol{X}'],[\boldsymbol{m}',\boldsymbol{X}'])$$&lt;/div&gt;
&lt;p&gt;在最后的相似性模块中，模型计算输入分子 &lt;span class="math"&gt;\(\boldsymbol{m}''\)&lt;/span&gt; 与 support set &lt;span class="math"&gt;\(\boldsymbol{X}''\)&lt;/span&gt; 中每个分子 &lt;span class="math"&gt;\(\boldsymbol{x}_n''\)&lt;/span&gt; 之间的相似性 &lt;span class="math"&gt;\(k(\boldsymbol{m}'',\boldsymbol{x}_n'')\)&lt;/span&gt;，并使用所有相似性的加权平均表示输入分子，用该表示计算输入分子的性质：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}=\mathrm{Sigmoid}\left(\tau^{-1}\frac 1N\sum_{n=1}^Ny_n'k(\boldsymbol{m}'',\boldsymbol{x}_n'')\right)$$&lt;/div&gt;
&lt;p&gt;文章这么做的理由是，考虑现实中的情况，当药物化学家对某系列化合物只有有限的活性数据（support set）而要预测（同一靶点或类似结构的）一化合物（query molecule）的活性时，化学家会将该化合物与手头已有数据的化合物对比，再在化合物库（context set）中对比，综合考虑各项因素后得出判断。模型所做的 MHN 计算以及平均相似性，就是简化了的上述过程，文章认为这样的设计有助于模型模仿化学家的思考方式。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用用于少样本药物发现的标准数据集 FS-Mol 作为模型的数据集。该数据集中的分子来自于 ChEMBL 27，其中定义了 4938 个训练任务，40 个验证任务与 157 个测试任务，平均每个任务下只有 94 个数据点。&lt;/p&gt;
&lt;p&gt;文章使用 ECFPs 分子指纹与 RDKit 描述符来作为初始的分子表示。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8890?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8890?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;实验结果如上表所示，文章对比了各个模型在 FS-Mol 测试集上的 &amp;Delta;AUC-PR，除了 ADKF-IFT 在 Hydol. 与 Oxid. 小部分任务上优于 MHNfs，其他模型的结果都不如 MHNfs，而且 MHNfs 在全部任务的整体结果上优于其他全部模型，所以文章认为 MHNfs 在 FS-Mol 测试集实现了目前药物发现少样本学习的最优性能。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 参考了现实中化学家面对少样本数据时的策略，通过设想的一种上下文增强的方式更新了输入模型的分子表示，使其具有更多大数据集中的背景信息。在实验中，测试结果表示这种增强的分子表示确实提高了模型预测的准确率，MHNfs 也在该任务上达到了最优的性能。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>从零起步的 Transformer 与代码拆解</title><link href="https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html" rel="alternate"></link><published>2023-04-21T00:00:00+08:00</published><updated>2023-04-21T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-21:/sui-sui-nian/2023-04-21-transformer-from-scratch.html</id><summary type="html">&lt;p&gt;自 Google 的论文 &lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Attention Is All You Need&lt;/a&gt; 发布后，几年内涌现了大量基于 Transformer 的模型，俨然形成了 Transformer 横扫人工智能领域的态势。&lt;/p&gt;
&lt;p&gt;网络上也出现了大量解读论文或是讲解 Transformer 的文章，其中也不乏许多高水平人工智能从业者的解读。虽然有些可以称得上是高屋建瓴，但 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;自 Google 的论文 &lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Attention Is All You Need&lt;/a&gt; 发布后，几年内涌现了大量基于 Transformer 的模型，俨然形成了 Transformer 横扫人工智能领域的态势。&lt;/p&gt;
&lt;p&gt;网络上也出现了大量解读论文或是讲解 Transformer 的文章，其中也不乏许多高水平人工智能从业者的解读。虽然有些可以称得上是高屋建瓴，但相当大部分难以避免地落入了知识的诅咒（curse of knowledge），起码在我初开始了解 Transformer 时难以读懂这些文章。&lt;/p&gt;
&lt;p&gt;随着 Transformer 广泛应用到各领域，学习 Transformer 也成了一门「显学」。尽管我已经能读懂一些更深层次的 Transformer 剖析，但我还是未找见一篇合我心意的入门文章，所以我希望能撰写一篇小文章，以初学者的角度来讲解 Transformer，是为序。&lt;/p&gt;
&lt;h2 id="xie-zi"&gt;楔子&lt;/h2&gt;
&lt;p&gt;Transformer 是设计用于 NLP 的一种模型，尽管目前 Transformer 所能完成的任务已经大大扩展，但这里还是以最原始的翻译任务为例。&lt;/p&gt;
&lt;p&gt;在翻译任务中，所需要的数据包括原始语句与目标语句，也就是 Transformer 原论文中所指的「input」和「output」，因为名字太容易混淆，还是将其原始语句与目标语句或是「source」与「target」。&lt;/p&gt;
&lt;p&gt;假设 source 为 &lt;code&gt;你好，世界！&lt;/code&gt;，target 为 &lt;code&gt;Hello, world!&lt;/code&gt;，完成这个中译英任务首先要将文本转化为利于模型处理的数值，这一步称为词嵌入（embedding）。&lt;/p&gt;
&lt;p&gt;常见的词嵌入方法有 word2vec 等等，在这里不做介绍。词嵌入步骤大致的流程是先将 &lt;code&gt;你好，世界！&lt;/code&gt; 转化为 &lt;code&gt;&amp;lt;start&amp;gt; 你好 ， 世界 ！ &amp;lt;end&amp;gt;&lt;/code&gt;，每个「词」都用空格划分开，其中 &lt;code&gt;&amp;lt;start&amp;gt;&lt;/code&gt; 与 &lt;code&gt;&amp;lt;end&amp;gt;&lt;/code&gt; 分别表示文本的起讫，这些「词」在 NLP 通常称为「token」。接着再为每个 token 分配索引，例如 &lt;code&gt;&amp;lt;start&amp;gt;&lt;/code&gt; 为 &lt;code&gt;1&lt;/code&gt;，&lt;code&gt;&amp;lt;end&amp;gt;&lt;/code&gt;为 &lt;code&gt;0&lt;/code&gt;，照这个思路，文本就可以转换为 &lt;code&gt;[1 2 3 4 5 0]&lt;/code&gt; 的表示。当然这是很简单的做法，实际上，每个 token 都会被转化为指定维度的向量，用这一连串向量就可以表示文本。&lt;/p&gt;
&lt;p&gt;将上述过程抽象出来，在词嵌入后，可以得到 source 的表示 &lt;span class="math"&gt;\(\boldsymbol{X}=(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_t)\)&lt;/span&gt; 与 target 的表示 &lt;span class="math"&gt;\(\boldsymbol{Y}=(\boldsymbol{y}_1,\boldsymbol{y}_2,\cdots,\boldsymbol{y}_t)\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\boldsymbol{x}_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{y}_i\)&lt;/span&gt; 都是指定维度 &lt;span class="math"&gt;\(d\)&lt;/span&gt; 的向量。&lt;/p&gt;
&lt;p&gt;那么如何使用 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 完成翻译任务呢？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一种&lt;/strong&gt;是使用 RNN 方法，使用当前的 source token &lt;span class="math"&gt;\(\boldsymbol{x}_t\)&lt;/span&gt; 与前一步中生成的 token &lt;span class="math"&gt;\(\hat{\boldsymbol{y}}_{t-1}\)&lt;/span&gt; 生成下一个 token，逐个生成直至句子末尾：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\hat{\boldsymbol{y}}_{t-1},\boldsymbol{x}_t)$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;第二种&lt;/strong&gt;是使用卷积的方法，定义一个窗口长度再通过小范围中的几个 &lt;span class="math"&gt;\(\boldsymbol{x}_i\)&lt;/span&gt; 计算输出：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_{t-1},\boldsymbol{x}_t,\boldsymbol{x}_{t+1})$$&lt;/div&gt;
&lt;p&gt;可以看出，&lt;dot&gt;RNN 很难学习到全局的信息&lt;/dot&gt;，而&lt;dot&gt;卷积方法只能学习到小范围的局部信息&lt;/dot&gt;。&lt;/p&gt;
&lt;p&gt;所以 Transformer 给出了&lt;strong&gt;第三种&lt;/strong&gt;方法，也就是自注意力方法。自注意力机制让模型就当前的 source token &lt;span class="math"&gt;\(\boldsymbol{x}_t\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 中其他 token 的关系给出输出 &lt;span class="math"&gt;\(\hat{\boldsymbol{y}}_t\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_t, \boldsymbol{X})$$&lt;/div&gt;
&lt;h2 id="transformer-jie-gou"&gt;Transformer 结构&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;标准 Transformer 的结构如上图所示，大致分为左侧的 Encoder 与右侧的 Decoder 两个部分。Inputs 与 Outputs 分别是上文所说的 source 与 target，Output Probabilities 是模型输出的各 token 概率，取其中最大概率的 token 就能组织成模型输出结果。&lt;/p&gt;
&lt;h3 id="wei-zhi-bian-ma"&gt;位置编码&lt;/h3&gt;
&lt;p&gt;Transformer 并没有采用 RNN 与卷积方法所使用的序列处理 token 的方法，因而能够实现并行计算并且很大程度上缓解了长期依赖问题（顺序处理长序列容易丢失多个步骤前的信息）。文本中多个 token 间显然有前后的顺序关系，Transformer 使用位置编码的方式来处理顺序信息。&lt;/p&gt;
&lt;p&gt;source 与 target 送入模型，经过常规的词嵌入过程后，还需要在得到的矩阵上加上位置编码，论文将位置编码定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{PE}_{(\mathrm{pos},2i)}=\sin(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathrm{PE}_{(\mathrm{pos},2i+1)}=\cos(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$&lt;/div&gt;
&lt;p&gt;Transformer 将 &lt;span class="math"&gt;\(\mathrm{pos}\)&lt;/span&gt; 位置映射为 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 维的向量，向量中的第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个元素即按上式计算。位置编码的计算公式是构造出的经验公式，不必深究，当然也有许多文章分析了如此构造的原因，这里从略。&lt;/p&gt;
&lt;h3 id="encoder-yu-decoder"&gt;Encoder 与 Decoder&lt;/h3&gt;
&lt;p&gt;许多完成 seq2seq 任务的模型都采用了 encoder-decoder 模式，Transformer 也不例外。简单来说，encoder 将输入编码得到一个中间变量，decoder 解码该中间变量得到输出。&lt;/p&gt;
&lt;p&gt;在 Transformer 中，source 与 target 分别送入 encoder 与 decoder，encoder 计算得到的中间结果再送入 decoder 中与 target 输入进行计算，得到最后的结果，这就是所谓「编码-解码」的工作方式。&lt;/p&gt;
&lt;p&gt;从 Transformer 的结构图中可以看出，模型具有 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 层 encoder 与 decoder 层。其中，encoder 与 decoder 都具有相同的多头注意力层（Multi-Head Attention）、前馈层（Feed Forward）。encoder 与 decoder 的不同在于 decoder 多了一个多头注意力层，在这一层中，encoder 的输出与 decoder 的输入计算注意力。&lt;/p&gt;
&lt;p&gt;还可以注意到，在 encoder 与 decoder 中，每一层后都有一个 Add &amp;amp; Norm 层，用于归一化计算结果。Add &amp;amp; Norm 层的计算方式是将前一层的输入与前一层的输出相加，然后归一化，可以表示为 &lt;span class="math"&gt;\(\mathrm{LayerNorm}(\boldsymbol{x}+\mathrm{Sublayer}(\boldsymbol{x}))\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4&gt;Attention 机制&lt;/h4&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;数据进入 encoder 与 decoder 的内部，首先要通过注意力机制进行计算，这也是 Transformer 的核心。&lt;/p&gt;
&lt;p&gt;文章中将所使用的注意力称为缩放点积注意力（scaled dot-product attention），定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{Q}_{n\times d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{K}_{m\times d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{V}_{m\times d_v}\)&lt;/span&gt; 分别是若干向量 &lt;span class="math"&gt;\(\boldsymbol{q}\in\mathbb{R}^{d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{k}\in\mathbb{R}^{d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{v}\in\mathbb{R}^{d_v}\)&lt;/span&gt; 组成的矩阵。&lt;/p&gt;
&lt;p&gt;单看矩阵的乘法稍显复杂，不妨先用向量说明计算步骤。通过以下方式可以从输入 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 得到向量 &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{q}=\boldsymbol{x}\boldsymbol{W}^Q,\,\boldsymbol{k}=\boldsymbol{x}\boldsymbol{W}^K,\,\boldsymbol{v}=\boldsymbol{x}\boldsymbol{W}^V$$&lt;/div&gt;
&lt;p&gt;其中，&lt;span class="math"&gt;\(\boldsymbol{W}^Q\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{W}^K\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{W}^V\)&lt;/span&gt; 分别表示相应的权重矩阵。&lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; 代表 query，&lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt; 代表 key，&lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt; 代表 value，目的是&lt;dot&gt;用 query 去寻找更匹配的 key-value 对&lt;/dot&gt;。&lt;/p&gt;
&lt;p&gt;因为数量积可以表示两向量的相似程度，一种简单的做法是使用 &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; 与若干个 &lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt; 计算数量积，将其作为匹配分数：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{score}=\boldsymbol{q}\cdot \boldsymbol{k}_i=\boldsymbol{q}\boldsymbol{k}^\top_i$$&lt;/div&gt;
&lt;p&gt;但这样的「注意力」太过于简单，Google 从上述的数量积出发，设计了更为可靠的注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{q},\boldsymbol{k}_i,\boldsymbol{v}_i)=\frac 1 Z\sum_i\exp\left(\frac{\boldsymbol{q}\boldsymbol{k}^\top_i}{\sqrt{d_k}}\right)\boldsymbol{v}_i$$&lt;/div&gt;
&lt;p&gt;首先，式中 &lt;span class="math"&gt;\(1/Z\sum_i x_i\)&lt;/span&gt; 形式的部分是 Softmax 函数的简写，Softmax 函数由下式定义：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Softmax}(x_i)=\frac{\exp(x_i)}{\sum_j\exp(x_j)}$$&lt;/div&gt;
&lt;p&gt;Softmax 函数的作用是将若干数值 &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 归一化，得到的 &lt;span class="math"&gt;\(\mathrm{Softmax}(x_i)\)&lt;/span&gt; 具有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\sum_i\mathrm{Softmax}(x_i)=1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{Softmax}(x_i)\in[0, 1]\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两点性质，所以与概率具有相似的特征，可以用作概率处理。&lt;/p&gt;
&lt;p&gt;其次，式中新增的 &lt;span class="math"&gt;\(\sqrt{d_k}\)&lt;/span&gt; 用于调节内积 &lt;span class="math"&gt;\(\boldsymbol{q}\boldsymbol{k}^\top_i\)&lt;/span&gt; 的大小。当若干内积的大小过于悬殊时，Softmax 函数很容易将其推向 &lt;span class="math"&gt;\(0\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(1\)&lt;/span&gt; 的边界值，这样的数值处理起来没什么意义。&lt;/p&gt;
&lt;p&gt;最后，再次回忆 Transformer 的注意力机制是用 query 去寻找更匹配的 key-value 对。那么上式的意义就很了然了，就是将 query 与各个 key 的匹配分数转化为各个概率，再按各个概率取各个 key 所对应的 value，组合各 value 分量即得到注意力。&lt;/p&gt;
&lt;p&gt;以具有两个 value 的情况为例，需要得到的中间量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;（理解为注意力亦可）可以通过下式计算：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \boldsymbol{z}_1=\theta_{11}\boldsymbol{v}_1+\theta_{12}\boldsymbol{v}_2\\
    \boldsymbol{z}_2=\theta_{21}\boldsymbol{v}_1+\theta_{22}\boldsymbol{v}_2
\end{align}$$&lt;/div&gt;
&lt;p&gt;权值 &lt;span class="math"&gt;\(\theta_{ij}\)&lt;/span&gt;（即上文所说概率）通过下式得到：&lt;/p&gt;
&lt;div class="math"&gt;$$\theta_{ij}=\mathrm{Softmax}\left(\frac{\boldsymbol{q}_i\boldsymbol{k}^\top_j}{\sqrt{d_k}}\right)$$&lt;/div&gt;
&lt;p&gt;将上述运算转为矩阵形式会简洁许多：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{pmatrix}
    \boldsymbol{z}_1 \\
    \boldsymbol{z}_2
\end{pmatrix}=
\begin{pmatrix}
    \theta_{11} &amp;amp; \theta_{12} \\
    \theta_{21} &amp;amp; \theta_{22}
\end{pmatrix}
\begin{pmatrix}
    \boldsymbol{v}_1 \\
    \boldsymbol{v}_2
\end{pmatrix}\\
$$&lt;/div&gt;
&lt;p&gt;可以记作 &lt;span class="math"&gt;\(\boldsymbol{Z}=\boldsymbol{\theta}\boldsymbol{V}\)&lt;/span&gt;，也就是&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$&lt;/div&gt;
&lt;h4&gt;Multi-Head Attention&lt;/h4&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;前一节中解释了 Transformer 中的缩放点积注意力，但在模型中实际并非通过上述方式直接计算，而是通过多头注意力的方式计算注意力。&lt;/p&gt;
&lt;p&gt;如上图所示，多头注意力同样是在计算缩放点积注意力，但与纯粹缩放点积注意力的不同之处在于多头注意力将多个注意力计算步骤叠加了起来。&lt;/p&gt;
&lt;p&gt;叠加的次数为 &lt;span class="math"&gt;\(h\)&lt;/span&gt;，即代表 head，多少个 head 表示需要进行多少次叠加计算。矩阵 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{K}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{V}\)&lt;/span&gt; 进入多头注意力计算步骤后，首先要分别在第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个 head 中进行线性变换并计算注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{head}_i=\mathrm{Attention}(\boldsymbol{Q}\boldsymbol{W}^Q_i,\boldsymbol{K}\boldsymbol{W}^K_i,\boldsymbol{V}\boldsymbol{W}^V_i)$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}^Q_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^K_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^V_i\in\mathbb{R}^{d_\mathrm{model}\times d_v}\)&lt;/span&gt;，注意不同 head 中的线性变换并不同，输出也不同。然后将所有输出 &lt;span class="math"&gt;\(\mathrm{head}_i\)&lt;/span&gt; 拼合在一起，经线性变换后作为注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{MultiHead}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\mathrm{Concat}(\mathrm{head}_1,\mathrm{head}_2,\cdots,\mathrm{head}_h)\boldsymbol{W}^O$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}^O\in\mathbb{R}^{hd_v\times d_\mathrm{model}}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;注意这个过程中数据维数的变化 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 为单头注意力中模型所处理的维数，&lt;span class="math"&gt;\(\boldsymbol{W}^Q_i\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^K_i\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^V_i\)&lt;/span&gt; 的线性变换将 query、key 的维数从 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 提升到 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;，将 value 的维数从 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 提升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;。最后的 &lt;span class="math"&gt;\(\boldsymbol{W}^O\)&lt;/span&gt; 又将拼合起来维数为 &lt;span class="math"&gt;\(hd_v\)&lt;/span&gt; 的注意力转换为模型所处理的维数 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt;。这些线性变换矩阵 &lt;span class="math"&gt;\(\boldsymbol{W}_i\)&lt;/span&gt; 实际上就是模型训练过程中需要学习的一部分参数。&lt;/p&gt;
&lt;p&gt;至于为什么要用多头的方式计算注意力，这就是个很复杂的问题了。就我的理解而言，由于每个 head 中的线性变换矩阵 &lt;span class="math"&gt;\(\boldsymbol{W}_i\)&lt;/span&gt;，多头注意力实际上是将 query、key、value 映射到不同的子空间中，在多个不同的子空间中寻找与 query 最匹配的 key-value。由于不同子空间中具有不同方面的信息，最后将其拼接起来作为结果，这样可以更多地从多个方面捕获数据中的信息。&lt;/p&gt;
&lt;h4&gt;Feed-Forward 层&lt;/h4&gt;
&lt;p&gt;在多头注意力层之后，就是前馈层，前馈层只在位置方向上计算，所以原文描述其为 position-wise。进入前馈层的数据在该层中先做 1 次线性变换，维度升高，再经过 RELU 激活函数，最后再做 1 次线性变换，维度降低，输入与输出前馈层的维度相同。上述过程可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{FFN}(\boldsymbol{x})=\max(0,\boldsymbol{x}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2$$&lt;/div&gt;
&lt;p&gt;RELU 激活函数定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{ReLU}(x)=x^+=\max(0,x)$$&lt;/div&gt;
&lt;p&gt;即式中的 &lt;span class="math"&gt;\(\max\)&lt;/span&gt;，按原文中的例子，&lt;span class="math"&gt;\(\boldsymbol{W}_1\)&lt;/span&gt; 使 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 由 512 维升高到 2048 维，&lt;span class="math"&gt;\(\boldsymbol{W}_2\)&lt;/span&gt; 使 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 计算由 2048 维再降至 512 维，升维与降维的过程也是为了更好地获得数据中的信息。&lt;/p&gt;
&lt;h3 id="transformer-ji-suan-bu-zou"&gt;Transformer 计算步骤&lt;/h3&gt;
&lt;p&gt;Transformer 模型大致就由上述的几个层连接在一起构成，但或许还是觉得朦朦胧胧，比如究竟什么才是 query、key、value 等等。不妨再来看看 Transformer 的结构图，这一次已熟知大部分模块的工作原理了，所以只看数据流入与流出各模块的路线。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;作为 source 的 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与作为 target 的 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 分别从下方的左右两侧进入模型。&lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 都要经过词嵌入并加上位置编码，按以下方式更新：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    \boldsymbol{X}&amp;amp;\leftarrow\mathrm{Embedding}(\boldsymbol{X})+\mathrm{PE}(\boldsymbol{X})\\
    \boldsymbol{Y}&amp;amp;\leftarrow\mathrm{Embedding}(\boldsymbol{Y})+\mathrm{PE}(\boldsymbol{Y})
\end{align}
$$&lt;/div&gt;
&lt;p&gt;接着 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 分别进入 encoder 与 decoder，可以注意到数据分作 4 条路线，这意味着将数据复制 4 次。先看进入多头注意力层的 3 条数据，以 encoder 为例，在这一层中就是在计算&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$&lt;/div&gt;
&lt;p&gt;不言自明，在这里的 query、key、value 三者都是 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt;，是在 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 内部计算注意力，因此称其为&lt;strong&gt;自注意力&lt;/strong&gt;（self-attention）。&lt;/p&gt;
&lt;p&gt;在后续的 Add &amp;amp; Norm 层中，计算&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X}))$$&lt;/div&gt;
&lt;p&gt;在前馈层与后续的 Add &amp;amp; Norm 层输的输出结果也可想而知：&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\max(0,\boldsymbol{X}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2)$$&lt;/div&gt;
&lt;p&gt;这里的 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 分作两路进入到 decoder 中，在 decoder 的该多头注意力层中，query 与 key 为 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt;，而 value 为类似步骤得到的 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt;，该层的输出为&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{Z}=\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{Y})$$&lt;/div&gt;
&lt;p&gt;这也是 decoder 与 encoder 的关键不同。输出结果 &lt;span class="math"&gt;\(\boldsymbol{Z}\)&lt;/span&gt; 完成后续的计算过程后，就得到各 token 的概率，用各 token 替换即可得到模型输出的文本结果。&lt;/p&gt;
&lt;p&gt;{note begin}有兴趣的读者不妨根据各矩阵的形状尝试计算一下各个变量的维度在 Transformer 在各步骤中是如何变化的，一定会对 Transformer 的计算过程收获更深的了解。{note end}&lt;/p&gt;
&lt;h2 id="dai-ma-chai-jie_1"&gt;代码拆解&lt;/h2&gt;
&lt;p&gt;有了对 Transformer 原理的基本认识，就可以动手实现一个 Transformer 了，通过代码更深入了解 Transformer 的一些细节。这里使用 PyTorch 搭建一个标准的 Transformer，参考代码见 &lt;a href="https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; aladdinpersson / Machine-Learning-Collection &lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;代码中的各模块如下图所示，接下来对各模块逐个拆解。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="positionembedding"&gt;PositionEmbedding&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;import math
import torch
import torch.nn as nn


class PositionEmbedding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        # d_model 为模型处理数据的维数，即公式中 d_k
        # max_len 表示模型处理的最大 token 数量
        super(PositionEmbedding, self).__init__()

        # 生成大小为 max_len * d_model 的零矩阵
        pe = torch.zeros(max_len, d_model)
        # 生成大小为 max_len * 1 的位置矩阵
        position = torch.arange(max_len).unsqueeze(1)
        # 计算位置编码
        div_term = torch.exp(torch.arange(0, d_model, 2) * - (math.log(10000.0) / d_model))
        x = position * div_term
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = self.pe[:, :x.size(1)]
        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;首先实现位置编码模块。在 PyTorch 中，用于搭建神经网络的模块都要继承 &lt;code&gt;nn.Module&lt;/code&gt;，PyTorch 会通过 &lt;code&gt;__call__()&lt;/code&gt; 调用模块的 &lt;code&gt;forward()&lt;/code&gt; 的方法进行前向传播。简单来讲就是，&lt;code&gt;PositionEmbedding(x)&lt;/code&gt; 的功能等同于 &lt;code&gt;PositionEmbedding.forward(x)&lt;/code&gt;，但不能使用 &lt;code&gt;PositionEmbedding.forward(x)&lt;/code&gt;，因为 PyTorch 做了许多条件的判定和优化。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;torch.arange(num)&lt;/code&gt; 的功能类似于 Python 中的 &lt;code&gt;range(num)&lt;/code&gt;，用于生成文本各 token 的顺序位置索引。&lt;code&gt;unsqueeze(dim)&lt;/code&gt; 会令 Tensor 在指定的维度 &lt;code&gt;dim&lt;/code&gt; 上扩张 1 维，这里是为了使 &lt;code&gt;pe&lt;/code&gt; 与 &lt;code&gt;position&lt;/code&gt; 两个矩阵的维度对齐，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; torch.arange(5)
tensor([0, 1, 2, 3, 4])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(0)
tensor([[0, 1, 2, 3, 4]])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(1)
tensor([[0],
        [1],
        [2],
        [3],
        [4]])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).size()
torch.Size([5])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(0).size()
torch.Size([1, 5])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(1).size()
torch.Size([5, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;代码中的位置编码并不是直接按公式计算的，而是做了一些变换，先计算一个中间量 &lt;code&gt;div_term&lt;/code&gt;，其中 &lt;code&gt;torch.arange(0, d_model, 2)&lt;/code&gt; 即为 &lt;span class="math"&gt;\(2i\)&lt;/span&gt;，可以整理出&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \mathrm{div\_term}_i&amp;amp;=\exp\left[2i\times(-\frac{\ln10000}{d_k})\right]\\
    &amp;amp;=\left[\exp(-\frac{\ln10000}{d_k})\right]^{2i}\\
    &amp;amp;=\left[10000^{-\frac{1}{d_k}}\right]^{2i}\\
    &amp;amp;=10000^{-2i/d_k}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;所以 &lt;code&gt;position * div_term&lt;/code&gt; 就可以得到&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{position}\times \mathrm{div\_term}_i=\mathrm{pos}/10000^{2i/d_k}$$&lt;/div&gt;
&lt;p&gt;就是位置编码中的一项。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pe[:, 0::2]&lt;/code&gt; 与 &lt;code&gt;pe[:, 1::2]&lt;/code&gt; 是 Pytorch 中的高级索引操作。索引中用 &lt;code&gt;,&lt;/code&gt; 分隔不同维度，例中以 &lt;code&gt;,&lt;/code&gt; 为分界，前面是对第 1 维的索引，后面是对第 2 维的索引。索引操作也遵守 Python 的规则，即 &lt;code&gt;a:b:c&lt;/code&gt; 中 &lt;code&gt;a&lt;/code&gt; 为起始，&lt;code&gt;b&lt;/code&gt; 为末尾，&lt;code&gt;c&lt;/code&gt; 为步长。&lt;/p&gt;
&lt;p&gt;所以 &lt;code&gt;pe[:, 0::2]&lt;/code&gt; 与 &lt;code&gt;pe[:, 1::2]&lt;/code&gt; 取出全部第 1 维中的元素，即行方向上不操作，再在第 2 维中分别从 &lt;code&gt;0&lt;/code&gt; 或 &lt;code&gt;1&lt;/code&gt; 开始以步长 &lt;code&gt;2&lt;/code&gt; 取出元素，即取出第 &lt;span class="math"&gt;\(2i\)&lt;/span&gt; 或第 &lt;span class="math"&gt;\(2i+1\)&lt;/span&gt; 列。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;forward()&lt;/code&gt; 部分，输出的位置编码为 &lt;code&gt;pe[:, :x.size(1)]&lt;/code&gt;，这主要是为了确保矩阵形状在加法过程中不会因非法输入的广播而改变。其实在输入合法的情况下，&lt;code&gt;x.size(1)&lt;/code&gt; 就是 &lt;code&gt;d_model&lt;/code&gt;，等价于 &lt;code&gt;pe[:, :]&lt;/code&gt;，也等价于 &lt;code&gt;pe&lt;/code&gt;。&lt;/p&gt;
&lt;!-- 指定 `requires_grad_(False)` 是因为 PyTorch 会自动保存 Tensor 的来源，用于更快地计算梯度，而这里的加法计算并不是训练过程，取消保存能节省一部分资源。 --&gt;
&lt;h3 id="selfattention"&gt;SelfAttention&lt;/h3&gt;
&lt;p&gt;在进入 Transformer 核心部分之前，我们需要再次明确一下输入模型的数据格式。上文中仅以输入模型一条数据（由若干 token 组成的一条句子）为例，在实际操作中，为了提高训练效率，会同时输入若干条数据，在构建模型时也要考虑到这一点。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，一次输入模型的数据条数就称为 batch size，所以模型所处理的其实是一个 &lt;span class="math"&gt;\(\mathrm{batch\_size}\times\mathrm{max\_len}\times\mathrm{d\_model}\)&lt;/span&gt; 的高维矩阵。也就是说，&lt;code&gt;x.size()&lt;/code&gt; 的结果是 &lt;code&gt;[batch_size, max_len, d_model]&lt;/code&gt;，务必注意三者顺序。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        # 确保 embed_size 能被 heads 整除
        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(embed_size, embed_size)
        self.keys = nn.Linear(embed_size, embed_size)
        self.queries = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;先看 &lt;code&gt;SelfAttention&lt;/code&gt; 的初始化部分，明白了注意力机制的计算过程就不难理解上面的各个属性了。&lt;code&gt;head_dim&lt;/code&gt; 是每一个 head 中注意力的维度，&lt;code&gt;embeds_size&lt;/code&gt; 必须能被 &lt;code&gt;heads&lt;/code&gt; 整除，否则将多头注意力拼接在一起的维数不等于模型处理的维数就会出现问题。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;values&lt;/code&gt;、&lt;code&gt;keys&lt;/code&gt;、&lt;code&gt;queries&lt;/code&gt; 都是计算多头注意力前的线性变换，&lt;code&gt;fc_out&lt;/code&gt; 是拼接多头注意力后的线性变换。线性变换可以直接调用 &lt;code&gt;nn.Linear(in_dim, out_dim)&lt;/code&gt;，只需要指定线性变换前后的维数即可，这里线性变换前后维数没有变化。&lt;/p&gt;
&lt;p&gt;可能会有读者疑惑为什么这里所设定的线性变换不改变维数，原文中所描述的步骤不是应该将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt; 再计算注意力吗？这是正确的，原文中的计算流程确实如此。如下图所示，在线性变换后复制 &lt;code&gt;h&lt;/code&gt; 份（例中为 2） &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt;，用若干份 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 分别计算注意力再拼合起来，得到注意力的维数自然就是 &lt;code&gt;h * d_v&lt;/code&gt; （例中为 2 * 6），再用一个线性变换将其转化回模型所处理的维数 &lt;code&gt;d_model&lt;/code&gt;（例中为 5）。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;但代码中优化了一部分比较繁琐的操作，也有其他版本的代码使用了更接近原文的实现方式，如  &lt;a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; jadore801120 / attention-is-all-you-need-pytorch &lt;/a&gt;，流程就如下图所示，勉强称之为「单头注意力变多头注意力」的一种代码实现吧。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;例中 &lt;code&gt;d_model&lt;/code&gt; 也就是词嵌入的维数还是 5，&lt;code&gt;heads&lt;/code&gt; 仍为 2，&lt;code&gt;d_value&lt;/code&gt; 仍为 6，但模型不再是将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;，而是将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 直接升至 &lt;span class="math"&gt;\(hd_v\)&lt;/span&gt;，然后将 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 分成 &lt;code&gt;h&lt;/code&gt; 份，每份分别用于计算并拼接为注意力。与上例相比，本质上其实并无区别，区别仅仅是上例先复制多个矩阵再分别做线性变换，而该例只使用了一个更大的矩阵乘法就完成了上述操作，效率上更优。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;多头注意力还有一种实现方法，也是这里展示代码所使用的方法。如上图所示，这种方法对词嵌入的维数有要求，在词嵌入的步骤中就将 token 表示为 &lt;code&gt;d_v * h&lt;/code&gt; 维，这也是前文代码在初始化中使用 &lt;code&gt;assert&lt;/code&gt; 语句缘由。后续的线性变换不改变维数，计算多头注意力时直接将 &lt;code&gt;d_v * h&lt;/code&gt; 维切分为 &lt;code&gt;h&lt;/code&gt; 份作为每个 head 计算的对象。拼接各 head 的注意力后，最后的线性变换也不改变维数。&lt;/p&gt;
&lt;p&gt;在我看来，这种方法应该是对前两种方法的简化，三个例子中用于计算多头注意力的 &lt;code&gt;d_value&lt;/code&gt; 都为 6，计算量相同。第 3 种方法需要更大的 &lt;code&gt;d_model&lt;/code&gt;，而且计算多头注意力时没有使用到全部的 embedding，虽说效果类似，但总觉有些奇怪。这或许是为了计算上的方便，不用做过多的矩阵变换 🤔&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;# class SelfAttention(nn.Module):
    def forward(self, values, keys, query, mask):
        # 获取 batch_size
        N = query.shape[0]
        # d_v, d_k, d_q
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # 对 query, key, value 做线性变换
        values = self.values(values)    # (N, value_len, embed_size)
        keys = self.keys(keys)          # (N, key_len, embed_size)
        queries = self.queries(query)   # (N, query_len, embed_size)

        # 将 token 的词嵌入划分为 heads 份
        # d_model = embed_size = d_v * heads
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = queries.reshape(N, query_len, self.heads, self.head_dim)

        # queries: (N, query_len, heads, heads_dim),
        # keys: (N, key_len, heads, heads_dim)
        # energy: (N, heads, query_len, key_len)
        energy = torch.einsum("nqhd,nkhd-&amp;gt;nhqk", [queries, keys])

        # 将掩码矩阵中为 0 的对应项设为 -inf，不参与计算
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        # 得到的点积除以 sqrt(d_k) 并用 Softmax 归一化
        # attention: (N, heads, query_len, key_len)
        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)

        # attention: (N, heads, query_len, key_len)
        # values: (N, value_len, heads, heads_dim)
        # out after matrix multiply: (N, query_len, heads, head_dim), then
        # we reshape and flatten the last two dimensions.
        out = torch.einsum("nhql,nlhd-&amp;gt;nqhd", [attention, values]).reshape(
            N, query_len, self.heads * self.head_dim
        )

        # 拼接多头注意力后的线性变换
        # out: (N, query_len, embed_size)
        out = self.fc_out(out)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 部分描述了上述计算多头重意力的过程。线性变换后，使用 &lt;code&gt;reshape()&lt;/code&gt; 方法将 Tensor 转化化为指定维度，也就是将词嵌入划分为 &lt;code&gt;heads&lt;/code&gt; 份的操作，Tensor 的形状由 &lt;code&gt;[N, query_len, embed_size]&lt;/code&gt; 变为 &lt;code&gt;[N, query_len, self.heads, self.head_dim]&lt;/code&gt;，把 &lt;code&gt;embed_size&lt;/code&gt; 拆成 &lt;code&gt;heads * head_dim&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;接着使用 &lt;code&gt;torch.einsum()&lt;/code&gt; 得到注意力计算的一个中间量 &lt;code&gt;energy&lt;/code&gt;。&lt;code&gt;torch.einsum()&lt;/code&gt; 称为爱因斯坦求和约定，可以非常简洁地进行矩阵乘法、转置待操作，但会有些难以理解。&lt;/p&gt;
&lt;p&gt;例如矩阵乘法 &lt;span class="math"&gt;\(\boldsymbol{A}_{i\times j}\boldsymbol{B}_{j\times k}=\boldsymbol{C}_{i\times k}\)&lt;/span&gt;，可以表示为 &lt;code&gt;"ij,jk-&amp;gt;ik"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; B = torch.randn(4, 5)
&amp;gt;&amp;gt;&amp;gt; C = torch.einsum("ij,jk-&amp;gt;ik", [A, B])
&amp;gt;&amp;gt;&amp;gt; C.size()
torch.Size([3, 5])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;例如矩阵转置 &lt;span class="math"&gt;\((\boldsymbol{A}_{i\times j})^\top=\boldsymbol{B}_{j\times i}\)&lt;/span&gt;，可以表示为 &lt;code&gt;"ij-&amp;gt;ji"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; B = torch.einsum("ij-&amp;gt;ji", [A])
&amp;gt;&amp;gt;&amp;gt; B.size()
torch.Size([4, 3])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;定义了矩阵乘法的表示后，相应的数量积与向量积就也能表示了，不再赘述。求和操作将矩阵转化为数值，行与列都会消失，所以 &lt;span class="math"&gt;\(\sum a_{ij}\in\boldsymbol{A}_{i\times j}\)&lt;/span&gt; 可以记作 &lt;code&gt;"ij-&amp;gt;"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; torch.einsum("ij-&amp;gt;", [A])
tensor(0.5634)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此外，爱因斯坦求和约定还可以表示在指定维度上求和、做数量积等一系列的复杂操作，读者可以自行试验。&lt;/p&gt;
&lt;p&gt;代码中 &lt;code&gt;queries&lt;/code&gt; 的形状为 &lt;code&gt;[N, query_len, heads, heads_dim]&lt;/code&gt;，记作 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times q\times h \times d}\)&lt;/span&gt;，&lt;code&gt;keys&lt;/code&gt; 的形状为 &lt;code&gt;[N, key_len, heads, heads_dim]&lt;/code&gt;，记作 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times k\times h \times d}\)&lt;/span&gt;，那么 &lt;code&gt;torch.einsum("nqhd,nkhd-&amp;gt;nhqk", [queries, keys])&lt;/code&gt; 所做的操作就是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times q\times h \times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times h \times q\times d}\)&lt;/span&gt;，将 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times k\times h \times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h\times k \times d}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;两个矩阵中的 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 是 &lt;code&gt;batch_size&lt;/code&gt; 与 &lt;code&gt;heads&lt;/code&gt; 的乘积，仅仅是表示数量，所以 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times k\times d}\)&lt;/span&gt; 可以视作由 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 个 &lt;span class="math"&gt;\((\boldsymbol{K}_i)_{\ k\times d}\)&lt;/span&gt; 子矩阵构成的大矩阵。那么固定前两维不变，转置后两维，相当于&lt;strong&gt;转置&lt;/strong&gt;所有子矩阵，得到 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times d\times k}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;固定前两维，令 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times h \times q\times d}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times d\times k}\)&lt;/span&gt; 在后两维上做乘法，得到 &lt;span class="math"&gt;\((\boldsymbol{QK})_{N\times h \times q \times k}\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;仔细思考上述的转置和乘法过程，实际上就是在做多头注意力中的 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;掩码部分的操作先略过。接着 &lt;code&gt;torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)&lt;/code&gt; 先将前一步中得到 &lt;code&gt;energy&lt;/code&gt; 除以 &lt;span class="math"&gt;\(\sqrt{d_k}\)&lt;/span&gt; 再用 Softmax 归一化。指定的 &lt;code&gt;dim=3&lt;/code&gt; 与 &lt;code&gt;dim=-1&lt;/code&gt; 等价，其目的是在最后一维的方向上归一化。&lt;/p&gt;
&lt;p&gt;以一个简单的 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt; 乘法为例，如下图所示，&lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{K}\)&lt;/span&gt; 的每一行都是一个 token 的词嵌入表示。计算得到 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt; 后需要归一化，&lt;code&gt;softmax(dim=0)&lt;/code&gt; 是在行方向上归一化，在得到的结果中，全部行加起来，各元素为 1；&lt;code&gt;softmax(dim=1)&lt;/code&gt; 是在列方向上归一化，结果中的全部列加起来，各元素为 1。&lt;/p&gt;
&lt;p&gt;计算注意力还是为了得到更准确的 token 表示，所以归一化的方向应该与原始的 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 方向相同，即 &lt;code&gt;softmax(dim=1)&lt;/code&gt;。代码中也是一样，&lt;span class="math"&gt;\((\boldsymbol{QK})_{N\times h \times q \times k}\)&lt;/span&gt; 是 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 个 &lt;span class="math"&gt;\((\boldsymbol{Q}\boldsymbol{K}_i)_{q\times k}\)&lt;/span&gt; 子矩阵，要在所有子矩阵的列方向上做归一化，那么就是在第 4 个维度上做 Softmax，即 &lt;code&gt;softmax(dim=3)&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此时，上述过程已经完成了多头注意力中的 &lt;span class="math"&gt;\(\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\)&lt;/span&gt;，将结果记作 &lt;span class="math"&gt;\(\boldsymbol{A}_{N\times h\times q\times k}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;在下一步中，用 &lt;code&gt;"nhql,nlhd-&amp;gt;nqhd"&lt;/code&gt; 表示了 &lt;span class="math"&gt;\(\boldsymbol{A}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{V}\)&lt;/span&gt; 的乘法，具体操作是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times v\times h\times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times h\times v\times d}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;固定前两维，令 &lt;span class="math"&gt;\(\boldsymbol{A}_{N\times h\times q\times k}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times h\times v\times d}\)&lt;/span&gt; 在后两维上做乘法，这里有 &lt;span class="math"&gt;\(q=k=v\)&lt;/span&gt;，所以结果为 &lt;span class="math"&gt;\((AV)_{N\times h \times q\times d}\)&lt;/span&gt;，到这一步已经计算了 &lt;span class="math"&gt;\(\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\boldsymbol{V}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;将结果转置为 &lt;span class="math"&gt;\((AV)_{N\times q \times h\times d}\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后代码使用 &lt;code&gt;reshape()&lt;/code&gt; 合并后两维，将结果转化为 &lt;span class="math"&gt;\((AV)_{N\times q \times hd}\)&lt;/span&gt;，很巧妙地拼接了多个 head 的注意力，最后通过线性层再输出结果。&lt;/p&gt;
&lt;p&gt;至此，Transformer 中的 &lt;code&gt;SelfAttention&lt;/code&gt; 部分已经结束，读者或许会觉得头昏脑胀。不必担心，最为艰涩的一部分已经过去，接下来是一路下坡 🚩&lt;/p&gt;
&lt;h3 id="transformerblock"&gt;TransformerBlock&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class TransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TransformerBlock, self).__init__()
        # 前一层的多头注意力
        self.attention = SelfAttention(embed_size, heads)
        # Add &amp;amp; Norm 层
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        # 前馈层
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, value, key, query, mask):
        attention = self.attention(value, key, query, mask)

        x = self.dropout(self.norm1(attention + query))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;TransformerBlock&lt;/code&gt; 模块包括多头注意力与后接的 Add &amp;amp; Norm、Feed Forward、Add &amp;amp; Norm 三层。&lt;/p&gt;
&lt;p&gt;初始化部分使用 &lt;code&gt;nn.Sequential()&lt;/code&gt; 将 &lt;code&gt;nn.Linear()&lt;/code&gt;、&lt;code&gt;nn.ReLU()&lt;/code&gt;、&lt;code&gt;nn.Linear&lt;/code&gt; 依次连接起来形成前馈层，正如前文所说的，数据进入前馈层先升维再激活，最后再降回原来维度，&lt;code&gt;forward_expansion&lt;/code&gt; 决定升维的倍数。&lt;code&gt;dropout&lt;/code&gt; 用于随机弃用一部分数据防止过拟合，直接调用 &lt;code&gt;nn.Dropout()&lt;/code&gt; 类，接收的数值决定了弃用数据的比例。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 部分也很简单，计算的多头注意力依次做 Add &amp;amp; Norm、Feed Forward、Add &amp;amp; Norm 三层后输出数据。&lt;/p&gt;
&lt;h3 id="encoder"&gt;Encoder&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Encoder(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        embed_size,
        num_layers,
        heads,
        device,
        forward_expansion,
        dropout,
        max_length,
    ):

        super(Encoder, self).__init__()
        self.embed_size = embed_size
        # CPU or GPU
        self.device = device
        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)
        self.position_embedding = PositionalEncoding(embed_size, max_length)

        self.layers = nn.ModuleList(
            [
                TransformerBlock(
                    embed_size,
                    heads,
                    dropout=dropout,
                    forward_expansion=forward_expansion,
                )
                for _ in range(num_layers)
            ]
        )

        self.dropout = nn.Dropout(dropout)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Encoder 是 Transformer 中的左边部分，Transformer 中有 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个 &lt;code&gt;TransformerBlock&lt;/code&gt; 顺序叠放在一起组成 encoder。所以在初始化部分，使用列表推导式在 &lt;code&gt;layers&lt;/code&gt; 中放置了 &lt;code&gt;num_layers&lt;/code&gt; 层 &lt;code&gt;TransformerBlock&lt;/code&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;# class Encoder(nn.Module):
    def forward(self, x, mask):
        # 输入数据的 batch_size 与长度
        N, seq_length = x.shape
        # 从输入数据计算位置索引
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        # 由位置索引得到位置编码，并 dropout 一部分数据
        out = self.dropout(
            (self.word_embedding(x) + self.position_embedding(positions))
        )

        # 让数据逐层经过 encoder，计算自注意力
        for layer in self.layers:
            out = layer(out, out, out, mask)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 &lt;code&gt;forward()&lt;/code&gt; 部分中，使用 &lt;code&gt;torch.arange()&lt;/code&gt; 得到位置索引，再用 &lt;code&gt;expand()&lt;/code&gt; 方法将位置索引矩阵的形状变为与输入数据相同，&lt;code&gt;expand()&lt;/code&gt; 方法的主要作用是复制，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; torch.arange(0, 5)
tensor([0, 1, 2, 3, 4])
&amp;gt;&amp;gt;&amp;gt; torch.arange(0, 5).expand(2, 5)
tensor([[0, 1, 2, 3, 4],
        [0, 1, 2, 3, 4]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;to()&lt;/code&gt; 方法用于指定 Tensor 存储的设备，例如 &lt;code&gt;"CPU"&lt;/code&gt; 或 &lt;code&gt;"GPU"&lt;/code&gt;。将词嵌入加上位置编码得到 &lt;code&gt;out&lt;/code&gt;，再将 &lt;code&gt;out&lt;/code&gt; 送入 encoder 中计算结果。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;layer(out, out, out)&lt;/code&gt; 看起来或许有些奇怪，请留意，前文已经讨论过，在 encoder 中计算的是&lt;strong&gt;自注意力&lt;/strong&gt;，所以此时的 query、key、value 都是相同的，而在 decoder 中就会有所不同了。&lt;/p&gt;
&lt;h3 id="decoderblock"&gt;DecoderBlock&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class DecoderBlock(nn.Module):
    def __init__(self, embed_size, heads, forward_expansion, dropout, device):
        super(DecoderBlock, self).__init__()
        self.norm = nn.LayerNorm(embed_size)
        self.attention = SelfAttention(embed_size, heads=heads)
        self.transformer_block = TransformerBlock(
            embed_size, heads, dropout, forward_expansion
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, value, key, src_mask, trg_mask):
        attention = self.attention(x, x, x, trg_mask)
        query = self.dropout(self.norm(attention + x))
        out = self.transformer_block(value, key, query, src_mask)
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;类似地，Decoder 是 Transformer 结构图中的右侧部分，也是由 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 层 &lt;code&gt;DecoderBlock&lt;/code&gt; 组成。decoder 只比 encoder 多了一个掩码注意力层，其他结构相同，所以 &lt;code&gt;DecoderBlock&lt;/code&gt; 的初始化中直接调用了先前定义的 &lt;code&gt;TransformerBlock&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 中，target 进入 decoder 后，先计算&lt;strong&gt;自注意力&lt;/strong&gt;（&lt;code&gt;attention(x, x, x)&lt;/code&gt;），再经过 Add &amp;amp; Norm 层得到 &lt;code&gt;query&lt;/code&gt;，再与 encoder 中的结果做多头注意力（&lt;code&gt;attention(value, key, query)&lt;/code&gt;），输出结果。留意两种注意力计算的不同，参考 Transformer 结构图理解一下就会很明确。&lt;/p&gt;
&lt;h3 id="decoder"&gt;Decoder&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Decoder(nn.Module):
    def __init__(
        self,
        trg_vocab_size,
        embed_size,
        num_layers,
        heads,
        forward_expansion,
        dropout,
        device,
        max_length,
    ):
        super(Decoder, self).__init__()
        self.device = device
        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)
        self.position_embedding = PositionEmbedding(embed_size,max_length)

        self.layers = nn.ModuleList(
            [
                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)
                for _ in range(num_layers)
            ]
        )
        self.fc_out = nn.Linear(embed_size, trg_vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, src_mask, trg_mask):
        N, seq_length = x.shape
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))

        for layer in self.layers:
            x = layer(x, enc_out, enc_out, src_mask, trg_mask)

        out = self.fc_out(x)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;实现了 &lt;code&gt;DecoderBlock&lt;/code&gt; 后，&lt;code&gt;Decoder&lt;/code&gt; 就没有什么内容了，与 encoder 类似，就是将多个 &lt;code&gt;DecoderBlock&lt;/code&gt; 组装起来，按接口传入数据进行计算。&lt;/p&gt;
&lt;h3 id="transformer"&gt;Transformer&lt;/h3&gt;
&lt;p&gt;最后的 &lt;code&gt;Transformer&lt;/code&gt; 将各个模块都组合起来：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        trg_vocab_size,
        src_pad_idx,
        trg_pad_idx,
        embed_size=512,
        num_layers=6,
        forward_expansion=4,
        heads=8,
        dropout=0,
        device="cpu",
        max_length=100,
    ):

        super(Transformer, self).__init__()

        self.encoder = Encoder(
            src_vocab_size,
            embed_size,
            num_layers,
            heads,
            device,
            forward_expansion,
            dropout,
            max_length,
        )

        self.decoder = Decoder(
            trg_vocab_size,
            embed_size,
            num_layers,
            heads,
            forward_expansion,
            dropout,
            device,
            max_length,
        )

        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        self.device = device

    def make_src_mask(self, src):
        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)
        # (N, 1, 1, src_len)
        return src_mask.to(self.device)

    def make_trg_mask(self, trg):
        N, trg_len = trg.shape
        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(
            N, 1, trg_len, trg_len
        )

        return trg_mask.to(self.device)

    def forward(self, src, trg):
        src_mask = self.make_src_mask(src)
        trg_mask = self.make_trg_mask(trg)
        enc_src = self.encoder(src, src_mask)
        out = self.decoder(trg, enc_src, src_mask, trg_mask)
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;初如化部分主要是设定了默认的参数，并引入前面定义好的 &lt;code&gt;Encoder&lt;/code&gt; 与 &lt;code&gt;Decoder&lt;/code&gt; 模块。&lt;code&gt;Transformer&lt;/code&gt; 中还多了 &lt;code&gt;make_src_mask()&lt;/code&gt; 与 &lt;code&gt;make_trg_mask()&lt;/code&gt; 两个函数，这就不得不谈谈 Transformer 中的掩码机制了。&lt;/p&gt;
&lt;p&gt;考虑一个情境，需要使用 Transformer 翻译一批（若干条）句子，各句子的长度自然是不同的，那么输入模型的数据的形状也是不同的，这在后续步骤中就会出现很多问题。在实际中，通常会找到文本中最长的句子（&lt;code&gt;max_len&lt;/code&gt;），再将所有句子都变为该长度，这种操作称为 padding。&lt;/p&gt;
&lt;p&gt;具体做法如下图所示，分别用 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 与 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 标记句子的起讫，用 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 填充 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 后的空位，各数据的长度就会一致。然后根据设定的词典，将 token 转化为索引，接着再做词嵌入。&lt;code&gt;make_src_mask()&lt;/code&gt; 就是根据 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 的索引，将 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 所在位置都标记为 &lt;code&gt;False&lt;/code&gt;，其他位置标记为 &lt;code&gt;True&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;后续 &lt;code&gt;unsqueeze()&lt;/code&gt; 的操作比较费解，其实它是利用了 PyTorch 的广播机制，用于自动匹配矩阵的形状。图中的例子可以看作是将矩阵翻转再在第 3 个方向上拉长。因为代码中的掩码要用于掩盖形状为 &lt;code&gt;[N, heads, query_len, key_len]&lt;/code&gt; 具有 4 个方向的 &lt;code&gt;energy&lt;/code&gt;，所以要额外再做一次 &lt;code&gt;unsqueeze()&lt;/code&gt;。最后将掩码用于掩盖词嵌入数据，掩码就像一个罩子盖在词嵌入数据上，模型只计算 &lt;code&gt;True&lt;/code&gt; 位置上的数据。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;使用掩码可以让模型灵活地处理不同长度的数据，数据的长度由掩码决定，改变掩码就相当于改变处理的数据，而不去改变存储在硬件中的数据，这对于计算更有利。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;make_trg_mask()&lt;/code&gt; 函数产生用于 target 数据的掩码，在 target 上使用掩码的原因与 source 不同。在 decoder 中，模型要根据输入数据的计算结果给出新 token，而生成文本的过程是顺序的，依赖于前一步生成的结果。具体来说就是，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;序列以 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 标记起始；&lt;/li&gt;
&lt;li&gt;根据已有的 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 生成 &lt;code&gt;A&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;根据生成的 &lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt; 生成 &lt;code&gt;B&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;根据生成的 &lt;code&gt;&amp;lt;s&amp;gt; A B&lt;/code&gt; 生成 &lt;code&gt;C&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;以此类推，直至模型生成 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt;，句子结束。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;前文已经讨论过，这种方法有很多局限性，而 Transformer 的巧妙之处就在于能够并行完成这个过程。&lt;/p&gt;
&lt;p&gt;我们可以考虑训练过程，实际上与生成过程类似，训练过程就是要根据已经生成的 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 建立与下一个 token &lt;code&gt;A&lt;/code&gt; 的关系，而不能是与后续 &lt;code&gt;B&lt;/code&gt; 或 &lt;code&gt;C&lt;/code&gt; 的关系，将这种关系以参数的形式存储到模型中，推理阶段就能顺利地根据 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 生成 &lt;code&gt;A&lt;/code&gt;。这样的训练过程可以表示为一个下三角矩阵，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Transformer 不需要逐个 token 生成再建立关系，可以通过下三角矩阵一次直接取出 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt;、&lt;code&gt;&amp;lt;s&amp;gt; A B&lt;/code&gt; 等 token 序列，并行地训练模型与对应的下一个 token 建立关系。最后将 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 与每一步骤中新生成 token &lt;code&gt;A&lt;/code&gt;、&lt;code&gt;B&lt;/code&gt;、&lt;code&gt;C&lt;/code&gt;、&lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 拼合起来，即得到生成的文本。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;make_trg_mask()&lt;/code&gt; 就是在构建这个下三角的掩码。&lt;code&gt;torch.ones()&lt;/code&gt; 用于生成指定大小元素全为 &lt;code&gt;1&lt;/code&gt; 的矩阵，然后用 &lt;code&gt;torch.tril()&lt;/code&gt; 取该矩阵的下三角，再用 &lt;code&gt;expand()&lt;/code&gt; 方法将该矩阵复制到与 &lt;code&gt;batch_size&lt;/code&gt; 匹配。&lt;/p&gt;
&lt;h3 id="train"&gt;Train&lt;/h3&gt;
&lt;p&gt;从前面讨论的模型生成过程还可以知道的一点是，模型永远不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，所以 target 中没有 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，而 source 则必须由 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 起始。在实际中，一种做法是，用预处理的脚本在原始训练数据（例如 &lt;code&gt;.csv&lt;/code&gt;、&lt;code&gt;.txt&lt;/code&gt; 文件）中标上标记；另一种方法是，在训练代码中加入预处理的功能，读取数据时分别为数据做上相应标记。为了方便起见，本文就不实现这一部分功能，使用 Transformer 可以直接处理的数据。&lt;/p&gt;
&lt;p&gt;生成训练数据的函数为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def generate_random_batch(batch_size, max_length=16):
    src = []
    for i in range(batch_size):
        # 随机指定有效数据的长度
        random_len = random.randint(1, max_length - 2)
        # 在数据起讫处加上标记，"&amp;lt;s&amp;gt;": 0, "&amp;lt;e&amp;gt;": 1
        random_nums = [0] + [random.randint(3, 9) for _ in range(random_len)] + [1]
        # padding 填满数据长度，"&amp;lt;p&amp;gt;": [2]
        random_nums = random_nums + [2] * (max_length - random_len - 2)
        src.append(random_nums)

    src = torch.LongTensor(src)
    # tgt 去除末尾的 token
    tgt = src[:, :-1]
    # tgt_y 去除首个 &amp;lt;s&amp;gt;，即模型需要预测的 token，用于计算损失
    tgt_y = src[:, 1:]
    # 模型需要预测的 token 数量（不计 &amp;lt;p&amp;gt;），用于计算损失函数
    n_tokens = (tgt_y != 2).sum()

    return src, tgt, tgt_y, n_tokens
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;generate_random_batch()&lt;/code&gt; 能够生成 Transformer 可以直接计算的相同的 source 与 target，该模型的任务目标就是生成与输入相同的序列。模型不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，所以&lt;code&gt;tgt_y&lt;/code&gt; 去除 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 用于与生成的序列对比计算损失，这很容易理解。但为什么 &lt;code&gt;tgt&lt;/code&gt; 需要去除最后一个 token 呢？这一点我将在后文生成序列的 Predict 一节讨论。训练与测试模型的代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# &amp;lt;p&amp;gt; 索引
src_pad_idx = 2
trg_pad_idx = 2
# 词表大小，即全部 token 数量，包括 &amp;lt;s&amp;gt; &amp;lt;e&amp;gt; &amp;lt;p&amp;gt; 等标记
src_vocab_size = 10
trg_vocab_size = 10
# 文本最大长度
max_len = 16

model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,
                    embed_size=128, num_layers=2, dropout=0.1, max_length=max_len,
                    device=device).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
criteria = nn.CrossEntropyLoss()
total_loss = 0

for step in range(2000):
    src, tgt, tgt_y, n_tokens = generate_random_batch(batch_size=2, max_length=max_len)
    optimizer.zero_grad()
    out = model(src, tgt)

    # contiguous() 与 view() 将矩阵在各行首尾相连为一行（即向量）
    # 在两向量间计算损失函数
    # tgt_y 中元素的值是索引，除以 n_tokens 将其缩放到 [0, 1]
    loss = criteria(out.contiguous().view(-1, out.size(-1)),
                    tgt_y.contiguous().view(-1)) / n_tokens
    loss.backward()
    optimizer.step()

    total_loss += loss

    if step != 0 and step % 40 == 0:
        print(f"Step {step}, total_loss: {total_loss}")
        total_loss = 0

# Predict
copy_test(model, max_len)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PyTorch 使用 &lt;code&gt;torch.optim&lt;/code&gt; 定义模型的训练过程，其中可以选择非常多种的优化过程，这里选择了 &lt;code&gt;Adam()&lt;/code&gt;，&lt;code&gt;lr=3e-4&lt;/code&gt; 指定了训练步骤的学习率。&lt;code&gt;nn.CrossEntropyLoss()&lt;/code&gt; 用于计算两个向量的交叉熵损失，作为训练过程的损失函数。&lt;/p&gt;
&lt;p&gt;在训练循环中，每一个循环处理 1 个 batch 的数据，在同一个 batch 中 PyTorch 自动计算梯度的反向传播并更新参数。但在新的 batch 中，因为已经更新到参数中了，我们不希望保留上一个 batch 的梯度，所以用 &lt;code&gt;optimizer.zero_grad()&lt;/code&gt; 将梯度清空。&lt;/p&gt;
&lt;p&gt;将 &lt;code&gt;src&lt;/code&gt; 与 &lt;code&gt;tgt&lt;/code&gt; 传入模型，&lt;code&gt;out&lt;/code&gt; 就是 Transformer 的计算结果。&lt;code&gt;loss.backward()&lt;/code&gt; 与 &lt;code&gt;optimizer.step()&lt;/code&gt; 两行代码就是前面所说的让 PyTorch 自动计算梯度的反向传播并更新参数。&lt;/p&gt;
&lt;h3 id="predict"&gt;Predict&lt;/h3&gt;
&lt;p&gt;训练结束后，我用 &lt;code&gt;copy_test()&lt;/code&gt; 函数测试模型的效果，这个测试函数定义为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def copy_test(model, max_len):
    model = model.eval()
    src = torch.LongTensor([[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]])
    # 模型从 &amp;lt;s&amp;gt; 开始生成序列，但不会生成 &amp;lt;s&amp;gt;，所以指定起始的 &amp;lt;s&amp;gt;
    tgt = torch.LongTensor([[0]])

    for i in range(max_len):
        # out： (1, i + 1, 10)
        # i + 1 模型输出的 token 数量
        # 10 为 vocab_size，是词表中 token 数量，out 是词表中各 token 在此处出现的概率
        out = model(src, tgt)
        # 取输出的 i + 1 个 token 中的最后一个
        # predict: (1, 10)
        predict = out[:, -1]
        # 取得概率最大的 token 索引
        # y: (1, )
        y = torch.argmax(predict, dim=1)
        # 逐个拼合 token 索引
        # y.unsqueeze(0): (1, 1)
        # tgt: (1, i + 1 )
        tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)
        # 若生成 token &amp;lt;e&amp;gt;，表示句子结束，退出循环
        if y == 1:
            break
    print(tgt)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;eval()&lt;/code&gt; 方法令模型退出训练模式，会关闭 dropout 等训练过程中才需要的功能。在循环中逐个拼合生成的 token，就能得到生成的句子。循环中的操作如下图所示，在第 1 次循环中，&lt;code&gt;tgt&lt;/code&gt; 为 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，通过与 &lt;code&gt;src&lt;/code&gt; 的注意力与下三角矩阵得到计算结果 &lt;code&gt;out&lt;/code&gt; 为 &lt;code&gt;A&lt;/code&gt;，然后将 &lt;code&gt;tgt&lt;/code&gt; 更新为 &lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt;，在第 2 次循环中，得到的 &lt;code&gt;out&lt;/code&gt; 为 &lt;code&gt;A B&lt;/code&gt;，所以在每次循环中都只取新生成的 &lt;code&gt;out[-1]&lt;/code&gt; 更新 &lt;code&gt;tgt&lt;/code&gt;，最后将结果拼接起来得到完整的输出结果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;或许读者会有疑惑，既然使用下三角矩阵并行计算是 Transformer 的优势，为什么这里却是用循环顺序地生成呢？为什么计算上图中最后一个矩阵的 &lt;code&gt;out&lt;/code&gt;，而是要用一个个的 &lt;code&gt;out[-1]&lt;/code&gt; 呢？&lt;/p&gt;
&lt;p&gt;要注意的是，训练与生成有重要的一个不同，就是生成中的 &lt;code&gt;tgt&lt;/code&gt; 是空白的、模型不可知的，而训练中的 &lt;code&gt;tgt&lt;/code&gt; 是完整的、模型可知的。如上图中，&lt;code&gt;tgt&lt;/code&gt; 在每个循环中都在变长，只有 &lt;code&gt;tgt&lt;/code&gt; 变成了 &lt;code&gt;&amp;lt;s&amp;gt; A B C &amp;hellip;&lt;/code&gt; 才会有最后一个矩阵中的 &lt;code&gt;out&lt;/code&gt;。如果说只要最后一个矩阵中的 &lt;code&gt;out&lt;/code&gt; 而不要前面的步骤，就变成了「吃两个馒头吃饱，所以只吃后一个能吃得饱的馒头」的笑话。&lt;/p&gt;
&lt;p&gt;所以&lt;dot&gt;生成过程并不是并行的，Transformer 的并行指的是训练过程&lt;/dot&gt;。如下图所示，在训练过程中 Transformer 只需要做一次下三角矩阵的运算就可以建立多个 token 间的关系。这张图还解释了模型永远不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 但 &lt;code&gt;tgt&lt;/code&gt; 必须以 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 起始的原因。图中还可以很明白的看出为什么先前的训练代码要去除 &lt;code&gt;tgt&lt;/code&gt; 末尾的 token，因为 Transformer 的输出 &lt;code&gt;out&lt;/code&gt; 计算的是 &lt;code&gt;tgt&lt;/code&gt; 下一个 token（及此前）的计算结果，若不去除末位就超出范围了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后训练与测试的结果为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;cpu
Step 40, total_loss: 4.021485328674316
Step 80, total_loss: 2.8817126750946045
&amp;hellip;&amp;hellip;
Step 1920, total_loss: 0.9760974049568176
Step 1960, total_loss: 0.8644390106201172
tensor([[0, 6, 3, 4, 5, 7, 6, 4, 3, 1]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出的结果没有输出 source &lt;code&gt;[[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]]&lt;/code&gt; 中末尾代表 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 的 &lt;code&gt;2&lt;/code&gt;，前面的 token 索引也与 source 相差无几，说明模型正确复制了输入序列，训练是成功的。&lt;/p&gt;
&lt;h2 id="hou-ji_1"&gt;后记&lt;/h2&gt;
&lt;p&gt;至此，这篇 Transformer 的介绍终于告一段落了。从起草、绘图再到最后的代码梳理，前后花了一周多的时间。虽名为介绍，其实还是为自己在做梳理，边写边想、边想边查，终于把 Transformer 中的一些细节弄明白了，这篇笔记也能为读者勾勒出一个大致的图景。&lt;/p&gt;
&lt;p&gt;当然，限于篇幅，限于「从零起步」的初衷，也限于笔力，还有许多更深层次问题都没有探讨，但我相信，在看懂了这篇笔记之后，再去阅读那些文章已经不成问题了，这也符合我的初心。&lt;/p&gt;
&lt;p&gt;或许读者还很困惑，疑惑为什么数学推导上并不那么严谨的模型居然能有效，甚至具有极好的表现，那就说明需要钻入研究 Transformer 的底层了，不可不再读些更专业的文章。我也把写这篇文章时所参考以及较好的相关资料罗列于后，以飨读者。&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Vaswani, A. et al. Attention Is All You Need (2017) - arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spaces.ac.cn/archives/4765" rel="noopener" target="_blank"&gt;《Attention is All You Need》浅读（简介+代码）- 科学空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spaces.ac.cn/archives/6933" rel="noopener" target="_blank"&gt;从语言模型到 Seq2Seq：Transformer 如戏，全靠 Mask - 科学空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" rel="noopener" target="_blank"&gt;Language Modeling with nn.Transformer and torchtext - PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener" target="_blank"&gt;The Illustrated Transformer - Jay Alammar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/wevolf/p/12484972.html" rel="noopener" target="_blank"&gt;Transformer 源码中 Mask 机制的实现 - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/434232512" rel="noopener" target="_blank"&gt;torch.einsum 详解 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.csdn.net/zhaohongfei_358/article/details/126019181" rel="noopener" target="_blank"&gt;Pytorch 中 nn.Transformer 的使用详解与 Transformer 的黑盒讲解 - CSDN 博客&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Python"></category><category term="PyTorch"></category><category term="Transformer"></category></entry><entry><title>文献总结｜药物发现中的匹配分子对分析：方法与当前应用</title><link href="https://leonis.cc/sui-sui-nian/2023-04-15-summary-doi.org/10.1021/acs.jmedchem.2c01787.html" rel="alternate"></link><published>2023-04-15T00:00:00+08:00</published><updated>2023-04-15T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-15:/sui-sui-nian/2023-04-15-summary-doi.org/10.1021/acs.jmedchem.2c01787.html</id><summary type="html">&lt;p&gt;本文介绍 2023 年由曹东升与侯廷军研究团队发表在 &lt;em&gt;Journal of Medicinal Chemistry&lt;/em&gt; 上的一篇展望，文章原标题为 Matched Molecular Pair Analysis in Drug Discovery: Methods and Recent Applications，文章介绍了主要介绍了匹配分子对分析的理论与目前基于匹配分子对分析的实际应用。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/acs.jmedchem.2c01787" rel="noopener" target="_blank"&gt;doi.org/10.1021/acs.jmedchem.2c01787&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍 2023 年由曹东升与侯廷军研究团队发表在 &lt;em&gt;Journal of Medicinal Chemistry&lt;/em&gt; 上的一篇展望，文章原标题为 Matched Molecular Pair Analysis in Drug Discovery: Methods and Recent Applications，文章介绍了主要介绍了匹配分子对分析的理论与目前基于匹配分子对分析的实际应用。&lt;/p&gt;
&lt;p&gt;匹配分子对（matched molecular pair, MMP）的概念自提出以来，已成为了从化合物中提取药物化学知识并用于指导先导化合物优化的标准方法，MMP 的定义是只在局部具有较小的结构差异的一对化合物。合成化学家、药物化学家借助匹配分子对分析（molecular matched pair analysis, MMPA）的手段，可以从人类研究过的海量化合物中总结出化学改造的方法、化学改造对于化合物性质的影响等重要经验知识。&lt;/p&gt;
&lt;h2 id="mmpa-li-lun"&gt;MMPA 理论&lt;/h2&gt;
&lt;h3 id="mmp-sou-suo-suan-fa"&gt;MMP 搜索算法&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8784?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8784?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在需要对大量分子数据做 MMPA 时，首要任务就是提取出其中的 MMP，MMP 搜索算法可以分为 3 类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;预设的变换规则：使用人为设计的切分规则分割分子，寻找分子数据中的 MMP，常用规则如 retrosynthetic combinatorial analysis procedure（RECAP）和 breaking of retrosynthetically interesting chemical substructures（BRICS）。这种方法的局限性也很明显，例如忽略了预设规则以外的 MMP 并且只能处理单点的化学结构变换。&lt;/li&gt;
&lt;li&gt;基于最大公共子结构（maximum common substructure, MCS）的方法：先寻找指定分子的的公共结构，将其设定为固定部分，只有具有公共结构的分子才能构成 MMP，分子中除去公共结构所剩余的结构就是改变部分，所以该方法通常用用于表示化学变换的 SMIRKS 存储 MMP。这种方法的问题在于计算 MCS 的计算开销很大。&lt;/li&gt;
&lt;li&gt;片段与索引（fragmentation and indexing, F+I）方法：该方法是目前寻找 MMP 最通用的方法，主要方法是在两非氢原子间的非环单键处切断，构建 key 与 value 片段的对应索引，通过键值对间的匹配寻找 MMP，具体方法可以&lt;a href="https://leonis.cc/sui-sui-nian/2023-02-25-summary-doi.org/10.1021/ci900450m.html" rel="noopener" target="_blank"&gt;参看前文&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="ying-xiang-mmpa-de-guan-jian-yin-su"&gt;影响 MMPA 的关键因素&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8785?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8785?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MMPA 的基本假设是，分子结构中一些小的结构改变将引起特定物理性质或是生物活性的改变。然而现实中化合物性质改变的原因更为复杂，表现出更为偶然的现象，例如分子改造中的活性悬崖（对分子仅做微小的改造而生物活性变化巨大）等，所以在 MMPA 中也要考虑到许多因素的影响。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分子表示：2D 与 3D 分子结构都被用于 MMPA 研究中，2D 分子描述的主要优点是处理简单，但许多实践表明 3D 分子表示方法表示了分子的空间信息，使其对于微小的结构差异更为敏感，这对 MMPA 十分重要。&lt;/li&gt;
&lt;li&gt;环境特征：在早期的研究中，人们认为只有 MMP 中的化学转换改变了分子的性质，因此只针对化学转换进行研究，而没有考虑具体分子。如今人们已经意识到，在 MMPA 还需要考虑具体分子的结构以及改造位点等环境特征，不能只研究 MMP 中的化学转化规则。目前，大部分研究使用分子图或 SMILES 来表示 MMP 中的完整分子，用于 MMPA 研究。除了分子信息以外，也有研究将蛋白口袋的信息也融入 MMPA，这有助于更深入研究 MMP 转化对受体与配体间结合作用影响。&lt;/li&gt;
&lt;li&gt;统计显著性：MMPA 的统计分析对于研究 MMP 间性质的变化十分重要，因为一种化学转换可以引起多种性质的改变，多种化学转换也可能使分子的某些性质不发生改变。MMPA 的统计学研究发现，在同一化合物上所做的两个结构改造所产生的影响远不同于单一结构改造影响的加和，这也称为「不可加和性」效应，这意味着简单的单一结构改造间存在着相互作用。不可加和性同样影响了分子的溶解度等性质，在 MMPA 中对可加和性进行统计分析，可以更好地识别药物分子的构效关系与分子中潜在的相互作用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="mmpa-shi-ji-ying-yong_1"&gt;MMPA 实际应用&lt;/h2&gt;
&lt;p&gt;MMPA 已经广泛应用在寻找得到目标性质分子所需的化学改造中（ADMET 优化），除了应用在先导化合物优化，MMPA 也用于靶点预测、生物电子等排体替换、构效关系确定、全新药物设计等任务中，这里主要介绍 MMPA 在分子结构改造和全新药物设计中的应用。&lt;/p&gt;
&lt;h3 id="pi-pei-fen-zi-xu-lie"&gt;匹配分子序列&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8786?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8786?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;将多个仅具有一个子结构区别的分子组织起来，就得到了匹配分子序列（matching molecular series, MMS），该方法最早被用于药物分子构效关系的分析，将不同 MMS 组织起来还得到形成匹配分子序列图，用于决策分子改造的路线。称为 SAR 转移的方法通过对比两个 MMS 间化合物性质的变化，可以判断替换结构的效果与。&lt;/p&gt;
&lt;h3 id="ji-yu-mmpa-de-quan-xin-yao-wu-she-ji"&gt;基于 MMPA 的全新药物设计&lt;/h3&gt;
&lt;p&gt;将 MMP 化学变换规则用于分子生成是全新药物设计中的重要步骤，输入的分子首先被分割为片段，然后通过 MMP 数据库搜索找到相应的化学转换，将这些化学转换用于输入分子就得到了新分子。也有研究提出了基于片段的 MMP 分子生成方法，主要步骤是收集 MMP 片段信息，通过遗传算法等方法合理地相互组合 MMP 片段，得到新分子。&lt;/p&gt;
&lt;p&gt;也有研究使用分子骨架和分子骨架以外的子结构来构建分子生成模型，模型是使用 SMILES 的 RNN 模型，第一步是生成正确的分子骨架，第二步在分子骨架上添加结构改造得到正确的分子。此外，MMS 方法可以很容易地将分子分为若干类的类似物，也可以很方便地用于全新药物设计。DeepSARM 模型的目标是寻找生物作用类似而化学结构新颖的类似物，就使用了 MMS 方法，模型同时还考虑了靶点信息，扩大的 MMS 方法的应用范围。&lt;/p&gt;
&lt;h2 id="zhan-wang_1"&gt;展望&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8787?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8787?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;分子设计所面临的一个重要难题是如何基于有限的实验数据决定下一步的分子改造，MMPA 有助于人们从已有的分子改造数据中得到化学转换的信息。为了能更好地利用 MMPA，文章提出了以下几点展望：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 QSAR 与 MMPA 相结合。QSAR 模型着重于整体的结构特征，MMPA 主要用于确定局部子结构的改变，在一定程度上二者是互补的，在未来 MMPA 也可能对 QSAR 模型的预测有帮助。&lt;/li&gt;
&lt;li&gt;将 MMPA 的概念用于蛋白质等大分子。&lt;/li&gt;
&lt;li&gt;融合 MMPA 相关的分子优化方法，构建自动化的分子优化流程。尽管目前 MMP 已经应用于分子生成，但 MMP 数据的提取等步骤还需要人工处理。文章提出了上图所示的预期 MMPA 工作流程，希望能够实现 MMP 的自动提取、组织、应用和评估。&lt;/li&gt;
&lt;/ol&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Review"></category></entry><entry><title>我的 2023 年春播计划</title><link href="https://leonis.cc/zai-lu-shang/2023-04-11-my-gardening-plan.html" rel="alternate"></link><published>2023-04-11T00:00:00+08:00</published><updated>2023-04-11T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-11:/zai-lu-shang/2023-04-11-my-gardening-plan.html</id><summary type="html">&lt;p&gt;过了惊蛰之后，万物萌动，看到园林工人正在整饬路旁的花圃，我的心里也跟着躁动起来，想着非要在阳台上种点什么才好，于是在一番调查与纠结之后确定了这篇春播计划。&lt;/p&gt;</summary><content type="html">&lt;p&gt;过了惊蛰之后，万物萌动，看到园林工人正在整饬路旁的花圃，我的心里也跟着躁动起来，想着非要在阳台上种点什么才好，于是在一番调查与纠结之后确定了这篇春播计划。&lt;/p&gt;
&lt;h2 id="pin-chong"&gt;品种&lt;/h2&gt;
&lt;p&gt;我的阳台正朝南方，一整天都可以受到阳光，非常适合种些花草，之所以一直闲置，还是因为漂泊在外，总担心有时无暇顾及这些无言的小小生灵。所以在我下定决心后，首先要考虑的就是草木的品种。由于春节会回家，到时就无人照看这些花草了，我优先考虑一年生的植物，并且植株的越冬所需要的操作一定要越简单越好。&lt;/p&gt;
&lt;p&gt;结合以上客观因素和个人喜好等主观因素下，我敲定了这几个大类别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;大丽花&lt;/strong&gt;：大丽花喜欢光照，光照越充足花开得越盛，正好符合阳台的光照条件，而在冬天枯萎后可以挖出种球保存，也不需要专门照看，所以非常合适；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;甘菊等一年生的切花&lt;/strong&gt;：我个人特别喜欢切花，不仅盛开时美，还可以剪下做成干花，某种意义上也算是经冬不凋，而天津的秋冬季十分干燥，制作干花实在太合适了；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;番茄等一年生的蔬果&lt;/strong&gt;：许多人调侃中国人的天赋是种菜，喜欢种蔬果胜过于种花草，其实蔬果也有许多园艺品种，除了食用以外也有很好的观赏价值，蔬果的生长周期短，播种时间可以很灵活，而且谁不想一尝丰收的喜悦呢？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我的春播计划包括 5 个具体的品种，用店家的图做个参考吧：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="朵拉大丽花" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8760?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="朵拉大丽花" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8760?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 朵拉大丽花 Dahlia 'Melody Dora'&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="小甘菊" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8761?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="小甘菊" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8761?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 小甘菊&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="文森特向日葵" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8762?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="文森特向日葵" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8762?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 文森特向日葵 Helianthus annuus 'Vincent's Fresh'&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="橙色番茄" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8763?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="橙色番茄" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8763?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 橙色番茄&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="墨西哥小番茄" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8764?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="墨西哥小番茄" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8764?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 墨西哥小番茄 Solanum lycopersicum 'Mexico Midget'&lt;/p&gt;
&lt;p&gt;{warn begin}品种学名来自于植物数据库 &lt;a href="https://garden.org/plants/" rel="noopener" target="_blank"&gt;Plants Database&lt;/a&gt;，主要靠对照图片推测，不一定准确。{warn end}&lt;/p&gt;
&lt;h2 id="gong-ju"&gt;工具&lt;/h2&gt;
&lt;p&gt;接着是一些园艺用具，包括花盆、花土、花肥等等。&lt;/p&gt;
&lt;p&gt;包括我在内的很多人都没有地栽的条件，只能购置花盆选择盆栽。花盆的上选当然是红陶盆，不仅透水透气还美观耐看，红陶盆的一个问题是太重，所以运费会导致价格比较高，也不适合漂泊在外没有自己固定居所的人；再一个问题是在植株换盆时，为了避免伤根，通常会把盆给敲碎，像我这样拮据的人很难容忍这种浪费。基于这些考虑，我选择了更便捷一些的塑料盆。&lt;/p&gt;
&lt;p&gt;盆的大小要依种植的植株品种来定，比较常用的尺寸是直径 15 cm 左右的花盆，容量大约 2 L。大丽花需要较大的花盆，可以按大丽花的株高选盆：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;株高 50 cm 以下，盆径 18 cm&lt;/li&gt;
&lt;li&gt;株高 50-90 cm，盆径 25 cm&lt;/li&gt;
&lt;li&gt;株高 90 cm 以上，盆径 30 cm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于番茄需要注意的是，番茄可以按生长模式分为两类&amp;mdash;&amp;mdash;有限生长型与无限生长型。不要把生长模式与植物的寿命混淆，番茄大部分都是一年生的，有限生长是指番茄的果实都在同一时期成熟，收完一茬就结束了；而无限生长型番茄的果实会在一段较长的时间内次序成熟，犹如永远收获不完，因而得名。&lt;dot&gt;有限生长型番茄大多是矮株，无限生长型番茄大多是高株&lt;/dot&gt;，所以要选择匹配的盆径。有限生长型番茄用 20 cm 的盆尚可，无限生长型番茄就需要用 30 cm 左右的花盆了，因为它们甚至能长到 2 米多高。&lt;/p&gt;
&lt;p&gt;在番茄品种的选择上出现了一些失误，把小番茄误当作了矮株番茄，收到种子后才得知我购买的两个品种都是无限生长型的番茄，于是慌忙之下又购买了几个大盆。&lt;/p&gt;
&lt;p&gt;那么我所种植的品种与花盆的搭配就是，矮型大丽花朵拉和两种番茄使用 25 cm 的大盆，向日葵搭配 20 cm 的花盆，小甘菊可以使用 17 cm 的盆。&lt;/p&gt;
&lt;p&gt;我购买的花土也简单，就是普通的泥炭土，先后买了大约 18 L，还不太确定用量，后续不够可以再买，这样也能先试试土的质量。&lt;/p&gt;
&lt;p&gt;至于肥料，我只购买了一些非常便宜的有机肥，没有购买无机肥，主要是因为花肥的品牌琳琅满目，花肥又不像花土一般适用，购买了不适配的就容易浪费，并且我对花肥的品牌不太了解也难以选择。再者作为学化学的，有什么化学试剂是我难以获得的呢，我相信我的专业水平更多于那些名不见经传的品牌。&lt;/p&gt;
&lt;h2 id="bo-chong"&gt;播种&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;朵拉大丽花&lt;/strong&gt;：大丽花在 3 月中旬左右就可以种下了，大丽花的花期很长，从夏季持续到秋季，所以在天气暖和了之后早些种下也有利于大丽花蓄积养份，为盛开做好准备。收到大丽花的球根后，我提前泡了一天的水，剪去一些枯干的残根，然后确认茎的方向，茎向上埋入土中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;小甘菊&lt;/strong&gt;：播种温度 15～20℃，我在 4 月 10 日种下。使用育苗块播种，种子用手指小心按进湿润的泥土中，等待发芽长大后再移栽到盆中，一个花盆中种植 3 棵左右。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文森特向日葵&lt;/strong&gt;：播种温度 15～30℃，向日葵大约 2 个月就能开花，我打算在 4 月 20 日左右种下，希望恰好在夏至时能看到黄灿灿的向日葵。据店家的说明，向日葵的发芽率很高，不需要育苗，直接在盆中挖 5 mm 左右的小坑，将葵花籽横放埋入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;橙色番茄&lt;/strong&gt;：播种温度 15～30℃，可以把播种时间向后推一些，在 4 月至 5 月种下，也有人喜欢秋播，但不太适合我。我打算在 4 月 15 日左右种下，同样使用育苗块播种，长大后再移栽到盆中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;墨西哥小番茄&lt;/strong&gt;：与橙色番茄相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在春天也忙碌起来了，快快发芽吧 🌱&lt;/p&gt;
&lt;h2 id="zhang-dan"&gt;账单&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;价格（CNY）&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;球根与种子&lt;/td&gt;
&lt;td&gt;38.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;花盆（5 个）&lt;/td&gt;
&lt;td&gt;54.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;泥炭土（18 L）&lt;/td&gt;
&lt;td&gt;21.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;鸡粪有机肥&lt;/td&gt;
&lt;td&gt;3.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;育苗块 + 育苗盒&lt;/td&gt;
&lt;td&gt;11.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;总计（计运费）&lt;/td&gt;
&lt;td&gt;142.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="在路上"></category><category term="园艺"></category></entry><entry><title>文献总结｜DrugEx v3：使用基于图 Transformer 的强化学习进行以分子骨架为约束的药物设计</title><link href="https://leonis.cc/sui-sui-nian/2023-04-06-summary-doi.org/10.1186/s13321-023-00694-z.html" rel="alternate"></link><published>2023-04-06T00:00:00+08:00</published><updated>2023-04-06T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-06:/sui-sui-nian/2023-04-06-summary-doi.org/10.1186/s13321-023-00694-z.html</id><summary type="html">&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 DrugEx v3: scaffold‑constrained drug design with graph transformer‑based reinforcement learning，文章介绍了使用包括 Transformer 和 LSTM 模型实现以分子骨架为约束的药物设计的方法并对比了使用 SMILES 与图两种方式的分子表示在分子生成中的区别。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1186/s13321-023-00694-z" rel="noopener" target="_blank"&gt;doi.org/10.1186/s13321-023-00694-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 DrugEx v3: scaffold‑constrained drug design
with graph transformer‑based reinforcement learning，文章介绍了使用包括 Transformer 和 LSTM 模型实现以分子骨架为约束的药物设计的方法，并且对比了使用 SMILES 与图两种分子表示方式在分子生成中的区别。&lt;/p&gt;
&lt;p&gt;在先前的工作中，作者设计了名为 DrugEx 的 RNN 模型，它能够通过基于分布的方式探索化学空间并通过强化学习的策略实现基于目标的分子生成，但它无法接受用户的输入，无法基于先验知识给出结果，只能在已有的化学空间中给出结果，当任务改变后又需要重新训练模型，这些方面的问题使其在具体应用上具有很大的局限性。&lt;/p&gt;
&lt;p&gt;因此这篇文章使用多种深度学习模型重构了 DrugEx，DrugEx 可以接受用户指定的分子骨架生成具有目标结构的分子，并且在模型中引入强化学习策略，更加有效地控制生成分子的目标性质，此外文章对比了不同深度学习模型和以 SMILES 或图两种不同编码方式实现以分子骨架为约束的药物设计的效果。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8738?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8738?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;由来源于 ChEMBL 的约 170 万条分子数据构成 ChEMBL 数据集，用于预训练模型，由对人腺苷受体具有活性的 10828 条分子数据构成 LIGAND 数据集，用于微调生成模型。&lt;/p&gt;
&lt;p&gt;所有数据都构建为「输入- 输出」对的形式，使用 BRICS 规则将每个分子分割为最多 4 个的一系列片段，片段的排列组合就作为输入部分，被分割的分子就作为输出部分。分割完成后，用于预训练生成模型的分子对数据有 9335410 万条。&lt;/p&gt;
&lt;p&gt;若以 SMILES 作为分子表示，则是使用词表将每条 SMILES 序列分为若干 token，就可以将 SMILES 表示为 token 的索引序列，用于模型计算。&lt;/p&gt;
&lt;p&gt;若以图作为分子表示，首先需要计算分子的临接矩阵，接着每个分子都会被表示为具有 5 行的矩阵，前两行分别代表原子类型和化学键类型，第三行表示连接原子的索引，第四行表示目前原子的索引，第五行表示片段索引。按列连接 start、fragment、growing、end 和 linking 五个部分的上述五种信息，start 与 end 两部分分别具有一列，只有标记分隔的作用，fragment 部分中组织了各分子骨架中的原子信息，growing 部分组织了分子中去除分子骨架后剩余原子的信息，linking 部分组织了 fragment 与 growing 部分间相互连接的化学键信息。&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8739?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8739?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章中设计了 4 种模型用于完成分子生成成任务：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;图 Transformer&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;LSTM + 注意力机制&lt;/li&gt;
&lt;li&gt;序列 Transformer&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中分子的图表示只用于图 Transformer 模型中，其他三种模型都使用 SMILES 表示分子。&lt;/p&gt;
&lt;p&gt;由于图 Transformer 无法同时处理原子与化学键的信息，因此按下式组合原子和化学键索引：&lt;/p&gt;
&lt;div class="math"&gt;$$W=T_\mathrm{atom}\times 4+T_\mathrm{bond}$$&lt;/div&gt;
&lt;p&gt;通过将原子类型与化学键类型的总数相乘再加上化学键类型得到结果 &lt;span class="math"&gt;\(W\)&lt;/span&gt;，用于计算词向量。&lt;/p&gt;
&lt;p&gt;由于图 Transformer 处理的不是序列信息，原有的位置编码计算方式同样无法使用，文章设计了以下位置编码：&lt;/p&gt;
&lt;div class="math"&gt;$$P=I_\mathrm{atom}\times L_\mathrm{max}+I_\mathrm{connected}$$&lt;/div&gt;
&lt;p&gt;式中将当前原子索引 &lt;span class="math"&gt;\(I_\mathrm{atom}\)&lt;/span&gt; 与最大长度 &lt;span class="math"&gt;\(L_\mathrm{max}\)&lt;/span&gt; 相乘，然后再加上连接原子的索引 &lt;span class="math"&gt;\(I_\mathrm{connected}\)&lt;/span&gt; 得到位置编码。&lt;/p&gt;
&lt;h3 id="ping-gu-zhi-biao"&gt;评估指标&lt;/h3&gt;
&lt;p&gt;为了更好评估生成分子的多样性，除了常见的分子指标外，文章中还使用了 Solow Polasky measurement，由下式给出：&lt;/p&gt;
&lt;div class="math"&gt;$$I(A)=\frac{1}{|A|}\boldsymbol{e}^\mathrm{T}F(\boldsymbol{s})^{-1}\boldsymbol{e}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 表示分子数据集，&lt;span class="math"&gt;\(|A|\)&lt;/span&gt; 为数据集大小，&lt;span class="math"&gt;\(\boldsymbol{e}\)&lt;/span&gt; 为 &lt;span class="math"&gt;\(|A|\)&lt;/span&gt; 维元素全为 &lt;span class="math"&gt;\(1\)&lt;/span&gt; 的向量，&lt;span class="math"&gt;\(F(s)=[f(d_{ij})]\)&lt;/span&gt;，&lt;span class="math"&gt;\(f(d_{ij})\)&lt;/span&gt; 是表示每对分子间距离的函数：&lt;/p&gt;
&lt;div class="math"&gt;$$f(d)=\mathrm{e}^{-\theta d_{ij}}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 为常数，取 &lt;span class="math"&gt;\(\theta=0.5\)&lt;/span&gt;，&lt;span class="math"&gt;\(d_{ij}\)&lt;/span&gt; 为分子 &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(s_j\)&lt;/span&gt; 的分子指纹间的谷本距离。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun_1"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8751?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8751?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先分别使用 ChEMBL 数据集预训练四种模型，再用 LIGAND 数据集微调预训练的生成模型，使用测试集生成分子的结果如上表所示。&lt;/p&gt;
&lt;p&gt;同样使用 SMILES 表示分子，相比于 LSTM 模型，训练 Transformer 模型需要的计算资源更多，但训练时间更短而且效果更好。使用 SMILES 的模型在微调后表现有所上升，但还是差于使用图的模型，这主要是由于用图表示分子更容易获得原子的几何关系信息。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8752?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8752?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章绘制了图 Transformer 生成分子的降维结果，可以看出图 Transformer 生成的分子很好地覆盖了 ChEMBL 和 LIGAND 两个数据集的化学空间。在对生成分子进一步评估中发现，图 Transformer 生成分子的可合成性低于使用 SMILES 的模型，作者认为这是因为基于图的模型能够生成更复杂的结构，导致可合成性降低。&lt;/p&gt;
&lt;p&gt;文章总结了图 Transformer 的 4 点优势：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;局部尺度上的不变性：图 Transformer 能够很好地识别输入的分子骨架，并使输出的生成分子中具有相同的结构；&lt;/li&gt;
&lt;li&gt;全局尺度上的可扩展性：图 Transformer 在生成分子的过程中，可以将生成部分直接插入到表示图的矩阵中，具有很大灵活性；&lt;/li&gt;
&lt;li&gt;无语法约束：图 Transformer 不需要关注 SMILES 语法要求，模型不需要额外学习分子中的语法特征；&lt;/li&gt;
&lt;li&gt;可引入化学规则：可以在图 Transformer 中引入化学规则，例如价键匹配规则，提高生成分子的准确性。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后文章还在图 Transformer 中引入了强化学习的策略，模型能够生成对 A&lt;sub&gt;2A&lt;/sub&gt;AR 的亲合力和 QED 分数更高的分子。文章中输入模型的分子骨架与生成分子如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8753?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8753?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章重构了以前的 DrugEx 的模型，在对多种深度模型的试验中，图 Transformer 具有最好的分子生成效果。相比于 SMILES，分子的图表示在分子生成任务中可以更好地识别输入的分子结构，并且可以很容易地改造分子结构生成分子，这一点在发现先导化合物以及先导化合物的优化上都能发挥很大的作用。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>文献总结｜用于从蛋白序列进行药物设计的深度生成模型</title><link href="https://leonis.cc/sui-sui-nian/2023-04-01-summary-doi.org/10.1186/s13321-023-00702-2.html" rel="alternate"></link><published>2023-04-01T00:00:00+08:00</published><updated>2023-04-01T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-01:/sui-sui-nian/2023-04-01-summary-doi.org/10.1186/s13321-023-00702-2.html</id><summary type="html">&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Deep generative model for drug design from protein target sequence，文章设计了一种基于 GAN 的蛋白配体分子生成模型，该模型只需要获取氨基酸序列的信息就可以生成相应蛋白口袋的配体。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1186/s13321-023-00702-2" rel="noopener" target="_blank"&gt;doi.org/10.1186/s13321-023-00702-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Deep generative model for drug design from protein target sequence，文章设计了一种基于 GAN 的蛋白配体分子生成模型，该模型只需要获取氨基酸序列的信息就可以生成相应蛋白口袋的配体。&lt;/p&gt;
&lt;p&gt;目前的分子生成方法可以分为两类，其中一种是基于配体的分子生成（ligand-based molecule generation, LBMG），另一种是基于口袋的分子生成（pocketbased
molecule generation, PBMG）。LBMG 方法难以跳出目前的化空间，因而难以生成具有新颖结构的分子；PBMG 方法需要获取更多蛋白口袋的信息，但计算蛋白 3D 构象通常开销巨大。&lt;/p&gt;
&lt;p&gt;文章提出了一种输入蛋白序列即可获得配体的分子生成模型，称为 DeepTarget。DeepTarget 既不需要考虑蛋白口袋的构象信息，也不需要在特定的分子库上微调，有效避免了上述两种方法的局限。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用了来自于 ChEMBL 的分子-蛋白对数据，经数据清洗后，共得到 551223 个分子-蛋白对，涉及 1970 种蛋白质与 333399 种分子的 SMILES 序列。&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8733?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8733?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;DeepTarget 由 3 个部分构成，分别是氨基酸序列嵌入（Amino Acid Sequence Embedding, AASE）、结构特征推理（Structural Feature Inference, SFI）和分子生成（Molecule Generation, MG）。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;氨基酸序列嵌入：AASE 是模型的嵌入层，使用了 Transformer 的架构，主要用于将序列数据转化为模型计算并处理的特征向量。&lt;/li&gt;
&lt;li&gt;结构特征推理：SFI 部分采用了 GAN 的结构，其主要任务是在 AASE 中得到蛋白特征表示上加上一定的噪声，再通过多层感知机得到潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;分子生成：MG 部分使用了 LSTM 结构，是一个预训练的解码器，它将潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 解码为目标分子。该解码器是一个在 ChEMBL 大数据集上训练好的模型，能将分子的潜变量转换为相似的分子。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;SFI 中的 GAN 模型是 DeepTarget 生成蛋白配体的关键，GAN 由生成器与分别器两个模块构成，生成器从潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 生成分子，而分别器则识别生成分子与该蛋白口袋之间的关系，训练 GAN 就是让生成器不断生成分子，直至生成的分子可以「欺骗」分别器，也就是此时生成的分子满足该蛋白口袋的配体分布特征。在推理阶段，则将此生成分子的表示送入 MG 中得到分子的 SMILES 编码。&lt;/p&gt;
&lt;p&gt;传统的生成模型关注于生成器与生成结果之间的关系，而在文章所设计的任务中，不同的蛋白口袋与配体分子在化学空间中有着不同的分布，这也是影响分子生成的因素。因此文章引入了对比学习（Contrastive Learning, CL）的手段，将不同的蛋白作为标签而使分子分簇，在相应的化学空间中生成分子。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun_1"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8734?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8734?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章首先针对 DRD2 和 PARP1 两个蛋白的活性口袋生成分子。图 a 展示了对应的真实配体分子与生成分子的对接打分，生成分子相对于真实分子向打分更低方向偏移，说明具有更高的亲合力。&lt;/p&gt;
&lt;p&gt;图 b 挑出了生成分子中的代表分子，与训练集分子计算相似性，两个分子与训练集的相似性都在 0.2-0.6 左右，说明这两个分子与训练集分子存在一定差异，DeepTarget 能生成新颖的分子。&lt;/p&gt;
&lt;p&gt;图 c 展示了是否在模型中引入对比学习的了生成结果，使用对比学习策略模型生成的分子明显向打分更低处偏移，具有更好的效果。&lt;/p&gt;
&lt;p&gt;图 d 测试了模型的泛化能力，先在训练集中删去 DRD2 和 PARP1 两个蛋白的数据，将生成分子与先前生成的分子对比，从测试结果中可以看出，删除相应训练数据后，生成分子的对接打分上升，但还是生成了相当数量打分低于 -6 的分子，作者认为这可以说明 DeepTarget 能针对训练集中不存在的活性口袋生成配体分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8735?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8735?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章对 DeepTarget 生成的分子与其他模型针对这两个口袋生成的分子做了评估，结果如上表所示，DeepTarget 生成分子的 Valid 高于两个 GAN 模型，其他指标与其他模型相当。&lt;/p&gt;
&lt;p&gt;作者认为这些指标只能做为模型的参考，因为模型并没有针对生成分子的 Valid 和 Unique 等指标进行优化，DeepTarget 的目标更着重于生成与指定口袋真正具有相互作用的配体分子。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章设计了一种基于 GAN 的蛋白配体分子生成模型，模型只需要获取氨基酸序列的信息就可以生成相应蛋白口袋的配体，文章验证了生成分子具有较好的对接打分，并且模型表现出了一定的泛化能力，可用于针对训练集以外的蛋白生成配体。但文章中设计的模型评估实验有限，只针对两个靶点生成了分子，从文章中的数据来看，与其他模型相比该模型没有特别明显的优势。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GAN"></category></entry><entry><title>在明清小说中索隐：读《中国叙事学》</title><link href="https://leonis.cc/gu-zhi-dui/2023-03-30-book-chinese-narratology.html" rel="alternate"></link><published>2023-03-30T00:00:00+08:00</published><updated>2023-03-30T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-30:/gu-zhi-dui/2023-03-30-book-chinese-narratology.html</id><summary type="html">&lt;p&gt;浦安迪的《中国叙事学》是一本讨论中国叙事传统和明清小说的小书，页数并不多，不消四五天即可翻完。虽然这是一本学术著作，但语言流畅、分析丝丝入扣，读起来时并不觉得枯燥乏味，反而觉得酣畅淋漓。书题虽为《中国叙事学》，但全书中专门论述 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;浦安迪的《中国叙事学》是一本讨论中国叙事传统和明清小说的小书，页数并不多，不消四五天即可翻完。虽然这是一本学术著作，但语言流畅、分析丝丝入扣，读起来时并不觉得枯燥乏味，反而觉得酣畅淋漓。书题虽为《中国叙事学》，但全书中专门论述中国叙事的篇幅并不多，只是在全书的前面部分章节立起「中国叙事」的概念与分析方法，在后文中则全是基于这些概念与方法来分析中国的奇书文体，也就是范围更窄的明清小说，我在后文中即以「明清小说」这一更为通俗的文学类别指代书中所说的「奇书文体」。&lt;/p&gt;
&lt;p&gt;全书只有前一部分中才真正讨论了「中国叙事学」，但这一部分中不乏许多精要切当的论述，让人觉得豁然开朗；作者将其作为分析方法，一以贯之用于剖析明清小说，又深让人惊异于明清小说原来还能这么读，读完这本书后再去重读明清小说，相信又会有另一番收获。基于以上的原因，本文另起了一个标题&amp;mdash;&amp;mdash;「在明清小说中索隐」，我认为能够更准确地概括全书的主题。&lt;/p&gt;
&lt;div class="bookshelf"&gt;
&lt;div class="book"&gt;
&lt;img referrerpolicy="no-referrer" src="https://img2.doubanio.com/view/subject/s/public/s33747413.jpg"/&gt;
&lt;div class="infos"&gt;
&lt;a class="title" href="https://book.douban.com/subject/30244064/"&gt;中国叙事学&lt;/a&gt;
&lt;div class="作者"&gt;作者：[美] 浦安迪 (Andrew H. Plaks)&lt;/div&gt;
&lt;div class="出版社"&gt;出版社：北京大学出版社&lt;/div&gt;
&lt;div class="出版年"&gt;出版年：2018-8&lt;/div&gt;
&lt;div class="页数"&gt;页数：292&lt;/div&gt;
&lt;div class="定价"&gt;定价：45.00元&lt;/div&gt;
&lt;div class="ISBN"&gt;ISBN：9787301295960&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="he-wei-xu-shi"&gt;何为叙事&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;任何时代，任何地方，任何社会，都少不了叙述。它从远古时代就开始存在，古往今来，哪里有人，哪里就有叙述。&lt;/p&gt;
&amp;mdash;&amp;mdash;法国当代文论家 罗兰・巴特&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;书中认为文学作品是在传递某种人生本质，那么文学中的三大体式就也可以按传递人生本质的方式区别，即&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;抒情诗：直接描绘绘静态的人生本质，有叙述人但没有故事&lt;/li&gt;
&lt;li&gt;戏剧：关注人生矛盾，通过舞台传达人生本质，有场面、故事但没有叙述人&lt;/li&gt;
&lt;li&gt;叙事文：不直接描绘人生本质，而以传事为主要目标，从而展示延绵不断的经验流中的人生本质&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们不妨回忆我们曾经接触过的文学作品，一定是可以将大体的内容归置到上述的 3 个类别中去的。之所以说「大体的内容」，就是因为三种体式间不是完全孤立的，实际上三者相互交融渗透，抒情中存在着叙事，叙事中同样存在着抒情。这里用这种机械的分类有助于我们厘清全书讨论的对象&amp;mdash;&amp;mdash;那也就是叙事文，在三者对比之下，相信我们对「何为叙事」已经有了朦胧的答案。&lt;/p&gt;
&lt;h2 id="xu-shi-wen-xue-de-yuan-liu"&gt;叙事文学的源流&lt;/h2&gt;
&lt;p&gt;西方叙事文学的历史可以概括为「&lt;dot&gt;史诗（epic）&amp;rarr; 罗曼史（romance）&amp;rarr; 小说（novel）&lt;/dot&gt;」。&lt;/p&gt;
&lt;p&gt;而中国的古代传统文学则是一条「三百篇 - 骚 - 赋 - 乐府 - 律诗 - 词曲 - 小说」的发展脉络，也就是说，中国传统文学的重心是抒情。&lt;/p&gt;
&lt;p&gt;中国的叙事文学源头应当是《尚书》以及《左传》，作者认为二者深刻地影响了后世的叙事文学，并画下了一定的定式。二者都属于史文，而后世的虚构文学则是从六朝志怪发源，分化为「文言小说」和「白话小说」两大类别。&lt;/p&gt;
&lt;p&gt;作者强调，文言小说和白话小说泾渭分明，「杂录」「志怪」等文言小说又被称为「史余」，与史文的关系更为密切，书中提到纪昀将《山海经》等书从史部抽出，纳入小说，可为一证。而白话小说则可能是由民间说书的「通俗文学」发展而来（鲁迅等持此说），也可能是由当时文人所创作的「才子书」（作者持此说）。&lt;/p&gt;
&lt;p&gt;书中将中国的叙事文学历史总结为「&lt;dot&gt;神话 &amp;rarr; 史文 &amp;rarr; 明清奇书&lt;/dot&gt;」，正如在对 novel 的批评不得不在 epic 的传统中一样，在批评中国的明清奇书时也脱不开神话与史文对其的影响。&lt;/p&gt;
&lt;h2 id="shi-wen-de-ying-xiang"&gt;史文的影响&lt;/h2&gt;
&lt;p&gt;在介绍书中分析史文与神话的内容之前，我想先引用顾颉刚先生的观点奠定一个基调，那就是应当审慎地对待传世文献，未经检验的文献是不可信的。经常有人被民族主义裹挟，把「疑古」简单地想象为「否认一切历史」并言之凿凿地抨击顾颉刚先生，我只能对此深表遗憾。在面对浩如烟海的史料时，「疑古」才是去伪存真、还原历史的科学方法。&lt;/p&gt;
&lt;p&gt;书中的想法与顾颉刚先生的观点不谋而合，我不了解作者是否参考了顾颉刚先生的著作，如若不然，就是顾颉刚先生从历史研究的经验中总结出了「疑古」的观点，而作者从文学分析的角度告诉读者对史文的应当审慎，可以称得上合作。原书中的文字足够精彩，无需我的赘言，兹引录原文如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;中国的史书虽然力图给我们造成一种客观记载的感觉，但实际上不外乎一种美学上的幻觉，是用各种人为的方法和手段造成的「拟客观」效果。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;由于中国历代长期形成的对史近乎宗教的狂热崇拜，也由于在清亡以前史料永远只对史官开放的历史事实，中国正史叙述者总是摆出一副「全知全能者」的姿态；然而，这种全知全能却只是局限在冠冕堂皇的庙堂里。它的触角甚至伸不进皇家的后院，当然更难看见「处江湖之远」的草民百姓的众生相。一种纯客观的叙事幻觉由此产生，并且成为一种经久不坏的模式，从史官实录到虚构文本，横贯中国叙事的各种文体。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;西方的史诗原则上是虚构的艺术，只与历史传说有些微弱的关联；而中国的史文对于「虚构」和「实事」却从来就没有严格的分界线。西方文学理论家一般认为，历史讲实事，小说讲虚构。中国古代批评家则强调，「历史中有小说，小说中有历史」。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;西人重「模仿」，等于假定所讲述的一切都是出于虚构，中国人尚「传述」，等于宣称所述的一切都出于实事。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="shen-hua-de-ying-xiang"&gt;神话的影响&lt;/h2&gt;
&lt;p&gt;由史文上溯至远古神话，可以发现中国神话具有「人本位」倾向，也就是通常借神话来表人事，因而常常把历史与神话混作一团，譬如说我们都知道宙斯是虚构的，而禹究竟是神是人仍无定论，再例如我们都知道十个太阳是断无可能的，但后羿是远古的君主还是人格化的神呢？或许我们习于这些神话，在面对这样的疑难时会将其推作中国的神话过于琐碎繁杂而不成体系，但希腊神话何尝不零碎，却没有这种明显的「人本位」特征。&lt;/p&gt;
&lt;p&gt;再来考查中神话的叙事，在中国古代典籍中，对神话的具体情节都是话焉不详的，这可以说是&lt;dot&gt;在中国的美学原动力中缺乏一种要求「头身尾」连贯的结构原型&lt;/dot&gt;，这种「非叙述性」美学原型导致了中西叙事传统的分流。这两种不同的叙事传统在神话中表现为希腊神话以时间为轴心，故重过程而善于讲述故事；中国神话以空间为宗旨，故重本体而善于画图案。&lt;/p&gt;
&lt;p&gt;这里要注意，神话是诞生于远古时代的故事，但其叙述未并产生于远古时代，例如《淮南子》《庄子》中的神话显然不能用于作神话叙述的分析，所以作者更花精力着眼于例如《尚书》中的神话。&lt;/p&gt;
&lt;h2 id="zhong-guo-de-xu-shi"&gt;中国的叙事&lt;/h2&gt;
&lt;p&gt;在神话与史文的孳乳中，中国的叙事形成了不同于西方的叙事传统，这种叙事传统的影响绵延至明清小说乃至其后。中国的叙事传统中，普遍将重点放在「事隙」上，与西方恰恰相反，也就是真正的具有动作的「事」，都被诸如宴会等「无事之事」包围。&lt;/p&gt;
&lt;p&gt;至于为什么中国的叙事传统如此，没有形成或是拋弃了「叙述性」，甚至在远古神话中已经接受了了这一范式，作者认为这可能是因为中国人重礼的传统，因而中国人把诸如阴阳交替、四时更换等仪礼形式作为了一种基本原则，从而将其「空间化」，抹杀了其中的「时间性」。&lt;/p&gt;
&lt;h2 id="ming-qing-xiao-shuo-zhong-de-xu-shi"&gt;明清小说中的叙事&lt;/h2&gt;
&lt;p&gt;今人在读明清小说时，相当大部分人会因为叙事中的时间性或因果性不强感觉琐碎或是乏味，想必很多人在中学时代读四大名著时，除了《西游记》与《三国演义》以外都觉得昏昏欲睡，这很大程度上就是因为《西游记》具有「取经」这条真正的具有动作的「事」贯穿始终，《三国演义》的时间性则明显更强，就算书中没有明显地以时间为线索，但汉末三分天下终而三家归晋的历史在开卷之前已经为读者熟知。&lt;/p&gt;
&lt;p&gt;西人在读明清小说时亦有这种感受，他们用「缀段性」批评明清小说的叙事方法。缀段性来源于西方对诗歌的文学批评，亚里士多德曾说「缀段性的情节是所有情节中最坏的一种。我所谓的缀段性情节，是指前后毫无因果关系而串接成的情节。」&lt;/p&gt;
&lt;p&gt;但是我们要注意，这是批评西方诗歌的方法，将这一术语用于评价明清小说时已经陷入了西方视角。西方之所以贬斥缀段性，是因为西方文学中有重视「头身尾」结构完整性的传统，这种叙述完整性的要求下，缀段性显然是一种低格。但这种结构完整性的要求并不是金科玉律，我们将眼光放回到「叙事」这一行为上，就会发现叙事天然就带有一定缀段性的特征，因为叙事就是在处理人类经验的一个个片段单元，例如中国的史文，就是由史官将一长段的时间线的人类经验分割为一个个事件片段而创作出的巨制。&lt;/p&gt;
&lt;p&gt;明清小说在结构上的一个明显特征就是「百回」，古人刻意将一部小说分为百回或是百廿回本身就反映了一种中式的美感，一种中国人追求平衡的美学倾向。再者，明清小说的百回完帙在出版时又惯被分作十卷，这是今人难以注意到的细节，同时这也不是出于偶然。作者发现，每十回就能组成一个小故事，百回中的十个小故事又现出一种特殊的韵律感。&lt;/p&gt;
&lt;p&gt;若以《水浒传》为为例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;武（松）十回&lt;/li&gt;
&lt;li&gt;林（冲）十回&lt;/li&gt;
&lt;li&gt;宋（江）十回&lt;/li&gt;
&lt;li&gt;第七十二至八十二回：受招安&lt;/li&gt;
&lt;li&gt;第八十三至九十回：平辽&lt;/li&gt;
&lt;li&gt;第九十一至一百回：平田虎&lt;/li&gt;
&lt;li&gt;第一百一至一百十回：平王庆&lt;/li&gt;
&lt;li&gt;第一百十一至一百二十回：平方腊&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若以《三国演义》为例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一至九回：董卓传，至董卓身死&lt;/li&gt;
&lt;li&gt;第十至十九回：吕布传，至吕布战死白门楼&lt;/li&gt;
&lt;li&gt;第二十至三十一回：曹操传，至大破袁绍&lt;/li&gt;
&lt;li&gt;第三十二至三十八回：刘备传，至隆中对&lt;/li&gt;
&lt;li&gt;第三十九至五十回：诸葛亮传，至华容道&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在这些十回的小故事中，又有相应的重心，这个重心大约是在三四回左右，在全书之中丝毫不乱，作者也基于这些结构上的巧妙安排而认定明清小说属于独运匠心的「才子书」，绝不可能脱胎于民间说书人口中。&lt;/p&gt;
&lt;p&gt;再将全书的十回目故事缀合起来，对于全书而言还有一个结构上的经典范式，那就是「二十-六十-二十」的叙述程式。《金瓶梅》就是具有这种对称美的典型，在前二十回与后二十回中，书中都是在描写西门庆院墙外的故事，在中间六十回转入展开这间深宅大院中离合，同时前二十回叙述家中新添金、瓶、梅三小妾，奠定全书格局，后二十回则是西门庆死去，树倒猢孙散，这种对称的安排不能说不是作者的心思。&lt;/p&gt;
&lt;p&gt;作者还发现，明清小说中的另一个叙事特点就是情节的高潮在终点前就已经发生了，大约在全书四分之三位置处，同时全书又可以分成上下两截相互照应。以《三国演义》为例子最为简单，第五十回赤壁之战确定了天下三分的格局，在此全书分为两截，而在第七十八回曹操死去，全书自此高潮突然收束，一路走向下坡。《金瓶梅》要更为规整，以第四十九回为上下截的分水岭，上截描述西门庆升官发财、步步高升的经历，下截则是其春风得意而加速自毁的过程，同样在第七十九回死去，达到故事的最高潮。&lt;/p&gt;
&lt;p&gt;作者认为这种绵延不绝的故事布局是中国叙事独特的「转轮式」布局，故事的发展正像一个不断旋转的法轮，暗给人天道循环的感受。正如《三国演义》所要强调的「合久并分，分久并合」，故事开始于东汉末年的纷乱，结束于一统于晋，故事从刘、关、张等主人公手中交递给他们的后辈，正像长江后浪推前浪，呈现出一种无了局的形式，作者想表达的思想也暗含其中了。&lt;/p&gt;
&lt;p&gt;作者在书的最后部分上述方法分析了四大奇书中的中心思想，对于只接触教科书式解读的我，其分析方法与结果都可谓是别开生面，例如《西游记》不能简单理解为明末政治的黑暗，在一定程度上它还与明末的思想主流「心学」有关，书中「心猿」等等用语肯定另有所指，作者将其解读为告诫人们「诚意正心」。但部分观点有些冒进而难免令人觉得略显穿凿，但我认为作者只是提出了他的观点，真正怎样理解和解读明清小说还是依赖于读者。我相信在看过作者的一系列剖析后，再去重新读一读四大奇书，一定会有新的理解，这些新收获才是作者浦安迪撰写此书想要传达给我们的东西。&lt;/p&gt;</content><category term="故纸堆"></category><category term="阅读"></category><category term="文学"></category></entry><entry><title>文献总结｜通过连接感知模版挖掘实现从头生成分子</title><link href="https://leonis.cc/sui-sui-nian/2023-03-25-summary-doi.org/10.48550/arXiv.2302.01129.html" rel="alternate"></link><published>2023-03-25T00:00:00+08:00</published><updated>2023-03-25T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-25:/sui-sui-nian/2023-03-25-summary-doi.org/10.48550/arXiv.2302.01129.html</id><summary type="html">&lt;p&gt;本文介绍由中科大于 2023 年发布在 ICLR 2023 上的一篇文章，文章原标题为 De Novo Molecular Generation via Connection-aware Motif Mining，文章提出了一种从分子数据集中挖掘模版结构的算法，同时设计了一种通过组合模版结构实现分子生成的模型。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2302.01129" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2302.01129&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍由中科大于 2023 年发布在 ICLR 2023 上的一篇文章，文章原标题为 De Novo Molecular Generation via Connection-aware Motif Mining，文章提出了一种从分子数据集中挖掘模版结构的算法，同时设计了一种通过组合模版结构实现分子生成的模型 MiCaM（Mined Connection-aware Motifs）。&lt;/p&gt;
&lt;h2 id="suan-fa"&gt;算法&lt;/h2&gt;
&lt;p&gt;文章中提出的连接感知模版挖掘能够从数据集中构建模版词汇，用于后续的分子生成，该算法包括两个主要步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;合并操作（Merging-operation Learning Phase）&lt;/li&gt;
&lt;li&gt;构建模版词汇（Motif-vocabulary Construction Phase）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="算法" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8702?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="算法" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8702?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="he-bing-cao-zuo"&gt;合并操作&lt;/h3&gt;
&lt;p&gt;数据集 &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 中的每个分子都可以表示为图 &lt;span class="math"&gt;\(\mathcal{G(V,E)}\)&lt;/span&gt;，结点 &lt;span class="math"&gt;\(\mathcal{V}\)&lt;/span&gt; 表示原子，边 &lt;span class="math"&gt;\(\mathcal{E}\)&lt;/span&gt; 表示化学键。接着定义与之类似的 &lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M(\mathcal{V}^{(k)}_M,\mathcal{E}^{(k)}_M)\)&lt;/span&gt; 表示第 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 次合并后的分子，其中 &lt;span class="math"&gt;\(\mathcal{F}\in\mathcal{V}_M\)&lt;/span&gt; 表示分子的子结构，可以由一个原子构成，也可以由多个原子构成，那么边 &lt;span class="math"&gt;\(\mathcal{E}_M\)&lt;/span&gt; 相应就表示子结构之间的连接方式。显然，&lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M\)&lt;/span&gt; 由 &lt;span class="math"&gt;\(\mathcal{G}\)&lt;/span&gt; 初始化得到，也就是 &lt;span class="math"&gt;\(\mathcal{G}^{(0)}_M(\mathcal{V}^{(0)}_M,\mathcal{E}^{(0)}_M)=\mathcal{G(V,E)}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;接下来介绍由 &lt;span class="math"&gt;\(\mathcal{G}^{(0)}_M\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M\)&lt;/span&gt; 的合并操作，文章将操作「&lt;span class="math"&gt;\(\oplus\)&lt;/span&gt;」定义将子结构合并为新的子结构，即 &lt;span class="math"&gt;\(\mathcal{F}_{ij}=\mathcal{F}_i\oplus\mathcal{F}_j\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\mathcal{F}_{ij}\)&lt;/span&gt; 就包含了 &lt;span class="math"&gt;\(\mathcal{F}_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\mathcal{F}_j\)&lt;/span&gt; 中所有的结点与边。&lt;span class="math"&gt;\(\mathcal{G}^{(0)}_M\)&lt;/span&gt; 包含了边 &lt;span class="math"&gt;\(\mathcal{E}^{(0)}_M\)&lt;/span&gt;，边又连接了相应的子结构，所以对于通过边连接的子结构 &lt;span class="math"&gt;\((\mathcal{F}^{(0)}_i,\mathcal{F}^{(0)}_j)\in\mathcal{E}^{(0)}_M\)&lt;/span&gt; 计算 &lt;span class="math"&gt;\(\mathcal{F}^{(0)}_{ij}=\mathcal{F}^{(0)}_i\oplus\mathcal{F}^{(0)}_j\)&lt;/span&gt;，其中出现频率最高的 &lt;span class="math"&gt;\(\mathcal{F}^{(0)}_{ij}\)&lt;/span&gt; 就记作 &lt;span class="math"&gt;\(\mathcal{M}^{(0)}\)&lt;/span&gt;。再次遍历子结构 &lt;span class="math"&gt;\((\mathcal{F}^{(0)}_i,\mathcal{F}^{(0)}_j)\in\mathcal{E}^{(0)}_M\)&lt;/span&gt;，只要 &lt;span class="math"&gt;\(\mathcal{F}^{(0)}_i\oplus\mathcal{F}^{(0)}_j==\mathcal{M}^{(0)}\)&lt;/span&gt;，就将这两个子结构合并，完成后得到的所有新子结构就是 &lt;span class="math"&gt;\(\mathcal{V}^{(1)}_M\)&lt;/span&gt;，相应的新连接边就是 &lt;span class="math"&gt;\(\mathcal{E}^{(1)}_M\)&lt;/span&gt;，二者构成了经过 1 次合并的分子 &lt;span class="math"&gt;\(\mathcal{G}^{(1)}_M\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;以上合并操作可以推广，在第 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 次合并中，可以从分子 &lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M(\mathcal{V}^{(k)}_M,\mathcal{E}^{(k)}_M)\)&lt;/span&gt; 得到分子 &lt;span class="math"&gt;\(\mathcal{G}^{(k+1)}_M(\mathcal{V}^{(k+1)}_M,\mathcal{E}^{(k+1)}_M)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id="gou-jian-mo-ban-ci-hui"&gt;构建模版词汇&lt;/h3&gt;
&lt;p&gt;数据集中的所有分子在合并操作后都变为 &lt;span class="math"&gt;\(\mathcal{G}^{(K)}_M(\mathcal{V}^{(K)}_M,\mathcal{E}^{(K)}_M)\)&lt;/span&gt;，此时分子已经大大简化，将分子中的结点分割开来，并添加上连接位置的标记 &lt;code&gt;*&lt;/code&gt;，就得到了分子结构模版。&lt;/p&gt;
&lt;h2 id="mo-xing_1"&gt;模型&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="模型" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8703?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="模型" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8703?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MiCaM 通过多个逐渐组合模版结构的步骤生成分子，这样的组合有两种方式，一种是直接在连接位置上连上另一个模版结构，另一种是分子中的两个连接位置相连（成环）。&lt;/p&gt;
&lt;p&gt;将第 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 步得到的分子记作 &lt;span class="math"&gt;\(\mathcal{G}_t\)&lt;/span&gt;，用 &lt;span class="math"&gt;\(\mathcal{C}_{\mathcal{G}_t}\)&lt;/span&gt; 表示该分子的所有连接位置，使用序列 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt; 记录 &lt;span class="math"&gt;\(\mathcal{C}_{\mathcal{G}_t}\)&lt;/span&gt; 中所有连接位置的顺序。在第 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 步取出 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt; 的首个元素 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt;，也就是当前操作的连接位置，在该处连接或者成环后，就得到了新分子。新分子中可能具有新的连接位置，所以还使用 RDKit 更新序列 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt;。分子生成步骤就是不断重复以上过程，直至 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt; 为空，此时分子中的所有连接位置都被填满，就得到了输出分子。&lt;/p&gt;
&lt;p&gt;因为具有连接和成环两种组合方式，所以在每个生成步骤中还需要确定与当前操作的连接位置 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt; 相连的位置与子结构。&lt;/p&gt;
&lt;p&gt;文章为此设计了以下步骤，首先使用 GNN&lt;sub&gt;pmol&lt;/sub&gt; 编码当前步骤得到的分子 &lt;span class="math"&gt;\(\mathcal{G}_t\)&lt;/span&gt; 和当前操作的连接位置 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt;，分别得到相应的表示 &lt;span class="math"&gt;\(\boldsymbol{h}_{v_t}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{h}_{\mathcal{G}_t}\)&lt;/span&gt;，使用 GNN&lt;sub&gt;motif&lt;/sub&gt; 编码模版库中的所有结构模版，得到所有连接位置 &lt;span class="math"&gt;\(v\in\mathcal{C}_\mathrm{Vocab}\)&lt;/span&gt; 的表示 &lt;span class="math"&gt;\(\boldsymbol{h}_{v}\)&lt;/span&gt;。使用神经网络将相应的表示转换为 key 向量与 query 向量，寻找与 query 向量最相似的 key 向量，也就是根据以下概率选择与当前操作的连接位置 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt; 相连的位置：&lt;/p&gt;
&lt;div class="math"&gt;$$P_v=\mathop{\mathrm{softmax}}_{v\in\mathcal{C}_\mathrm{Vocab}\cup\mathcal{C}_{\mathcal{G}_t}\backslash\{v_t\}}(\mathrm{NN_{query}}([\boldsymbol{z},\boldsymbol{h}_{\mathcal{G}_t},\boldsymbol{h}_{v_t}])\cdot\mathrm{NN_{key}}(\boldsymbol{h}_v))$$&lt;/div&gt;
&lt;p&gt;若候选的连接位置 &lt;span class="math"&gt;\(v\in\mathcal{C}_\mathrm{Vocab}\)&lt;/span&gt;，则在模版库中取得相应的 &lt;span class="math"&gt;\(\mathcal{F}^*\)&lt;/span&gt;，并将其接入分子 &lt;span class="math"&gt;\(\mathcal{G}_t\)&lt;/span&gt;，完成连接；若 &lt;span class="math"&gt;\(v\in\mathcal{C}_{\mathcal{G}_t}\)&lt;/span&gt;，那么就合并 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(v\)&lt;/span&gt;，完成成环。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun"&gt;结果与讨论&lt;/h2&gt;
&lt;h3 id="fen-bu-xue-xi"&gt;分布学习&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8704?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8704?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章在 QM9、ZINC 和 GuacaMol 三个数据集上分别训练了 MiCaM，并使用 MiCaM 生成分子，检查生成的分子集是否接近训练集的分布。实验结果如上表所示，在 QM9 和 GuacaMol 数据集上，MiCaM 生成分子的 Uniqueness 和 Novelty 低于 MoLeR 和 GP-VAE 等模型，但在 KL Div 和 FCD score 上，MiCaM 完全优于其他模型。KL Div 与 FCD score 都衡量了生成分子集与训练集分布的相似程度，也就是说，MiCaM 生成的分子最接近训练集的分布。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8705?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8705?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在算法一节提到的参数 &lt;span class="math"&gt;\(K\)&lt;/span&gt; 决定了合并次数，文章在 QM9 数据集上测试了该参数对生成分子的影响。随着合并次数增加，KL Div 与 FCD score 都随之增加，而生成分子的 Novelty 却在下降。这是因为合并次数越多，分子简化程度越大，最后进入模版库中的模版结构也会更加复杂、更加接近训练集中的分子，最后使用这些模版结构构造的分子就会趋于与训练集分子「雷同」，这也一点程度解释了为什么 MiCaM 生成分子的 Uniqueness 和 Novelty 低于部分模型。&lt;/p&gt;
&lt;h3 id="die-dai-you-hua"&gt;迭代优化&lt;/h3&gt;
&lt;p&gt;文章还使用 MiCaM 进行了迭代目标增强（Iterative Target Augmentation, ITA）的分子生成。首先选出数据集中针对目标要求打分最高的 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个分子，接着使用模型在该训练集上微调并产生新分子，每次迭代过程中，在新分子与训练集中再选出打分最高的 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个分子，用作为下一次迭代的训练集。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8706?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8706?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在文章设计的 5 种任务中，MiCaM 优于其他所有模型。&lt;/p&gt;
&lt;p&gt;最后文章展示了 MiCaM 生成分子的过程，经过 5 个步骤，MiCaM 就能生成相当复杂的分子，同时分子的各性质分数也相应提高。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8707?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8707?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun_1"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种从分子数据中挖掘模版结构的算法，能够用于提取分子数据集中频繁出现的子结构并构建模版结构库，同时文章设计了基于该模版结构库的分子生成模型 MiCaM。MiCaM 通过逐步组合模版结构实现分子生成，很大程度解决了以往启发式分子生成随机性大、难以生成复杂结构的问题，使用模版结构组合生成的分子也更接近于现实中化学家对化合物改造的策略。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category><category term="VAE"></category></entry><entry><title>春日漫步小记</title><link href="https://leonis.cc/zai-lu-shang/2023-03-24-spring-trip-in-tianjin.html" rel="alternate"></link><published>2023-03-24T00:00:00+08:00</published><updated>2023-03-24T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-24:/zai-lu-shang/2023-03-24-spring-trip-in-tianjin.html</id><summary type="html">&lt;p&gt;五六天前刚起了几阵沙尘暴，入眼的一切都是黄蒙蒙的。黄沙中的行人都拉紧着衣服、戴着口罩，可能是为了防沙，也可能是为了防病毒。唯一不避风沙的只有两旁的行道树，经冬之后满是枯枝，更添了几分萧索。就是在这样一片了无生气之中，竟在 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;五六天前刚起了几阵沙尘暴，入眼的一切都是黄蒙蒙的。黄沙中的行人都拉紧着衣服、戴着口罩，可能是为了防沙，也可能是为了防病毒。唯一不避风沙的只有两旁的行道树，经冬之后满是枯枝，更添了几分萧索。就是在这样一片了无生气之中，竟在几天之中气温陡升，绽开了桃花，催绿了柳梢，赶在春分前酝酿好了春天的氛围。我想，也只有中原与华北的气候能如此依时令而行，不误物候，也只有在这时，看着次第开放的春花，才深感只有北方的花如此重诺而能被称为「花信」。&lt;/p&gt;
&lt;p&gt;虽是在尘埃满扑之中，但桃红柳绿的色彩也使春光亮丽了起来，现出了新鲜的气象。自新冠管控放松之后，趁着东风，京津间的通勤也恢复如常了，于是与北京的好友几人约在周末沿海河散散步。&lt;/p&gt;
&lt;p&gt;海河是天津最有风采的景致，虽说不如长江黄河之于中国，但若是失了海河，天津的趣味就要大打折扣。此行的起点就在三岔河口前的丁字沽，北运河、子牙河与海河在此交汇，形成「丁」形，因而得名「丁字沽」。朋友看着路边介绍京杭大运河的牌子，打趣这里该算天津的陆家嘴，运河虽是依旧却再难见到繁忙来往的船舶。&lt;/p&gt;
&lt;p&gt;丁字沽一带的桃花园与西沽公园是赏春的好去处，不用多言，其中最负盛名的自然是桃花。节临三月，占得春花头筹的就是桃花，连通往景点的路上也游人如织，非摩肩接朣不得过。也许是为了不打消游人赏花的兴致，原先告示的「预约入园」也成了君子协定，凡是到场即可入内，园中的人流便可以推测了。&lt;/p&gt;
&lt;p&gt;桃花园是北运河边的一条长堤，沿堤遍植桃树，因此没有「园」之实，称为桃花堤更为贴切。长堤上人头攒动，头顶上两边的桃树相互环合，拱成了一条花廊。游人在花廊下行走，而时不时几阵薰风吹过，就有几枚桃花脱了梢头，落入人流之中。人在花群中，花亦在人群中。&lt;/p&gt;
&lt;p&gt;花廊难以目力穷尽，只得顺人流向前。沿桃花堤前行，一旁的北运河也沿着桃花堤悠悠向前流淌。正当乱花迷眼而觉得乏味之时，桃花堤悄然南折，北运河也在此转而向南一路奔流汇注海河，我也在此转身向南。赫然映入眼中的是一株高三丈许的玉兰，玉兰的枝干上着满了白花，不杂一叶，开得是那样浓烈，温润洁白的玉兰浑如古玉，又是那样祥和。还不及细想缘何没有人驻足树下，才发觉前方悬挂着河北工业大学的匾额，同时提醒谢绝游人的参观。静静伫立在校园中，仅有的喧闹也只是汩汩的运河水与朗朗书声，悄悄花开花落，委心任乎去留，这株洁白的玉兰也变得更加洒脱起来。&lt;/p&gt;
&lt;p&gt;南折不远，便出了桃花堤，前方遥遥与之照应的是西沽公园。西沽公园与桃花堤不同，所占面积要大许多，所以尽管设置了巨大的广告牌招徕游客，园中也不至于像桃花堤那样拥挤。西沽公园中的游人更多是附近的居民，按天津人的闲致，家旁有如此山水湖林，自然免不了来此唱几句京戏，园中也得以檀板丝弦不绝于耳，平添了生活气息。&lt;/p&gt;
&lt;p&gt;午后日光稍晻，沿海河一路步行至旧意租界。随着渐渐临近春分，东风解冻了海河，虽没有鸭子来试试水温，但漂浮在河面上随波逐流的海鸥也提示着春水已暖。海河曾是繁忙的航道，如今常见的船只也只有观光轮渡，安静下来的海河于是成了海鸥休憩觅食的场所。每有渡船经过，稍谨慎的扑翅腾空而上，盘旋数周，待船只曳开的波澜平息后又落入水中，稍慵懒的也不动弹，宁随着艏波漂荡，一派春日融融的景象。&lt;/p&gt;
&lt;p&gt;与海河作别，进入旧意租界，眼见日头已经西颓，于是匆匆穿过条条欧式风情的街道寻觅晚餐，又匆匆送朋友往火车站，而后分别。一天的漫步果然让我收获了腰酸腿痛，但兴致、友人与良辰美景总是难以聚齐，难得能够这样共同游目聘怀，不仅幸甚，也算是不负如此韶光了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="桃花堤" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8688?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="桃花堤" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8688?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 游人如织的桃花堤&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="桃花堤" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8689?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="桃花堤" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8689?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="桃花堤" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8690?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="桃花堤" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8690?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="河北工业大学" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8691?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="河北工业大学" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8691?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 河工大中独自盛开的玉兰&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="海河" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8694?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="海河" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8694?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 海河中戏水的海鸥&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="海河" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8692?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="海河" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8692?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 与骑手相伴而飞&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="海河" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8693?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="海河" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8693?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="但丁广场" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8695?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="但丁广场" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8695?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 但丁握着哪部文集的稿子？&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="意式风情街" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8696?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="意式风情街" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8696?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 某无人居住的故居，窗前桃花已开，主人归未？&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="游记"></category><category term="摄影"></category></entry><entry><title>文献总结｜使用等变扩散模型进行基于结构的药物设计</title><link href="https://leonis.cc/sui-sui-nian/2023-03-17-summary-doi.org/10.48550/arXiv.2210.13695.html" rel="alternate"></link><published>2023-03-17T00:00:00+08:00</published><updated>2023-03-17T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-17:/sui-sui-nian/2023-03-17-summary-doi.org/10.48550/arXiv.2210.13695.html</id><summary type="html">&lt;p&gt;本文介绍由洛桑联邦理工学院等研究单位于 2022 年发布在 arXiv 上的一篇文章，文章原标题为 Structure-based Drug Design with Equivariant Diffusion Models，文章首次将等变扩散模型用于基于结构的药物设计，实现针对特定的蛋白靶点生成多样且具有高亲合力的的配体分子。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2210.13695" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2210.13695&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍由洛桑联邦理工学院等研究单位于 2022 年发布在 arXiv 上的一篇文章，文章原标题为 Structure-based Drug Design with Equivariant Diffusion Models，文章首次将等变扩散模型用于基于结构的药物设计，实现针对特定的蛋白靶点生成多样且具有高亲合力的的配体分子。&lt;/p&gt;
&lt;p&gt;近年来，越来越多的深度学习模型被应于用基于结构的药物设计，但这些模型主要采用了序列生成的方式，这些方法所面临的一个重要问题是序列生成忽视了原子的几何顺序，可能并不能反映设计分子的真实原理，最后无法获取到目标分子的化学空间。&lt;/p&gt;
&lt;p&gt;针对于这个问题，文章考虑了分子中各原子的空间坐标，使用等变扩散模型完成了针对特定靶点的分子生成，称为 DiffSBDD（Equivariant Diffusion Model
for Structure-Based Drug Design），实验结果表明该模型能够获取指定蛋白活性口袋的信息并生成多样、具有类药性且具有高亲合力的分子。&lt;/p&gt;
&lt;h2 id="mo-xing"&gt;模型&lt;/h2&gt;
&lt;h3 id="qu-zao-sheng-kuo-san-gai-lu-mo-xing"&gt;去噪声扩散概率模型&lt;/h3&gt;
&lt;p&gt;去噪声扩散概率模型（Denoising Diffusion Probabilistic Models, DDPMs）是近年应用于多个领域的一类生成模型，DDPM 通过马尔可夫链逐次向样本数据上添加噪声，然后由神经网络学习该马尔可夫链的逆过程，实现从噪声中重建采样数据。&lt;/p&gt;
&lt;p&gt;就分子生成任务而言，样本数据是原子点云（简称分子） &lt;span class="math"&gt;\(\boldsymbol{z}_\mathrm{data}=[\boldsymbol{x},\boldsymbol{h}]\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\boldsymbol{x}\in\mathbb{R}^{N\times3}\)&lt;/span&gt;，表示原子的空间坐标，&lt;span class="math"&gt;\(\boldsymbol{h}\in\mathbb{R}^{N\times d}\)&lt;/span&gt;，表示原子的特征。那么对原子点由 &lt;span class="math"&gt;\(t=0\)&lt;/span&gt; 至 &lt;span class="math"&gt;\(t=T\)&lt;/span&gt; 逐次加噪声的过程就可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$q(\boldsymbol{z}_t|\boldsymbol{z}_\mathrm{data})=\mathcal{N}(\boldsymbol{z}_t|\alpha_t\boldsymbol{z}_\mathrm{data},\sigma^2_t\boldsymbol{I})$$&lt;/div&gt;
&lt;p&gt;由 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 至 &lt;span class="math"&gt;\(s&amp;lt;t\)&lt;/span&gt; 的去噪声的过程同是马尔可夫链，记作 &lt;span class="math"&gt;\(q(\boldsymbol{z}_s|\boldsymbol{z}_\mathrm{data},\boldsymbol{z}_t)\)&lt;/span&gt;，可以看出去噪声的过程依赖于样本数据 &lt;span class="math"&gt;\(\boldsymbol{z}_\mathrm{data}\)&lt;/span&gt;，即标签数据。&lt;/p&gt;
&lt;p&gt;但当使用模型进行预测时，由于预测的分子 &lt;span class="math"&gt;\(\boldsymbol{z}_\mathrm{data}\)&lt;/span&gt; 是未知的，该模型使用神经网络 &lt;span class="math"&gt;\(\phi_\theta\)&lt;/span&gt; 拟合得到 &lt;span class="math"&gt;\(\hat{\boldsymbol{z}}_\mathrm{data}\)&lt;/span&gt;。具体来说，加噪声后的分子可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{z}_t=\alpha_t\boldsymbol{z}_\mathrm{data}+\sigma_t\boldsymbol{\epsilon},\ \epsilon\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})$$&lt;/div&gt;
&lt;p&gt;神经网络就是用于预测噪声 &lt;span class="math"&gt;\(\hat{\boldsymbol{\epsilon}}_\theta=\phi_\theta(\boldsymbol{z}_t,t)\)&lt;/span&gt;，那么显然有&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{z}}_\mathrm{data}=\frac{1}{\alpha_t}\boldsymbol{z}_t-\frac{\sigma_t}{\alpha_t}\hat{\boldsymbol{\epsilon}}_\theta$$&lt;/div&gt;
&lt;p&gt;所以训练的目标也就是最小化神经网络预测值 &lt;span class="math"&gt;\(\hat{\boldsymbol{\epsilon}}_\theta\)&lt;/span&gt; 与真实值 &lt;span class="math"&gt;\(\boldsymbol{\epsilon}\)&lt;/span&gt; 间的差距，损失函数为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}_\mathrm{train}=\frac{1}{2}||\boldsymbol{\epsilon}-\phi_\theta(\boldsymbol{z}_t,t)||^2$$&lt;/div&gt;
&lt;h3 id="en-deng-bian-tu-shen-jing-wang-luo"&gt;&lt;em&gt;E(n)&lt;/em&gt; - 等变图神经网络&lt;/h3&gt;
&lt;p&gt;在使生成的分子与蛋白活性口袋配合的过程中，需要重新打乱原子生成新的结构，所以需要对原子点云做置换、旋转等变换。传统的图神经网络无法很好处理这类具有等变对称性质的特征，因而提出的一类改进后用于处理等变性质的图神经网络就被称为等变图神经网络。&lt;/p&gt;
&lt;h3 id="diffsbdd"&gt;DiffSBDD&lt;/h3&gt;
&lt;p&gt;结合以上两种模型，就得到了文章中所设计的 DiffSBDD，DiffSBDD 具有两种分子生成方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;蛋白口袋条件分子生成（Conditional generation）&lt;/li&gt;
&lt;li&gt;通过联合分布实现分子生成（Inpainting）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8663?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8663?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h4&gt;Conditional generation&lt;/h4&gt;
&lt;p&gt;在 conditional generation 模式中，给模型的每个去噪声过程都指定了相同的蛋白口袋，简单来讲，就是在分子生成过程中，原子点云会不断发生置换、旋转等变换，而蛋白口袋不发生变化，使配体分子去「适配」蛋白口袋。&lt;/p&gt;
&lt;h3 id="inpainting"&gt;Inpainting&lt;/h3&gt;
&lt;p&gt;而在 inpainting 模式中，需要首先训练一个无指定条件的 DDPM 用于拟合配体分子与蛋白口袋的联合概率分布 &lt;span class="math"&gt;\(p(\boldsymbol{z}^{(L)}_\mathrm{data},\boldsymbol{z}^{(P)}_\mathrm{data})\)&lt;/span&gt;，该概率分布的主要用于从噪声数据中得到配合的配体分子与蛋白口袋对。&lt;/p&gt;
&lt;p&gt;在预测过程中，首先将加噪声后的样本 &lt;span class="math"&gt;\([\boldsymbol{z}^{(L)}_t,\boldsymbol{z}^{(P)}_t]\)&lt;/span&gt; 中的蛋白口袋掩盖掉，将得到其中的配体部分与加噪声后的蛋白口袋重新组合为 &lt;span class="math"&gt;\([\boldsymbol{z}^{(L)}_t,\boldsymbol{z}^{(P')}_t]\)&lt;/span&gt;，最后用已训练得到的联合概率分布 DDPM 为该组合去噪声，即生成目标分子。&lt;/p&gt;
&lt;h2 id="shu-ju_1"&gt;数据&lt;/h2&gt;
&lt;p&gt;文章分别使用了 CrossDocked 中 10 万个蛋白-配体对和 Binding MOAD 中 40354 个蛋白-配体对作为模型的数据集。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8664?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8664?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先文章对生成分子进行了评估，可以看出不管使用哪一个数据集训练模型、使用哪一个模式生成分子，最终结果中只有很小一部分生成的分子不合法，生成分子中大部分都满足新颖、合法的要求。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8665?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8665?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;接着文章将分子生成结果与目前在基于结构的药物设计上表现最好的 3D-SBDD 和 Pocaket2Mol 两种模型比较。从表中的数据可以看出，在 Vina Score 上，DiffSBDD 与另外两种模型都比较接近，也就是模型都识别到了配体与蛋白口袋间的相互作用，生成了具有亲合力的分子。DiffSBDD 在 QED、Lipinski 这些分子性质上并没有实现优化，而是与测试集保持相似。其中，DiffSBDD 在 SA 上显著低于其他两种模型，文章认为 SA 在一定程度上并不能反映分子真实的合成难度，低 SA 反而说明 DiffSBDD 能够探索更大的化学空间，因此具有最高的 Diversity。最后，DiffSBDD 所需的计算时间远远少于另外两种模型，相比之下更加高效。&lt;/p&gt;
&lt;p&gt;文章展示了针对靶点 2jjg 和 3kc1 生成的分子，其中为 3kc1 生成的第二个分子具有三环结构，该分子也曾在传统的基于结构的药物设计中被设计出来，说明 DiffSBDD 具有应用潜力。在生成的许多分子，还可以找到大环、三元环化合物，这些分子很难合成，所以 DiffSBDD 可能还需要考虑分子的可合成性。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8667?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8667?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章中所提出的 DiffSBDD 首次将等变扩散模型应用于基于结构的药物设计领域，实验证明了 DiffSBDD 在完成分子生成任务上不仅高效而且有效，能够针对给定的靶点生成多样且具有高亲合力的配体分子，该模型不需要再训练就可以直接应用于先导化合物优化等药物设计实践。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category></entry><entry><title>为 Pelican 博客加入搜索功能</title><link href="https://leonis.cc/sui-sui-nian/2023-03-14-deploy-pelican-search.html" rel="alternate"></link><published>2023-03-14T00:00:00+08:00</published><updated>2023-03-14T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-14:/sui-sui-nian/2023-03-14-deploy-pelican-search.html</id><summary type="html">&lt;p&gt;博客上的文章越来越多了，靠文章 tag 来检索文章总是有些麻烦，不妨为博客配置一个搜索功能吧。在中文互联网上搜索不到半点配置 Pelican Search 的相关信息，希望这篇文章能帮到后来人。&lt;/p&gt;</summary><content type="html">&lt;p&gt;在目前的博客生态圈中，静态博客占了相当大的比例。静态博客因为足够轻量、便于备份和迁移而受到包括我在内的许多用户喜欢，但由于静态博客没有数据库，在组织内容上就不免有所不足，这就涉及到题中所说的搜索功能了。为静态博客部署搜索功能比较麻烦，一个稍简单的方法就是借于「必应」「谷歌」等搜索引擎的 API，用它们来搜索站内的内容，但是这种方法的效果并不好，如果网页还未被收录或是相应头键字在网页中的占比太小，就很难搜索到目标信息。另一种方法就是在站内建立本地的搜索，例如 Hexo 等博客框架都提供了相应的插件，在本地生成搜索匹配所需的文件，将其一并推送至服务器实现全站搜索，这种方法的兼容性、准确性都要更好，也是我选择的方案。&lt;/p&gt;
&lt;p&gt;Pelican 也有类似的搜索插件 &lt;a href="https://github.com/pelican-plugins/search" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; pelican-plugins/search&lt;/a&gt;，它主要是借助了 &lt;a href="https://stork-search.net/" rel="noopener" target="_blank"&gt;Stork&lt;/a&gt; 来实现搜索功能。Stork 在官方文档中指出，它可以用于为静态站点构建关速美观的搜索接口，所以理应可以用于所有类型的静态博客，我不了解其他博客框架是否使用了这个工具，但不得不提其搜索体验非常不错，十分值得一试。&lt;/p&gt;
&lt;h2 id="an-zhuang-stork"&gt;安装 Stork&lt;/h2&gt;
&lt;h3 id="an-zhuang-c-sheng-cheng-gong-ju"&gt;安装 C++ 生成工具&lt;/h3&gt;
&lt;p&gt;Stork 是基于 Rust 构建的工具，需要使用 Rust 的包管理器 Cargo 安装，若是在 macOS 或 Linux 系统上，直接按照官方文档给出的方法安装即可，而在 Windows 上就会比较麻烦，我在这里介绍一下 Windows 的操作方法。&lt;/p&gt;
&lt;p&gt;{note begin}后文所涉及的操作系统都是 Windows 10 系统，终端指的是 Windows 终端（Windows Terminal）。{note end}&lt;/p&gt;
&lt;p&gt;首先，在 Windows 上，Rust 需要某些 C++ 生成工具，可以选择安装 Visual Studio 或仅安装 Microsoft C++ 生成工具。安装 Visual Studio 的方法非常简单（推荐），按下不表，若仅安装 Microsoft C++ 生成工具，可以在终端中输入&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;winget install Microsoft.VisualStudio.2022.BuildTools
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;由于 winget 下载速度感人，实在不推荐这种方法。&lt;/p&gt;
&lt;p&gt;在安装好 Visual Studio 后，在开始界面搜索并打开 Visual Studio Installer，选择 &lt;code&gt;修改&lt;/code&gt; - &lt;code&gt;使用 C++ 的桌面开发&lt;/code&gt;，等待安装完成。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Visual Studio" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8654?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Visual Studio" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8654?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="an-zhuang-rust"&gt;安装 Rust&lt;/h3&gt;
&lt;p&gt;Rust 也可以使用 winget 安装，命令很简单：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;winget install rustup
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是不太好用，所以我还是选择到 &lt;a href="https://www.rust-lang.org/zh-CN/tools/install" rel="noopener" target="_blank"&gt;Rust 官网&lt;/a&gt;下载。下载完成后打开安装程序，弹出的是命令行窗口，默认安装在 &lt;code&gt;C:\Users&lt;/code&gt; 路径下的目录中，如果不需要额外的设置，键入 &lt;code&gt;1&lt;/code&gt; 后按回车即可。&lt;/p&gt;
&lt;p&gt;但由于 C 盘空间不太够了，我需要修改一下安装的路径，在目标路径下创建以下两个文件夹并新建环境变量：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;CARGO_HOME: E:\RUST\.cargo
RUSTUP_HOME: E:\RUST\.rustup
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 &lt;code&gt;PATH&lt;/code&gt; 中添加变量 &lt;code&gt;%CARGO_HOME%&lt;/code&gt;、&lt;code&gt;%RUSTUP_HOME%&lt;/code&gt; 和 &lt;code&gt;%CARGO_HOME%\bin&lt;/code&gt;，然后再打开安装程序，默认路径就已经改变，键入 &lt;code&gt;1&lt;/code&gt; 按回车安装。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-txt"&gt;Rustup metadata and toolchains will be installed into the Rustup
home directory, located at:

  E:\RustT\.rustup

This can be modified with the RUSTUP_HOME environment variable.

The Cargo home directory is located at:

  E:\Rust\.cargo

This can be modified with the CARGO_HOME environment variable.

The cargo, rustc, rustup and other commands will be added to
Cargo's bin directory, located at:

  E:\Rust\.cargo\bin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装完成后，在终端中输入 &lt;code&gt;cargo --version&lt;/code&gt; 理应输出版本信息。&lt;/p&gt;
&lt;h3 id="tong-guo-cargo-an-zhuang-stork"&gt;通过 Cargo 安装 Stork&lt;/h3&gt;
&lt;p&gt;虽然安装好了 Cargo，但不出意外的话，与其他包管理器类似，从官方的源下载内容的速度非常之慢，所以需要修改配置使用镜像源。&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;.cargo&lt;/code&gt; 目录下创建文件 &lt;code&gt;config.toml&lt;/code&gt;，写入以下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-toml"&gt;[source.crates-io]
replace-with = 'ustc'

[source.ustc]
registry = "git://mirrors.ustc.edu.cn/crates.io-index"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;就使用了科大的镜像源，然后在终端中使用以下命令安装 Stork：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;cargo install stork-search --locked
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后可以使用 &lt;code&gt;stork --version&lt;/code&gt; 验证是否成功安装 Stork。&lt;/p&gt;
&lt;h2 id="bu-shu-pelican-search_1"&gt;部署 Pelican Search&lt;/h2&gt;
&lt;p&gt;安装好 Stork 后的步骤就很简单了，在 Pelican 的 Python 环境中安装插件：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;python -m pip install pelican-search
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后在 Pelican 设置中的 &lt;code&gt;PLUGINS&lt;/code&gt; 引入 &lt;code&gt;search&lt;/code&gt;，在主题的模板文件（一般是 &lt;code&gt;base.html&lt;/code&gt;）中引入 Stork CSS 的 CDN（当然也可以改写后自己部署）：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;link rel="stylesheet" href="https://files.stork-search.net/basic.css" /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以及 JavaScript：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;script src="https://files.stork-search.net/releases/v1.5.0/stork.js"&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;
    stork.register("sitesearch", "{{ SITEURL }}/search-index.st")
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后就可以在设计的搜索区域通过以下方式调用 Stork 搜索了：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;Search: &amp;lt;input data-stork="sitesearch" /&amp;gt;
&amp;lt;div data-stork="sitesearch-output"&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{warn begin}以上 CSS 应在页面的头部，例如 &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; 中引入，而 JavaScript 则应在 &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; 的尾部引入。这是因为 HTML 文件是自上至下顺序执行的，如果加载 JavaScript 的位置靠前，比如在 &lt;code&gt;&amp;lt;input data-stork="sitesearch" /&amp;gt;&lt;/code&gt; 元素之前引入，在执行时该元素还未生成，就会找不到匹配的搜索框，提示没有 &lt;code&gt;query selector `input[data-stork="sitesearch"]`&lt;/code&gt;，导致 &lt;code&gt;Uncaught StorkError&lt;/code&gt;。{warn end}&lt;/p&gt;
&lt;h3 id="pelican-search-de-she-zhi-xiang"&gt;Pelican Search 的设置项&lt;/h3&gt;
&lt;p&gt;pelican-search 的设置项只有两项，一项 &lt;code&gt;SEARCH_MODE&lt;/code&gt; 设定从 Markdown 文件建立索引还是从 HTML 建立索引，&lt;code&gt;SEARCH_HTML_SELECTOR&lt;/code&gt; 可以用于指定从 HTML 的哪些内容中建立索引。但 Stork 的设置项不止这些，将其整合进 pelican-search 应该也不太难，留到以后有精力的时候尝试一下。&lt;/p&gt;
&lt;p&gt;我浏览了一下 &lt;a href="https://stork-search.net/docs/config-ref" rel="noopener" target="_blank"&gt;Stork 官方文档&lt;/a&gt;中的内容，发现了很多很有意思的东西。例如 &lt;code&gt;minimum_indexed_substring_length&lt;/code&gt; 一项设置了建立索引的匹配项最短长度，默认值为 3，通俗来讲就是长于 3 的单词才会用于建立索引，那么在搜索时也需要起码键入 3 个字母才有结果。但这种做法对于中文来说就存在很大的问题，中文中最普遍的是双字词，所以文档中也提到 &lt;code&gt;minimum_index_ideographic_substring_length&lt;/code&gt; 一项设置，默认值是 1，对于 CJK 字符而言，长于 1 的词就可以建立索引。可惜在我的试验中这个设置貌似并没有效果，在我使用汉字搜索时，也必须输入 3 个汉字才有结果，若要搜索双字词，只好用两个汉字加上空格的方法将就一下。除了这个问题之外就是使用中文搜索的精度不高，很难找到匹配项，所以 Stork 的最大不足其实就在于对中文的支持不好。据作者的消息，他也很希望能够提高 Stork 在中文搜索上的表现，可以期待一下后续的更新。&lt;/p&gt;
&lt;p&gt;再来说说 Stork 的优点，那就是「快」。Stork 搜索的速度特别快，不论是汉字还是字母，击入三个字符秒出搜索结果，这一点的体验就特别好。另外 Stork 还支持包括英语在内的多种欧洲语言的词根检索，例如输入「get」，它亦能返回「getting」的检索结果，这个功能对于静态站点而言可谓强大。如果是英文语境下，Stork 搜索精度高、速度快、支持词根检索，简直是最为优雅的静态站点搜索插件，也期待一下后续它能否在中文搜索上也能提供如此流畅的体验。&lt;/p&gt;
&lt;p&gt;{note begin}不知是我的原因还是 Stork 的问题，生成的索引文件巨大，足足有 16 MB，完全不能在网页上使用。于是我花了一整天的时间升级 Nginx 并配置上了 Brotli 压缩传输，压缩后只有大约 900 KB 了，加载速度大大提升。{note end}&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references_1"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/maphew/95fb9e986edfab887e4ff36547d5da59" rel="noopener" target="_blank"&gt;Install Stork-search on Windows - GitHub Gist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/zh-cn/windows/dev-environment/rust/setup" rel="noopener" target="_blank"&gt;在 Windows 上针对 Rust 设置开发环境 - Microsoft Learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/skzxc/p/12129353.html" rel="noopener" target="_blank"&gt;Win10 Rust 语言安装与环境变量配置(+VSCode) - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://course.rs/first-try/slowly-downloading.html" rel="noopener" target="_blank"&gt;下载依赖太慢了？ - Rust语言圣经(Rust Course)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="Blog"></category><category term="Pelican"></category><category term="Rust"></category><category term="Windows"></category></entry><entry><title>文献总结｜为蛋白质口袋定制分子：用于基于结构药物设计的 Transformer 分子生成方法</title><link href="https://leonis.cc/sui-sui-nian/2023-03-11-summary-doi.org/10.48550/arXiv.2209.06158.html" rel="alternate"></link><published>2023-03-11T00:00:00+08:00</published><updated>2023-03-11T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-11:/sui-sui-nian/2023-03-11-summary-doi.org/10.48550/arXiv.2209.06158.html</id><summary type="html">&lt;p&gt;本文介绍由微软研究团队于 2022 年发布在 arXiv 上的一篇文章，文章原标题为 Tailoring Molecules for Protein Pockets: a Transformer-based Generative Solution for Structured-based Drug Design，文章使用 Transformer 构建了一种能够获取受体 3 维信息的分子生成模型 TamGent，其中分子生成部分使用了预训练模型，避免了训练数据有限的问题。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2209.06158" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2209.06158&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍由微软研究团队于 2022 年发布在 arXiv 上的一篇文章，文章原标题为 Tailoring Molecules for Protein Pockets: a Transformer-based Generative Solution for Structured-based Drug Design，文章使用 Transformer 构建了一种能够获取受体 3 维信息的分子生成模型 TamGent，其中分子生成部分使用了预训练模型，避免了训练数据有限的问题。&lt;/p&gt;
&lt;p&gt;随着人工智能技术的发展，深度学习也进入到基于结构的药物设计（Structure Based Drug Design, SBDD）领域。SBDD 基于受体蛋白的结构设计与之适配的分子，是药物化学中的重要方法，而在深度学习辅助下的 SBDD 也将大大提升药物设计的效率，但目前这一方向还存在两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用于训练模型的「靶点-药物分子对」有限；&lt;/li&gt;
&lt;li&gt;SBDD AI 模型还不能很好利用靶点活性口袋的 3 维信息。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;针对以上两个问题，文章首先使用分子数据预训练 Transformer 生成模型，使其学习到分子数据中更通用的特征，避免标签不足；其次，文章设计了一种变种的 Transformer encoder，通过 encoder 获得氨基酸序列中的 3 维结构信息，文章将最后得到的模型称为 TamGent（Target-aware molecule generator with Transformer）。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用来自于 PubChem 数据库中的 1000 万个分子的 SMILES 序列预训练用于分子生成的 Transformer decoder 模型，使用来源于文献（&lt;a href="https://arxiv.org/abs/2205.07249" rel="noopener" target="_blank"&gt;Luo et al.&lt;/a&gt;）的 12.3 万个靶点-配体对训练配体生成模型。&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8650?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8650?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;使用 &lt;span class="math"&gt;\(\boldsymbol{a}=(a_1,a_2,\cdots,a_N)\)&lt;/span&gt; 表示氨基酸序列，其中 &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; 为长度为 20 的 ont-hot 向量，可以用于表示 20 种氨基酸，使用 &lt;span class="math"&gt;\(\boldsymbol{r}=(r_1,r_2,\cdots,r_N)\)&lt;/span&gt; 表示相应的 3 维坐标，其中 &lt;span class="math"&gt;\(r_i\in\mathbb{R}^3\)&lt;/span&gt;。将配体分子的 SMILES 编码转化为向量 &lt;span class="math"&gt;\(\boldsymbol{y}=(y_1,y_2,\cdots,y_M)\)&lt;/span&gt;，那么模型训练的目标就是学习从 &lt;span class="math"&gt;\(\boldsymbol{x}=(\boldsymbol{a},\boldsymbol{r})\)&lt;/span&gt; 到 &lt;span class="math"&gt;\(\boldsymbol{y}\)&lt;/span&gt; 的映射。&lt;/p&gt;
&lt;p&gt;TamGent 的架构参考了变分自编码器的工作模式，也就是主要由活性口袋 encoder 和配体分子 decoder 构成，encoder 与 decoder 都使用了 Transformer 中的结构。&lt;/p&gt;
&lt;p&gt;配体分子 decoder 部分与 Transformer 完全一样，具有 self-attention 机制，能够根据生成的 toekn 生成下一个 token，完成分子生成，因此使用 1000 万个分子数据预训练该模型，使其能够根据数据集中分子的普遍特征生成分子。&lt;/p&gt;
&lt;p&gt;活性口袋 encoder 部分修改了其中的 attention 机制，文章中称为 distance-aware attention。具体来说，就是认为距离较远的氨基酸与配体的相互作用更小，所以将输入的氨基酸序列和坐标转化为特征矩阵后，再与 &lt;span class="math"&gt;\(\exp(-\mathrm{distance}^2/\tau)\)&lt;/span&gt; 相乘，距离越远的氨基酸的权重就会越小。&lt;/p&gt;
&lt;p&gt;在推断过程中，将氨基酸序列及其坐标输入模型，embedding 为特征矩阵后进入活性口袋 encoder 部分计算 distance-aware attention，得到活性口袋的表示，最后将其作为配体分子 decoder 部分中的 pocket-SMILES attention，生成分子得到预测的活性配体结果。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun_1"&gt;结果与讨论&lt;/h2&gt;
&lt;h3 id="sheng-cheng-fen-zi-jie-guo"&gt;生成分子结果&lt;/h3&gt;
&lt;p&gt;文章使用 DrugBank 数据库中 1641 个靶点-配体对的数据用于测试模型效果，随机抽取其中的 100 个靶点-配体对，使用 TamGent、3DGen 和 SECSE 三种模型针对每个靶点生成 20 个分子，对比生成效果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8651?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8651?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;三种模型生成分子的对接打分中，TamGent 生成的分子明显更低，说明分子与靶点具有更好的亲和力，同时其平均值也与标签数据最为接近。对比三种模型生成的分子与标签分子的相似性，同样是 TamGent 具有更大的相似性，生成的分子最接近标签分子。同时，在 QED、MD 和 SA 几项的分子指标上，TamGent 也都高于其他两种模型，以上几点可以表明 TamGent 在 DrugBank 数据上根据靶点生成分子具有明显的优势。&lt;/p&gt;
&lt;h3 id="an-li-yan-jiu"&gt;案例研究&lt;/h3&gt;
&lt;p&gt;接下来文章使用 TamGent 针对于具体的靶点生成配体，分析模型表现。文章选择 SARS-CoV-2 主糖蛋白酶（&lt;em&gt;M&lt;/em&gt; &lt;sup&gt;pro&lt;/sup&gt;）作为靶点生成分子，收集了 415 个高分辨率结构后，使用模型生成了 4563 个分子，其中找到了先前报道过的一种 &lt;em&gt;M&lt;/em&gt; &lt;sup&gt;pro&lt;/sup&gt; 候选抑制剂（GC-376）和 6 个可能的先导化合物片段。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8652?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8652?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;上图展示了相应分子的二维分布，其中灰色表示在 PubChem 中随机选取的 3 万个分子，蓝色表示 TamGent 生成的分子，黄色表示在先前报道中提到了可能的 &lt;em&gt;M&lt;/em&gt; &lt;sup&gt;pro&lt;/sup&gt; 抑制剂。&lt;/p&gt;
&lt;p&gt;明显可以看出 TamGent 生成的分子与随机选取的分子具有不同的分布并且成簇聚集，主要分为 ① 和 ② 两簇。在第 ① 簇中，生成分子与 GC-376 的谷本相似度达到 0.82，并且此前报道的 6 种候选抑制剂都位于该簇中。但在第 ② 簇中没有找到对接分数较好的分子，只在分子中找到了一些可能的活性片段。&lt;/p&gt;
&lt;p&gt;最后文章选出了第 ① 簇中两个结构不同的分子与 &lt;em&gt;M&lt;/em&gt; &lt;sup&gt;pro&lt;/sup&gt; 对接，两种分子都能很好地填充活性口袋，对接分数分别为 -10.2 和 -9.5，而先前的 GC-376 是 -9.4，说明 TamGent 能够根据口性口袋生成具有良好活性的分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8653?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8653?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun_1"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章参考变分自编码器的结构使用 Transformer 构建了一种能够获取受体 3 维信息的分子生成模型 TamGent，其中分子生成部分使用了预训练模型，避免模型依赖于有限的「靶点-药物分子对」。在分子生成任务中，TamGent 生成分子的效果优于以往的两种模型，使用 TamGent 针对 SARS-CoV-2 主糖蛋白酶生成活性分子，甚至找到了比先前报道的候选抑制剂具有更好对接打分的分子，表现出 TamGent 的优异性能。&lt;/p&gt;
&lt;p&gt;对于 TamGent，文章提出了 3 点改进措施，第一是使用更多实验测试得到的「靶点-药物分子对」进一步优化模型，第二是在模型中整合考虑 ADMET 待药理性质，第三是在具体靶点上微调模型，使其帮助提升针对相应靶点的药物研发效率。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category><category term="VAE"></category></entry><entry><title>文献总结｜MolGPT：使用 Transformer 解码器模型实现分子生成</title><link href="https://leonis.cc/sui-sui-nian/2023-03-03-summary-doi.org/10.1021/acs.jcim.1c00600.html" rel="alternate"></link><published>2023-03-03T00:00:00+08:00</published><updated>2023-03-03T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-03:/sui-sui-nian/2023-03-03-summary-doi.org/10.1021/acs.jcim.1c00600.html</id><summary type="html">&lt;p&gt;本文介绍于 2022 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 MolGPT: Molecular Generation Using a Transformer-Decoder Model，在 GPT 模型已经在自然语言处理领域得到了成功应用的背景下，这篇文章首次将 GPT 模型应用于完成分子生成的任务，实现了分子性质和结构两个方面的优化。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/acs.jcim.1c00600" rel="noopener" target="_blank"&gt;doi.org/10.1021/acs.jcim.1c00600&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2022 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 MolGPT: Molecular Generation Using a Transformer-Decoder Model，在生成预训练（Generative Pre-training, GPT）模型已经在自然语言处理领域得到了成功应用的背景下，这篇文章首次将 GPT 模型应用于完成分子生成的任务，实现了分子性质和结构两个方面的优化。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;分子数据来自于 MOSES 和 GuacaMol 的数据集，其中包括源于 Zinc 的 190 万个类先导化合物与源于 ChEMBL 的 160 万个分子，分子为 SMILES 形式，使用 RDKit 提取分子的骨架用于模型训练。&lt;/p&gt;
&lt;p&gt;此外，使用 RDKit 计算出分子的logP、SA、拓扑极性表面积（TPSA）和 QED，用于训练具有性质约束的模型。&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8625?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8625?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在得到原始分子的分子骨架与性质信息后，将分子性质与分子骨架序列连接在一起，称为「条件」，那么原始分子就成为需要 MolGPT 根据条件生成的「目标分子」。&lt;/p&gt;
&lt;p&gt;在训练过程中，将条件和目标分子序列一同送入 MolGPT，使模型建立条件与目标分子的关系。GPT 模型通过顺序读取每个 token，由当前 token 预测下一个 token，从而获得采样的权重。&lt;/p&gt;
&lt;p&gt;具体来说，分子 SMILES 词嵌入为 256 维的向量后，将性质条件和骨架条件也分别词嵌入为 256 维的向量，将其直接拼接在 SMILES 向量的起始端，就构成了实际输入模型的信息。&lt;/p&gt;
&lt;p&gt;在推理过程中，模型对训练集中的所有 token 根据权重随机取样得到第一个字符，接着模型就根据输入的条件（即目标性质与需要改造的分子骨架）和第一个字符生成下一个字符，直至生成整个分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8626?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8626?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在 Transformer 中，encoder 模块对输入编码得到状态向量 &lt;span class="math"&gt;\(c\)&lt;/span&gt;，再由 decoder 模块对状态 &lt;span class="math"&gt;\(c\)&lt;/span&gt; 解码并运算产生输出，由于输入的情况是多种多样的，将其转化为等长的 &lt;span class="math"&gt;\(c\)&lt;/span&gt; 就有很大的局限性。GPT 模型减去了 Transformer 中的 encoder 模块，而保留了 Transformer 中例如 self-attention 在内的其他机制，具有更好的长文本处理能力。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun_1"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8632?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8632?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先使模型在不给定条件的情况下生成分子，即 MolGPT 根据从训练数据集中学习到的化学空间中的分子分布生成生成分子，生成的分子与训练集分子具有相似的特征。&lt;/p&gt;
&lt;p&gt;文章分析了 MolGPT 生成一个分子的过程，上图中的黑色横线表示当前步骤生成的字符（token），其他 token 上颜色的深浅表示了与生成该 token 之间的权重关系。&lt;/p&gt;
&lt;p&gt;可以看出，MolGPT 首先从已经学习到的分布中随机抽取出 &lt;code&gt;C&lt;/code&gt;，接着根据它继续生成后续 token，每个 token 都是由先前生成的 token 决定。同时还可以发现 MolGPT 在生成分子的过程中具有一定的「化学知识」，例如第一行中生成 &lt;code&gt;O&lt;/code&gt; 时，明显由前面的 &lt;code&gt;=&lt;/code&gt; 与 &lt;code&gt;N&lt;/code&gt; 决定，所以 MolGPT 不仅能够在双键上连接氧原子，还会构建酰胺结构使分子更稳定。&lt;/p&gt;
&lt;h3 id="xing-zhi-tiao-jian"&gt;性质条件&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8627?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8627?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章接着只使用性质条件作为输入分子的条件，测试模型是否能按照要求生成满足约束的分子，结果如上图所示，与训练集中分子的性质分布不同，生成的分子集中在设定的性质条件（黑线）两侧，评估生成分子，各组分子的 validity、unique 和 novelty 都在0.97 以上，具有很好的效果。&lt;/p&gt;
&lt;p&gt;此外文章还同时使用多种性质约束作为模型条件，在保持较高的 validity、unique 和 novelty 条件下，生成的分子散落在设置性质条件的周围，所以模型对也能很好地处理多性质约束的分子生成。&lt;/p&gt;
&lt;h3 id="xing-zhi-yu-gu-jia-tiao-jian"&gt;性质与骨架条件&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8628?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8628?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;接下来文章测试了将性质与分子骨架同时作为模型的生成条件，分析了生成分子的性质分布以及生成分子与设定分子骨架的谷本相似度。与仅性质条件的结果相比，额外加入分子骨架条件后生成分子的性质虽然有一些偏移（如上图 g），但仍然能大致满足性质约束。同时生成的分子与设定的分子骨架具有极高的相似性，从这两个可以证明 MolGPT 可以用于对给定的分子骨架进行指定性质的优化。&lt;/p&gt;
&lt;p&gt;最后文章展示了使用 MolGPT 实现分子骨架优化的例子：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8631?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8631?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun_1"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章使用 GPT 构建了分子生成模型 MolGPT，MolGPT 生成的分子具有很高的 validity 和 uniqueness，在对 MolGPT 生成分子过程中的权重分析发现，MolGPT 能够很好学习到 SMILES 中所包含的化学语义。在实际应用上，MolGPT 可以根据指定的多种分子性质和（或）指定的分子骨架生成目标的分子，生成的分子能够很好满足预先设定的要求，有助于指导化合物优化的方向。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category><category term="GPT"></category></entry><entry><title>文献总结｜在大数据集中有效识别匹配分子对（MMPs）的算法</title><link href="https://leonis.cc/sui-sui-nian/2023-02-25-summary-doi.org/10.1021/ci900450m.html" rel="alternate"></link><published>2023-02-25T00:00:00+08:00</published><updated>2023-02-25T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-02-25:/sui-sui-nian/2023-02-25-summary-doi.org/10.1021/ci900450m.html</id><summary type="html">&lt;p&gt;本文介绍于 2010 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Computationally Efficient Algorithm to Identify Matched Molecular Pairs (MMPs) in Large Data Sets，文章介绍了一种在大规模数据中识别匹配分子对的算法，这种算法也就是目前用于生成匹配分子对的最常用方法。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/ci900450m" rel="noopener" target="_blank"&gt;doi.org/10.1021/ci900450m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2010 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Computationally Efficient Algorithm to Identify Matched Molecular Pairs (MMPs) in Large Data Sets，文章介绍了一种在大规模数据中识别匹配分子对的算法，这种算法也就是目前用于生成匹配分子对的最常用方法。&lt;/p&gt;
&lt;p&gt;匹配分子对（Matched Molecular Pairs, MMP）是指化合物 A 与化合物 B 之间只在单一位置具有结构差异的一对分子，所以 MMP 中的分子可以很容易地从一个化合物改变为另一个化合物。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8602?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8602?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在面对先导化合物优化等问题时，传统且主流的做法仍然是依靠化学团队的经验。然而目前已经拥有化合物优化的海量数据，如果能得到其中蕴藏的经验和知识，将给予化学家很大的帮助并加快化合物优化的效率。MMP 就提供了从一批分子中获取化合物优化知识的一种方法，所以针对于识别分子数据集中 MMP 的算法，文章提出了两个要求：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;应当能识别数据集中所有的 MMP；&lt;/li&gt;
&lt;li&gt;计算效率高，能够适用于具有大量分子的数据集。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;p&gt;文章中提出的识别 MMP 算法可以归纳为以下几步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每次在一个化合中一处两个非氢原子间的非环单键处切断，在每个切断位点执行该操作，直至将数据集中所有分子分割为所有可能的片段。&lt;/li&gt;
&lt;li&gt;每次切断后，都要索引两个片段。具体来说，分子 &lt;span class="math"&gt;\(\rm{A&amp;mdash;B}\)&lt;/span&gt; 切断为 &lt;span class="math"&gt;\(\rm{A*}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\rm{* B}\)&lt;/span&gt; 后，需要建立 &lt;span class="math"&gt;\(\{\rm{key:value}\}\)&lt;/span&gt; 的索引，即 &lt;span class="math"&gt;\(\{\rm{A*:* B}\}\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\{\rm{* B:A*}\}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;下一次切断得到的片段若与已建立索引的 &lt;span class="math"&gt;\(\rm{key}\)&lt;/span&gt; 相同，就更新到相应的字典中。例如又得到了 &lt;span class="math"&gt;\(\rm{A* }\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\rm{* C}\)&lt;/span&gt;，那么索引就会更新为 &lt;span class="math"&gt;\(\{\rm{A*:* B,* C}\}\)&lt;/span&gt;，新增 &lt;span class="math"&gt;\(\{\rm{* C: A*}\}\)&lt;/span&gt;，保留 &lt;span class="math"&gt;\(\{\rm{* B: A*}\}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;在处理完所有分子后，相同 &lt;span class="math"&gt;\(\rm{key}\)&lt;/span&gt; 索引中的片段就揭示了 MMP 的化学转化，例如 &lt;span class="math"&gt;\(\rm{*B\rightarrow*C}\)&lt;/span&gt;，将键值对重新组合后同时也得到了目标的 MMP，例如 &lt;span class="math"&gt;\([\rm{A&amp;mdash;B,A&amp;mdash;C}]\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这种方法还可以推广到在同一分子中多次切断，识别单次切断可能会遗漏的 MMP。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8603?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8603?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;对于化学家而言，例如 &lt;span class="math"&gt;\(\rm{*B\rightarrow*C}\)&lt;/span&gt; 的转化规则，若该结构太大，那么通过这种转化得到的 MMP 也是没有实际意义的。所以为了避免用于替换的子结构过大，还可以设定只切断不超过 n 个非氢原子的子结构。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;文章所使用的分子数据库包含有 333332 个化合物，通过该算法使用单核 CPU 经过约 850 min 后，得到了 5310964 对 MMP。&lt;/p&gt;
&lt;p&gt;在得到的 MMP 中也存在一些问题，例如羧酸到酰胺是一类常见的结构转化，但在得到的结果中，羧酸到酰胺的转化的结果也会混入到醇到胺的转化中，因为这两种结构转化从分子表示上来看都是将羟基替换为氨基，而这两种结构转化的性质截然不同，就会导致下游的分子性质预测、结构改造任务出现问题。&lt;/p&gt;
&lt;p&gt;一种妥善的解决方法就是在切断步骤中，不切断官能团间的单键，保证 MMP 中的分子都具有相同的官能团，不互相混淆。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8604?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8604?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;另一个问题是一些化学转化是在环结构上进行结构改造，由于切断的操作只会在单键之间执行，所以对于这一类的化学转化，MMP 必须找到单键切下整个环结构。而对于一些稠环结构，很容易就超出了设定的 n 个非氢原子的约束，无法获得这一类化学转化。&lt;/p&gt;
&lt;p&gt;一个改进的方向就是设定若切下的子结构为纯粹的环结构，那么不计算原子数量直接建立索引，这样既获得到环结构上的化学转化，又避免了混入超过 n 原子的无意义转化。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8605?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8605?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章只将在一个分子中切断 1 次的操作推广到了 3 次。实验结果发现，在一个分子中切断 3 次后，就可以找到数据集中几乎全部的 MMP，切断次数越多，效果提升越少，计算开销越大，所以在实际使用中，3 次切断就已经是可行的。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8606?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8606?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种适用于在较大的分子数据集中识别 MMP 的算法，这种算法能够帮助人们从过去所累积的分子改造数据中获取化学改造的知识，结合分子的各种实际评估数据，这种方法能够更全面地构建出药物分子的构效关系，解决实际中先导化物优化方面的问题。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Algorithm"></category></entry><entry><title>为 Pelican 博客写插件——在文章中插入豆瓣图书</title><link href="https://leonis.cc/sui-sui-nian/2023-02-22-create-pelican-plugin.html" rel="alternate"></link><published>2023-02-22T00:00:00+08:00</published><updated>2023-02-22T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-02-22:/sui-sui-nian/2023-02-22-create-pelican-plugin.html</id><summary type="html">&lt;p&gt;突然想把豆瓣图书信息插入到文章中，便于在博客里记录阅读笔记。寻找了一圈无果，只好自己动手写个插件，也正好研究了下 Pelican 插件的工作方式。&lt;/p&gt;</summary><content type="html">&lt;p&gt;在博客上记录读书笔记是一件寻常事，但是怎么在文章中插入图书信息却是件恼人的事。直接用书名号引出？不不不，这也太不够美观了。参考一下国内图书信息最全面的豆瓣：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8594?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8594?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;错不了，就要像这样把图书封面、出版社还有 ISBN 等等全部展示出来，才能让人满意。但是一个个截图也太麻烦了，而且信息没法更新，将这些信息都嵌入在网页中才是最好的实现方式。比较流行的博客框架例如 Hexo 和 WordPress 都有实现类似功能的插件，而我使用的是比较小众的 Pelican，只好自己动手了。&lt;/p&gt;
&lt;h2 id="mu-biao"&gt;目标&lt;/h2&gt;
&lt;p&gt;我在博客上写作的流程是 &lt;code&gt;撰写 Markdown&lt;/code&gt; ⇨ &lt;code&gt;使用 Pelican 生成 HTML&lt;/code&gt;，为了实现在文章中插入豆瓣图书，比较方便的方法是在 Markdown 写作时，在需要插入图书的地方写下 &lt;code&gt;{GET BOOK URL}&lt;/code&gt;，其中 &lt;code&gt;BOOK&lt;/code&gt; 填写书籍名称，&lt;code&gt;URL&lt;/code&gt; 填写书籍链接。当 URL 无效时，保留这条指令，避免遗失原来的信息；当从 URL 成功获取了图书元数据后，就将该条命令转换为 HTML 格式的图书信息，就顺利插入文章中了。&lt;/p&gt;
&lt;p&gt;本文介绍的代码可以在 &lt;a href="https://github.com/Tseing/pelican-bookshelf" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; Tseing/pelican-bookshelf&lt;/a&gt;  上找到。&lt;/p&gt;
&lt;h2 id="pelican-mo-kuai"&gt;Pelican 模块&lt;/h2&gt;
&lt;p&gt;先从 Pelican 插件的原理讲起，Pelican 插件使用了信号机制，所谓信号机制就是在工作流程中，完成特定任务后就会给出特定信号，可以使用这种信号触发插件，完成正常工作流程以外额外的工作。那么在正式写插件之前，还需要了解一下 Pelican 的正常工作模块，具体可以分为以下 3 种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Writers：负责写文件的模块，例如生成 HTML、RSS 等，Writers 模块需要创建对象，并将其传递给 Generators 模块。&lt;/li&gt;
&lt;li&gt;Readers：用于读文件的模块，读取文件并返回元数据（作者、日期等）和用于生成 HTML 网页的内容。&lt;/li&gt;
&lt;li&gt;Generators：产生各种各样的输出，是工作流程中的最后部分。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;回头考虑我的目标，我所要编写的插件显然是一个 Reader 模块，当读取到 Markdown 中的图书指令时开始工作，并将替换的内容传递给 Generators 模块生成最终页面。&lt;/p&gt;
&lt;p&gt;但是由于 Pelican 文档中给出的相关介绍太少，再读了几遍源码之后仍觉得无从下手，最后我并没有使用官方推荐的方法，用了更简单粗暴的办法，可能以后有精力了会尝试再改成 Reader 模块。&lt;/p&gt;
&lt;h2 id="dou-ban-pa-chong"&gt;豆瓣爬虫&lt;/h2&gt;
&lt;p&gt;从指定的 URL 获取图书信息就要用爬虫了，网上关于爬虫的介绍多如牛毛，我只简单讲述一下我的设计过程。爬网解析 HTML 页面可以使用 Beautiful Soup 和 lxml 两个库，因为我需要使用的这个爬虫将在 Pelican 生成网页的过程中调用，虽然 Beautiful Soup 的功能强大也更好上手，但它的性能是不如 lxml 的，所以最后我还是选择使用 lxml。&lt;/p&gt;
&lt;p&gt;爬虫并不复杂，通过 request 获取 HTML 信息后，使用 Xpath 定位到目标信息的节点，例如目标信息的结构为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;div id="info" class=""&amp;gt;
    &amp;lt;span&amp;gt;
      &amp;lt;span class="pl"&amp;gt; 作者&amp;lt;/span&amp;gt;:
            &amp;lt;a class="" href="/author/4550936"&amp;gt;海子&amp;lt;/a&amp;gt;
    &amp;lt;/span&amp;gt;&amp;lt;br/&amp;gt;
    &amp;lt;span class="pl"&amp;gt;出版社:&amp;lt;/span&amp;gt;
      &amp;lt;a href="https://book.douban.com/press/2145"&amp;gt;江西人民出版社&amp;lt;/a&amp;gt;
    &amp;lt;br&amp;gt;
    &amp;lt;span class="pl"&amp;gt;出品方:&amp;lt;/span&amp;gt;
      &amp;lt;a href="https://book.douban.com/producers/10"&amp;gt;果麦文化&amp;lt;/a&amp;gt;
    &amp;lt;br&amp;gt;
    &amp;lt;span class="pl"&amp;gt;出版年:&amp;lt;/span&amp;gt; 2017-10&amp;lt;br/&amp;gt;
    &amp;lt;span class="pl"&amp;gt;页数:&amp;lt;/span&amp;gt; 193&amp;lt;br/&amp;gt;
    &amp;lt;span class="pl"&amp;gt;定价:&amp;lt;/span&amp;gt; 42.00元&amp;lt;br/&amp;gt;
    &amp;lt;span class="pl"&amp;gt;装帧:&amp;lt;/span&amp;gt; 精装&amp;lt;br/&amp;gt;
    &amp;lt;span class="pl"&amp;gt;丛书:&amp;lt;/span&amp;gt;
    &amp;lt;a href="https://book.douban.com/series/43038"&amp;gt;果麦经典&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;
    &amp;lt;span class="pl"&amp;gt;ISBN:&amp;lt;/span&amp;gt; 9787210097136&amp;lt;br/&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;获取出版社信息的脚本就可以这么写：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def get_press(meta, selector):
    regex = '//div[@id="info"]/child::span[contains(text(), "出版社")]/following-sibling::*[1]/text()'
    match = selector.xpath(regex)
    if match:
        meta["出版社"] = str(match[0])
    else:
        meta["出版社"] = "暂无"
    return meta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;regex&lt;/code&gt; 是需要匹配的 Xpath 路径，&lt;code&gt;//div[@id="info"]&lt;/code&gt; 为全文查找属性为 &lt;code&gt;id="info"&lt;/code&gt; 的 &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; 节点。&lt;code&gt;child::span[contains(text(), "出版社")]&lt;/code&gt; 为在其子节点查找包含「出版社」文字的 &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt; 节点，在这个节点之后的一个节点就是出版社信息了。&lt;/p&gt;
&lt;p&gt;用类似的方法就可以得到全部的出版信息，我把所有 Xpath 路径列在下方，随时可以取用，但是注意如果豆瓣更改了网页结构可能就会失效。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# 封面图片
regex = '//img[@rel="v:photo"]/@src'
# 作者
regex = '//div[@id="info"]/span[child::span[@class="pl"][contains(text(), "作者")]]//text()'
# 出版社、出品方、丛书等 &amp;lt;a&amp;gt; 标签内容
tags = ["出版社", "出品方", "丛书"]
regex = f'//div[@id="info"]/child::span[contains(text(), "{tag}")]/following-sibling::*[1]/text()'
# 其他非 &amp;lt;a&amp;gt; 标签出版信息
tags = ["出版年", "页数", "定价", "装帧", "ISBN"]
regex = f'//text()[preceding-sibling::span[1][contains(text(),"{tag}")]][following-sibling::br[1]]'
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="he-xin-dai-ma"&gt;核心代码&lt;/h2&gt;
&lt;p&gt;在解决了爬虫之后剩下的就是替换文本的一个简单任务了，由于爬虫的速度相对较慢，如果每次生成网页都要把所有 URL 都重新爬一遍，一来会大大降低生成速度，二来如此大量的请求很容易被网页反爬，所以我将原来的目标分成了两个功能。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;生成网页后，将 HTML 文件中所有 &lt;code&gt;{GET BOOK URL}&lt;/code&gt; 字段替换为图书信息。&lt;/li&gt;
&lt;li&gt;如果指定了 &lt;code&gt;SAVE_TO_MD&lt;/code&gt;，那么除生成的网页之外，原始的 Markdown 文件也会被修改，下一次生成网页时就不需要重新爬虫。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这两个功能的具体操作相同，所以用同一个函数实现即可：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def replace(path, context=None):
    suffix = os.path.splitext(str(path))[-1]
    if suffix != ".html" and suffix != ".md":
        pass
    elif suffix == ".md" and not BOOKSHELF_SETTING["SAVE_TO_MD"]:
        pass
    else:
        pattern = r"\{GET\s\S+\s[a-zA-z]+://[^\s]*\}"
        with open(str(path), 'r', encoding="utf-8") as f:
            s = f.read()
            search_target = re.search(pattern, s)
            while search_target is not None:
                _, book, url = search_target.group().strip("{}").split()
                html = get_page(url)
                time.sleep(BOOKSHELF_SETTING["WAIT_TIME"])
                if html is not None:
                    meta = parse_page(html)
                    s = s.replace(search_target.group(), generate_bookshelf(meta, book))
                    search_target = re.search(pattern, s)
                else:
                    search_target = re.search(pattern, s)
                    continue
        with open(str(path), 'w', encoding="utf-8") as f:
            f.write(s)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;代码也十分简单，就是根据路径打开文件，通过反复通过正则表达式搜索匹配的内容并用生成的 HTML 片段替换。在每次爬虫请求之后，我加入了一个 2 秒钟的延时，否则太容易被豆瓣屏蔽了。&lt;/p&gt;
&lt;h2 id="zhu-ce-cha-jian"&gt;注册插件&lt;/h2&gt;
&lt;p&gt;就像上文所说的，这是因为 Pelican 可以通过信号像管道一样将插件接入正常的工作流中，这里介绍两个信号：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;content_object_init(content_object)&lt;/code&gt;：在读取完所有文件后，准备通过 Generators 模块生成 HTML 时的信号。这个信号传递的参数是 &lt;code&gt;content_object&lt;/code&gt;，也就是目前读取完文件的对象，使用 &lt;code&gt;str(content_object)&lt;/code&gt; 能直接将文件转换为文件路径，将其传递给 &lt;code&gt;replace&lt;/code&gt; 函数就能将修改原始的 Markdown 文件，但由于文件已经读取完成了，所以不会影响输出文件中的内容。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;content_written(path, context)&lt;/code&gt;：Generators 模块输出文件后的信号，每输出一个文件就会引发一次该信号。信号传递的参数是 &lt;code&gt;path&lt;/code&gt; 和 &lt;code&gt;context&lt;/code&gt;，&lt;code&gt;path&lt;/code&gt; 是输出文件的路径，&lt;code&gt;context&lt;/code&gt; 是例如修改日期等的其他信息，所以 &lt;code&gt;replace&lt;/code&gt; 函数同样接受 &lt;code&gt;path&lt;/code&gt; 参数进行处理，Pelican 每生成一个文件，&lt;code&gt;replace&lt;/code&gt; 函数就检查是否有匹配的命令并替换，而不会改变原始 Markdown 文件。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后通过以下方式将 &lt;code&gt;replace&lt;/code&gt; 注册到相应的信号上，就可以使用了。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def register():
    pelican.signals.initialized.connect(init_config)
    pelican.signals.content_object_init.connect(replace)
    pelican.signals.content_written.connect(replace)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当然插件的功能只是在文件中插入了 HTML 片段，以下是个样例；&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;div class="bookshelf"&amp;gt;
  &amp;lt;div class="book"&amp;gt;
    &amp;lt;img src="https://img2.doubanio.com/view/subject/s/public/s29610741.jpg" referrerPolicy="no-referrer"/&amp;gt;
    &amp;lt;div class="infos"&amp;gt;
      &amp;lt;a class="title" href="https://book.douban.com/subject/27154094/"&amp;gt;海子的诗&amp;lt;/a&amp;gt;
      &amp;lt;div class="作者"&amp;gt;作者：海子&amp;lt;/div&amp;gt;
      &amp;lt;div class="出版社"&amp;gt;出版社：江西人民出版社&amp;lt;/div&amp;gt;
      &amp;lt;div class="出版年"&amp;gt;出版年：2017-10&amp;lt;/div&amp;gt;
      &amp;lt;div class="页数"&amp;gt;页数：193&amp;lt;/div&amp;gt;
      &amp;lt;div class="定价"&amp;gt;定价：42.00元&amp;lt;/div&amp;gt;
      &amp;lt;div class="ISBN"&amp;gt;ISBN：9787210097136&amp;lt;/div&amp;gt;
    &amp;lt;/div&amp;gt;
  &amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;只要编写好 CSS 样式再套上去，就能得到很不错的效果啦。&lt;/p&gt;
&lt;h2 id="demo"&gt;Demo&lt;/h2&gt;
&lt;p&gt;最后看一看插件的效果吧 :）&lt;/p&gt;
&lt;div class="bookshelf"&gt;
&lt;div class="book"&gt;
&lt;img referrerpolicy="no-referrer" src="https://img2.doubanio.com/view/subject/s/public/s29610741.jpg"&gt;
&lt;div class="infos"&gt;
&lt;a class="title" href="https://book.douban.com/subject/27154094/"&gt;海子的诗&lt;/a&gt;
&lt;div class="作者"&gt;作者：海子&lt;/div&gt;
&lt;div class="出版社"&gt;出版社：江西人民出版社&lt;/div&gt;
&lt;div class="出版年"&gt;出版年：2017-10&lt;/div&gt;
&lt;div class="页数"&gt;页数：193&lt;/div&gt;
&lt;div class="定价"&gt;定价：42.00元&lt;/div&gt;
&lt;div class="ISBN"&gt;ISBN：9787210097136&lt;/div&gt;
&lt;/div&gt;
&lt;/img&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="zi-ji-de-hua"&gt;自己的话&lt;/h2&gt;
&lt;p&gt;其实我之前从未用过爬虫，写这个插件也是一时兴起，最后花了 2 天时间简单地完成了。虽然国内使用 Pelican 的人非常少，甚至在国际上使用的人也不多，但正因为这是一个如此小的社区，并没有丰富的插件，所以我也能自豪地参与其中并为它做贡献。&lt;/p&gt;
&lt;p&gt;关于上文所提到的目标，最好的实现方式并不是爬虫，因为爬虫速度慢、发起的大量请求对服务器也并不友善，更好的方式是使用官方的 API。使用官方 API 发起的请求与对网页的请求不是同一个入口，不会影响网站的正常访问，而且需要的数据都在 JSON 中，不用费力不讨好地去解析 HTML。&lt;/p&gt;
&lt;p&gt;然而遗憾的是，四五年前豆瓣就并闭了它的 API，原来使用 API 的「互联网难民」也转向了使用爬虫，面对于海量的爬虫，豆瓣与网友之间最终陷入到了反爬虫与爬虫的零和博弈。我相信不用多久，文章中提到的爬虫方式就会无法使用，又不得不继续参加到这场博弈中去。不知在哪看到的一句话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;cite&gt;中国互联网公司之间，是没有 API 的，通用的 API 就是硬爬。&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;不禁苦笑，真是「萧瑟秋风今又是，换了人间」，我们正在见证着互联网走向封闭，明天又会是什么样呢？&lt;/p&gt;</content><category term="碎碎念"></category><category term="Blog"></category><category term="Pelican"></category><category term="Python"></category><category term="XPath"></category></entry><entry><title>文献总结｜通过大规模化学语言表示捕获分子结构和性质</title><link href="https://leonis.cc/sui-sui-nian/2023-02-17-summary-doi.org/10.1038/s42256-022-00580-7.html" rel="alternate"></link><published>2023-02-17T00:00:00+08:00</published><updated>2023-02-17T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-02-17:/sui-sui-nian/2023-02-17-summary-doi.org/10.1038/s42256-022-00580-7.html</id><summary type="html">&lt;p&gt;本文介绍于 2022 年发表在 &lt;em&gt;Nature Machine Intelligence&lt;/em&gt; 上的一篇文章，文章原标题为 Large-scale chemical language representations capture molecular structure and properties，文章使用大量来自 PubChem 和 ZINC 的分子训练了基于 Transformer 的化学语言模型，该模型将原始的 SMILES 序列转化为向量，文章结果表明，使用这种方式实现的分子词嵌入在许多任务上都表现优异。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1038/s42256-022-00580-7" rel="noopener" target="_blank"&gt;doi.org/10.1038/s42256-022-00580-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 IBM 研究中心于 2022 年发表在 &lt;em&gt;Nature Machine Intelligence&lt;/em&gt; 上的一篇文章，文章原标题为 Large-scale chemical language representations capture molecular structure and properties，文章使用大量来自 PubChem 和 ZINC 的分子训练了基于 Transformer 的化学语言模型，该模型将原始的 SMILES 序列转化为向量，文章结果表明，使用这种方式实现的分子词嵌入在许多任务上都表现优异。&lt;/p&gt;
&lt;p&gt;在深度学习领域，SMILES 是最为常用的分子表示形式，但越来越多研究结果表明，SMILES 无法反映分子的拓扑结果，在许多任务上都有局限性。这使得许多研究人员转向使用基于图的分子表示方式，然而由于缺少用于训练的标记数据，研究这一类模型也具有很大挑战。&lt;/p&gt;
&lt;p&gt;预训练语言模型是近年 NLP 领域中的热点，先使用大量语料训练语言模型，再微调该预训练模型并用于解决具体任务。借鉴这一概念，文章的目标就是使用庞大的分子数据集（11 亿个分子）训练化学语言模型，让该模型能够捕获分子的深层特征，并用它来完成各种分子预测的下游任务。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8580?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8580?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;文章所使用的模型称为 MoLFormer，该模型通过对 Transformer 改造得到，减小了计算开销，适用于处理大量数据。在 Transformer 中，输入序列中的每个位置上的 token 都会通过位置嵌入将位置信息加入到词向量中。由于 self-attention 机制，Transformer 在每个位置 &lt;span class="math"&gt;\(m\)&lt;/span&gt; 上需要计算：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}_m(Q,K,V)=\frac{\sum_{n=1}^N\exp(\langle q_m,k_n\rangle)v_n}{\sum_{n=1}^N\exp(\langle q_m,k_n\rangle)}$$&lt;/div&gt;
&lt;p&gt;指数计算的计算开销很大，可以用核函数近似的方法简化计算（Vanilla Transforme），得到线性注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}_m(Q,K,V)=\frac{\sum_{n=1}^N\langle\varphi(q_m),\varphi(k_n)\rangle v_n}{\sum_{n=1}^N\langle\varphi(q_m),\varphi(k_n)\rangle}$$&lt;/div&gt;
&lt;p&gt;此外，文章还引入了旋转位置嵌入（Rotary Position Embeddings, RoPE）替换了原来的位置嵌入方式。RoPE 是一种新的位置编码技术，旨在理解文本中单词的顺序表示，在化学语言模型中，它可能能够更好地捕捉 token 之间的内在关系，表示出分子的拓扑结构。&lt;/p&gt;
&lt;p&gt;在训练上，文章使用了掩码语言模型方法，选出 15% 的 token 作为噪声，其中 80% 的 token 将随机地被 &lt;code&gt;[MASK]&lt;/code&gt; 代替，10% 随机替换为其他 token，其他 10% 不变。通过这种方法可以强迫模型不依赖于 token，而是依赖于上下文做出判断，增强模型的稳健性。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;来自 PubChem 的 1.1 亿个分子和来自 ZINC 的 10 亿个分子构成了训练数据，所有分子经过 RDKit 规范化后用于分词，总共得到 2357 个不重复的 token，设置 202 token 为模型输出的最长序列。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;MoLFormer 作为一种通用的预训练模型，在经过大数据集的训练后，再使用不同的标准分子数据集微调模型，完成 MoleculeNet 中的分类与回归任务，并将其结果与多种基线模型的表现对比，验证 MoLFormer 的编码效果。在 AUC-ROC 数据中，MoLFormer 在 6 组数据集上获得了 3 组最优表现，显著优于其他基线模型。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8581?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8581?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;除此之外，文章还对比了 MoLFormer 和与之类似的预训练模型 ChemBerta 编码分子的能力。文章从 PubChem 数据集中随机挑选 10000 对分子，计算每对分子指纹相似性与预训练模型得到的词嵌入相似性之间的相关系数，计算每对分子最大公共子图（Maximum Common Subgraph, MCS）中原子数量与两词嵌入欧氏距离之间的相关系数。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8591?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8591?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;实验结果中，在两个对比的指标上，MoLFormer 都具有更大的相关性，说明通过它得到的词嵌入能够更好地反映分子结构特征，即两分子的分子指纹相似性越高，表示两分子的向量也越相似；两分子的共有结构越大，表示两分子向量之间的距离也更小。&lt;/p&gt;
&lt;p&gt;最后，文章分析了 MoLFormer 学习到的注意力矩阵。随机选取分子，绘制各原子间的化学键连接矩阵和 3D 距离矩阵，与相应的 MoLFormer 全注意力矩阵和线性注意力矩阵对比，可以看出，线性注意力矩阵与分子的化学键连接矩阵和 3D 距离矩阵具有部分相似的权重，这可能解释了 MoLFormer 编码后为什么能保留分子的结构特征。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8592?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8592?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章使用大规模数据训练了基于 Transformer 的化学语言模型，经过实验对比，这种通用的预训练模型可以很好地通过微调迁移到其他数据集上完成包括分类、回归在内的任务，并且取得了优异的表现。此外，文章还确定了通过该模型得到的词嵌入能够反映分子的结构，同时比以往类似模型具有更好效果。&lt;/p&gt;
&lt;p&gt;文章主要对 Transformer 中的注意力机制做了改造，使用线性注意力减少计算开销，大大加快了训练速度，因此才能在 11 亿个分子的大规模数据上完成学习过程。文章还使用 RoPE 提高模型识别上下文的效果，这可能也是该模型具有更优表现的原因。目前预训练的通用语言模型如 GPT 和 Bart 等在 NLP 领域大放异彩，而对于这一类化学语言模型的研究还很少，它具有助力完成下游分子预测等任务的潜力。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>Pelican + Nginx 在服务器上搭载静态博客</title><link href="https://leonis.cc/sui-sui-nian/2023-02-13-deploy-pelican-by-nginx.html" rel="alternate"></link><published>2023-02-13T00:00:00+08:00</published><updated>2023-02-13T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-02-13:/sui-sui-nian/2023-02-13-deploy-pelican-by-nginx.html</id><summary type="html">&lt;p&gt;去年购置了一个服务器，由于后来太忙，一直没有时间折腾，终于有一段空闲的时间了，就尝试把我的 Pelican Blog 从 GitHub Pages 搬迁到服务器上，再通过 Nginx 部署我的站点。&lt;/p&gt;</summary><content type="html">&lt;p&gt;去年黑色星期五低价购置了一个海外服务器，由于后来太忙，一直没有时间折腾，终于有一段空闲的时间了。想到的我博客一直部署在 GitHub Pages 上，访问速度慢不说，还会由于各种的原因时常无法连接，所以动手把它搬到自己的服务器上吧。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8514?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8514?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;上图是我的部署方案，首先使用 Pelican（或其他静态博客生成器）在本地生成静态网页，再将静态网页 push 到 GitHub 仓库，这两步也是在 GitHub Pages 上部署、更新博客的步骤，所以对我而言没有额外的负担。&lt;/p&gt;
&lt;p&gt;接着在服务器上 pull 静态网页，然后使用 Nginx 作为 HTTP 服务器就可以通过域名或是公网 IP 访问到网站了。虽然 Pelican 也自带有 HTTP 服务器，可以不使用 Nginx，但我觉得它的性能和通用性方面不够好，而且我主要在 PC 上写文章，在服务器上安装 Pelican 并在服务器上生成静态网页未免太繁琐。&lt;/p&gt;
&lt;p&gt;最后为了便于管理服务器上的文件和排查问题，可以使用 SFTP 直接上传和下载服务器文件。&lt;/p&gt;
&lt;p&gt;{warn begin}在境内服务器上部署网站必须经过 TCP 备案，否则会被拦截或是限流，引起非常多的麻烦。{warn end}&lt;/p&gt;
&lt;h2 id="pei-zhi-nginx"&gt;配置 Nginx&lt;/h2&gt;
&lt;p&gt;在 Debian 系统上可以使用以下命令安装 Nginx：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Debian
$ sudo apt install nginx
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装完成后可 &lt;code&gt;nginx -v&lt;/code&gt; 检查 Nginx 的版本。使用 &lt;code&gt;http://服务器 IP&lt;/code&gt; 访问 Web 服务器，不出意外的话，会出现如下页面，提示已经成功安装了 Nginx：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8515?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8515?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;如果没有出现 Nginx 的欢迎页面，就需要检查服务器防火墙的设置，由于各个系统的防火墙设置不同，这里就不给出设置的方法了。&lt;/p&gt;
&lt;p&gt;然后需要在 &lt;code&gt;/etc/nginx/sites-available&lt;/code&gt; 中建立我们站点的 Nginx 配置:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;$ cd /etc/nginx/sites-available
$ sudo touch blog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 Nginx 设计中，在 &lt;code&gt;sites-available&lt;/code&gt; 目录下存放站点的配置文件，并将其链接到 &lt;code&gt;sites-enabled&lt;/code&gt; 目录下。&lt;code&gt;sites-enabled&lt;/code&gt; 目录下存放了启用站点的配置，若要关闭该站点的配置，删除该链接即可。&lt;/p&gt;
&lt;p&gt;通过以下命令进入 &lt;code&gt;sites-enabled&lt;/code&gt; 并建立软链接：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;$ cd /etc/nginx/sites-enabled
$ ln -s /etc/nginx/sites-available/blog /etc/nginx/sites-enabled/blog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;接着再回到 &lt;code&gt;sites-available&lt;/code&gt; 中，使用 &lt;code&gt;vim blog&lt;/code&gt; 编辑配置，粘贴以下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-nginx"&gt;server {
    listen       80;
    server_name  _;
    root         /home/Leo/web/blog;

    location / {
    };
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中 &lt;code&gt;listen 80&lt;/code&gt; 指监听 80 端口，也就是 HTTP 服务的端口，&lt;code&gt;server_name&lt;/code&gt; 是网站的 url 地址，如果需要通过域名访问，例如填入 &lt;code&gt;example.com&lt;/code&gt; 即可，&lt;code&gt;root&lt;/code&gt; 是静态站点的文件目录，在该目录下存放了站点的 HTML 和 CSS 文件，&lt;code&gt;location&lt;/code&gt; 设置了网站路由，不需要修改。&lt;/p&gt;
&lt;p&gt;最后需要将同一文件夹内 &lt;code&gt;default&lt;/code&gt; 文件中的所有内容都注释掉。&lt;/p&gt;
&lt;h2 id="cong-github-pull-jing-tai-wang-ye"&gt;从 GitHub pull 静态网页&lt;/h2&gt;
&lt;p&gt;在本地电脑上，使用 Pelican（或其他静态网页生成器）生成静态网页，Pelican 的命令是 &lt;code&gt;pelican content -s publishconf.py&lt;/code&gt;，然后就会生成 &lt;code&gt;output&lt;/code&gt; 文件夹，只需要将该文件夹 push 到 GitHub 仓库中，这里的操作十分简单，也不展开介绍了。&lt;/p&gt;
&lt;p&gt;在服务器中进入 &lt;code&gt;root&lt;/code&gt; 文件夹，也就是 &lt;code&gt;/home/Leo/web/blog&lt;/code&gt; 中，&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;git init
git remote add origin https://github.com/Tseing/tseing.github.io.git
git pull origin master
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用 &lt;code&gt;sudo git init&lt;/code&gt; 初始化后，从链接的 GitHub 仓库中 pull 静态文件。最后使用 &lt;code&gt;sudo nginx -s reload&lt;/code&gt; 重启 Nginx 服务，在 &lt;code&gt;http://服务器 IP&lt;/code&gt; 上就能看到博客页面啦。&lt;/p&gt;
&lt;p&gt;可以把在服务器上发布文章的流程总结为以下几步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在本地使用 Markdown 撰写文章；&lt;/li&gt;
&lt;li&gt;使用 Pelican 生成静态网页；&lt;/li&gt;
&lt;li&gt;将生成的静态网页 push 至 GitHub 仓库；&lt;/li&gt;
&lt;li&gt;连接服务器，在相应目录下 pull GitHub 仓库中的更新。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这样的流程比较繁琐，每次更新都需要连接到服务器上操作，要简化这样的流程并实现自动化部署就需要借助 webhook 的功能。简单来说，在 GitHub 仓库设置中开启 webhook，用户对仓库执行每个动作（这里为 push）后，仓库都会向目标服务器发送一段 JSON 报文（回调），那么服务器只需要一直运行着监听该报文的脚本，一旦收到 push 成功的报文就执行 pull 操作。这样就简化了工作流程，push 静态网页后，服务器上的内容也会自动更新。&lt;/p&gt;
&lt;p&gt;webhook 脚本可以使用不同语言实现，例如 PHP、JavaScript 和 Python 等等。但是考虑了服务器的性能，我不打算在服务器上配置过于重的环境。于是我选择了基于 Go 语言的 &lt;a href="https://github.com/adnanh/webhook" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; adnanh/webhook&lt;/a&gt;，十分轻量，配置的方法也很简单，可以参考&lt;a href="https://www.cnblogs.com/pingyeaa/p/12777626.html" rel="noopener" target="_blank"&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;{warn begin}旧版 webhook 存在一些令人困恼的 bug，例如无法读取相对路径，请使用最新版本。{warn end}&lt;/p&gt;
&lt;h2 id="guan-bi-github-pages"&gt;关闭 GitHub Pages&lt;/h2&gt;
&lt;p&gt;在一切部署工作都完成之后，也是时候和 GitHub Pages 说再见了。避免搜索引擎抓取数据时出现大量重复网页，所以就关闭原来 GitHub Pages 上的博客吧。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8516?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8516?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;Hello world!&lt;/p&gt;&lt;br/&gt;
&amp;mdash;&amp;mdash; &lt;em&gt;From my server&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr/&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.jianshu.com/p/87e26e644a5a" rel="noopener" target="_blank"&gt;使用 Nginx 搭建静态网站 - 简书&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/jenkin1991/p/8301983.html" rel="noopener" target="_blank"&gt;nginx 配置详解 - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="Blog"></category><category term="Pelican"></category><category term="Nginx"></category><category term="Linux"></category></entry></feed>