<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Leo's blog</title><link href="https://leonis.cc/" rel="alternate"></link><link href="https://leonis.cc/feed.xml" rel="self"></link><id>https://leonis.cc/</id><updated>2023-05-27T00:00:00+08:00</updated><subtitle>A nook to hoard my manuscripts.</subtitle><entry><title>文献总结｜可以同时完成分子语言序列回归和生成的 Regression Transformer</title><link href="https://leonis.cc/sui-sui-nian/2023-05-27-summary-doi.org/10.1038/s42256-023-00639-z.html" rel="alternate"></link><published>2023-05-27T00:00:00+08:00</published><updated>2023-05-27T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-27:/sui-sui-nian/2023-05-27-summary-doi.org/10.1038/s42256-023-00639-z.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年 IBM 研究团队发表在 &lt;em&gt;Nature Machine Intelligence&lt;/em&gt; 上的一篇文章，文章原标题为 Regression Transformer enables concurrent sequence regression and generation for molecular language modelling，文章提出了一种可以同时处理序列中的数值与文本并完成回归与生成的多任务的 Transformer 模型。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1038/s42256-023-00639-z" rel="noopener" target="_blank"&gt;doi.org/10.1038/s42256-023-00639-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年 IBM 研究团队发表在 &lt;em&gt;Nature Machine Intelligence&lt;/em&gt; 上的一篇文章，文章原标题为 Regression Transformer enables concurrent sequence regression and generation for molecular language modelling，文章提出了一种可以同时处理序列中的数值与文本并完成回归与生成的多任务的 Transformer 模型。&lt;/p&gt;
&lt;p&gt;基于 Transformer 的模型是化学任务中常用的模型，但由于 Transformer 最早是用于自然语言处理的模型，难以处理回归任务，这些模型只能完成性质预测或条件分子生成，无法同时完成指定结构的生成和性质预测。若要实现有约束的分子生成，即根据指定的性质生成分子，则不得不通过在多个模型间传递参数再得到反馈的方法不断调节并得到目标的分子，如下图中 &lt;strong&gt;a&lt;/strong&gt; 所示。&lt;/p&gt;
&lt;p&gt;文章尝试将回归任务融入到文本序列建模的过程中，提出了一种可以同时处理序列中的数值与文本并完成回归与生成的多任务模型，称为 回归 Transformer（Regression Transformer, RT）。在实验部分，文章使用化学领域中常见的分子生成、性质预测、化学反应预测、生物领域中蛋白质性质预测以及自然语言处理中的文本生成等多种任务测试了模型效果，证明 RT 是一种可以通用于多种任务且可以同时完成序列回归和生成的模型。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9166?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9166?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;Transformer 原为由左至右逐次由前一个 token 预测下一个 token 的自回归模型，而在分子语言，如 SMILES 中，序列中各原子的顺序是没有特定意义的，序列中的原子也并非由前一个原子决定，因此文章选择使用非自回归模型。BERT、XLNet 都是 Transformer 的变种，BERT 使用掩码的方式随机掩盖序列中的 token，并根据周围的 token 预测被掩盖的 token，因为这个过程使用周围信息编码掩盖的 token，这类模型称为自编码模型。&lt;/p&gt;
&lt;p&gt;XLNet 结合了自回归模型与自编码模型的优势，尽管 XLNet 还是由左至右预测 token，但它使用排列置换的方法将随机选择的待预测 token 放至序列末端，与 BERT 的掩码机制实际上相同，称为排列语言模型（Permutation language modeling, PLM）。文章使用 XLNet 作为主要的模型。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9167?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9167?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h4&gt;数值编码器&lt;/h4&gt;
&lt;p&gt;如上图所示，输入的数据格式为 &lt;code&gt;&amp;lt;ESOL&amp;gt;-2.92|SMILES&lt;/code&gt;，&lt;code&gt;&amp;lt;ESOL&amp;gt;&lt;/code&gt; 标识了预测的性质，&lt;code&gt;-2.92&lt;/code&gt; 为该性质的数值。由于 Transformer 无法识别数值，会将其识别为数字字符，文章设计了数值编码器（numeric encoder, NE）获取数值信息。&lt;/p&gt;
&lt;p&gt;先将 &lt;code&gt;-2.92&lt;/code&gt; 分为 &lt;code&gt;_-_&lt;/code&gt; &lt;code&gt;_2_0_&lt;/code&gt; &lt;code&gt;_._&lt;/code&gt; &lt;code&gt;_9_-1_&lt;/code&gt; &lt;code&gt;_2_-2_&lt;/code&gt; 几个 token，其中的 &lt;code&gt;_-_&lt;/code&gt; 与 &lt;code&gt;_._&lt;/code&gt; 分别表示负号与小数点，数字 &lt;code&gt;9&lt;/code&gt; 就以 &lt;code&gt;_9_-1_&lt;/code&gt; 表示，其中 &lt;code&gt;9&lt;/code&gt; 表示数值为 9，&lt;code&gt;-1&lt;/code&gt; 表示该值位于十分位（10&lt;sup&gt;-1&lt;/sup&gt;）。&lt;/p&gt;
&lt;p&gt;对于数值 token &lt;span class="math"&gt;\(t_{v,p}\)&lt;/span&gt;，&lt;span class="math"&gt;\(v\)&lt;/span&gt; 表示该 token 的数值，&lt;span class="math"&gt;\(p\)&lt;/span&gt; 表示该 token 数值的位置，词嵌入的第 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 维按下式计算：&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{NE_{Float}}(v,p,j)=(-1)^j\cdot\frac{v\cdot 10^p}{j+1}
$$&lt;/div&gt;
&lt;p&gt;然后与 SMILES 的常规词嵌入一起加上位置编码进入 XLNet 中进行计算。&lt;/p&gt;
&lt;h4&gt;XLNet&lt;/h4&gt;
&lt;p&gt;输入 RT 的 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 是由 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 个性质 token &lt;span class="math"&gt;\([\boldsymbol{x}^p]_k\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(l\)&lt;/span&gt; 个文本 token &lt;span class="math"&gt;\([\boldsymbol{x}^t]_l\)&lt;/span&gt; 拼接而成，即&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{x}=[\boldsymbol{x}^p,\boldsymbol{x}^t]_T=[x^p_1,\cdots,x^p_k,x^t_1,\cdots,x^t_l]
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(T=k+l\)&lt;/span&gt;，为整个序列的 token 数量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PLM objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{PLM}\)&lt;/span&gt;）：在原始的 XLNet 中，输入的序列就要做 &lt;span class="math"&gt;\(T!\)&lt;/span&gt; 次的排列，将掩盖的 token 放置到序列末端，训练目标是使模型能够预测出掩盖的 token。如上图中 PLM objective 所示，由于这种训练方法是随机选取，打断了整体的 &lt;span class="math"&gt;\(\boldsymbol{x}^p\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(\boldsymbol{x}^t\)&lt;/span&gt;，因而不适合该任务，仅用于预训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Property prediction objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{P}\)&lt;/span&gt;）：对于分子性质预测的回归任务，将表示分子性质的 &lt;span class="math"&gt;\(\boldsymbol{x}^p\)&lt;/span&gt; 全部掩盖并排列置换位置，使用分子的文本 &lt;span class="math"&gt;\(\boldsymbol{x}^t\)&lt;/span&gt; 预测被掩盖的分子性质。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conditional text generation objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{G}\)&lt;/span&gt;）：对于分子生成任务，正与上述过程相反，将表示分子的 &lt;span class="math"&gt;\(\boldsymbol{x}^t\)&lt;/span&gt; 全部掩盖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Self-consistency (SC) objective&lt;/strong&gt;（&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{SC}\)&lt;/span&gt;）：为了使 RT 能够同时完成回归和生成任务，文章设计了该训练目标：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{J}_\mathrm{SC}=\mathcal{J}_\mathrm{G}(\boldsymbol{x})+\alpha\cdot\mathcal{J}_\mathrm{P}(\hat{\boldsymbol{x}})$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; 为权重，&lt;span class="math"&gt;\(\hat{\boldsymbol{x}}=[\boldsymbol{x}^p,\hat{\boldsymbol{x}}^t]\)&lt;/span&gt; 为生成的样本。该训练任务就是先使用分子性质生成分子，再用生成的分子预测其性质。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;使用 SELFIES 作为分子表示，许多研究表明，相比 SMILES，SELFIES 在分子生成任务上更具有优势。&lt;/p&gt;
&lt;p&gt;Synthetic QED dataset：由 ChEMBL 得到的约 160 万个分子，约 140 万用于训练，1000 条数据用于验证，10000 条数据用于测试。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;p&gt;文章中使用 RT 在化学反应、蛋白质性质预测等任务上测试了模型性能，这里仅以分子生成与分子性质预测的任务为例。&lt;/p&gt;
&lt;p&gt;在 QED 数据集上，先使用 &lt;span class="math"&gt;\(\mathcal{J}_\mathrm{PLM}\)&lt;/span&gt; 训练模型，至验证集数据的指标收敛后，再每 50 轮用 &lt;span class="math"&gt;\(\mathcal{J}_\mathrm{P}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\mathcal{J}_\mathrm{G}\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(\mathcal{J}_\mathrm{SC}\)&lt;/span&gt; 轮流微调（Alternate），不同模型设定的结果如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9168?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9168?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;从实验结果中可以看出，（1）SELFIES 在生成任务上更有优势，但在回归任务上稍逊于 SMILES；（2）不论是回归还是生成任务，预训练使模型的表现提升；（3）设计的数值编码器有利于模型识别数值信息，提升模型表现；（4）在微调阶段轮流使用不同的训练任务，使模型在回归和生成两种任务上的泛化能力更好，在回归和生成单个任务上都具有与单任务模型接近甚至更优的表现。&lt;/p&gt;
&lt;p&gt;能够处理回归与生成两种任务的模型也可以用于实现分子的性质优化，具体过程是设定一个 seed 分子以及目标的性质（primer），模型随机掩盖分子中 token 再通过 primer 将 token 预测出来，得到优化后的新分子，再通过新分子计算其性质的预测值，下图展示了在两种不同的数据集上微调得到的模型实现分子性质优化的样例。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9169?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9169?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了回归 Transformer（RT）模型，该模型以 XLNet 为主要的结构，文章增加了数值编码器用于获取数值信息，并设计了不同的训练模式使模型在预训练-微调后能够完成数值回归与序列生成两种不同的任务。RT 设计用于数值回归与序列生成，因此也可以用于蛋白性质预测、反应预测等。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>旧书市场淘书记</title><link href="https://leonis.cc/zai-lu-shang/2023-05-22-wander-in-old-book-shop.html" rel="alternate"></link><published>2023-05-22T00:00:00+08:00</published><updated>2023-05-22T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-22:/zai-lu-shang/2023-05-22-wander-in-old-book-shop.html</id><summary type="html">&lt;p&gt;不得不说，在北方诸多城市中，天津的二手旧物市场可以说是相当火热的。我猜测的原因有二，一则是天津的老龄人口占比多，古玩旧物收藏有很大的受众；二则是得益于近代天津经济、文化的繁荣，许多官商士绅定居在此，天津民间仍流通有十分 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;不得不说，在北方诸多城市中，天津的二手旧物市场可以说是相当火热的。我猜测的原因有二，一则是天津的老龄人口占比多，古玩旧物收藏有很大的受众；二则是得益于近代天津经济、文化的繁荣，许多官商士绅定居在此，天津民间仍流通有十分具有价值的骨董。&lt;/p&gt;
&lt;p&gt;我对古玩是一窍不通，再加之俚谚「多看少买」的教育，更有许多低劣到我都能看出的赝品，常引得我在心中暗笑，所以我对那些地摊上的古玩也一点不感兴趣。但旧物中有一门类却是我的心头好，那就是旧书。&lt;/p&gt;
&lt;p&gt;我一向认为书应当是用来读的，次之才是历史等其他价值。由于许多好书由于各种原因不再出版了，或是更改了原来的版本，没有旧版本更好读了，于是有了「藏书」的群体去搜罗这些旧书。所以「藏书」的「藏」不该是像对待金银珠宝那样的「秘藏」，而是作「保存」解。这是我的藏书主张，也是我搜集旧书的信条。&lt;/p&gt;
&lt;p&gt;之所以提及以上原则，还是因为天津旧货市场上的旧书实在太多了，近乎可以同逛新书店一般，不带任何想法去，抱回好几摞的书，为了避免这种无谓的金钱开销，必须要有筛选的准绳。前些天在反复告诉自己&lt;dot&gt;买书是为了读书&lt;/dot&gt;后，终于敢大胆淘了几本书，对其中几本实在喜欢得紧，也算小有收获。&lt;/p&gt;
&lt;h2 id="qie-jie-ting-za-wen-qie-jie-ting-za-wen-mo-bian"&gt;《且介亭杂文》《且介亭杂文末编》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="且介亭杂文" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9071?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="且介亭杂文" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9071?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;人文社 1973 年出版的鲁迅作品集应该是最优良的鲁迅作品版本，因为印量大，价格也不贵，但有几本很少见，凑齐全套并不容易。所以我一般是遇见了品相较好且为手中所无才购买，一切都随缘，并不特意搜集。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="且介亭杂文内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9070?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="且介亭杂文内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9070?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《且介亭杂文末编》，是我在一堆未经整理的书堆中翻找出来的，售 5 元。人民文学出版社 1973 年 4 月北京 1 版 1 印，扉枼钤「天津市第一机械工业学校图书舘藏书」，内枼整洁，纸张坚韧泛黄。唯一不美的是封面钤「不外借」圆印且封面有墨渍污损。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="且介亭杂文扉枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9072?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="且介亭杂文扉枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9072?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《且介亭杂文》，由我在另一书摊上访得，要价 4 元。人民文学出版社 1973 年 6 月山西 1 版 1 印，扉枼有 82 年的购书识记，内枼整洁，纸张洁白，可惜曾遭水浸，整册书都有湿后的压痕。鲁迅冠以「且介亭杂文」为名的集子共有 3 册，那么我还差一册《且介亭杂文二集》就可成一小帙了。&lt;/p&gt;
&lt;h2 id="tang-shi-xuan-yi-wei-liu-zhao-shi-xuan"&gt;《唐诗选》《汉魏六朝诗选》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐诗选与汉魏六朝诗选" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9078?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐诗选与汉魏六朝诗选" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9078?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;人文社的文学类古籍也具有口碑，可这一套《中国古典文学读本丛书》让我颇为困惑。这一套丛书中的书籍都具有类似的封面和题签，古雅简洁，装帧精美，而且编者与注者都是各领域的权威，内容也很精良，我十分喜欢。可是这一套丛书中兼有简体横排本和繁体竖排本，例如《唐诗选》和《汉魏六朝诗选》就都是简体横排，对简体横排介怀者在挑选这一套书时务必留意。能翻看时一看便知，但有时书商将书用塑料纸包装起来，不允翻看内枼，这时可以根据书口方向分辨，书口向右者为简体横排，书口向左者为繁体竖排本。在读古典文学时，我当然更喜欢用繁体竖排，这套丛书夹杂的两种版本让我困惑又纠结。&lt;/p&gt;
&lt;p&gt;此一册《唐诗选（上）》，中国社科院文学研究所编，全套为上下两册，因此仅售我 5 元，待有机会再访下册，这套《唐诗选》印量很大，可货比三家，寻找品相好，更适合翻阅的版本。人民文学出版社 1978 年 4 月北京 1 版 1 印，内枼整洁，纸张泛黄。许多人认为这套集子选的诗并不好，但我只以其简体横排为遗憾。唐诗存世量极大，唐朝诗人又如群星璀璨，无论怎么选都有顾此失彼之嫌，这套集子已经尽可能选出唐朝代表性诗人的作品，注释详略得当，限于篇幅可能未选许多代表作，但对于业余的爱好者概览唐诗完全是足够的了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="汉魏六朝诗选内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9079?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="汉魏六朝诗选内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9079?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《汉魏六朝诗选》，余冠英选注，可能由于印量少些，再加上这家书商的眼光比其他家更利，竟要价 10 元，不过余冠英的注本，加之品相不错，这个价格并不亏。人民文学出版社 1979 年 3 月北京 1 版 2 印，内枼整洁，纸张微黄。&lt;/p&gt;
&lt;h2 id="du-fu-shi-xuan-song-shi-xuan-zhu"&gt;《杜甫诗选》《宋诗选注》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="杜甫诗选与宋诗选注" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9080?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="杜甫诗选与宋诗选注" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9080?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;同样是《中国古典文学读本丛书》，《杜甫诗选》和《宋诗选注》都是繁体竖排，版式相同，老铅字实在赏心悦目，我十分钟爱这两本。&lt;/p&gt;
&lt;p&gt;此一册《杜甫诗选》，冯至选，要价 10 元，还价不允，无奈购下。人民文学出版社 1987 年北京 1 版 11 印，内枼整洁，纸张洁白，摩挲纸面铅字凹痕明显，字画如新，真令人边不释手。《杜甫诗选》并不是最好的杜甫诗集，但又是读杜诗难以绕开的选本。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="杜甫诗选内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9081?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="杜甫诗选内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9081?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《宋诗选注》，钱锺书选注，售 2 元，可以说是捡到的最大漏。人民文学出版社 1982 年 7 月北京 1 版重庆 1 印，内枼整洁，纸张洁白柔韧，字画清晰，惜其封面有折痕。这本集子是由钱锺书选、钱锺书选的宋诗，读宋诗的人可能不多，但是这本集子是宋诗最好的选注本。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="宋诗选注内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9083?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="宋诗选注内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9083?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="tang-song-ci-xuan-shi"&gt;《唐宋詞選釋》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐宋詞選釋" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9082?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐宋詞選釋" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9082?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《唐宋詞選釋》，俞平伯编，售 10 元，叹无人识此宝又恐有人抢购，立马购入。人民文学出版社 1979 年北京 1 版 1 印，扉枼钤「天津自行车二厂图书舘」，内枼整洁，纸张洁白柔韧，铅字的字画虽不如前面两本清晰，但同样令人赏玩不忍释手。俞平伯的《唐宋詞選釋》是读宋词的入门，选、释皆精良，说是读宋词必读并不为过。其实家中已有一本《唐宋詞選釋》，但已经快要脱胶散枼，遇到品相如此好的一本，真令我欣喜！&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐宋詞選釋内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9084?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐宋詞選釋内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9084?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="tang-shi-san-bai-shou-xin-zhu"&gt;《唐詩三百首新注》&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐詩三百首新注" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9094?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐詩三百首新注" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9094?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此一册《唐詩三百首新注》，金性尧注，售 5 元。上海古籍出版社 1980 年上海 1 版 1 印，扉枼有 00 年购书识记，竟购于内蒙而流入我手。内枼如新，纸张洁白柔韧，适合翻阅。看多了总感觉上古的铅字整体比人文的更好，字画更清晰，字形也更优美，但若是真让我哪些细节上有差异则有些困难。《唐诗三百首》是家喻户晓的唐诗集子，中华书局前几年覆刻的《唐诗三百首》是更好的版本，版式古雅且价格低廉，现在也很容易买到，但肯定是激光排印而不是铅印了。这本《唐詩三百首新注》静静躺在一角，封面的金字闪耀动人，立马吸引了我的注意，展卷翻阅，心甚悦之，遂购入。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐詩三百首新注扉枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9093?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐詩三百首新注扉枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9093?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="唐詩三百首新注内枼" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9098?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="唐詩三百首新注内枼" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9098?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后以全部收获的合影作结吧，共计约 50 元，从堆积成山的书堆里挑出这几本，真可谓是如大浪淘沙一般的「淘」书。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="淘书收获" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9099?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="淘书收获" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9099?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="藏书"></category></entry><entry><title>文献总结｜一种用于基于结构药物设计的 3D 生成模型</title><link href="https://leonis.cc/sui-sui-nian/2023-05-19-summary-doi.org/10.48550/arXiv.2203.10446.html" rel="alternate"></link><published>2023-05-19T00:00:00+08:00</published><updated>2023-05-19T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-19:/sui-sui-nian/2023-05-19-summary-doi.org/10.48550/arXiv.2203.10446.html</id><summary type="html">&lt;p&gt;本文介绍于 2021 年彭健课题组发表在 NeurIPS 2021 上的一篇文章，文章原标题为 A 3D Generative Model for Structure-Based Drug Design，文章提出了一种能够针对指定的蛋白生成药物分子的 3D 生成模型，在利用蛋白空间信息的情况下生成分子，实现基于结构的药物设计。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2203.10446" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2203.10446&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2021 年彭健课题组发表在 NeurIPS 2021 上的一篇文章，文章原标题为 A 3D Generative Model for Structure-Based Drug Design。&lt;/p&gt;
&lt;p&gt;基于结构药物设计中的一个基本问题是针对指定的蛋白结合位点生成分子，目前解决这一问题的深度学习方法可以分为两类：基于字符序列与基于图的方法。但不论是基于字符序列的 1 维模型，还是基于图的 2 维模型，其本质上缺少蛋白质 3 维空间中的信息。为了获取空间信息，目前也出现了在 3D 空间中实现分子生成的模型，但这些模型只能生成较小的分子，无法有效生成类药的更大分子。&lt;/p&gt;
&lt;p&gt;因此，文章提出了一种能够针对指定的蛋白生成药物分子的 3D 生成模型，在利用蛋白空间信息的情况下生成分子，实现基于结构的药物设计。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;蛋白的结合位点可以定义为原子的集合 &lt;span class="math"&gt;\(\mathcal{C}=\{(\boldsymbol{a}_i,\boldsymbol{r}_i)\}^{N_b}_{i=1}\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(N_b\)&lt;/span&gt; 是结合位点原子的数量，&lt;span class="math"&gt;\(\boldsymbol{a}_i\)&lt;/span&gt; 是第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个原子的特征，&lt;span class="math"&gt;\(\boldsymbol{r}_i\)&lt;/span&gt; 是其空间坐标。可以将在结合位点生成原子的任务视作为模拟结合位点中各位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 上出现原子的概率，也就是模拟原子在结合位点上出现的概率密度 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(e\in\mathcal{E}=\{\mathrm{H},\mathrm{C},\mathrm{O},\cdots\}\)&lt;/span&gt; 代表生成分子中的原子。&lt;/p&gt;
&lt;p&gt;为了对 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt; 建模，文章设计了两个模块：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;上下文编码器（Context Encoder）：使用图神经网络（graph neural networks, GNN）学习环境 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 下各原子的表示；&lt;/li&gt;
&lt;li&gt;空间分类器（Spatial Classifier）：输入任意位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt;，集合该位置附近所有上下文原子的表示，输出预测结果 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;上下文编码器&lt;/h4&gt;
&lt;p&gt;上下文编码器用于提取特征，获得各原子的表示，在该任务中，对原子表示有两个要求：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;原子表示不应只具有本身的信息，还应具有环境中的信息；&lt;/li&gt;
&lt;li&gt;在旋转和平移变换后，原子性质的性质不会发生改变，原子表示应具有旋转和平移不变性。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基于以上两点要求，文章使用了旋转平移不变的图神经网络。&lt;/p&gt;
&lt;p&gt;首先，针对蛋白结合位点构建 k-近邻图，基于结合位点 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 中各原子的距离得到图 &lt;span class="math"&gt;\(\mathcal{G}=\langle\mathcal{C},\boldsymbol{A}\rangle\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\boldsymbol{A}\)&lt;/span&gt; 为邻接矩阵，将 k-近邻中的第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个原子记作 &lt;span class="math"&gt;\(N_k(\boldsymbol{r}_i)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;接着，编码器将 &lt;span class="math"&gt;\(\mathcal{G}\)&lt;/span&gt; 中所有结点原子的特征 &lt;span class="math"&gt;\(\{\boldsymbol{a}_i\}\)&lt;/span&gt; 转化为嵌入表示 &lt;span class="math"&gt;\(\{\boldsymbol{h}^{(0)}_i\}\)&lt;/span&gt;，然后进入消息传递层。&lt;/p&gt;
&lt;p&gt;一般的 GNN 消息传递过程定义为&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{h}^{(\ell+1)}_i=\sigma\left(\boldsymbol{W}^\ell_\mathrm{self}\boldsymbol{h}^{(\ell)}_i+\boldsymbol{W}^\ell_\mathrm{nergh}\sum_{j\in\mathcal{N}}\boldsymbol{h}^{(\ell)}_j\right)
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}\)&lt;/span&gt; 为模型需要训练的参数，&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; 为激活函数。从上式中可以看出，GNN 的消息传递是在将 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 结点周围临近的 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 结点的信息按权重聚集起来。&lt;/p&gt;
&lt;p&gt;在文章中所使用的消息传递过程为&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{h}^{(\ell+1)}_i=\sigma\left(\boldsymbol{W}^\ell_0\boldsymbol{h}^{(\ell)}_i+\sum_{j\in N_k(\boldsymbol{r}_i)}\boldsymbol{W}^\ell_\mathrm{1}\boldsymbol{w}(d_{ij})\odot\boldsymbol{W}^\ell_2\boldsymbol{h}^{(\ell)}_j\right)
$$&lt;/div&gt;
&lt;p&gt;相比原式，文章在第 2 项中做了一些改动，&lt;span class="math"&gt;\(\boldsymbol{w}(\cdot)\)&lt;/span&gt; 是一个权重网络，&lt;span class="math"&gt;\(d_{ij}\)&lt;/span&gt; 为 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 两个结点间的距离。上述过程就是在聚集信息时，根据距离的远近分配权重，逐个原子计算后得到 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 中所有原子的嵌入表示集合 &lt;span class="math"&gt;\(\{\boldsymbol{h}^{(L)}_i\}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4&gt;空间分类器&lt;/h4&gt;
&lt;p&gt;在空间中的任意位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 上，空间分类器聚集由上下文编码器得到的原子的嵌入表示：&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{v}=\sum_{j\in N_k(\boldsymbol{r})}\boldsymbol{W}_0\boldsymbol{w}_\mathrm{aggr}(||\boldsymbol{r}-\boldsymbol{r}_j||)\odot\boldsymbol{W}_i\boldsymbol{h}^{(L)}_j
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{w}_\mathrm{aggr}(\cdot)\)&lt;/span&gt; 同样是一个权重网络。在这一步中，类似地根据任意位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 与周围结点间的距离 &lt;span class="math"&gt;\(||\boldsymbol{r}-\boldsymbol{r}_j||\)&lt;/span&gt; 分配权重，聚集该位置附近出现过原子的信息，得到特征 &lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;最后通过多层感知机、归一化后得到所求概率分布：&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{c}=\mathrm{MLP}(\boldsymbol{v})\\
p(e|\boldsymbol{r},\mathcal{C})=\frac{\exp(\boldsymbol{c}[e])}{1+\sum_{e'\in\mathcal{E}}\exp(\boldsymbol{c}[e'])}
$$&lt;/div&gt;
&lt;h4&gt;取样&lt;/h4&gt;
&lt;p&gt;因为 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt; 需要指定结合位点 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 和位置 &lt;span class="math"&gt;\(\boldsymbol{r}\)&lt;/span&gt; 得到预测的原子 &lt;span class="math"&gt;\(e\)&lt;/span&gt;，而分子生成需要根据 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 自动分配各原子的位置，所以由 &lt;span class="math"&gt;\(p(e|\boldsymbol{r},\mathcal{C})\)&lt;/span&gt; 导出&lt;/p&gt;
&lt;div class="math"&gt;$$p(e,\boldsymbol{r}|\mathcal{C})=\frac{\exp(\boldsymbol{c}[e])}{Z}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(Z\)&lt;/span&gt; 为未定的归一化常数。&lt;/p&gt;
&lt;p&gt;分子生成的过程为，在 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 步骤，使用结合位点（环境） &lt;span class="math"&gt;\(\mathcal{C}_t\)&lt;/span&gt; 由 &lt;span class="math"&gt;\(p(e,\boldsymbol{r}|\mathcal{C}_t)\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\((e_{t+1},\boldsymbol{r}_{t+1})\)&lt;/span&gt;，将 &lt;span class="math"&gt;\((e_{t+1},\boldsymbol{r}_{t+1})\)&lt;/span&gt; 加入到环境 &lt;span class="math"&gt;\(\mathcal{C}_t\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\(\mathcal{C_{t+1}}\)&lt;/span&gt;，再用于预测下一个原子的种类和位置，即&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    &amp;amp;(e_{t+1},\boldsymbol{r}_{t+1})\sim p(e,\boldsymbol{r}|\mathcal{C}_t)\\
    &amp;amp;\mathcal{C}_{t+1}\leftarrow\mathcal{C}_t\cup\{(e_{t+1},\boldsymbol{r}_{t+1})\}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;再增加一个辅助的分类网络用于判断生成原子是否为末端原子，若为末端原子则结束分子生成过程。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9062?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9062?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;CrossDocked 数据集中有 2.25 千万条对接得到的蛋白-配体对数据，经数据清洗后，使用其中的 100000 条数据训练模型，100 条数据作为测试集。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9063?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9063?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章首先测试了模型根据蛋白结合位点生成分子的整体效果。结果如上图所示，模型生成分子的对接打分略差于参考分子，但要更好于同类模型 liGAN，生成分子的 QED 与 SA 甚至好过于参考分子，生成分子的所有指标均好于 liGAN。在各分子性质分布的对比中，相比另两个数据集，生成分子的 QED 向右偏移，具有更好的类药性。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9064?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9064?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;以上结果也可以从生成分子的样例中看出，上图展示了模型针对两个蛋白生成的多个分子，生成分子的对接打分、QED 都要好于参考分子，同时许多生成分子还具有参考分子的类似结构。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9066?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9066?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章测试了模型在 linker 预测上的应用。模型不需要经过重新训练或都微调，只需将结合位点与片段作为初始的环境 &lt;span class="math"&gt;\(\mathcal{C}_0\)&lt;/span&gt;，模型就会根据环境补足片段间的 linker。测试结果如上图所示，与设计用于 linker 预测任务的 DeLinker 相比，文章中的模型在各方面都具有优势。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9065?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9065?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章也列举了 linker 预测结果的样例，虽然模型不一定能预测并找回参考分子，但预测生成的分子中都包含了指定的片段，同时模型是根据蛋白的 3D 信息生成 linker，这在基于结构的药物设计上可以作为应用工具。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章使用 GNN 构建了一种用于基于结构药物设计的分子生成模型，该模型使用 GNN 通过设计用于蛋白 3D 信息的消息传递过程提取结合位点中的空间信息，根据配体各原子在结合位点中各位置出现的概率建立模型，从该概率中取样实现分子生成。模型生成的分子具有蛋白的空间信息，在各方面指标上都具有较好的表现。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category></entry><entry><title>文献总结｜对编码器-解码器模型学习过程中化学结构识别的研究</title><link href="https://leonis.cc/sui-sui-nian/2023-05-13-summary-doi.org/10.1186/s13321-023-00713-z.html" rel="alternate"></link><published>2023-05-13T00:00:00+08:00</published><updated>2023-05-13T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-05-13:/sui-sui-nian/2023-05-13-summary-doi.org/10.1186/s13321-023-00713-z.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年东京大学发表在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Investigation of chemical structure recognition by encoder–decoder models in learning progress，文章，文章研究了编码器-解码器模型训练过程中对化学结构识别的过程以及将其潜变量作为分子表示用于下游任务的效果。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1186/s13321-023-00713-z" rel="noopener" target="_blank"&gt;doi.org/10.1186/s13321-023-00713-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年东京大学发表在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Investigation of chemical structure recognition by encoder&amp;ndash;decoder models in learning progress，文章，文章研究了编码器-解码器模型训练过程中对化学结构识别的过程以及将其潜变量作为分子表示用于下游任务的效果。&lt;/p&gt;
&lt;p&gt;基于结构的分子表示又被称为描述符，如何获得更好的描述符是化学信息学中很重要的问题。在近年兴起的深度学习领域，编码器-解码器（encoder-decoder, ED）类模型广受关注，以分子的字符序列 SMILES 作为输入，编码器模型会将其转化为一连串蕴含化学学信息的描述符，解码器模型通过该中间变量还原出原来的分子，这种由 SMILES（或其他）分子表示编码至隐空间中的潜变量就可以用作为数字形式的分子表示，用于各种下游任务，这也是自然语言处理中「预训练-微调」的范式。&lt;/p&gt;
&lt;p&gt;在传统方法中，也有例如 ECFP、NFP 一类的分子指纹，但他们只描述了分子中所具有的特定结构，无法由描述符再还原出分子结构。只有更好的表示才能在下游任务得到更好的结果，所以文章研究了在 ED 模型中化学结构的识别过程，ED 模型对化学结构的识别是指模型获得反映化学结构的数字信息和将该信息还原为化学结构的能力。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9036?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9036?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;模型的编码器部分是 3 层的 GRU，后面的全连接层将 GRU 的输出映射到 256 维的潜空间，解码器部分以该潜变量为输入，进入全连接层后同样是 3 层 GRU，输出还原的 SMILES。模型以输出 SMILES 与目标 SMILES 的交叉熵损失作为损失函数，训练模型的过程是使其能通过输入的随机化 SMILES 输出标准 SMILES。&lt;/p&gt;
&lt;p&gt;为了评估将 ED 模型中潜变量作为分子表示的效果，文章使用 ToxCast 中 113 个任务分别训练了 XGBoost，使用其预测结果（即下游任务结果）判断分子表示的优劣。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;模型所使用的训练数据来源于 ZINC15，从中随机获取了 3 千万个分子，后续通过去除非有机物常见原子、去除重原子等方式清洗数据。&lt;/p&gt;
&lt;h3 id="zhi-biao"&gt;指标&lt;/h3&gt;
&lt;p&gt;文章定义了两个指标用于评估 ED 模型的准确率，完全准确率定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{perfect\ accuracy}=\frac 1n\sum^n_iI(t=p)$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 表示正确的 SMILES，&lt;span class="math"&gt;\(p\)&lt;/span&gt; 表示预测的 SMILES，即计算与标签值相同的输出所占比例。&lt;/p&gt;
&lt;p&gt;另一个指标部分准确率定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{partial\ accuracy}=\frac 1n\sum^n_i\left\{\frac{1}{\max(l(t),l(p))}\sum^{\min(l(t),l(p))}_jI(t_i=p_i)\right\}$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(t_i\)&lt;/span&gt; 表示 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 位置上正确的 SMILES 字符，&lt;span class="math"&gt;\(p_i\)&lt;/span&gt; 表示 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 位置上预测的 SMILES 字符，该式即计算所有 SMILES 字符中，预测结果与标签值相同位置上相同的字符所占比例。&lt;/p&gt;
&lt;p&gt;在 XGBoost 模型中，使用 ROC 曲线下面积（AUROC）与 Matthews 相关系数（MCC）评估模型准确率。&lt;/p&gt;
&lt;h2 id="jie-guo_1"&gt;结果&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9035?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9035?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章首先分别使用 10 k、100 k、1 M 的测诫集测试了训练过程中的 ED 模型，结果如上图所示。随着训练轮次的增加，模型还原出化学结构的准确率也在上升，对比部分准确率与完全准确率，可以发现在相同时刻下，部分准确率要高于完全准确率，这说明模型先学习到了还原出分子中若干个字符组成的小片段，然后才将其拼合还原出整个分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9038?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9038?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;以训练后的还原准确率命名模型，例如 94% 的模型记为「Model_94」，测试各模型的分子表示在 ToxCast 任务上的效果，结果如上图（a）所示。可以看出，除了未经训练完全无法还原出分子的 Model_0，其他模型都能较好地完成分类任务，而且分类效果比较接近。接着文章选定了三类结构的分子，使用 UMAP 降维的方法绘制出了其分子表示在化学空间中的位置，如上图（b）所示，在 Model_0 中，三种结构混杂在一起，很难完成分类，而 ED 模型只需要经过训练，三种结构就区别开来，与前一个实验的结果也吻合，这说明 ED 模型生成的潜变量很适用于分子分类的任务。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!9037?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!9037?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章比较了训练过程中还原出分子的分子量与 SMILES 长度的准确关系，在上图中只有在黄线上的样本表明还原分子与标签值一致，可以看出虽然 DE 模型在训练的靠前阶段无法还原出分子的特定性质，但其潜变量在化学空间中已经有了区分度，这种区分度足可以完成分类任务，但其所蕴含的化学信息还不能使模型还原出结构。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章研究了 DE 模型训练过程中对化学结构的识别以及将其中的潜变量作为分子表示的效果，文章的实验结果展示了模型训练过程中分子表示的变化，证明了将其作为下游任务的分子表示的可行性。&lt;/p&gt;
&lt;p&gt;在目前，预训练-微调是广泛使用的模型训练范式，而在化学信息学领域，其中的关键步骤，也就是将潜变量作为分子表示尚缺乏研究。文章研究的是较早的 GRU（RNN）所构成的 DE 模型，还应该对 Transformer、GPT 等目前更广泛使用的模型进行潜变量的深入研究。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="RNN"></category></entry><entry><title>文献总结｜我们能用 Transformer 模型快速学会「翻译」活性分子吗？</title><link href="https://leonis.cc/sui-sui-nian/2023-04-28-summary-doi.org/10.1021/acs.jcim.2c01618.html" rel="alternate"></link><published>2023-04-28T00:00:00+08:00</published><updated>2023-04-28T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-28:/sui-sui-nian/2023-04-28-summary-doi.org/10.1021/acs.jcim.2c01618.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Can We Quickly Learn to “Translate” Bioactive Molecules with Transformer Models? 文章使用 MMP 数据训练 Transformer，使其生成具有活性的分子，文章结果表明 Transformer 对于未知靶点也能生成活性分子。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/acs.jcim.2c01618" rel="noopener" target="_blank"&gt;doi.org/10.1021/acs.jcim.2c01618&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Can We Quickly Learn to &amp;ldquo;Translate&amp;rdquo; Bioactive Molecules with Transformer Models? 文章使用 MMP 数据训练 Transformer，使其生成具有活性的分子，文章结果表明 Transformer 对于未知靶点也能生成活性分子。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章所使用的分子数据来源于 ChEMBL 29，包含有 950640 个分子。数据集中的分子都由 SMILES 表示，将其输入 MMP 软件匹配其中的相似分子。输出的数据中，每对数据都由两个 SMILES 构成，形成一个 MMP 对，用 SMIRK 表示两个 SMILES 间的化学转化，最后得到了约 5700 万条 MMP 对。&lt;/p&gt;
&lt;p&gt;接着文章对得到的 MMP 对数据进一步清洗，主要包括两个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;排除 SMIRK 出现次数少于 &lt;span class="math"&gt;\(N_1\)&lt;/span&gt; 的 MMP 对；&lt;/li&gt;
&lt;li&gt;在剩余的 MMP 对中，随机保留 &lt;span class="math"&gt;\(N_2\)&lt;/span&gt; 条数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第 1 步的清洗是为了除去数据中出现频率过少的化学转化，因为它们过于特殊，并不能普适地用于所有分子；第 2 步是为了避免数据中的极端偏向影响模型，因为在数据集中，简单的转化（如 -H &amp;rarr; -CH&lt;sub&gt;3&lt;/sub&gt;）出现频率极高，这会导致模型无法学习到那些复杂的转化。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8914?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8914?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;文章使用 OpenNMT 构建 Transformer 模型，在 SMILES 数据输入模型前，都将其转化为 SELFIES 形式。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8917?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8917?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在训练过程中，使用 OpenNMT 中默认的损失函数，使用困惑度（perplexity）评估模型训练效果。困惑度是自然语言处理中所使用的评估指标，它定义为 &lt;span class="math"&gt;\(ppl=\exp(L/N)\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(L\)&lt;/span&gt; 为损失函数，&lt;span class="math"&gt;\(N\)&lt;/span&gt; 为全部备选的 token 数量。在自然语言处理中，模型根据前一个 token 预测下一个 token，并将其连成句子，困惑度的意义就是模型在获取前一个 token 的情况下，概率较高的下一个 token 的数量，所以困惑度越小时，表明模型能在目前分布下生成更合理的句子。同样，将其应用于分子生成模型，困惑度就可以表示分枝原子上可以备选的连接原子。&lt;/p&gt;
&lt;p&gt;此外，文章测试了模型对未知靶点生成分子的效果。在上一步得到的数据中，分别除去对 COX2、DRD2 或 HERG 有活性的分子，分别用三种数据训练 Transformer，最后得到的各模型对指定靶点「不可知」。文章又将去除掉的活性分子根据活性大小分为前 5% 与后 95% 的子数据集，用后 95% 作为模型输入，测试模型是否能输出前 5% 的分子。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;h4&gt;Transformer 可以为未知靶点生成活性分子&lt;/h4&gt;
&lt;p&gt;分别从训练数据中除去 COX2、DRD2 或 HERG 的活性分子，训练了 3 个 Transformer 模型，模型无法得到关于特点靶点的信息。将对相应靶点具有活性的后 95% 分子作为输入，模型生成的分子不仅满足相应的化学规则，而且相当数量的活性分子，说明模型具有相当好的泛化能力。生成分子的结果如下图所示，横轴表示分子与前 5% 高活性分子的相似性，竖轴表示分子活性，其中蓝色圆点表示生成的分子，红色菱形表示输入模型的分子。图中只展示了一部分输入-输出的变换，用箭头表示，可以看出许多生成的分子都是往右上方向移动，也就是生成活性更高、与高活性分子更相似的结构。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8916?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8916?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h4&gt;Transformer 可以为苗头化合物发现生成新颖分子&lt;/h4&gt;
&lt;p&gt;文章统计了训练集与输入-输出分子中的 SMIRKS 化学转化，在模型生成的结果中有 1086 种化转化并未出现在训练集中，所以文章认为模型可以生成新颖的化学结构。一部分化学转化如下图所示，训练集中的化学转化都是 MMP 转化，而图中的化学换化显然并不符合规则，而是模型根据训练集中的信息所新造的化学转化。&lt;/p&gt;
&lt;p&gt;文章认为使用深度学习实现分子生成并不是将所有可能的 SMIRKS 规则放到输入分子上，这种排列组合的模式势必会大大增加生成的数据量，将有价值的信息淹没。Transformer 能够上下文相关地获取分子信息，并根据信息通过合适的 SMIRKS 规则构建新分子，这种分子生成的手段更有帮助，同时文章发现 Transformer 所使用的 SMIRKS 并不是完全照搬训练集中的数据，而是根据已有信息新构造出的转化规则，这一点可能也是提升 Transformer 分子生成效果的方向。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8915?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8915?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章还认为，相比于更常用的 SMILES，SELFIES 更有利于 Transformer 学习其中的化学信息，因为 SMILES 无法保证分子合法，必须先训练模型使其学习生成合乎规则的分子。相反，使用 SELFIES 的分子生成模型不需要先让模型学会表示分子的语法，极少出现分子不合法的情况，可能使用 SELFIES 表示分子也是能实现文章实验中效果的重要原因。&lt;/p&gt;
&lt;p&gt;文章也指出这只是一个尝试性的工作，还有很多的问题没有解决。首要的一点就是文章使用与活性分子的相似性来评价生成分子的效果，尽管两个分子十分相似，它们也可能具有十分悬殊的活性，这是使用深度学习手段进行药物发现尤需解决的问题。所以文章中的分子生成也只能起到「启发」的作用，并不能真接指导药物化学家找到活性更高的分子。另一点就是文章中并没有对模型做全面的评估与超参数的选择，只是验证的方法的可行性，并没有对比与其他模型的优势。文章中还推测 SELFIES 相比 SMILES 更具有优势，但也未对比两种模型的效果。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章将 Transformer 用于苗头化合物的发现，并且发现 Transformer 对于训练集中不存在的未知靶点也能生成相当数量的活性分子，其中一部分分子与高活性的配体具有很高的相似性。文章还发现，Transformer 生成的结果中，其化学变化并不局限于用于训练的 MMP 数据，这表明 Transformer 具有上下文相关的信息识别能力，利用好这一特性有利于实现活性分子的生成。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>津城海棠</title><link href="https://leonis.cc/zai-lu-shang/2023-04-27-crabapple-of-tianjin.html" rel="alternate"></link><published>2023-04-27T00:00:00+08:00</published><updated>2023-04-27T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-27:/zai-lu-shang/2023-04-27-crabapple-of-tianjin.html</id><summary type="html">&lt;p&gt;五大道的海棠花开了，天津又到了最美的时候。&lt;/p&gt;
&lt;p&gt;几年前初到天津时，给我印象最深刻的就是城中随处可见的海棠花。天津遍植海棠，却不是密密地栽种为若干排。而是漫步在街头时，走过几个街道，转过几个巷口，蓦地发现几枝洁白而间杂洋红的海 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;五大道的海棠花开了，天津又到了最美的时候。&lt;/p&gt;
&lt;p&gt;几年前初到天津时，给我印象最深刻的就是城中随处可见的海棠花。天津遍植海棠，却不是密密地栽种为若干排。而是漫步在街头时，走过几个街道，转过几个巷口，蓦地发现几枝洁白而间杂洋红的海棠花在那静静地开放，而无意间的拜访者也成了唯一的赏花者，独享这春光。至于如庆王府的几处，大概是因为人们喜爱海棠，几代经营之下竟植海棠如林。阳春三月，津城就被海棠花环抱，绝不是一句虚言。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8898?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8898?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;我在故乡不曾见过这样的景象，倒不是没有海棠，却没有天津的海棠这般而引人重视。我一度误把海棠当作天津的市花，我愿意把这一切都推作是天津与海棠有缘。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8906?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8906?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 向着窗口盛开的海棠&lt;/p&gt;
&lt;p&gt;我与朋友晃悠悠地骑着单车往五大道拜访海棠，过桂林路至大理道&amp;mdash;&amp;mdash;好一趟西南之行。天津与上海相似，在收回各国租界后便以国内诸大小城市为道路名，因而也成了一大特色。想必不少来天津的游人都玩过寻找家乡路牌的游戏，冥冥之中建立起了一种远跨大半个中国的联系。看着大理道旁盛开的海棠，万里之遥的大理也为这春景增色不少吧。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8900?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8900?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 大理道上的海棠花&lt;/p&gt;
&lt;p&gt;不及步入五大道，已经能望见一批批游人往前走去。天津市也乐于营造海棠的氛围，工作人员用围挡临时封闭了路段，仅允游人入内。但这并不意味着五大道内有多少宽敞余裕的空间，也休想从容地漫步海棠花下。汹涌的人潮占满了可及之处，我甚至难以看清路面，却能依人群铺开的形状分辨出街道巷衢来。盛放的海棠也不愿与赏花者示弱，两列海棠一字排开，树冠张得极大，好似擎不起满树的海棠花，一树压着一树；树上的海棠也开得极密，一簇压着一簇，全开的压着半开的，半开的压着未开的，组成了繁盛的花群。人群中的人伸着脖子望花群，花群中的花也伸着叶梗看人群。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8899?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8899?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 游人熙熙攘攘地往五大道走去&lt;/p&gt;
&lt;p&gt;可花枝柔弱，阵风吹过，花瓣就零落而被阵风裹至空中，引起人群中阵阵讶叹声&amp;mdash;&amp;mdash;看来这场对决还是人群占了上风。摇落的花瓣混入风中，风顿时有了形状，它们翻腾、盘旋，又扶摇而上，在高处击散而化作花雨，洒向一个个抬头瞪大着眼、惊异得还不及合拢嘴的游人身边。风顿时也有了色彩，大片的雪白与点点嫣红在空中交织，它不像颜料盘中两种颜色相混时那样柔和得出现游丝，而是随着花瓣的翻飞，白红两种颜色相互变换，像极了文人所用的花草笺纸。在阳光的照耀下，空中的花瓣闪出熠熠的光芒，看来这笺纸还多了一道洒金的工艺。风顿时还有了气味，风摘落的花瓣被送至每个人的鼻前，不似桃花那样甜腻，芬芳中更多了几分清爽，虽说海棠将所有芳华都留给了春季，但这种气味总能让我想起初夏的时节。据佛经上说，维摩诘讲经讲到妙处时，有天女散花，赞叹其智慧。这花决计不是海棠，这样色、香、味齐全的海棠岂不正犯佛家所说的「五贼」。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8901?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8901?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 来不及拍下风吹下的海棠花雨，捕捉到几点如霰般翻飞的花瓣&lt;/p&gt;
&lt;p&gt;花雨过后，地上自然也铺满了海棠。明人张岱曾描述他书屋中的海棠「花时积三尺香雪」，诚不诬也，果然处处都积满了如雪般的海棠花瓣。有些店家自然不愿意放过这样的景色，径在店门外摆出桌子招徕顾客。更有好事者撤去桌上的阳伞，恣意在花雨之中，任那些海棠落在盘中的糕点之上，落入茶水之中。虽说道上行人熙熙攘攘，但啜茶人亦可闹中取静，其风雅也如此，丝毫不下于古人。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8909?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8909?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 道上也有卖各种物件的小摊，也落满了海棠花&lt;/p&gt;
&lt;p&gt;我被裹挟在人群中，亦步亦趋地穿过海棠花海，流连的景色如万花筒般绚丽，而正所谓兴尽悲来，又深令人感慨春光易逝。天津人自小便生活在海棠之中，求学于海棠花下，他们是惯看了秋月春风而变得更为冷峻，还是年年见证着花朝花暮，相比他人更多了几分细心肠呢？传闻天津人大多缱绻于故乡，而他们的确是如浮萍一般的移民，终于定居在此，与繁茂的海棠杂处而居，其念兹在兹的心情亦可想而知。我不禁想象，奔走于异乡的游子是否会种下一株海棠，欣赏着它与津城的海棠同样开谢，正与记忆中同。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8908?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8908?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 俨然是粉红色的世界&lt;/p&gt;
&lt;p&gt;归来搦管撰写此文时，已经将近立夏，海棠或许早已落尽。清少纳言曾盛赞落花之后赏花为风雅之事，我虽未有此意，奈何已蹈古人之迹。寓居天津多年，我同样遥望着津城海棠的开谢，正与我的记忆同！&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8907?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8907?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="摄影"></category></entry><entry><title>文献总结｜使用上下文增强的分子表示提升少样本药物发现的效果</title><link href="https://leonis.cc/sui-sui-nian/2023-04-22-summary-openreview.html" rel="alternate"></link><published>2023-04-22T00:00:00+08:00</published><updated>2023-04-22T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-22:/sui-sui-nian/2023-04-22-summary-openreview.html</id><summary type="html">&lt;p&gt;本文介绍于 2023 年发表在 ICLR 2023 上的一篇文章，文章原标题为 Context-enriched molecule representations improve few-shot drug discovery，文章介绍了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 通过 Hopfield 网络用上下文数据集少样本的强化分子表示，提升了分子性质预测的准确度。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://openreview.net/forum?id=XrMWUuEevr" rel="noopener" target="_blank"&gt;OpenReview&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2023 年发表在 ICLR 2023 上的一篇文章，文章原标题为 Context-enriched molecule representations improve few-shot drug discovery，文章介绍了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 通过 Hopfield 网络用上下文数据集少样本的强化分子表示，提升了分子性质预测的准确度。&lt;/p&gt;
&lt;p&gt;深度学习已经成为了药物发现中的重要工具，但目前大部分深度学习方法都是通过大训练集获得分子信息。药物发现中的深度学习方法通常需要大量的生物试验数据，这在实际的药物研发过程中很难获取。少样本学习解决了药物发现中有效数据较少的问题，少样本学习主要有 3 种方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于数据增强的方法（Data-augmentation based approaches）：变换已有数据达到增加数据量的目的。&lt;/li&gt;
&lt;li&gt;基于词嵌入与最近邻的方法（Embedding-based and nearest neighbour approaches learn approaches）：学习词嵌入的空间，从已有数据邻近位置取得新数据（相似分子）。&lt;/li&gt;
&lt;li&gt;基于优化和微调的方法（Optimization-based or fine-tuning methods）：将大规模的预训练模型放在已有数据上微调，使其迁移到新的化学空间。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文章提出了一种新的 MHNfs 模型用于少样本的药物发现，模型使用联想记忆来提取原始数据中的共现和协变结构从而强化其分子表示，在少样本数据集 FS-Mol 上达到了最佳效果。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="yuan-li"&gt;原理&lt;/h3&gt;
&lt;p&gt;药物发现中所使用的模型 &lt;span class="math"&gt;\(g(\boldsymbol{m})\)&lt;/span&gt; 用于在给定分子表示 &lt;span class="math"&gt;\(\boldsymbol{m}\in\mathcal{M}\)&lt;/span&gt; 的情况下预测分子性质或活性 &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;。深度学习模型中的分子编码器将分子的一些低级表示（如 SMILES、分子图等）映射为模型空间的表示 &lt;span class="math"&gt;\(f^\mathrm{ME}:\mathcal{M}\rightarrow\mathbb{R}^d\)&lt;/span&gt;，再通过后续计算给出分子性质。&lt;/p&gt;
&lt;p&gt;在少样本的情况下，只有分子的小数据集 &lt;span class="math"&gt;\(\{\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N\}\)&lt;/span&gt; 与对应分子是否具有活性的数据 &lt;span class="math"&gt;\(\boldsymbol{y}=\{y_1,\cdots,y_N\}\)&lt;/span&gt;。这里将数据集 &lt;span class="math"&gt;\(\{(\boldsymbol{x}_n,y_n)\}_{n=1}^N\)&lt;/span&gt; 称为 support set，少样本学习就是要正确预测不在 support set 中 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 所对应的 &lt;span class="math"&gt;\(y\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;文章中的模型分为 3 个模块：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    \text{context module: }&amp;amp;\quad&amp;amp;\boldsymbol{m}'&amp;amp;=f^\mathrm{CM}(\boldsymbol{m},\boldsymbol{C})\\
    &amp;amp;\quad&amp;amp;\boldsymbol{X}'&amp;amp;=f^\mathrm{CM}(\boldsymbol{X},\boldsymbol{C})\\
    \text{cross-attention module: }&amp;amp;\quad&amp;amp;[\boldsymbol{m}'',\boldsymbol{X}'']&amp;amp;=f^\mathrm{CAM}([\boldsymbol{m}',\boldsymbol{X}'])\\
    \text{similarity module: }&amp;amp;\quad&amp;amp;\hat{y}&amp;amp;=f^\mathrm{SM}(\boldsymbol{m}'',\boldsymbol{X}'',\boldsymbol{y})
\end{align}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{m}\in\mathbb{R}^d\)&lt;/span&gt; 是分子的词嵌入表示，&lt;span class="math"&gt;\(\boldsymbol{X}\in\mathbb{R}^{d\times N}\)&lt;/span&gt; 是 support set 中分子的词嵌入表示，&lt;span class="math"&gt;\(\boldsymbol{C}\in\mathbb{R}^{d\times M}\)&lt;/span&gt; 是另一个更大的分子数据集（context set）中分子的词嵌入表示。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f^\mathrm{CM}\)&lt;/span&gt; 交换 &lt;span class="math"&gt;\((\boldsymbol{m},\boldsymbol{C})\)&lt;/span&gt; 间与 &lt;span class="math"&gt;\((\boldsymbol{X},\boldsymbol{C})\)&lt;/span&gt; 间的上下文信息，得到强化的表示 &lt;span class="math"&gt;\(\boldsymbol{m}'\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}'\)&lt;/span&gt;。拼合两个增强的表示，&lt;span class="math"&gt;\(f^\mathrm{CAM}\)&lt;/span&gt; 计算两者间注意力，得到进一步增强的 &lt;span class="math"&gt;\(\boldsymbol{m}''\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}''\)&lt;/span&gt;，最后结合二者的信息进行预测。上面的过程可以描述成&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    &amp;amp;\underset{\textsf{symbolic or}\atop\textsf{low-level repr.}}{m}\overset{f^\mathrm{ME}}{\longrightarrow}\underset{\textsf{molecule}\atop\textsf{embedding}}{\boldsymbol{m}}\overset{f^\mathrm{CM}}{\longrightarrow}\underset{\textsf{context}\atop\textsf{repr.}}{\boldsymbol{m}'}\overset{f^\mathrm{CAM}}{\longrightarrow}\underset{\textsf{similarity}\atop\textsf{repr.}}{\boldsymbol{m}''}\\
    &amp;amp;\underset{\textsf{symbolic or}\atop\textsf{low-level repr.}}{x_n}\overset{f^\mathrm{ME}}{\longrightarrow}\underset{\textsf{molecule}\atop\textsf{embedding}}{\boldsymbol{x}_n}\overset{f^\mathrm{CM}}{\longrightarrow}\underset{\textsf{context}\atop\textsf{repr.}}{\boldsymbol{x}_n'}\overset{f^\mathrm{CAM}}{\longrightarrow}\underset{\textsf{similarity}\atop\textsf{repr.}}{\boldsymbol{x}_n''}
\end{align}
$$&lt;/div&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8889?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8889?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MHNfs 由 Transformer 中的 encoder 部分构建，具有与 Transformer 类似的结构与工作方式。&lt;/p&gt;
&lt;p&gt;模型中的上下文模块由现代 Hopfield 网络（Modern Hopfield Network, MHN）实现：&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{Hopfield}(\boldsymbol{\Xi},\boldsymbol{C}):=(\boldsymbol{W}_E\boldsymbol{C})\mathrm{Softmax}\left(\beta(\boldsymbol{W}_C\boldsymbol{C})^\top(\boldsymbol{W}_\Xi\boldsymbol{\Xi})\right)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{m}'=\mathrm{Hopfield(\boldsymbol{m},\boldsymbol{C})},\quad\boldsymbol{X}'=\mathrm{Hopfield}(\boldsymbol{X},\boldsymbol{C})
$$&lt;/div&gt;
&lt;p&gt;MHN 能够计算两个输入间的注意力，最后更新得到的分子表示就具有参考分子集 &lt;span class="math"&gt;\(\boldsymbol{C}\)&lt;/span&gt; 中的联想记忆。&lt;/p&gt;
&lt;p&gt;交叉注意力模块替换了原来 Transformer 中的多头注意力机制，但功能仍然类似，用于记算输入分子 &lt;span class="math"&gt;\(\boldsymbol{m}'\)&lt;/span&gt; 与 support set &lt;span class="math"&gt;\(\boldsymbol{X}'\)&lt;/span&gt; 之间的注意力，再次更新分子表示：&lt;/p&gt;
&lt;div class="math"&gt;$$[\boldsymbol{m}'',\boldsymbol{X}'']=\mathrm{Hopfield}([\boldsymbol{m}',\boldsymbol{X}'],[\boldsymbol{m}',\boldsymbol{X}'])$$&lt;/div&gt;
&lt;p&gt;在最后的相似性模块中，模型计算输入分子 &lt;span class="math"&gt;\(\boldsymbol{m}''\)&lt;/span&gt; 与 support set &lt;span class="math"&gt;\(\boldsymbol{X}''\)&lt;/span&gt; 中每个分子 &lt;span class="math"&gt;\(\boldsymbol{x}_n''\)&lt;/span&gt; 之间的相似性 &lt;span class="math"&gt;\(k(\boldsymbol{m}'',\boldsymbol{x}_n'')\)&lt;/span&gt;，并使用所有相似性的加权平均表示输入分子，用该表示计算输入分子的性质：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}=\mathrm{Sigmoid}\left(\tau^{-1}\frac 1N\sum_{n=1}^Ny_n'k(\boldsymbol{m}'',\boldsymbol{x}_n'')\right)$$&lt;/div&gt;
&lt;p&gt;文章这么做的理由是，考虑现实中的情况，当药物化学家对某系列化合物只有有限的活性数据（support set）而要预测（同一靶点或类似结构的）一化合物（query molecule）的活性时，化学家会将该化合物与手头已有数据的化合物对比，再在化合物库（context set）中对比，综合考虑各项因素后得出判断。模型所做的 MHN 计算以及平均相似性，就是简化了的上述过程，文章认为这样的设计有助于模型模仿化学家的思考方式。&lt;/p&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用用于少样本药物发现的标准数据集 FS-Mol 作为模型的数据集。该数据集中的分子来自于 ChEMBL 27，其中定义了 4938 个训练任务，40 个验证任务与 157 个测试任务，平均每个任务下只有 94 个数据点。&lt;/p&gt;
&lt;p&gt;文章使用 ECFPs 分子指纹与 RDKit 描述符来作为初始的分子表示。&lt;/p&gt;
&lt;h2 id="shi-yan_1"&gt;实验&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8890?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8890?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;实验结果如上表所示，文章对比了各个模型在 FS-Mol 测试集上的 &amp;Delta;AUC-PR，除了 ADKF-IFT 在 Hydol. 与 Oxid. 小部分任务上优于 MHNfs，其他模型的结果都不如 MHNfs，而且 MHNfs 在全部任务的整体结果上优于其他全部模型，所以文章认为 MHNfs 在 FS-Mol 测试集实现了目前药物发现少样本学习的最优性能。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种可以用于药物发现的少样本学习模型 MHNfs，MHNfs 参考了现实中化学家面对少样本数据时的策略，通过设想的一种上下文增强的方式更新了输入模型的分子表示，使其具有更多大数据集中的背景信息。在实验中，测试结果表示这种增强的分子表示确实提高了模型预测的准确率，MHNfs 也在该任务上达到了最优的性能。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>从零起步的 Transformer 与代码拆解</title><link href="https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html" rel="alternate"></link><published>2023-04-21T00:00:00+08:00</published><updated>2023-04-21T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-21:/sui-sui-nian/2023-04-21-transformer-from-scratch.html</id><summary type="html">&lt;p&gt;自 Google 的论文 &lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Attention Is All You Need&lt;/a&gt; 发布后，几年内涌现了大量基于 Transformer 的模型，俨然形成了 Transformer 横扫人工智能领域的态势。&lt;/p&gt;
&lt;p&gt;网络上也出现了大量解读论文或是讲解 Transformer 的文章，其中也不乏许多高水平人工智能从业者的解读。虽然有些可以称得上是高屋建瓴，但 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;自 Google 的论文 &lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Attention Is All You Need&lt;/a&gt; 发布后，几年内涌现了大量基于 Transformer 的模型，俨然形成了 Transformer 横扫人工智能领域的态势。&lt;/p&gt;
&lt;p&gt;网络上也出现了大量解读论文或是讲解 Transformer 的文章，其中也不乏许多高水平人工智能从业者的解读。虽然有些可以称得上是高屋建瓴，但相当大部分难以避免地落入了知识的诅咒（curse of knowledge），起码在我初开始了解 Transformer 时难以读懂这些文章。&lt;/p&gt;
&lt;p&gt;随着 Transformer 广泛应用到各领域，学习 Transformer 也成了一门「显学」。尽管我已经能读懂一些更深层次的 Transformer 剖析，但我还是未找见一篇合我心意的入门文章，所以我希望能撰写一篇小文章，以初学者的角度来讲解 Transformer，是为序。&lt;/p&gt;
&lt;h2 id="xie-zi"&gt;楔子&lt;/h2&gt;
&lt;p&gt;Transformer 是设计用于 NLP 的一种模型，尽管目前 Transformer 所能完成的任务已经大大扩展，但这里还是以最原始的翻译任务为例。&lt;/p&gt;
&lt;p&gt;在翻译任务中，所需要的数据包括原始语句与目标语句，也就是 Transformer 原论文中所指的「input」和「output」，因为名字太容易混淆，还是将其原始语句与目标语句或是「source」与「target」。&lt;/p&gt;
&lt;p&gt;假设 source 为 &lt;code&gt;你好，世界！&lt;/code&gt;，target 为 &lt;code&gt;Hello, world!&lt;/code&gt;，完成这个中译英任务首先要将文本转化为利于模型处理的数值，这一步称为词嵌入（embedding）。&lt;/p&gt;
&lt;p&gt;常见的词嵌入方法有 word2vec 等等，在这里不做介绍。词嵌入步骤大致的流程是先将 &lt;code&gt;你好，世界！&lt;/code&gt; 转化为 &lt;code&gt;&amp;lt;start&amp;gt; 你好 ， 世界 ！ &amp;lt;end&amp;gt;&lt;/code&gt;，每个「词」都用空格划分开，其中 &lt;code&gt;&amp;lt;start&amp;gt;&lt;/code&gt; 与 &lt;code&gt;&amp;lt;end&amp;gt;&lt;/code&gt; 分别表示文本的起讫，这些「词」在 NLP 通常称为「token」。接着再为每个 token 分配索引，例如 &lt;code&gt;&amp;lt;start&amp;gt;&lt;/code&gt; 为 &lt;code&gt;1&lt;/code&gt;，&lt;code&gt;&amp;lt;end&amp;gt;&lt;/code&gt;为 &lt;code&gt;0&lt;/code&gt;，照这个思路，文本就可以转换为 &lt;code&gt;[1 2 3 4 5 0]&lt;/code&gt; 的表示。当然这是很简单的做法，实际上，每个 token 都会被转化为指定维度的向量，用这一连串向量就可以表示文本。&lt;/p&gt;
&lt;p&gt;将上述过程抽象出来，在词嵌入后，可以得到 source 的表示 &lt;span class="math"&gt;\(\boldsymbol{X}=(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_t)\)&lt;/span&gt; 与 target 的表示 &lt;span class="math"&gt;\(\boldsymbol{Y}=(\boldsymbol{y}_1,\boldsymbol{y}_2,\cdots,\boldsymbol{y}_t)\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\boldsymbol{x}_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{y}_i\)&lt;/span&gt; 都是指定维度 &lt;span class="math"&gt;\(d\)&lt;/span&gt; 的向量。&lt;/p&gt;
&lt;p&gt;那么如何使用 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 完成翻译任务呢？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一种&lt;/strong&gt;是使用 RNN 方法，使用当前的 source token &lt;span class="math"&gt;\(\boldsymbol{x}_t\)&lt;/span&gt; 与前一步中生成的 token &lt;span class="math"&gt;\(\hat{\boldsymbol{y}}_{t-1}\)&lt;/span&gt; 生成下一个 token，逐个生成直至句子末尾：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\hat{\boldsymbol{y}}_{t-1},\boldsymbol{x}_t)$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;第二种&lt;/strong&gt;是使用卷积的方法，定义一个窗口长度再通过小范围中的几个 &lt;span class="math"&gt;\(\boldsymbol{x}_i\)&lt;/span&gt; 计算输出：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_{t-1},\boldsymbol{x}_t,\boldsymbol{x}_{t+1})$$&lt;/div&gt;
&lt;p&gt;可以看出，&lt;dot&gt;RNN 很难学习到全局的信息&lt;/dot&gt;，而&lt;dot&gt;卷积方法只能学习到小范围的局部信息&lt;/dot&gt;。&lt;/p&gt;
&lt;p&gt;所以 Transformer 给出了&lt;strong&gt;第三种&lt;/strong&gt;方法，也就是自注意力方法。自注意力机制让模型就当前的 source token &lt;span class="math"&gt;\(\boldsymbol{x}_t\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 中其他 token 的关系给出输出 &lt;span class="math"&gt;\(\hat{\boldsymbol{y}}_t\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_t, \boldsymbol{X})$$&lt;/div&gt;
&lt;h2 id="transformer-jie-gou"&gt;Transformer 结构&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;标准 Transformer 的结构如上图所示，大致分为左侧的 Encoder 与右侧的 Decoder 两个部分。Inputs 与 Outputs 分别是上文所说的 source 与 target，Output Probabilities 是模型输出的各 token 概率，取其中最大概率的 token 就能组织成模型输出结果。&lt;/p&gt;
&lt;h3 id="wei-zhi-bian-ma"&gt;位置编码&lt;/h3&gt;
&lt;p&gt;Transformer 并没有采用 RNN 与卷积方法所使用的序列处理 token 的方法，因而能够实现并行计算并且很大程度上缓解了长期依赖问题（顺序处理长序列容易丢失多个步骤前的信息）。文本中多个 token 间显然有前后的顺序关系，Transformer 使用位置编码的方式来处理顺序信息。&lt;/p&gt;
&lt;p&gt;source 与 target 送入模型，经过常规的词嵌入过程后，还需要在得到的矩阵上加上位置编码，论文将位置编码定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{PE}_{(\mathrm{pos},2i)}=\sin(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathrm{PE}_{(\mathrm{pos},2i+1)}=\cos(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$&lt;/div&gt;
&lt;p&gt;Transformer 将 &lt;span class="math"&gt;\(\mathrm{pos}\)&lt;/span&gt; 位置映射为 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 维的向量，向量中的第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个元素即按上式计算。位置编码的计算公式是构造出的经验公式，不必深究，当然也有许多文章分析了如此构造的原因，这里从略。&lt;/p&gt;
&lt;h3 id="encoder-yu-decoder"&gt;Encoder 与 Decoder&lt;/h3&gt;
&lt;p&gt;许多完成 seq2seq 任务的模型都采用了 encoder-decoder 模式，Transformer 也不例外。简单来说，encoder 将输入编码得到一个中间变量，decoder 解码该中间变量得到输出。&lt;/p&gt;
&lt;p&gt;在 Transformer 中，source 与 target 分别送入 encoder 与 decoder，encoder 计算得到的中间结果再送入 decoder 中与 target 输入进行计算，得到最后的结果，这就是所谓「编码-解码」的工作方式。&lt;/p&gt;
&lt;p&gt;从 Transformer 的结构图中可以看出，模型具有 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 层 encoder 与 decoder 层。其中，encoder 与 decoder 都具有相同的多头注意力层（Multi-Head Attention）、前馈层（Feed Forward）。encoder 与 decoder 的不同在于 decoder 多了一个多头注意力层，在这一层中，encoder 的输出与 decoder 的输入计算注意力。&lt;/p&gt;
&lt;p&gt;还可以注意到，在 encoder 与 decoder 中，每一层后都有一个 Add &amp;amp; Norm 层，用于归一化计算结果。Add &amp;amp; Norm 层的计算方式是将前一层的输入与前一层的输出相加，然后归一化，可以表示为 &lt;span class="math"&gt;\(\mathrm{LayerNorm}(\boldsymbol{x}+\mathrm{Sublayer}(\boldsymbol{x}))\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4&gt;Attention 机制&lt;/h4&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;数据进入 encoder 与 decoder 的内部，首先要通过注意力机制进行计算，这也是 Transformer 的核心。&lt;/p&gt;
&lt;p&gt;文章中将所使用的注意力称为缩放点积注意力（scaled dot-product attention），定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{Q}_{n\times d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{K}_{m\times d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{V}_{m\times d_v}\)&lt;/span&gt; 分别是若干向量 &lt;span class="math"&gt;\(\boldsymbol{q}\in\mathbb{R}^{d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{k}\in\mathbb{R}^{d_k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{v}\in\mathbb{R}^{d_v}\)&lt;/span&gt; 组成的矩阵。&lt;/p&gt;
&lt;p&gt;单看矩阵的乘法稍显复杂，不妨先用向量说明计算步骤。通过以下方式可以从输入 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 得到向量 &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{q}=\boldsymbol{x}\boldsymbol{W}^Q,\,\boldsymbol{k}=\boldsymbol{x}\boldsymbol{W}^K,\,\boldsymbol{v}=\boldsymbol{x}\boldsymbol{W}^V$$&lt;/div&gt;
&lt;p&gt;其中，&lt;span class="math"&gt;\(\boldsymbol{W}^Q\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{W}^K\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{W}^V\)&lt;/span&gt; 分别表示相应的权重矩阵。&lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; 代表 query，&lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt; 代表 key，&lt;span class="math"&gt;\(\boldsymbol{v}\)&lt;/span&gt; 代表 value，目的是&lt;dot&gt;用 query 去寻找更匹配的 key-value 对&lt;/dot&gt;。&lt;/p&gt;
&lt;p&gt;因为数量积可以表示两向量的相似程度，一种简单的做法是使用 &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; 与若干个 &lt;span class="math"&gt;\(\boldsymbol{k}\)&lt;/span&gt; 计算数量积，将其作为匹配分数：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{score}=\boldsymbol{q}\cdot \boldsymbol{k}_i=\boldsymbol{q}\boldsymbol{k}^\top_i$$&lt;/div&gt;
&lt;p&gt;但这样的「注意力」太过于简单，Google 从上述的数量积出发，设计了更为可靠的注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{q},\boldsymbol{k}_i,\boldsymbol{v}_i)=\frac 1 Z\sum_i\exp\left(\frac{\boldsymbol{q}\boldsymbol{k}^\top_i}{\sqrt{d_k}}\right)\boldsymbol{v}_i$$&lt;/div&gt;
&lt;p&gt;首先，式中 &lt;span class="math"&gt;\(1/Z\sum_i x_i\)&lt;/span&gt; 形式的部分是 Softmax 函数的简写，Softmax 函数由下式定义：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Softmax}(x_i)=\frac{\exp(x_i)}{\sum_j\exp(x_j)}$$&lt;/div&gt;
&lt;p&gt;Softmax 函数的作用是将若干数值 &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 归一化，得到的 &lt;span class="math"&gt;\(\mathrm{Softmax}(x_i)\)&lt;/span&gt; 具有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\sum_i\mathrm{Softmax}(x_i)=1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathrm{Softmax}(x_i)\in[0, 1]\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两点性质，所以与概率具有相似的特征，可以用作概率处理。&lt;/p&gt;
&lt;p&gt;其次，式中新增的 &lt;span class="math"&gt;\(\sqrt{d_k}\)&lt;/span&gt; 用于调节内积 &lt;span class="math"&gt;\(\boldsymbol{q}\boldsymbol{k}^\top_i\)&lt;/span&gt; 的大小。当若干内积的大小过于悬殊时，Softmax 函数很容易将其推向 &lt;span class="math"&gt;\(0\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(1\)&lt;/span&gt; 的边界值，这样的数值处理起来没什么意义。&lt;/p&gt;
&lt;p&gt;最后，再次回忆 Transformer 的注意力机制是用 query 去寻找更匹配的 key-value 对。那么上式的意义就很了然了，就是将 query 与各个 key 的匹配分数转化为各个概率，再按各个概率取各个 key 所对应的 value，组合各 value 分量即得到注意力。&lt;/p&gt;
&lt;p&gt;以具有两个 value 的情况为例，需要得到的中间量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;（理解为注意力亦可）可以通过下式计算：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \boldsymbol{z}_1=\theta_{11}\boldsymbol{v}_1+\theta_{12}\boldsymbol{v}_2\\
    \boldsymbol{z}_2=\theta_{21}\boldsymbol{v}_1+\theta_{22}\boldsymbol{v}_2
\end{align}$$&lt;/div&gt;
&lt;p&gt;权值 &lt;span class="math"&gt;\(\theta_{ij}\)&lt;/span&gt;（即上文所说概率）通过下式得到：&lt;/p&gt;
&lt;div class="math"&gt;$$\theta_{ij}=\mathrm{Softmax}\left(\frac{\boldsymbol{q}_i\boldsymbol{k}^\top_j}{\sqrt{d_k}}\right)$$&lt;/div&gt;
&lt;p&gt;将上述运算转为矩阵形式会简洁许多：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{pmatrix}
    \boldsymbol{z}_1 \\
    \boldsymbol{z}_2
\end{pmatrix}=
\begin{pmatrix}
    \theta_{11} &amp;amp; \theta_{12} \\
    \theta_{21} &amp;amp; \theta_{22}
\end{pmatrix}
\begin{pmatrix}
    \boldsymbol{v}_1 \\
    \boldsymbol{v}_2
\end{pmatrix}\\
$$&lt;/div&gt;
&lt;p&gt;可以记作 &lt;span class="math"&gt;\(\boldsymbol{Z}=\boldsymbol{\theta}\boldsymbol{V}\)&lt;/span&gt;，也就是&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$&lt;/div&gt;
&lt;h4&gt;Multi-Head Attention&lt;/h4&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;前一节中解释了 Transformer 中的缩放点积注意力，但在模型中实际并非通过上述方式直接计算，而是通过多头注意力的方式计算注意力。&lt;/p&gt;
&lt;p&gt;如上图所示，多头注意力同样是在计算缩放点积注意力，但与纯粹缩放点积注意力的不同之处在于多头注意力将多个注意力计算步骤叠加了起来。&lt;/p&gt;
&lt;p&gt;叠加的次数为 &lt;span class="math"&gt;\(h\)&lt;/span&gt;，即代表 head，多少个 head 表示需要进行多少次叠加计算。矩阵 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{K}\)&lt;/span&gt;、&lt;span class="math"&gt;\(\boldsymbol{V}\)&lt;/span&gt; 进入多头注意力计算步骤后，首先要分别在第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个 head 中进行线性变换并计算注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{head}_i=\mathrm{Attention}(\boldsymbol{Q}\boldsymbol{W}^Q_i,\boldsymbol{K}\boldsymbol{W}^K_i,\boldsymbol{V}\boldsymbol{W}^V_i)$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}^Q_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^K_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^V_i\in\mathbb{R}^{d_\mathrm{model}\times d_v}\)&lt;/span&gt;，注意不同 head 中的线性变换并不同，输出也不同。然后将所有输出 &lt;span class="math"&gt;\(\mathrm{head}_i\)&lt;/span&gt; 拼合在一起，经线性变换后作为注意力：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{MultiHead}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\mathrm{Concat}(\mathrm{head}_1,\mathrm{head}_2,\cdots,\mathrm{head}_h)\boldsymbol{W}^O$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\boldsymbol{W}^O\in\mathbb{R}^{hd_v\times d_\mathrm{model}}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;注意这个过程中数据维数的变化 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 为单头注意力中模型所处理的维数，&lt;span class="math"&gt;\(\boldsymbol{W}^Q_i\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^K_i\)&lt;/span&gt;，&lt;span class="math"&gt;\(\boldsymbol{W}^V_i\)&lt;/span&gt; 的线性变换将 query、key 的维数从 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 提升到 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;，将 value 的维数从 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 提升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;。最后的 &lt;span class="math"&gt;\(\boldsymbol{W}^O\)&lt;/span&gt; 又将拼合起来维数为 &lt;span class="math"&gt;\(hd_v\)&lt;/span&gt; 的注意力转换为模型所处理的维数 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt;。这些线性变换矩阵 &lt;span class="math"&gt;\(\boldsymbol{W}_i\)&lt;/span&gt; 实际上就是模型训练过程中需要学习的一部分参数。&lt;/p&gt;
&lt;p&gt;至于为什么要用多头的方式计算注意力，这就是个很复杂的问题了。就我的理解而言，由于每个 head 中的线性变换矩阵 &lt;span class="math"&gt;\(\boldsymbol{W}_i\)&lt;/span&gt;，多头注意力实际上是将 query、key、value 映射到不同的子空间中，在多个不同的子空间中寻找与 query 最匹配的 key-value。由于不同子空间中具有不同方面的信息，最后将其拼接起来作为结果，这样可以更多地从多个方面捕获数据中的信息。&lt;/p&gt;
&lt;h4&gt;Feed-Forward 层&lt;/h4&gt;
&lt;p&gt;在多头注意力层之后，就是前馈层，前馈层只在位置方向上计算，所以原文描述其为 position-wise。进入前馈层的数据在该层中先做 1 次线性变换，维度升高，再经过 RELU 激活函数，最后再做 1 次线性变换，维度降低，输入与输出前馈层的维度相同。上述过程可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{FFN}(\boldsymbol{x})=\max(0,\boldsymbol{x}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2$$&lt;/div&gt;
&lt;p&gt;RELU 激活函数定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{ReLU}(x)=x^+=\max(0,x)$$&lt;/div&gt;
&lt;p&gt;即式中的 &lt;span class="math"&gt;\(\max\)&lt;/span&gt;，按原文中的例子，&lt;span class="math"&gt;\(\boldsymbol{W}_1\)&lt;/span&gt; 使 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 由 512 维升高到 2048 维，&lt;span class="math"&gt;\(\boldsymbol{W}_2\)&lt;/span&gt; 使 &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; 计算由 2048 维再降至 512 维，升维与降维的过程也是为了更好地获得数据中的信息。&lt;/p&gt;
&lt;h3 id="transformer-ji-suan-bu-zou"&gt;Transformer 计算步骤&lt;/h3&gt;
&lt;p&gt;Transformer 模型大致就由上述的几个层连接在一起构成，但或许还是觉得朦朦胧胧，比如究竟什么才是 query、key、value 等等。不妨再来看看 Transformer 的结构图，这一次已熟知大部分模块的工作原理了，所以只看数据流入与流出各模块的路线。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;作为 source 的 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与作为 target 的 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 分别从下方的左右两侧进入模型。&lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 都要经过词嵌入并加上位置编码，按以下方式更新：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
    \boldsymbol{X}&amp;amp;\leftarrow\mathrm{Embedding}(\boldsymbol{X})+\mathrm{PE}(\boldsymbol{X})\\
    \boldsymbol{Y}&amp;amp;\leftarrow\mathrm{Embedding}(\boldsymbol{Y})+\mathrm{PE}(\boldsymbol{Y})
\end{align}
$$&lt;/div&gt;
&lt;p&gt;接着 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt; 分别进入 encoder 与 decoder，可以注意到数据分作 4 条路线，这意味着将数据复制 4 次。先看进入多头注意力层的 3 条数据，以 encoder 为例，在这一层中就是在计算&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$&lt;/div&gt;
&lt;p&gt;不言自明，在这里的 query、key、value 三者都是 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt;，是在 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 内部计算注意力，因此称其为&lt;strong&gt;自注意力&lt;/strong&gt;（self-attention）。&lt;/p&gt;
&lt;p&gt;在后续的 Add &amp;amp; Norm 层中，计算&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X}))$$&lt;/div&gt;
&lt;p&gt;在前馈层与后续的 Add &amp;amp; Norm 层输的输出结果也可想而知：&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\max(0,\boldsymbol{X}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2)$$&lt;/div&gt;
&lt;p&gt;这里的 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt; 分作两路进入到 decoder 中，在 decoder 的该多头注意力层中，query 与 key 为 &lt;span class="math"&gt;\(\boldsymbol{X}\)&lt;/span&gt;，而 value 为类似步骤得到的 &lt;span class="math"&gt;\(\boldsymbol{Y}\)&lt;/span&gt;，该层的输出为&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{Z}=\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{Y})$$&lt;/div&gt;
&lt;p&gt;这也是 decoder 与 encoder 的关键不同。输出结果 &lt;span class="math"&gt;\(\boldsymbol{Z}\)&lt;/span&gt; 完成后续的计算过程后，就得到各 token 的概率，用各 token 替换即可得到模型输出的文本结果。&lt;/p&gt;
&lt;p&gt;{note begin}有兴趣的读者不妨根据各矩阵的形状尝试计算一下各个变量的维度在 Transformer 在各步骤中是如何变化的，一定会对 Transformer 的计算过程收获更深的了解。{note end}&lt;/p&gt;
&lt;h2 id="dai-ma-chai-jie_1"&gt;代码拆解&lt;/h2&gt;
&lt;p&gt;有了对 Transformer 原理的基本认识，就可以动手实现一个 Transformer 了，通过代码更深入了解 Transformer 的一些细节。这里使用 PyTorch 搭建一个标准的 Transformer，参考代码见 &lt;a href="https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; aladdinpersson / Machine-Learning-Collection &lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;代码中的各模块如下图所示，接下来对各模块逐个拆解。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="positionembedding"&gt;PositionEmbedding&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;import math
import torch
import torch.nn as nn


class PositionEmbedding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        # d_model 为模型处理数据的维数，即公式中 d_k
        # max_len 表示模型处理的最大 token 数量
        super(PositionEmbedding, self).__init__()

        # 生成大小为 max_len * d_model 的零矩阵
        pe = torch.zeros(max_len, d_model)
        # 生成大小为 max_len * 1 的位置矩阵
        position = torch.arange(max_len).unsqueeze(1)
        # 计算位置编码
        div_term = torch.exp(torch.arange(0, d_model, 2) * - (math.log(10000.0) / d_model))
        x = position * div_term
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = self.pe[:, :x.size(1)]
        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;首先实现位置编码模块。在 PyTorch 中，用于搭建神经网络的模块都要继承 &lt;code&gt;nn.Module&lt;/code&gt;，PyTorch 会通过 &lt;code&gt;__call__()&lt;/code&gt; 调用模块的 &lt;code&gt;forward()&lt;/code&gt; 的方法进行前向传播。简单来讲就是，&lt;code&gt;PositionEmbedding(x)&lt;/code&gt; 的功能等同于 &lt;code&gt;PositionEmbedding.forward(x)&lt;/code&gt;，但不能使用 &lt;code&gt;PositionEmbedding.forward(x)&lt;/code&gt;，因为 PyTorch 做了许多条件的判定和优化。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;torch.arange(num)&lt;/code&gt; 的功能类似于 Python 中的 &lt;code&gt;range(num)&lt;/code&gt;，用于生成文本各 token 的顺序位置索引。&lt;code&gt;unsqueeze(dim)&lt;/code&gt; 会令 Tensor 在指定的维度 &lt;code&gt;dim&lt;/code&gt; 上扩张 1 维，这里是为了使 &lt;code&gt;pe&lt;/code&gt; 与 &lt;code&gt;position&lt;/code&gt; 两个矩阵的维度对齐，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; torch.arange(5)
tensor([0, 1, 2, 3, 4])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(0)
tensor([[0, 1, 2, 3, 4]])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(1)
tensor([[0],
        [1],
        [2],
        [3],
        [4]])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).size()
torch.Size([5])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(0).size()
torch.Size([1, 5])
&amp;gt;&amp;gt;&amp;gt; torch.arange(5).unsqueeze(1).size()
torch.Size([5, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;代码中的位置编码并不是直接按公式计算的，而是做了一些变换，先计算一个中间量 &lt;code&gt;div_term&lt;/code&gt;，其中 &lt;code&gt;torch.arange(0, d_model, 2)&lt;/code&gt; 即为 &lt;span class="math"&gt;\(2i\)&lt;/span&gt;，可以整理出&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \mathrm{div\_term}_i&amp;amp;=\exp\left[2i\times(-\frac{\ln10000}{d_k})\right]\\
    &amp;amp;=\left[\exp(-\frac{\ln10000}{d_k})\right]^{2i}\\
    &amp;amp;=\left[10000^{-\frac{1}{d_k}}\right]^{2i}\\
    &amp;amp;=10000^{-2i/d_k}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;所以 &lt;code&gt;position * div_term&lt;/code&gt; 就可以得到&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{position}\times \mathrm{div\_term}_i=\mathrm{pos}/10000^{2i/d_k}$$&lt;/div&gt;
&lt;p&gt;就是位置编码中的一项。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pe[:, 0::2]&lt;/code&gt; 与 &lt;code&gt;pe[:, 1::2]&lt;/code&gt; 是 Pytorch 中的高级索引操作。索引中用 &lt;code&gt;,&lt;/code&gt; 分隔不同维度，例中以 &lt;code&gt;,&lt;/code&gt; 为分界，前面是对第 1 维的索引，后面是对第 2 维的索引。索引操作也遵守 Python 的规则，即 &lt;code&gt;a:b:c&lt;/code&gt; 中 &lt;code&gt;a&lt;/code&gt; 为起始，&lt;code&gt;b&lt;/code&gt; 为末尾，&lt;code&gt;c&lt;/code&gt; 为步长。&lt;/p&gt;
&lt;p&gt;所以 &lt;code&gt;pe[:, 0::2]&lt;/code&gt; 与 &lt;code&gt;pe[:, 1::2]&lt;/code&gt; 取出全部第 1 维中的元素，即行方向上不操作，再在第 2 维中分别从 &lt;code&gt;0&lt;/code&gt; 或 &lt;code&gt;1&lt;/code&gt; 开始以步长 &lt;code&gt;2&lt;/code&gt; 取出元素，即取出第 &lt;span class="math"&gt;\(2i\)&lt;/span&gt; 或第 &lt;span class="math"&gt;\(2i+1\)&lt;/span&gt; 列。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;forward()&lt;/code&gt; 部分，输出的位置编码为 &lt;code&gt;pe[:, :x.size(1)]&lt;/code&gt;，这主要是为了确保矩阵形状在加法过程中不会因非法输入的广播而改变。其实在输入合法的情况下，&lt;code&gt;x.size(1)&lt;/code&gt; 就是 &lt;code&gt;d_model&lt;/code&gt;，等价于 &lt;code&gt;pe[:, :]&lt;/code&gt;，也等价于 &lt;code&gt;pe&lt;/code&gt;。&lt;/p&gt;
&lt;!-- 指定 `requires_grad_(False)` 是因为 PyTorch 会自动保存 Tensor 的来源，用于更快地计算梯度，而这里的加法计算并不是训练过程，取消保存能节省一部分资源。 --&gt;
&lt;h3 id="selfattention"&gt;SelfAttention&lt;/h3&gt;
&lt;p&gt;在进入 Transformer 核心部分之前，我们需要再次明确一下输入模型的数据格式。上文中仅以输入模型一条数据（由若干 token 组成的一条句子）为例，在实际操作中，为了提高训练效率，会同时输入若干条数据，在构建模型时也要考虑到这一点。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，一次输入模型的数据条数就称为 batch size，所以模型所处理的其实是一个 &lt;span class="math"&gt;\(\mathrm{batch\_size}\times\mathrm{max\_len}\times\mathrm{d\_model}\)&lt;/span&gt; 的高维矩阵。也就是说，&lt;code&gt;x.size()&lt;/code&gt; 的结果是 &lt;code&gt;[batch_size, max_len, d_model]&lt;/code&gt;，务必注意三者顺序。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        # 确保 embed_size 能被 heads 整除
        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(embed_size, embed_size)
        self.keys = nn.Linear(embed_size, embed_size)
        self.queries = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;先看 &lt;code&gt;SelfAttention&lt;/code&gt; 的初始化部分，明白了注意力机制的计算过程就不难理解上面的各个属性了。&lt;code&gt;head_dim&lt;/code&gt; 是每一个 head 中注意力的维度，&lt;code&gt;embeds_size&lt;/code&gt; 必须能被 &lt;code&gt;heads&lt;/code&gt; 整除，否则将多头注意力拼接在一起的维数不等于模型处理的维数就会出现问题。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;values&lt;/code&gt;、&lt;code&gt;keys&lt;/code&gt;、&lt;code&gt;queries&lt;/code&gt; 都是计算多头注意力前的线性变换，&lt;code&gt;fc_out&lt;/code&gt; 是拼接多头注意力后的线性变换。线性变换可以直接调用 &lt;code&gt;nn.Linear(in_dim, out_dim)&lt;/code&gt;，只需要指定线性变换前后的维数即可，这里线性变换前后维数没有变化。&lt;/p&gt;
&lt;p&gt;可能会有读者疑惑为什么这里所设定的线性变换不改变维数，原文中所描述的步骤不是应该将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt; 再计算注意力吗？这是正确的，原文中的计算流程确实如此。如下图所示，在线性变换后复制 &lt;code&gt;h&lt;/code&gt; 份（例中为 2） &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt;，用若干份 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 分别计算注意力再拼合起来，得到注意力的维数自然就是 &lt;code&gt;h * d_v&lt;/code&gt; （例中为 2 * 6），再用一个线性变换将其转化回模型所处理的维数 &lt;code&gt;d_model&lt;/code&gt;（例中为 5）。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;但代码中优化了一部分比较繁琐的操作，也有其他版本的代码使用了更接近原文的实现方式，如  &lt;a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; jadore801120 / attention-is-all-you-need-pytorch &lt;/a&gt;，流程就如下图所示，勉强称之为「单头注意力变多头注意力」的一种代码实现吧。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;例中 &lt;code&gt;d_model&lt;/code&gt; 也就是词嵌入的维数还是 5，&lt;code&gt;heads&lt;/code&gt; 仍为 2，&lt;code&gt;d_value&lt;/code&gt; 仍为 6，但模型不再是将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 升至 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt;，而是将 &lt;span class="math"&gt;\(d_\mathrm{model}\)&lt;/span&gt; 直接升至 &lt;span class="math"&gt;\(hd_v\)&lt;/span&gt;，然后将 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 分成 &lt;code&gt;h&lt;/code&gt; 份，每份分别用于计算并拼接为注意力。与上例相比，本质上其实并无区别，区别仅仅是上例先复制多个矩阵再分别做线性变换，而该例只使用了一个更大的矩阵乘法就完成了上述操作，效率上更优。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;多头注意力还有一种实现方法，也是这里展示代码所使用的方法。如上图所示，这种方法对词嵌入的维数有要求，在词嵌入的步骤中就将 token 表示为 &lt;code&gt;d_v * h&lt;/code&gt; 维，这也是前文代码在初始化中使用 &lt;code&gt;assert&lt;/code&gt; 语句缘由。后续的线性变换不改变维数，计算多头注意力时直接将 &lt;code&gt;d_v * h&lt;/code&gt; 维切分为 &lt;code&gt;h&lt;/code&gt; 份作为每个 head 计算的对象。拼接各 head 的注意力后，最后的线性变换也不改变维数。&lt;/p&gt;
&lt;p&gt;在我看来，这种方法应该是对前两种方法的简化，三个例子中用于计算多头注意力的 &lt;code&gt;d_value&lt;/code&gt; 都为 6，计算量相同。第 3 种方法需要更大的 &lt;code&gt;d_model&lt;/code&gt;，而且计算多头注意力时没有使用到全部的 embedding，虽说效果类似，但总觉有些奇怪。这或许是为了计算上的方便，不用做过多的矩阵变换 🤔&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;# class SelfAttention(nn.Module):
    def forward(self, values, keys, query, mask):
        # 获取 batch_size
        N = query.shape[0]
        # d_v, d_k, d_q
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # 对 query, key, value 做线性变换
        values = self.values(values)    # (N, value_len, embed_size)
        keys = self.keys(keys)          # (N, key_len, embed_size)
        queries = self.queries(query)   # (N, query_len, embed_size)

        # 将 token 的词嵌入划分为 heads 份
        # d_model = embed_size = d_v * heads
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = queries.reshape(N, query_len, self.heads, self.head_dim)

        # queries: (N, query_len, heads, heads_dim),
        # keys: (N, key_len, heads, heads_dim)
        # energy: (N, heads, query_len, key_len)
        energy = torch.einsum("nqhd,nkhd-&amp;gt;nhqk", [queries, keys])

        # 将掩码矩阵中为 0 的对应项设为 -inf，不参与计算
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        # 得到的点积除以 sqrt(d_k) 并用 Softmax 归一化
        # attention: (N, heads, query_len, key_len)
        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)

        # attention: (N, heads, query_len, key_len)
        # values: (N, value_len, heads, heads_dim)
        # out after matrix multiply: (N, query_len, heads, head_dim), then
        # we reshape and flatten the last two dimensions.
        out = torch.einsum("nhql,nlhd-&amp;gt;nqhd", [attention, values]).reshape(
            N, query_len, self.heads * self.head_dim
        )

        # 拼接多头注意力后的线性变换
        # out: (N, query_len, embed_size)
        out = self.fc_out(out)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 部分描述了上述计算多头重意力的过程。线性变换后，使用 &lt;code&gt;reshape()&lt;/code&gt; 方法将 Tensor 转化化为指定维度，也就是将词嵌入划分为 &lt;code&gt;heads&lt;/code&gt; 份的操作，Tensor 的形状由 &lt;code&gt;[N, query_len, embed_size]&lt;/code&gt; 变为 &lt;code&gt;[N, query_len, self.heads, self.head_dim]&lt;/code&gt;，把 &lt;code&gt;embed_size&lt;/code&gt; 拆成 &lt;code&gt;heads * head_dim&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;接着使用 &lt;code&gt;torch.einsum()&lt;/code&gt; 得到注意力计算的一个中间量 &lt;code&gt;energy&lt;/code&gt;。&lt;code&gt;torch.einsum()&lt;/code&gt; 称为爱因斯坦求和约定，可以非常简洁地进行矩阵乘法、转置待操作，但会有些难以理解。&lt;/p&gt;
&lt;p&gt;例如矩阵乘法 &lt;span class="math"&gt;\(\boldsymbol{A}_{i\times j}\boldsymbol{B}_{j\times k}=\boldsymbol{C}_{i\times k}\)&lt;/span&gt;，可以表示为 &lt;code&gt;"ij,jk-&amp;gt;ik"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; B = torch.randn(4, 5)
&amp;gt;&amp;gt;&amp;gt; C = torch.einsum("ij,jk-&amp;gt;ik", [A, B])
&amp;gt;&amp;gt;&amp;gt; C.size()
torch.Size([3, 5])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;例如矩阵转置 &lt;span class="math"&gt;\((\boldsymbol{A}_{i\times j})^\top=\boldsymbol{B}_{j\times i}\)&lt;/span&gt;，可以表示为 &lt;code&gt;"ij-&amp;gt;ji"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; B = torch.einsum("ij-&amp;gt;ji", [A])
&amp;gt;&amp;gt;&amp;gt; B.size()
torch.Size([4, 3])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;定义了矩阵乘法的表示后，相应的数量积与向量积就也能表示了，不再赘述。求和操作将矩阵转化为数值，行与列都会消失，所以 &lt;span class="math"&gt;\(\sum a_{ij}\in\boldsymbol{A}_{i\times j}\)&lt;/span&gt; 可以记作 &lt;code&gt;"ij-&amp;gt;"&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; A = torch.randn(3, 4)
&amp;gt;&amp;gt;&amp;gt; torch.einsum("ij-&amp;gt;", [A])
tensor(0.5634)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此外，爱因斯坦求和约定还可以表示在指定维度上求和、做数量积等一系列的复杂操作，读者可以自行试验。&lt;/p&gt;
&lt;p&gt;代码中 &lt;code&gt;queries&lt;/code&gt; 的形状为 &lt;code&gt;[N, query_len, heads, heads_dim]&lt;/code&gt;，记作 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times q\times h \times d}\)&lt;/span&gt;，&lt;code&gt;keys&lt;/code&gt; 的形状为 &lt;code&gt;[N, key_len, heads, heads_dim]&lt;/code&gt;，记作 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times k\times h \times d}\)&lt;/span&gt;，那么 &lt;code&gt;torch.einsum("nqhd,nkhd-&amp;gt;nhqk", [queries, keys])&lt;/code&gt; 所做的操作就是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times q\times h \times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times h \times q\times d}\)&lt;/span&gt;，将 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times k\times h \times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h\times k \times d}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;两个矩阵中的 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 是 &lt;code&gt;batch_size&lt;/code&gt; 与 &lt;code&gt;heads&lt;/code&gt; 的乘积，仅仅是表示数量，所以 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times k\times d}\)&lt;/span&gt; 可以视作由 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 个 &lt;span class="math"&gt;\((\boldsymbol{K}_i)_{\ k\times d}\)&lt;/span&gt; 子矩阵构成的大矩阵。那么固定前两维不变，转置后两维，相当于&lt;strong&gt;转置&lt;/strong&gt;所有子矩阵，得到 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times d\times k}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;固定前两维，令 &lt;span class="math"&gt;\(\boldsymbol{Q}_{N\times h \times q\times d}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{K}_{N\times h \times d\times k}\)&lt;/span&gt; 在后两维上做乘法，得到 &lt;span class="math"&gt;\((\boldsymbol{QK})_{N\times h \times q \times k}\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;仔细思考上述的转置和乘法过程，实际上就是在做多头注意力中的 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;掩码部分的操作先略过。接着 &lt;code&gt;torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)&lt;/code&gt; 先将前一步中得到 &lt;code&gt;energy&lt;/code&gt; 除以 &lt;span class="math"&gt;\(\sqrt{d_k}\)&lt;/span&gt; 再用 Softmax 归一化。指定的 &lt;code&gt;dim=3&lt;/code&gt; 与 &lt;code&gt;dim=-1&lt;/code&gt; 等价，其目的是在最后一维的方向上归一化。&lt;/p&gt;
&lt;p&gt;以一个简单的 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt; 乘法为例，如下图所示，&lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{K}\)&lt;/span&gt; 的每一行都是一个 token 的词嵌入表示。计算得到 &lt;span class="math"&gt;\(\boldsymbol{Q}\boldsymbol{K}^\top\)&lt;/span&gt; 后需要归一化，&lt;code&gt;softmax(dim=0)&lt;/code&gt; 是在行方向上归一化，在得到的结果中，全部行加起来，各元素为 1；&lt;code&gt;softmax(dim=1)&lt;/code&gt; 是在列方向上归一化，结果中的全部列加起来，各元素为 1。&lt;/p&gt;
&lt;p&gt;计算注意力还是为了得到更准确的 token 表示，所以归一化的方向应该与原始的 &lt;span class="math"&gt;\(\boldsymbol{Q}\)&lt;/span&gt; 方向相同，即 &lt;code&gt;softmax(dim=1)&lt;/code&gt;。代码中也是一样，&lt;span class="math"&gt;\((\boldsymbol{QK})_{N\times h \times q \times k}\)&lt;/span&gt; 是 &lt;span class="math"&gt;\(N\times h\)&lt;/span&gt; 个 &lt;span class="math"&gt;\((\boldsymbol{Q}\boldsymbol{K}_i)_{q\times k}\)&lt;/span&gt; 子矩阵，要在所有子矩阵的列方向上做归一化，那么就是在第 4 个维度上做 Softmax，即 &lt;code&gt;softmax(dim=3)&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;此时，上述过程已经完成了多头注意力中的 &lt;span class="math"&gt;\(\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\)&lt;/span&gt;，将结果记作 &lt;span class="math"&gt;\(\boldsymbol{A}_{N\times h\times q\times k}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;在下一步中，用 &lt;code&gt;"nhql,nlhd-&amp;gt;nqhd"&lt;/code&gt; 表示了 &lt;span class="math"&gt;\(\boldsymbol{A}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{V}\)&lt;/span&gt; 的乘法，具体操作是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times v\times h\times d}\)&lt;/span&gt; 转置为 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times h\times v\times d}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;固定前两维，令 &lt;span class="math"&gt;\(\boldsymbol{A}_{N\times h\times q\times k}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{V}_{N\times h\times v\times d}\)&lt;/span&gt; 在后两维上做乘法，这里有 &lt;span class="math"&gt;\(q=k=v\)&lt;/span&gt;，所以结果为 &lt;span class="math"&gt;\((AV)_{N\times h \times q\times d}\)&lt;/span&gt;，到这一步已经计算了 &lt;span class="math"&gt;\(\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\boldsymbol{V}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;将结果转置为 &lt;span class="math"&gt;\((AV)_{N\times q \times h\times d}\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后代码使用 &lt;code&gt;reshape()&lt;/code&gt; 合并后两维，将结果转化为 &lt;span class="math"&gt;\((AV)_{N\times q \times hd}\)&lt;/span&gt;，很巧妙地拼接了多个 head 的注意力，最后通过线性层再输出结果。&lt;/p&gt;
&lt;p&gt;至此，Transformer 中的 &lt;code&gt;SelfAttention&lt;/code&gt; 部分已经结束，读者或许会觉得头昏脑胀。不必担心，最为艰涩的一部分已经过去，接下来是一路下坡 🚩&lt;/p&gt;
&lt;h3 id="transformerblock"&gt;TransformerBlock&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class TransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TransformerBlock, self).__init__()
        # 前一层的多头注意力
        self.attention = SelfAttention(embed_size, heads)
        # Add &amp;amp; Norm 层
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        # 前馈层
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, value, key, query, mask):
        attention = self.attention(value, key, query, mask)

        x = self.dropout(self.norm1(attention + query))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;TransformerBlock&lt;/code&gt; 模块包括多头注意力与后接的 Add &amp;amp; Norm、Feed Forward、Add &amp;amp; Norm 三层。&lt;/p&gt;
&lt;p&gt;初始化部分使用 &lt;code&gt;nn.Sequential()&lt;/code&gt; 将 &lt;code&gt;nn.Linear()&lt;/code&gt;、&lt;code&gt;nn.ReLU()&lt;/code&gt;、&lt;code&gt;nn.Linear&lt;/code&gt; 依次连接起来形成前馈层，正如前文所说的，数据进入前馈层先升维再激活，最后再降回原来维度，&lt;code&gt;forward_expansion&lt;/code&gt; 决定升维的倍数。&lt;code&gt;dropout&lt;/code&gt; 用于随机弃用一部分数据防止过拟合，直接调用 &lt;code&gt;nn.Dropout()&lt;/code&gt; 类，接收的数值决定了弃用数据的比例。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 部分也很简单，计算的多头注意力依次做 Add &amp;amp; Norm、Feed Forward、Add &amp;amp; Norm 三层后输出数据。&lt;/p&gt;
&lt;h3 id="encoder"&gt;Encoder&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Encoder(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        embed_size,
        num_layers,
        heads,
        device,
        forward_expansion,
        dropout,
        max_length,
    ):

        super(Encoder, self).__init__()
        self.embed_size = embed_size
        # CPU or GPU
        self.device = device
        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)
        self.position_embedding = PositionalEncoding(embed_size, max_length)

        self.layers = nn.ModuleList(
            [
                TransformerBlock(
                    embed_size,
                    heads,
                    dropout=dropout,
                    forward_expansion=forward_expansion,
                )
                for _ in range(num_layers)
            ]
        )

        self.dropout = nn.Dropout(dropout)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Encoder 是 Transformer 中的左边部分，Transformer 中有 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个 &lt;code&gt;TransformerBlock&lt;/code&gt; 顺序叠放在一起组成 encoder。所以在初始化部分，使用列表推导式在 &lt;code&gt;layers&lt;/code&gt; 中放置了 &lt;code&gt;num_layers&lt;/code&gt; 层 &lt;code&gt;TransformerBlock&lt;/code&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;# class Encoder(nn.Module):
    def forward(self, x, mask):
        # 输入数据的 batch_size 与长度
        N, seq_length = x.shape
        # 从输入数据计算位置索引
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        # 由位置索引得到位置编码，并 dropout 一部分数据
        out = self.dropout(
            (self.word_embedding(x) + self.position_embedding(positions))
        )

        # 让数据逐层经过 encoder，计算自注意力
        for layer in self.layers:
            out = layer(out, out, out, mask)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 &lt;code&gt;forward()&lt;/code&gt; 部分中，使用 &lt;code&gt;torch.arange()&lt;/code&gt; 得到位置索引，再用 &lt;code&gt;expand()&lt;/code&gt; 方法将位置索引矩阵的形状变为与输入数据相同，&lt;code&gt;expand()&lt;/code&gt; 方法的主要作用是复制，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; torch.arange(0, 5)
tensor([0, 1, 2, 3, 4])
&amp;gt;&amp;gt;&amp;gt; torch.arange(0, 5).expand(2, 5)
tensor([[0, 1, 2, 3, 4],
        [0, 1, 2, 3, 4]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;to()&lt;/code&gt; 方法用于指定 Tensor 存储的设备，例如 &lt;code&gt;"CPU"&lt;/code&gt; 或 &lt;code&gt;"GPU"&lt;/code&gt;。将词嵌入加上位置编码得到 &lt;code&gt;out&lt;/code&gt;，再将 &lt;code&gt;out&lt;/code&gt; 送入 encoder 中计算结果。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;layer(out, out, out)&lt;/code&gt; 看起来或许有些奇怪，请留意，前文已经讨论过，在 encoder 中计算的是&lt;strong&gt;自注意力&lt;/strong&gt;，所以此时的 query、key、value 都是相同的，而在 decoder 中就会有所不同了。&lt;/p&gt;
&lt;h3 id="decoderblock"&gt;DecoderBlock&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class DecoderBlock(nn.Module):
    def __init__(self, embed_size, heads, forward_expansion, dropout, device):
        super(DecoderBlock, self).__init__()
        self.norm = nn.LayerNorm(embed_size)
        self.attention = SelfAttention(embed_size, heads=heads)
        self.transformer_block = TransformerBlock(
            embed_size, heads, dropout, forward_expansion
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, value, key, src_mask, trg_mask):
        attention = self.attention(x, x, x, trg_mask)
        query = self.dropout(self.norm(attention + x))
        out = self.transformer_block(value, key, query, src_mask)
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;类似地，Decoder 是 Transformer 结构图中的右侧部分，也是由 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 层 &lt;code&gt;DecoderBlock&lt;/code&gt; 组成。decoder 只比 encoder 多了一个掩码注意力层，其他结构相同，所以 &lt;code&gt;DecoderBlock&lt;/code&gt; 的初始化中直接调用了先前定义的 &lt;code&gt;TransformerBlock&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;forward()&lt;/code&gt; 中，target 进入 decoder 后，先计算&lt;strong&gt;自注意力&lt;/strong&gt;（&lt;code&gt;attention(x, x, x)&lt;/code&gt;），再经过 Add &amp;amp; Norm 层得到 &lt;code&gt;query&lt;/code&gt;，再与 encoder 中的结果做多头注意力（&lt;code&gt;attention(value, key, query)&lt;/code&gt;），输出结果。留意两种注意力计算的不同，参考 Transformer 结构图理解一下就会很明确。&lt;/p&gt;
&lt;h3 id="decoder"&gt;Decoder&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Decoder(nn.Module):
    def __init__(
        self,
        trg_vocab_size,
        embed_size,
        num_layers,
        heads,
        forward_expansion,
        dropout,
        device,
        max_length,
    ):
        super(Decoder, self).__init__()
        self.device = device
        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)
        self.position_embedding = PositionEmbedding(embed_size,max_length)

        self.layers = nn.ModuleList(
            [
                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)
                for _ in range(num_layers)
            ]
        )
        self.fc_out = nn.Linear(embed_size, trg_vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, src_mask, trg_mask):
        N, seq_length = x.shape
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))

        for layer in self.layers:
            x = layer(x, enc_out, enc_out, src_mask, trg_mask)

        out = self.fc_out(x)

        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;实现了 &lt;code&gt;DecoderBlock&lt;/code&gt; 后，&lt;code&gt;Decoder&lt;/code&gt; 就没有什么内容了，与 encoder 类似，就是将多个 &lt;code&gt;DecoderBlock&lt;/code&gt; 组装起来，按接口传入数据进行计算。&lt;/p&gt;
&lt;h3 id="transformer"&gt;Transformer&lt;/h3&gt;
&lt;p&gt;最后的 &lt;code&gt;Transformer&lt;/code&gt; 将各个模块都组合起来：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        trg_vocab_size,
        src_pad_idx,
        trg_pad_idx,
        embed_size=512,
        num_layers=6,
        forward_expansion=4,
        heads=8,
        dropout=0,
        device="cpu",
        max_length=100,
    ):

        super(Transformer, self).__init__()

        self.encoder = Encoder(
            src_vocab_size,
            embed_size,
            num_layers,
            heads,
            device,
            forward_expansion,
            dropout,
            max_length,
        )

        self.decoder = Decoder(
            trg_vocab_size,
            embed_size,
            num_layers,
            heads,
            forward_expansion,
            dropout,
            device,
            max_length,
        )

        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        self.device = device

    def make_src_mask(self, src):
        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)
        # (N, 1, 1, src_len)
        return src_mask.to(self.device)

    def make_trg_mask(self, trg):
        N, trg_len = trg.shape
        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(
            N, 1, trg_len, trg_len
        )

        return trg_mask.to(self.device)

    def forward(self, src, trg):
        src_mask = self.make_src_mask(src)
        trg_mask = self.make_trg_mask(trg)
        enc_src = self.encoder(src, src_mask)
        out = self.decoder(trg, enc_src, src_mask, trg_mask)
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;初如化部分主要是设定了默认的参数，并引入前面定义好的 &lt;code&gt;Encoder&lt;/code&gt; 与 &lt;code&gt;Decoder&lt;/code&gt; 模块。&lt;code&gt;Transformer&lt;/code&gt; 中还多了 &lt;code&gt;make_src_mask()&lt;/code&gt; 与 &lt;code&gt;make_trg_mask()&lt;/code&gt; 两个函数，这就不得不谈谈 Transformer 中的掩码机制了。&lt;/p&gt;
&lt;p&gt;考虑一个情境，需要使用 Transformer 翻译一批（若干条）句子，各句子的长度自然是不同的，那么输入模型的数据的形状也是不同的，这在后续步骤中就会出现很多问题。在实际中，通常会找到文本中最长的句子（&lt;code&gt;max_len&lt;/code&gt;），再将所有句子都变为该长度，这种操作称为 padding。&lt;/p&gt;
&lt;p&gt;具体做法如下图所示，分别用 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 与 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 标记句子的起讫，用 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 填充 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 后的空位，各数据的长度就会一致。然后根据设定的词典，将 token 转化为索引，接着再做词嵌入。&lt;code&gt;make_src_mask()&lt;/code&gt; 就是根据 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 的索引，将 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 所在位置都标记为 &lt;code&gt;False&lt;/code&gt;，其他位置标记为 &lt;code&gt;True&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;后续 &lt;code&gt;unsqueeze()&lt;/code&gt; 的操作比较费解，其实它是利用了 PyTorch 的广播机制，用于自动匹配矩阵的形状。图中的例子可以看作是将矩阵翻转再在第 3 个方向上拉长。因为代码中的掩码要用于掩盖形状为 &lt;code&gt;[N, heads, query_len, key_len]&lt;/code&gt; 具有 4 个方向的 &lt;code&gt;energy&lt;/code&gt;，所以要额外再做一次 &lt;code&gt;unsqueeze()&lt;/code&gt;。最后将掩码用于掩盖词嵌入数据，掩码就像一个罩子盖在词嵌入数据上，模型只计算 &lt;code&gt;True&lt;/code&gt; 位置上的数据。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;使用掩码可以让模型灵活地处理不同长度的数据，数据的长度由掩码决定，改变掩码就相当于改变处理的数据，而不去改变存储在硬件中的数据，这对于计算更有利。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;make_trg_mask()&lt;/code&gt; 函数产生用于 target 数据的掩码，在 target 上使用掩码的原因与 source 不同。在 decoder 中，模型要根据输入数据的计算结果给出新 token，而生成文本的过程是顺序的，依赖于前一步生成的结果。具体来说就是，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;序列以 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 标记起始；&lt;/li&gt;
&lt;li&gt;根据已有的 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 生成 &lt;code&gt;A&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;根据生成的 &lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt; 生成 &lt;code&gt;B&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;根据生成的 &lt;code&gt;&amp;lt;s&amp;gt; A B&lt;/code&gt; 生成 &lt;code&gt;C&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;以此类推，直至模型生成 &lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt;，句子结束。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;前文已经讨论过，这种方法有很多局限性，而 Transformer 的巧妙之处就在于能够并行完成这个过程。&lt;/p&gt;
&lt;p&gt;我们可以考虑训练过程，实际上与生成过程类似，训练过程就是要根据已经生成的 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 建立与下一个 token &lt;code&gt;A&lt;/code&gt; 的关系，而不能是与后续 &lt;code&gt;B&lt;/code&gt; 或 &lt;code&gt;C&lt;/code&gt; 的关系，将这种关系以参数的形式存储到模型中，推理阶段就能顺利地根据 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 生成 &lt;code&gt;A&lt;/code&gt;。这样的训练过程可以表示为一个下三角矩阵，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Transformer 不需要逐个 token 生成再建立关系，可以通过下三角矩阵一次直接取出 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt;、&lt;code&gt;&amp;lt;s&amp;gt; A B&lt;/code&gt; 等 token 序列，并行地训练模型与对应的下一个 token 建立关系。最后将 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 与每一步骤中新生成 token &lt;code&gt;A&lt;/code&gt;、&lt;code&gt;B&lt;/code&gt;、&lt;code&gt;C&lt;/code&gt;、&lt;code&gt;&amp;lt;e&amp;gt;&lt;/code&gt; 拼合起来，即得到生成的文本。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;make_trg_mask()&lt;/code&gt; 就是在构建这个下三角的掩码。&lt;code&gt;torch.ones()&lt;/code&gt; 用于生成指定大小元素全为 &lt;code&gt;1&lt;/code&gt; 的矩阵，然后用 &lt;code&gt;torch.tril()&lt;/code&gt; 取该矩阵的下三角，再用 &lt;code&gt;expand()&lt;/code&gt; 方法将该矩阵复制到与 &lt;code&gt;batch_size&lt;/code&gt; 匹配。&lt;/p&gt;
&lt;h3 id="train"&gt;Train&lt;/h3&gt;
&lt;p&gt;从前面讨论的模型生成过程还可以知道的一点是，模型永远不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，所以 target 中没有 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，而 source 则必须由 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 起始。在实际中，一种做法是，用预处理的脚本在原始训练数据（例如 &lt;code&gt;.csv&lt;/code&gt;、&lt;code&gt;.txt&lt;/code&gt; 文件）中标上标记；另一种方法是，在训练代码中加入预处理的功能，读取数据时分别为数据做上相应标记。为了方便起见，本文就不实现这一部分功能，使用 Transformer 可以直接处理的数据。&lt;/p&gt;
&lt;p&gt;生成训练数据的函数为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def generate_random_batch(batch_size, max_length=16):
    src = []
    for i in range(batch_size):
        # 随机指定有效数据的长度
        random_len = random.randint(1, max_length - 2)
        # 在数据起讫处加上标记，"&amp;lt;s&amp;gt;": 0, "&amp;lt;e&amp;gt;": 1
        random_nums = [0] + [random.randint(3, 9) for _ in range(random_len)] + [1]
        # padding 填满数据长度，"&amp;lt;p&amp;gt;": [2]
        random_nums = random_nums + [2] * (max_length - random_len - 2)
        src.append(random_nums)

    src = torch.LongTensor(src)
    # tgt 去除末尾的 token
    tgt = src[:, :-1]
    # tgt_y 去除首个 &amp;lt;s&amp;gt;，即模型需要预测的 token，用于计算损失
    tgt_y = src[:, 1:]
    # 模型需要预测的 token 数量（不计 &amp;lt;p&amp;gt;），用于计算损失函数
    n_tokens = (tgt_y != 2).sum()

    return src, tgt, tgt_y, n_tokens
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;generate_random_batch()&lt;/code&gt; 能够生成 Transformer 可以直接计算的相同的 source 与 target，该模型的任务目标就是生成与输入相同的序列。模型不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，所以&lt;code&gt;tgt_y&lt;/code&gt; 去除 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 用于与生成的序列对比计算损失，这很容易理解。但为什么 &lt;code&gt;tgt&lt;/code&gt; 需要去除最后一个 token 呢？这一点我将在后文生成序列的 Predict 一节讨论。训练与测试模型的代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# &amp;lt;p&amp;gt; 索引
src_pad_idx = 2
trg_pad_idx = 2
# 词表大小，即全部 token 数量，包括 &amp;lt;s&amp;gt; &amp;lt;e&amp;gt; &amp;lt;p&amp;gt; 等标记
src_vocab_size = 10
trg_vocab_size = 10
# 文本最大长度
max_len = 16

model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,
                    embed_size=128, num_layers=2, dropout=0.1, max_length=max_len,
                    device=device).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
criteria = nn.CrossEntropyLoss()
total_loss = 0

for step in range(2000):
    src, tgt, tgt_y, n_tokens = generate_random_batch(batch_size=2, max_length=max_len)
    optimizer.zero_grad()
    out = model(src, tgt)

    # contiguous() 与 view() 将矩阵在各行首尾相连为一行（即向量）
    # 在两向量间计算损失函数
    # tgt_y 中元素的值是索引，除以 n_tokens 将其缩放到 [0, 1]
    loss = criteria(out.contiguous().view(-1, out.size(-1)),
                    tgt_y.contiguous().view(-1)) / n_tokens
    loss.backward()
    optimizer.step()

    total_loss += loss

    if step != 0 and step % 40 == 0:
        print(f"Step {step}, total_loss: {total_loss}")
        total_loss = 0

# Predict
copy_test(model, max_len)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PyTorch 使用 &lt;code&gt;torch.optim&lt;/code&gt; 定义模型的训练过程，其中可以选择非常多种的优化过程，这里选择了 &lt;code&gt;Adam()&lt;/code&gt;，&lt;code&gt;lr=3e-4&lt;/code&gt; 指定了训练步骤的学习率。&lt;code&gt;nn.CrossEntropyLoss()&lt;/code&gt; 用于计算两个向量的交叉熵损失，作为训练过程的损失函数。&lt;/p&gt;
&lt;p&gt;在训练循环中，每一个循环处理 1 个 batch 的数据，在同一个 batch 中 PyTorch 自动计算梯度的反向传播并更新参数。但在新的 batch 中，因为已经更新到参数中了，我们不希望保留上一个 batch 的梯度，所以用 &lt;code&gt;optimizer.zero_grad()&lt;/code&gt; 将梯度清空。&lt;/p&gt;
&lt;p&gt;将 &lt;code&gt;src&lt;/code&gt; 与 &lt;code&gt;tgt&lt;/code&gt; 传入模型，&lt;code&gt;out&lt;/code&gt; 就是 Transformer 的计算结果。&lt;code&gt;loss.backward()&lt;/code&gt; 与 &lt;code&gt;optimizer.step()&lt;/code&gt; 两行代码就是前面所说的让 PyTorch 自动计算梯度的反向传播并更新参数。&lt;/p&gt;
&lt;h3 id="predict"&gt;Predict&lt;/h3&gt;
&lt;p&gt;训练结束后，我用 &lt;code&gt;copy_test()&lt;/code&gt; 函数测试模型的效果，这个测试函数定义为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;def copy_test(model, max_len):
    model = model.eval()
    src = torch.LongTensor([[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]])
    # 模型从 &amp;lt;s&amp;gt; 开始生成序列，但不会生成 &amp;lt;s&amp;gt;，所以指定起始的 &amp;lt;s&amp;gt;
    tgt = torch.LongTensor([[0]])

    for i in range(max_len):
        # out： (1, i + 1, 10)
        # i + 1 模型输出的 token 数量
        # 10 为 vocab_size，是词表中 token 数量，out 是词表中各 token 在此处出现的概率
        out = model(src, tgt)
        # 取输出的 i + 1 个 token 中的最后一个
        # predict: (1, 10)
        predict = out[:, -1]
        # 取得概率最大的 token 索引
        # y: (1, )
        y = torch.argmax(predict, dim=1)
        # 逐个拼合 token 索引
        # y.unsqueeze(0): (1, 1)
        # tgt: (1, i + 1 )
        tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)
        # 若生成 token &amp;lt;e&amp;gt;，表示句子结束，退出循环
        if y == 1:
            break
    print(tgt)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;eval()&lt;/code&gt; 方法令模型退出训练模式，会关闭 dropout 等训练过程中才需要的功能。在循环中逐个拼合生成的 token，就能得到生成的句子。循环中的操作如下图所示，在第 1 次循环中，&lt;code&gt;tgt&lt;/code&gt; 为 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;，通过与 &lt;code&gt;src&lt;/code&gt; 的注意力与下三角矩阵得到计算结果 &lt;code&gt;out&lt;/code&gt; 为 &lt;code&gt;A&lt;/code&gt;，然后将 &lt;code&gt;tgt&lt;/code&gt; 更新为 &lt;code&gt;&amp;lt;s&amp;gt; A&lt;/code&gt;，在第 2 次循环中，得到的 &lt;code&gt;out&lt;/code&gt; 为 &lt;code&gt;A B&lt;/code&gt;，所以在每次循环中都只取新生成的 &lt;code&gt;out[-1]&lt;/code&gt; 更新 &lt;code&gt;tgt&lt;/code&gt;，最后将结果拼接起来得到完整的输出结果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;或许读者会有疑惑，既然使用下三角矩阵并行计算是 Transformer 的优势，为什么这里却是用循环顺序地生成呢？为什么计算上图中最后一个矩阵的 &lt;code&gt;out&lt;/code&gt;，而是要用一个个的 &lt;code&gt;out[-1]&lt;/code&gt; 呢？&lt;/p&gt;
&lt;p&gt;要注意的是，训练与生成有重要的一个不同，就是生成中的 &lt;code&gt;tgt&lt;/code&gt; 是空白的、模型不可知的，而训练中的 &lt;code&gt;tgt&lt;/code&gt; 是完整的、模型可知的。如上图中，&lt;code&gt;tgt&lt;/code&gt; 在每个循环中都在变长，只有 &lt;code&gt;tgt&lt;/code&gt; 变成了 &lt;code&gt;&amp;lt;s&amp;gt; A B C &amp;hellip;&lt;/code&gt; 才会有最后一个矩阵中的 &lt;code&gt;out&lt;/code&gt;。如果说只要最后一个矩阵中的 &lt;code&gt;out&lt;/code&gt; 而不要前面的步骤，就变成了「吃两个馒头吃饱，所以只吃后一个能吃得饱的馒头」的笑话。&lt;/p&gt;
&lt;p&gt;所以&lt;dot&gt;生成过程并不是并行的，Transformer 的并行指的是训练过程&lt;/dot&gt;。如下图所示，在训练过程中 Transformer 只需要做一次下三角矩阵的运算就可以建立多个 token 间的关系。这张图还解释了模型永远不会生成 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 但 &lt;code&gt;tgt&lt;/code&gt; 必须以 &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt; 起始的原因。图中还可以很明白的看出为什么先前的训练代码要去除 &lt;code&gt;tgt&lt;/code&gt; 末尾的 token，因为 Transformer 的输出 &lt;code&gt;out&lt;/code&gt; 计算的是 &lt;code&gt;tgt&lt;/code&gt; 下一个 token（及此前）的计算结果，若不去除末位就超出范围了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后训练与测试的结果为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;cpu
Step 40, total_loss: 4.021485328674316
Step 80, total_loss: 2.8817126750946045
&amp;hellip;&amp;hellip;
Step 1920, total_loss: 0.9760974049568176
Step 1960, total_loss: 0.8644390106201172
tensor([[0, 6, 3, 4, 5, 7, 6, 4, 3, 1]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出的结果没有输出 source &lt;code&gt;[[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]]&lt;/code&gt; 中末尾代表 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; 的 &lt;code&gt;2&lt;/code&gt;，前面的 token 索引也与 source 相差无几，说明模型正确复制了输入序列，训练是成功的。&lt;/p&gt;
&lt;h2 id="hou-ji_1"&gt;后记&lt;/h2&gt;
&lt;p&gt;至此，这篇 Transformer 的介绍终于告一段落了。从起草、绘图再到最后的代码梳理，前后花了一周多的时间。虽名为介绍，其实还是为自己在做梳理，边写边想、边想边查，终于把 Transformer 中的一些细节弄明白了，这篇笔记也能为读者勾勒出一个大致的图景。&lt;/p&gt;
&lt;p&gt;当然，限于篇幅，限于「从零起步」的初衷，也限于笔力，还有许多更深层次问题都没有探讨，但我相信，在看懂了这篇笔记之后，再去阅读那些文章已经不成问题了，这也符合我的初心。&lt;/p&gt;
&lt;p&gt;或许读者还很困惑，疑惑为什么数学推导上并不那么严谨的模型居然能有效，甚至具有极好的表现，那就说明需要钻入研究 Transformer 的底层了，不可不再读些更专业的文章。我也把写这篇文章时所参考以及较好的相关资料罗列于后，以飨读者。&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank"&gt;Vaswani, A. et al. Attention Is All You Need (2017) - arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spaces.ac.cn/archives/4765" rel="noopener" target="_blank"&gt;《Attention is All You Need》浅读（简介+代码）- 科学空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spaces.ac.cn/archives/6933" rel="noopener" target="_blank"&gt;从语言模型到 Seq2Seq：Transformer 如戏，全靠 Mask - 科学空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" rel="noopener" target="_blank"&gt;Language Modeling with nn.Transformer and torchtext - PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener" target="_blank"&gt;The Illustrated Transformer - Jay Alammar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/wevolf/p/12484972.html" rel="noopener" target="_blank"&gt;Transformer 源码中 Mask 机制的实现 - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/434232512" rel="noopener" target="_blank"&gt;torch.einsum 详解 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.csdn.net/zhaohongfei_358/article/details/126019181" rel="noopener" target="_blank"&gt;Pytorch 中 nn.Transformer 的使用详解与 Transformer 的黑盒讲解 - CSDN 博客&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Python"></category><category term="PyTorch"></category><category term="Transformer"></category></entry><entry><title>文献总结｜药物发现中的匹配分子对分析：方法与当前应用</title><link href="https://leonis.cc/sui-sui-nian/2023-04-15-summary-doi.org/10.1021/acs.jmedchem.2c01787.html" rel="alternate"></link><published>2023-04-15T00:00:00+08:00</published><updated>2023-04-15T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-15:/sui-sui-nian/2023-04-15-summary-doi.org/10.1021/acs.jmedchem.2c01787.html</id><summary type="html">&lt;p&gt;本文介绍 2023 年由曹东升与侯廷军研究团队发表在 &lt;em&gt;Journal of Medicinal Chemistry&lt;/em&gt; 上的一篇展望，文章原标题为 Matched Molecular Pair Analysis in Drug Discovery: Methods and Recent Applications，文章介绍了主要介绍了匹配分子对分析的理论与目前基于匹配分子对分析的实际应用。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/acs.jmedchem.2c01787" rel="noopener" target="_blank"&gt;doi.org/10.1021/acs.jmedchem.2c01787&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍 2023 年由曹东升与侯廷军研究团队发表在 &lt;em&gt;Journal of Medicinal Chemistry&lt;/em&gt; 上的一篇展望，文章原标题为 Matched Molecular Pair Analysis in Drug Discovery: Methods and Recent Applications，文章介绍了主要介绍了匹配分子对分析的理论与目前基于匹配分子对分析的实际应用。&lt;/p&gt;
&lt;p&gt;匹配分子对（matched molecular pair, MMP）的概念自提出以来，已成为了从化合物中提取药物化学知识并用于指导先导化合物优化的标准方法，MMP 的定义是只在局部具有较小的结构差异的一对化合物。合成化学家、药物化学家借助匹配分子对分析（molecular matched pair analysis, MMPA）的手段，可以从人类研究过的海量化合物中总结出化学改造的方法、化学改造对于化合物性质的影响等重要经验知识。&lt;/p&gt;
&lt;h2 id="mmpa-li-lun"&gt;MMPA 理论&lt;/h2&gt;
&lt;h3 id="mmp-sou-suo-suan-fa"&gt;MMP 搜索算法&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8784?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8784?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在需要对大量分子数据做 MMPA 时，首要任务就是提取出其中的 MMP，MMP 搜索算法可以分为 3 类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;预设的变换规则：使用人为设计的切分规则分割分子，寻找分子数据中的 MMP，常用规则如 retrosynthetic combinatorial analysis procedure（RECAP）和 breaking of retrosynthetically interesting chemical substructures（BRICS）。这种方法的局限性也很明显，例如忽略了预设规则以外的 MMP 并且只能处理单点的化学结构变换。&lt;/li&gt;
&lt;li&gt;基于最大公共子结构（maximum common substructure, MCS）的方法：先寻找指定分子的的公共结构，将其设定为固定部分，只有具有公共结构的分子才能构成 MMP，分子中除去公共结构所剩余的结构就是改变部分，所以该方法通常用用于表示化学变换的 SMIRKS 存储 MMP。这种方法的问题在于计算 MCS 的计算开销很大。&lt;/li&gt;
&lt;li&gt;片段与索引（fragmentation and indexing, F+I）方法：该方法是目前寻找 MMP 最通用的方法，主要方法是在两非氢原子间的非环单键处切断，构建 key 与 value 片段的对应索引，通过键值对间的匹配寻找 MMP，具体方法可以&lt;a href="https://leonis.cc/sui-sui-nian/2023-02-25-summary-doi.org/10.1021/ci900450m.html" rel="noopener" target="_blank"&gt;参看前文&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="ying-xiang-mmpa-de-guan-jian-yin-su"&gt;影响 MMPA 的关键因素&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8785?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8785?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MMPA 的基本假设是，分子结构中一些小的结构改变将引起特定物理性质或是生物活性的改变。然而现实中化合物性质改变的原因更为复杂，表现出更为偶然的现象，例如分子改造中的活性悬崖（对分子仅做微小的改造而生物活性变化巨大）等，所以在 MMPA 中也要考虑到许多因素的影响。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分子表示：2D 与 3D 分子结构都被用于 MMPA 研究中，2D 分子描述的主要优点是处理简单，但许多实践表明 3D 分子表示方法表示了分子的空间信息，使其对于微小的结构差异更为敏感，这对 MMPA 十分重要。&lt;/li&gt;
&lt;li&gt;环境特征：在早期的研究中，人们认为只有 MMP 中的化学转换改变了分子的性质，因此只针对化学转换进行研究，而没有考虑具体分子。如今人们已经意识到，在 MMPA 还需要考虑具体分子的结构以及改造位点等环境特征，不能只研究 MMP 中的化学转化规则。目前，大部分研究使用分子图或 SMILES 来表示 MMP 中的完整分子，用于 MMPA 研究。除了分子信息以外，也有研究将蛋白口袋的信息也融入 MMPA，这有助于更深入研究 MMP 转化对受体与配体间结合作用影响。&lt;/li&gt;
&lt;li&gt;统计显著性：MMPA 的统计分析对于研究 MMP 间性质的变化十分重要，因为一种化学转换可以引起多种性质的改变，多种化学转换也可能使分子的某些性质不发生改变。MMPA 的统计学研究发现，在同一化合物上所做的两个结构改造所产生的影响远不同于单一结构改造影响的加和，这也称为「不可加和性」效应，这意味着简单的单一结构改造间存在着相互作用。不可加和性同样影响了分子的溶解度等性质，在 MMPA 中对可加和性进行统计分析，可以更好地识别药物分子的构效关系与分子中潜在的相互作用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="mmpa-shi-ji-ying-yong_1"&gt;MMPA 实际应用&lt;/h2&gt;
&lt;p&gt;MMPA 已经广泛应用在寻找得到目标性质分子所需的化学改造中（ADMET 优化），除了应用在先导化合物优化，MMPA 也用于靶点预测、生物电子等排体替换、构效关系确定、全新药物设计等任务中，这里主要介绍 MMPA 在分子结构改造和全新药物设计中的应用。&lt;/p&gt;
&lt;h3 id="pi-pei-fen-zi-xu-lie"&gt;匹配分子序列&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8786?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8786?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;将多个仅具有一个子结构区别的分子组织起来，就得到了匹配分子序列（matching molecular series, MMS），该方法最早被用于药物分子构效关系的分析，将不同 MMS 组织起来还得到形成匹配分子序列图，用于决策分子改造的路线。称为 SAR 转移的方法通过对比两个 MMS 间化合物性质的变化，可以判断替换结构的效果与。&lt;/p&gt;
&lt;h3 id="ji-yu-mmpa-de-quan-xin-yao-wu-she-ji"&gt;基于 MMPA 的全新药物设计&lt;/h3&gt;
&lt;p&gt;将 MMP 化学变换规则用于分子生成是全新药物设计中的重要步骤，输入的分子首先被分割为片段，然后通过 MMP 数据库搜索找到相应的化学转换，将这些化学转换用于输入分子就得到了新分子。也有研究提出了基于片段的 MMP 分子生成方法，主要步骤是收集 MMP 片段信息，通过遗传算法等方法合理地相互组合 MMP 片段，得到新分子。&lt;/p&gt;
&lt;p&gt;也有研究使用分子骨架和分子骨架以外的子结构来构建分子生成模型，模型是使用 SMILES 的 RNN 模型，第一步是生成正确的分子骨架，第二步在分子骨架上添加结构改造得到正确的分子。此外，MMS 方法可以很容易地将分子分为若干类的类似物，也可以很方便地用于全新药物设计。DeepSARM 模型的目标是寻找生物作用类似而化学结构新颖的类似物，就使用了 MMS 方法，模型同时还考虑了靶点信息，扩大的 MMS 方法的应用范围。&lt;/p&gt;
&lt;h2 id="zhan-wang_1"&gt;展望&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8787?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8787?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;分子设计所面临的一个重要难题是如何基于有限的实验数据决定下一步的分子改造，MMPA 有助于人们从已有的分子改造数据中得到化学转换的信息。为了能更好地利用 MMPA，文章提出了以下几点展望：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 QSAR 与 MMPA 相结合。QSAR 模型着重于整体的结构特征，MMPA 主要用于确定局部子结构的改变，在一定程度上二者是互补的，在未来 MMPA 也可能对 QSAR 模型的预测有帮助。&lt;/li&gt;
&lt;li&gt;将 MMPA 的概念用于蛋白质等大分子。&lt;/li&gt;
&lt;li&gt;融合 MMPA 相关的分子优化方法，构建自动化的分子优化流程。尽管目前 MMP 已经应用于分子生成，但 MMP 数据的提取等步骤还需要人工处理。文章提出了上图所示的预期 MMPA 工作流程，希望能够实现 MMP 的自动提取、组织、应用和评估。&lt;/li&gt;
&lt;/ol&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Review"></category></entry><entry><title>我的 2023 年春播计划</title><link href="https://leonis.cc/zai-lu-shang/2023-04-11-my-gardening-plan.html" rel="alternate"></link><published>2023-04-11T00:00:00+08:00</published><updated>2023-04-11T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-11:/zai-lu-shang/2023-04-11-my-gardening-plan.html</id><summary type="html">&lt;p&gt;过了惊蛰之后，万物萌动，看到园林工人正在整饬路旁的花圃，我的心里也跟着躁动起来，想着非要在阳台上种点什么才好，于是在一番调查与纠结之后确定了这篇春播计划。&lt;/p&gt;</summary><content type="html">&lt;p&gt;过了惊蛰之后，万物萌动，看到园林工人正在整饬路旁的花圃，我的心里也跟着躁动起来，想着非要在阳台上种点什么才好，于是在一番调查与纠结之后确定了这篇春播计划。&lt;/p&gt;
&lt;h2 id="pin-chong"&gt;品种&lt;/h2&gt;
&lt;p&gt;我的阳台正朝南方，一整天都可以受到阳光，非常适合种些花草，之所以一直闲置，还是因为漂泊在外，总担心有时无暇顾及这些无言的小小生灵。所以在我下定决心后，首先要考虑的就是草木的品种。由于春节会回家，到时就无人照看这些花草了，我优先考虑一年生的植物，并且植株的越冬所需要的操作一定要越简单越好。&lt;/p&gt;
&lt;p&gt;结合以上客观因素和个人喜好等主观因素下，我敲定了这几个大类别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;大丽花&lt;/strong&gt;：大丽花喜欢光照，光照越充足花开得越盛，正好符合阳台的光照条件，而在冬天枯萎后可以挖出种球保存，也不需要专门照看，所以非常合适；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;甘菊等一年生的切花&lt;/strong&gt;：我个人特别喜欢切花，不仅盛开时美，还可以剪下做成干花，某种意义上也算是经冬不凋，而天津的秋冬季十分干燥，制作干花实在太合适了；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;番茄等一年生的蔬果&lt;/strong&gt;：许多人调侃中国人的天赋是种菜，喜欢种蔬果胜过于种花草，其实蔬果也有许多园艺品种，除了食用以外也有很好的观赏价值，蔬果的生长周期短，播种时间可以很灵活，而且谁不想一尝丰收的喜悦呢？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我的春播计划包括 5 个具体的品种，用店家的图做个参考吧：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="朵拉大丽花" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8760?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="朵拉大丽花" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8760?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 朵拉大丽花 Dahlia 'Melody Dora'&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="小甘菊" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8761?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="小甘菊" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8761?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 小甘菊&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="文森特向日葵" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8762?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="文森特向日葵" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8762?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 文森特向日葵 Helianthus annuus 'Vincent's Fresh'&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="橙色番茄" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8763?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="橙色番茄" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8763?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 橙色番茄&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="墨西哥小番茄" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8764?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="墨西哥小番茄" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8764?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 墨西哥小番茄 Solanum lycopersicum 'Mexico Midget'&lt;/p&gt;
&lt;p&gt;{warn begin}品种学名来自于植物数据库 &lt;a href="https://garden.org/plants/" rel="noopener" target="_blank"&gt;Plants Database&lt;/a&gt;，主要靠对照图片推测，不一定准确。{warn end}&lt;/p&gt;
&lt;h2 id="gong-ju"&gt;工具&lt;/h2&gt;
&lt;p&gt;接着是一些园艺用具，包括花盆、花土、花肥等等。&lt;/p&gt;
&lt;p&gt;包括我在内的很多人都没有地栽的条件，只能购置花盆选择盆栽。花盆的上选当然是红陶盆，不仅透水透气还美观耐看，红陶盆的一个问题是太重，所以运费会导致价格比较高，也不适合漂泊在外没有自己固定居所的人；再一个问题是在植株换盆时，为了避免伤根，通常会把盆给敲碎，像我这样拮据的人很难容忍这种浪费。基于这些考虑，我选择了更便捷一些的塑料盆。&lt;/p&gt;
&lt;p&gt;盆的大小要依种植的植株品种来定，比较常用的尺寸是直径 15 cm 左右的花盆，容量大约 2 L。大丽花需要较大的花盆，可以按大丽花的株高选盆：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;株高 50 cm 以下，盆径 18 cm&lt;/li&gt;
&lt;li&gt;株高 50-90 cm，盆径 25 cm&lt;/li&gt;
&lt;li&gt;株高 90 cm 以上，盆径 30 cm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于番茄需要注意的是，番茄可以按生长模式分为两类&amp;mdash;&amp;mdash;有限生长型与无限生长型。不要把生长模式与植物的寿命混淆，番茄大部分都是一年生的，有限生长是指番茄的果实都在同一时期成熟，收完一茬就结束了；而无限生长型番茄的果实会在一段较长的时间内次序成熟，犹如永远收获不完，因而得名。&lt;dot&gt;有限生长型番茄大多是矮株，无限生长型番茄大多是高株&lt;/dot&gt;，所以要选择匹配的盆径。有限生长型番茄用 20 cm 的盆尚可，无限生长型番茄就需要用 30 cm 左右的花盆了，因为它们甚至能长到 2 米多高。&lt;/p&gt;
&lt;p&gt;在番茄品种的选择上出现了一些失误，把小番茄误当作了矮株番茄，收到种子后才得知我购买的两个品种都是无限生长型的番茄，于是慌忙之下又购买了几个大盆。&lt;/p&gt;
&lt;p&gt;那么我所种植的品种与花盆的搭配就是，矮型大丽花朵拉和两种番茄使用 25 cm 的大盆，向日葵搭配 20 cm 的花盆，小甘菊可以使用 17 cm 的盆。&lt;/p&gt;
&lt;p&gt;我购买的花土也简单，就是普通的泥炭土，先后买了大约 18 L，还不太确定用量，后续不够可以再买，这样也能先试试土的质量。&lt;/p&gt;
&lt;p&gt;至于肥料，我只购买了一些非常便宜的有机肥，没有购买无机肥，主要是因为花肥的品牌琳琅满目，花肥又不像花土一般适用，购买了不适配的就容易浪费，并且我对花肥的品牌不太了解也难以选择。再者作为学化学的，有什么化学试剂是我难以获得的呢，我相信我的专业水平更多于那些名不见经传的品牌。&lt;/p&gt;
&lt;h2 id="bo-chong"&gt;播种&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;朵拉大丽花&lt;/strong&gt;：大丽花在 3 月中旬左右就可以种下了，大丽花的花期很长，从夏季持续到秋季，所以在天气暖和了之后早些种下也有利于大丽花蓄积养份，为盛开做好准备。收到大丽花的球根后，我提前泡了一天的水，剪去一些枯干的残根，然后确认茎的方向，茎向上埋入土中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;小甘菊&lt;/strong&gt;：播种温度 15～20℃，我在 4 月 10 日种下。使用育苗块播种，种子用手指小心按进湿润的泥土中，等待发芽长大后再移栽到盆中，一个花盆中种植 3 棵左右。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文森特向日葵&lt;/strong&gt;：播种温度 15～30℃，向日葵大约 2 个月就能开花，我打算在 4 月 20 日左右种下，希望恰好在夏至时能看到黄灿灿的向日葵。据店家的说明，向日葵的发芽率很高，不需要育苗，直接在盆中挖 5 mm 左右的小坑，将葵花籽横放埋入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;橙色番茄&lt;/strong&gt;：播种温度 15～30℃，可以把播种时间向后推一些，在 4 月至 5 月种下，也有人喜欢秋播，但不太适合我。我打算在 4 月 15 日左右种下，同样使用育苗块播种，长大后再移栽到盆中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;墨西哥小番茄&lt;/strong&gt;：与橙色番茄相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在春天也忙碌起来了，快快发芽吧 🌱&lt;/p&gt;
&lt;h2 id="zhang-dan"&gt;账单&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;价格（CNY）&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;球根与种子&lt;/td&gt;
&lt;td&gt;38.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;花盆（5 个）&lt;/td&gt;
&lt;td&gt;54.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;泥炭土（18 L）&lt;/td&gt;
&lt;td&gt;21.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;鸡粪有机肥&lt;/td&gt;
&lt;td&gt;3.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;育苗块 + 育苗盒&lt;/td&gt;
&lt;td&gt;11.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;总计（计运费）&lt;/td&gt;
&lt;td&gt;142.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="在路上"></category><category term="园艺"></category></entry><entry><title>文献总结｜DrugEx v3：使用基于图 Transformer 的强化学习进行以分子骨架为约束的药物设计</title><link href="https://leonis.cc/sui-sui-nian/2023-04-06-summary-doi.org/10.1186/s13321-023-00694-z.html" rel="alternate"></link><published>2023-04-06T00:00:00+08:00</published><updated>2023-04-06T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-06:/sui-sui-nian/2023-04-06-summary-doi.org/10.1186/s13321-023-00694-z.html</id><summary type="html">&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 DrugEx v3: scaffold‑constrained drug design with graph transformer‑based reinforcement learning，文章介绍了使用包括 Transformer 和 LSTM 模型实现以分子骨架为约束的药物设计的方法并对比了使用 SMILES 与图两种方式的分子表示在分子生成中的区别。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1186/s13321-023-00694-z" rel="noopener" target="_blank"&gt;doi.org/10.1186/s13321-023-00694-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 DrugEx v3: scaffold‑constrained drug design
with graph transformer‑based reinforcement learning，文章介绍了使用包括 Transformer 和 LSTM 模型实现以分子骨架为约束的药物设计的方法，并且对比了使用 SMILES 与图两种分子表示方式在分子生成中的区别。&lt;/p&gt;
&lt;p&gt;在先前的工作中，作者设计了名为 DrugEx 的 RNN 模型，它能够通过基于分布的方式探索化学空间并通过强化学习的策略实现基于目标的分子生成，但它无法接受用户的输入，无法基于先验知识给出结果，只能在已有的化学空间中给出结果，当任务改变后又需要重新训练模型，这些方面的问题使其在具体应用上具有很大的局限性。&lt;/p&gt;
&lt;p&gt;因此这篇文章使用多种深度学习模型重构了 DrugEx，DrugEx 可以接受用户指定的分子骨架生成具有目标结构的分子，并且在模型中引入强化学习策略，更加有效地控制生成分子的目标性质，此外文章对比了不同深度学习模型和以 SMILES 或图两种不同编码方式实现以分子骨架为约束的药物设计的效果。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8738?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8738?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;由来源于 ChEMBL 的约 170 万条分子数据构成 ChEMBL 数据集，用于预训练模型，由对人腺苷受体具有活性的 10828 条分子数据构成 LIGAND 数据集，用于微调生成模型。&lt;/p&gt;
&lt;p&gt;所有数据都构建为「输入- 输出」对的形式，使用 BRICS 规则将每个分子分割为最多 4 个的一系列片段，片段的排列组合就作为输入部分，被分割的分子就作为输出部分。分割完成后，用于预训练生成模型的分子对数据有 9335410 万条。&lt;/p&gt;
&lt;p&gt;若以 SMILES 作为分子表示，则是使用词表将每条 SMILES 序列分为若干 token，就可以将 SMILES 表示为 token 的索引序列，用于模型计算。&lt;/p&gt;
&lt;p&gt;若以图作为分子表示，首先需要计算分子的临接矩阵，接着每个分子都会被表示为具有 5 行的矩阵，前两行分别代表原子类型和化学键类型，第三行表示连接原子的索引，第四行表示目前原子的索引，第五行表示片段索引。按列连接 start、fragment、growing、end 和 linking 五个部分的上述五种信息，start 与 end 两部分分别具有一列，只有标记分隔的作用，fragment 部分中组织了各分子骨架中的原子信息，growing 部分组织了分子中去除分子骨架后剩余原子的信息，linking 部分组织了 fragment 与 growing 部分间相互连接的化学键信息。&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8739?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8739?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章中设计了 4 种模型用于完成分子生成成任务：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;图 Transformer&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;LSTM + 注意力机制&lt;/li&gt;
&lt;li&gt;序列 Transformer&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中分子的图表示只用于图 Transformer 模型中，其他三种模型都使用 SMILES 表示分子。&lt;/p&gt;
&lt;p&gt;由于图 Transformer 无法同时处理原子与化学键的信息，因此按下式组合原子和化学键索引：&lt;/p&gt;
&lt;div class="math"&gt;$$W=T_\mathrm{atom}\times 4+T_\mathrm{bond}$$&lt;/div&gt;
&lt;p&gt;通过将原子类型与化学键类型的总数相乘再加上化学键类型得到结果 &lt;span class="math"&gt;\(W\)&lt;/span&gt;，用于计算词向量。&lt;/p&gt;
&lt;p&gt;由于图 Transformer 处理的不是序列信息，原有的位置编码计算方式同样无法使用，文章设计了以下位置编码：&lt;/p&gt;
&lt;div class="math"&gt;$$P=I_\mathrm{atom}\times L_\mathrm{max}+I_\mathrm{connected}$$&lt;/div&gt;
&lt;p&gt;式中将当前原子索引 &lt;span class="math"&gt;\(I_\mathrm{atom}\)&lt;/span&gt; 与最大长度 &lt;span class="math"&gt;\(L_\mathrm{max}\)&lt;/span&gt; 相乘，然后再加上连接原子的索引 &lt;span class="math"&gt;\(I_\mathrm{connected}\)&lt;/span&gt; 得到位置编码。&lt;/p&gt;
&lt;h3 id="ping-gu-zhi-biao"&gt;评估指标&lt;/h3&gt;
&lt;p&gt;为了更好评估生成分子的多样性，除了常见的分子指标外，文章中还使用了 Solow Polasky measurement，由下式给出：&lt;/p&gt;
&lt;div class="math"&gt;$$I(A)=\frac{1}{|A|}\boldsymbol{e}^\mathrm{T}F(\boldsymbol{s})^{-1}\boldsymbol{e}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 表示分子数据集，&lt;span class="math"&gt;\(|A|\)&lt;/span&gt; 为数据集大小，&lt;span class="math"&gt;\(\boldsymbol{e}\)&lt;/span&gt; 为 &lt;span class="math"&gt;\(|A|\)&lt;/span&gt; 维元素全为 &lt;span class="math"&gt;\(1\)&lt;/span&gt; 的向量，&lt;span class="math"&gt;\(F(s)=[f(d_{ij})]\)&lt;/span&gt;，&lt;span class="math"&gt;\(f(d_{ij})\)&lt;/span&gt; 是表示每对分子间距离的函数：&lt;/p&gt;
&lt;div class="math"&gt;$$f(d)=\mathrm{e}^{-\theta d_{ij}}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 为常数，取 &lt;span class="math"&gt;\(\theta=0.5\)&lt;/span&gt;，&lt;span class="math"&gt;\(d_{ij}\)&lt;/span&gt; 为分子 &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(s_j\)&lt;/span&gt; 的分子指纹间的谷本距离。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun_1"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8751?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8751?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先分别使用 ChEMBL 数据集预训练四种模型，再用 LIGAND 数据集微调预训练的生成模型，使用测试集生成分子的结果如上表所示。&lt;/p&gt;
&lt;p&gt;同样使用 SMILES 表示分子，相比于 LSTM 模型，训练 Transformer 模型需要的计算资源更多，但训练时间更短而且效果更好。使用 SMILES 的模型在微调后表现有所上升，但还是差于使用图的模型，这主要是由于用图表示分子更容易获得原子的几何关系信息。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8752?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8752?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章绘制了图 Transformer 生成分子的降维结果，可以看出图 Transformer 生成的分子很好地覆盖了 ChEMBL 和 LIGAND 两个数据集的化学空间。在对生成分子进一步评估中发现，图 Transformer 生成分子的可合成性低于使用 SMILES 的模型，作者认为这是因为基于图的模型能够生成更复杂的结构，导致可合成性降低。&lt;/p&gt;
&lt;p&gt;文章总结了图 Transformer 的 4 点优势：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;局部尺度上的不变性：图 Transformer 能够很好地识别输入的分子骨架，并使输出的生成分子中具有相同的结构；&lt;/li&gt;
&lt;li&gt;全局尺度上的可扩展性：图 Transformer 在生成分子的过程中，可以将生成部分直接插入到表示图的矩阵中，具有很大灵活性；&lt;/li&gt;
&lt;li&gt;无语法约束：图 Transformer 不需要关注 SMILES 语法要求，模型不需要额外学习分子中的语法特征；&lt;/li&gt;
&lt;li&gt;可引入化学规则：可以在图 Transformer 中引入化学规则，例如价键匹配规则，提高生成分子的准确性。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后文章还在图 Transformer 中引入了强化学习的策略，模型能够生成对 A&lt;sub&gt;2A&lt;/sub&gt;AR 的亲合力和 QED 分数更高的分子。文章中输入模型的分子骨架与生成分子如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8753?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8753?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章重构了以前的 DrugEx 的模型，在对多种深度模型的试验中，图 Transformer 具有最好的分子生成效果。相比于 SMILES，分子的图表示在分子生成任务中可以更好地识别输入的分子结构，并且可以很容易地改造分子结构生成分子，这一点在发现先导化合物以及先导化合物的优化上都能发挥很大的作用。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category></entry><entry><title>文献总结｜用于从蛋白序列进行药物设计的深度生成模型</title><link href="https://leonis.cc/sui-sui-nian/2023-04-01-summary-doi.org/10.1186/s13321-023-00702-2.html" rel="alternate"></link><published>2023-04-01T00:00:00+08:00</published><updated>2023-04-01T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-04-01:/sui-sui-nian/2023-04-01-summary-doi.org/10.1186/s13321-023-00702-2.html</id><summary type="html">&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Deep generative model for drug design from protein target sequence，文章设计了一种基于 GAN 的蛋白配体分子生成模型，该模型只需要获取氨基酸序列的信息就可以生成相应蛋白口袋的配体。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1186/s13321-023-00702-2" rel="noopener" target="_blank"&gt;doi.org/10.1186/s13321-023-00702-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍 2023 年发布在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的一篇文章，文章原标题为 Deep generative model for drug design from protein target sequence，文章设计了一种基于 GAN 的蛋白配体分子生成模型，该模型只需要获取氨基酸序列的信息就可以生成相应蛋白口袋的配体。&lt;/p&gt;
&lt;p&gt;目前的分子生成方法可以分为两类，其中一种是基于配体的分子生成（ligand-based molecule generation, LBMG），另一种是基于口袋的分子生成（pocketbased
molecule generation, PBMG）。LBMG 方法难以跳出目前的化空间，因而难以生成具有新颖结构的分子；PBMG 方法需要获取更多蛋白口袋的信息，但计算蛋白 3D 构象通常开销巨大。&lt;/p&gt;
&lt;p&gt;文章提出了一种输入蛋白序列即可获得配体的分子生成模型，称为 DeepTarget。DeepTarget 既不需要考虑蛋白口袋的构象信息，也不需要在特定的分子库上微调，有效避免了上述两种方法的局限。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用了来自于 ChEMBL 的分子-蛋白对数据，经数据清洗后，共得到 551223 个分子-蛋白对，涉及 1970 种蛋白质与 333399 种分子的 SMILES 序列。&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8733?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8733?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;DeepTarget 由 3 个部分构成，分别是氨基酸序列嵌入（Amino Acid Sequence Embedding, AASE）、结构特征推理（Structural Feature Inference, SFI）和分子生成（Molecule Generation, MG）。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;氨基酸序列嵌入：AASE 是模型的嵌入层，使用了 Transformer 的架构，主要用于将序列数据转化为模型计算并处理的特征向量。&lt;/li&gt;
&lt;li&gt;结构特征推理：SFI 部分采用了 GAN 的结构，其主要任务是在 AASE 中得到蛋白特征表示上加上一定的噪声，再通过多层感知机得到潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;分子生成：MG 部分使用了 LSTM 结构，是一个预训练的解码器，它将潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 解码为目标分子。该解码器是一个在 ChEMBL 大数据集上训练好的模型，能将分子的潜变量转换为相似的分子。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;SFI 中的 GAN 模型是 DeepTarget 生成蛋白配体的关键，GAN 由生成器与分别器两个模块构成，生成器从潜变量 &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; 生成分子，而分别器则识别生成分子与该蛋白口袋之间的关系，训练 GAN 就是让生成器不断生成分子，直至生成的分子可以「欺骗」分别器，也就是此时生成的分子满足该蛋白口袋的配体分布特征。在推理阶段，则将此生成分子的表示送入 MG 中得到分子的 SMILES 编码。&lt;/p&gt;
&lt;p&gt;传统的生成模型关注于生成器与生成结果之间的关系，而在文章所设计的任务中，不同的蛋白口袋与配体分子在化学空间中有着不同的分布，这也是影响分子生成的因素。因此文章引入了对比学习（Contrastive Learning, CL）的手段，将不同的蛋白作为标签而使分子分簇，在相应的化学空间中生成分子。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun_1"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8734?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8734?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章首先针对 DRD2 和 PARP1 两个蛋白的活性口袋生成分子。图 a 展示了对应的真实配体分子与生成分子的对接打分，生成分子相对于真实分子向打分更低方向偏移，说明具有更高的亲合力。&lt;/p&gt;
&lt;p&gt;图 b 挑出了生成分子中的代表分子，与训练集分子计算相似性，两个分子与训练集的相似性都在 0.2-0.6 左右，说明这两个分子与训练集分子存在一定差异，DeepTarget 能生成新颖的分子。&lt;/p&gt;
&lt;p&gt;图 c 展示了是否在模型中引入对比学习的了生成结果，使用对比学习策略模型生成的分子明显向打分更低处偏移，具有更好的效果。&lt;/p&gt;
&lt;p&gt;图 d 测试了模型的泛化能力，先在训练集中删去 DRD2 和 PARP1 两个蛋白的数据，将生成分子与先前生成的分子对比，从测试结果中可以看出，删除相应训练数据后，生成分子的对接打分上升，但还是生成了相当数量打分低于 -6 的分子，作者认为这可以说明 DeepTarget 能针对训练集中不存在的活性口袋生成配体分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8735?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8735?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章对 DeepTarget 生成的分子与其他模型针对这两个口袋生成的分子做了评估，结果如上表所示，DeepTarget 生成分子的 Valid 高于两个 GAN 模型，其他指标与其他模型相当。&lt;/p&gt;
&lt;p&gt;作者认为这些指标只能做为模型的参考，因为模型并没有针对生成分子的 Valid 和 Unique 等指标进行优化，DeepTarget 的目标更着重于生成与指定口袋真正具有相互作用的配体分子。&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章设计了一种基于 GAN 的蛋白配体分子生成模型，模型只需要获取氨基酸序列的信息就可以生成相应蛋白口袋的配体，文章验证了生成分子具有较好的对接打分，并且模型表现出了一定的泛化能力，可用于针对训练集以外的蛋白生成配体。但文章中设计的模型评估实验有限，只针对两个靶点生成了分子，从文章中的数据来看，与其他模型相比该模型没有特别明显的优势。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GAN"></category></entry><entry><title>在明清小说中索隐：读《中国叙事学》</title><link href="https://leonis.cc/gu-zhi-dui/2023-03-30-book-chinese-narratology.html" rel="alternate"></link><published>2023-03-30T00:00:00+08:00</published><updated>2023-03-30T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-30:/gu-zhi-dui/2023-03-30-book-chinese-narratology.html</id><summary type="html">&lt;p&gt;浦安迪的《中国叙事学》是一本讨论中国叙事传统和明清小说的小书，页数并不多，不消四五天即可翻完。虽然这是一本学术著作，但语言流畅、分析丝丝入扣，读起来时并不觉得枯燥乏味，反而觉得酣畅淋漓。书题虽为《中国叙事学》，但全书中专门论述 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;浦安迪的《中国叙事学》是一本讨论中国叙事传统和明清小说的小书，页数并不多，不消四五天即可翻完。虽然这是一本学术著作，但语言流畅、分析丝丝入扣，读起来时并不觉得枯燥乏味，反而觉得酣畅淋漓。书题虽为《中国叙事学》，但全书中专门论述中国叙事的篇幅并不多，只是在全书的前面部分章节立起「中国叙事」的概念与分析方法，在后文中则全是基于这些概念与方法来分析中国的奇书文体，也就是范围更窄的明清小说，我在后文中即以「明清小说」这一更为通俗的文学类别指代书中所说的「奇书文体」。&lt;/p&gt;
&lt;p&gt;全书只有前一部分中才真正讨论了「中国叙事学」，但这一部分中不乏许多精要切当的论述，让人觉得豁然开朗；作者将其作为分析方法，一以贯之用于剖析明清小说，又深让人惊异于明清小说原来还能这么读，读完这本书后再去重读明清小说，相信又会有另一番收获。基于以上的原因，本文另起了一个标题&amp;mdash;&amp;mdash;「在明清小说中索隐」，我认为能够更准确地概括全书的主题。&lt;/p&gt;
&lt;div class="bookshelf"&gt;
&lt;div class="book"&gt;
&lt;img referrerpolicy="no-referrer" src="https://img2.doubanio.com/view/subject/s/public/s33747413.jpg"/&gt;
&lt;div class="infos"&gt;
&lt;a class="title" href="https://book.douban.com/subject/30244064/"&gt;中国叙事学&lt;/a&gt;
&lt;div class="作者"&gt;作者：[美] 浦安迪 (Andrew H. Plaks)&lt;/div&gt;
&lt;div class="出版社"&gt;出版社：北京大学出版社&lt;/div&gt;
&lt;div class="出版年"&gt;出版年：2018-8&lt;/div&gt;
&lt;div class="页数"&gt;页数：292&lt;/div&gt;
&lt;div class="定价"&gt;定价：45.00元&lt;/div&gt;
&lt;div class="ISBN"&gt;ISBN：9787301295960&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="he-wei-xu-shi"&gt;何为叙事&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;任何时代，任何地方，任何社会，都少不了叙述。它从远古时代就开始存在，古往今来，哪里有人，哪里就有叙述。&lt;/p&gt;
&amp;mdash;&amp;mdash;法国当代文论家 罗兰・巴特&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;书中认为文学作品是在传递某种人生本质，那么文学中的三大体式就也可以按传递人生本质的方式区别，即&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;抒情诗：直接描绘绘静态的人生本质，有叙述人但没有故事&lt;/li&gt;
&lt;li&gt;戏剧：关注人生矛盾，通过舞台传达人生本质，有场面、故事但没有叙述人&lt;/li&gt;
&lt;li&gt;叙事文：不直接描绘人生本质，而以传事为主要目标，从而展示延绵不断的经验流中的人生本质&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们不妨回忆我们曾经接触过的文学作品，一定是可以将大体的内容归置到上述的 3 个类别中去的。之所以说「大体的内容」，就是因为三种体式间不是完全孤立的，实际上三者相互交融渗透，抒情中存在着叙事，叙事中同样存在着抒情。这里用这种机械的分类有助于我们厘清全书讨论的对象&amp;mdash;&amp;mdash;那也就是叙事文，在三者对比之下，相信我们对「何为叙事」已经有了朦胧的答案。&lt;/p&gt;
&lt;h2 id="xu-shi-wen-xue-de-yuan-liu"&gt;叙事文学的源流&lt;/h2&gt;
&lt;p&gt;西方叙事文学的历史可以概括为「&lt;dot&gt;史诗（epic）&amp;rarr; 罗曼史（romance）&amp;rarr; 小说（novel）&lt;/dot&gt;」。&lt;/p&gt;
&lt;p&gt;而中国的古代传统文学则是一条「三百篇 - 骚 - 赋 - 乐府 - 律诗 - 词曲 - 小说」的发展脉络，也就是说，中国传统文学的重心是抒情。&lt;/p&gt;
&lt;p&gt;中国的叙事文学源头应当是《尚书》以及《左传》，作者认为二者深刻地影响了后世的叙事文学，并画下了一定的定式。二者都属于史文，而后世的虚构文学则是从六朝志怪发源，分化为「文言小说」和「白话小说」两大类别。&lt;/p&gt;
&lt;p&gt;作者强调，文言小说和白话小说泾渭分明，「杂录」「志怪」等文言小说又被称为「史余」，与史文的关系更为密切，书中提到纪昀将《山海经》等书从史部抽出，纳入小说，可为一证。而白话小说则可能是由民间说书的「通俗文学」发展而来（鲁迅等持此说），也可能是由当时文人所创作的「才子书」（作者持此说）。&lt;/p&gt;
&lt;p&gt;书中将中国的叙事文学历史总结为「&lt;dot&gt;神话 &amp;rarr; 史文 &amp;rarr; 明清奇书&lt;/dot&gt;」，正如在对 novel 的批评不得不在 epic 的传统中一样，在批评中国的明清奇书时也脱不开神话与史文对其的影响。&lt;/p&gt;
&lt;h2 id="shi-wen-de-ying-xiang"&gt;史文的影响&lt;/h2&gt;
&lt;p&gt;在介绍书中分析史文与神话的内容之前，我想先引用顾颉刚先生的观点奠定一个基调，那就是应当审慎地对待传世文献，未经检验的文献是不可信的。经常有人被民族主义裹挟，把「疑古」简单地想象为「否认一切历史」并言之凿凿地抨击顾颉刚先生，我只能对此深表遗憾。在面对浩如烟海的史料时，「疑古」才是去伪存真、还原历史的科学方法。&lt;/p&gt;
&lt;p&gt;书中的想法与顾颉刚先生的观点不谋而合，我不了解作者是否参考了顾颉刚先生的著作，如若不然，就是顾颉刚先生从历史研究的经验中总结出了「疑古」的观点，而作者从文学分析的角度告诉读者对史文的应当审慎，可以称得上合作。原书中的文字足够精彩，无需我的赘言，兹引录原文如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;中国的史书虽然力图给我们造成一种客观记载的感觉，但实际上不外乎一种美学上的幻觉，是用各种人为的方法和手段造成的「拟客观」效果。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;由于中国历代长期形成的对史近乎宗教的狂热崇拜，也由于在清亡以前史料永远只对史官开放的历史事实，中国正史叙述者总是摆出一副「全知全能者」的姿态；然而，这种全知全能却只是局限在冠冕堂皇的庙堂里。它的触角甚至伸不进皇家的后院，当然更难看见「处江湖之远」的草民百姓的众生相。一种纯客观的叙事幻觉由此产生，并且成为一种经久不坏的模式，从史官实录到虚构文本，横贯中国叙事的各种文体。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;西方的史诗原则上是虚构的艺术，只与历史传说有些微弱的关联；而中国的史文对于「虚构」和「实事」却从来就没有严格的分界线。西方文学理论家一般认为，历史讲实事，小说讲虚构。中国古代批评家则强调，「历史中有小说，小说中有历史」。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;西人重「模仿」，等于假定所讲述的一切都是出于虚构，中国人尚「传述」，等于宣称所述的一切都出于实事。&lt;/p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="shen-hua-de-ying-xiang"&gt;神话的影响&lt;/h2&gt;
&lt;p&gt;由史文上溯至远古神话，可以发现中国神话具有「人本位」倾向，也就是通常借神话来表人事，因而常常把历史与神话混作一团，譬如说我们都知道宙斯是虚构的，而禹究竟是神是人仍无定论，再例如我们都知道十个太阳是断无可能的，但后羿是远古的君主还是人格化的神呢？或许我们习于这些神话，在面对这样的疑难时会将其推作中国的神话过于琐碎繁杂而不成体系，但希腊神话何尝不零碎，却没有这种明显的「人本位」特征。&lt;/p&gt;
&lt;p&gt;再来考查中神话的叙事，在中国古代典籍中，对神话的具体情节都是话焉不详的，这可以说是&lt;dot&gt;在中国的美学原动力中缺乏一种要求「头身尾」连贯的结构原型&lt;/dot&gt;，这种「非叙述性」美学原型导致了中西叙事传统的分流。这两种不同的叙事传统在神话中表现为希腊神话以时间为轴心，故重过程而善于讲述故事；中国神话以空间为宗旨，故重本体而善于画图案。&lt;/p&gt;
&lt;p&gt;这里要注意，神话是诞生于远古时代的故事，但其叙述未并产生于远古时代，例如《淮南子》《庄子》中的神话显然不能用于作神话叙述的分析，所以作者更花精力着眼于例如《尚书》中的神话。&lt;/p&gt;
&lt;h2 id="zhong-guo-de-xu-shi"&gt;中国的叙事&lt;/h2&gt;
&lt;p&gt;在神话与史文的孳乳中，中国的叙事形成了不同于西方的叙事传统，这种叙事传统的影响绵延至明清小说乃至其后。中国的叙事传统中，普遍将重点放在「事隙」上，与西方恰恰相反，也就是真正的具有动作的「事」，都被诸如宴会等「无事之事」包围。&lt;/p&gt;
&lt;p&gt;至于为什么中国的叙事传统如此，没有形成或是拋弃了「叙述性」，甚至在远古神话中已经接受了了这一范式，作者认为这可能是因为中国人重礼的传统，因而中国人把诸如阴阳交替、四时更换等仪礼形式作为了一种基本原则，从而将其「空间化」，抹杀了其中的「时间性」。&lt;/p&gt;
&lt;h2 id="ming-qing-xiao-shuo-zhong-de-xu-shi"&gt;明清小说中的叙事&lt;/h2&gt;
&lt;p&gt;今人在读明清小说时，相当大部分人会因为叙事中的时间性或因果性不强感觉琐碎或是乏味，想必很多人在中学时代读四大名著时，除了《西游记》与《三国演义》以外都觉得昏昏欲睡，这很大程度上就是因为《西游记》具有「取经」这条真正的具有动作的「事」贯穿始终，《三国演义》的时间性则明显更强，就算书中没有明显地以时间为线索，但汉末三分天下终而三家归晋的历史在开卷之前已经为读者熟知。&lt;/p&gt;
&lt;p&gt;西人在读明清小说时亦有这种感受，他们用「缀段性」批评明清小说的叙事方法。缀段性来源于西方对诗歌的文学批评，亚里士多德曾说「缀段性的情节是所有情节中最坏的一种。我所谓的缀段性情节，是指前后毫无因果关系而串接成的情节。」&lt;/p&gt;
&lt;p&gt;但是我们要注意，这是批评西方诗歌的方法，将这一术语用于评价明清小说时已经陷入了西方视角。西方之所以贬斥缀段性，是因为西方文学中有重视「头身尾」结构完整性的传统，这种叙述完整性的要求下，缀段性显然是一种低格。但这种结构完整性的要求并不是金科玉律，我们将眼光放回到「叙事」这一行为上，就会发现叙事天然就带有一定缀段性的特征，因为叙事就是在处理人类经验的一个个片段单元，例如中国的史文，就是由史官将一长段的时间线的人类经验分割为一个个事件片段而创作出的巨制。&lt;/p&gt;
&lt;p&gt;明清小说在结构上的一个明显特征就是「百回」，古人刻意将一部小说分为百回或是百廿回本身就反映了一种中式的美感，一种中国人追求平衡的美学倾向。再者，明清小说的百回完帙在出版时又惯被分作十卷，这是今人难以注意到的细节，同时这也不是出于偶然。作者发现，每十回就能组成一个小故事，百回中的十个小故事又现出一种特殊的韵律感。&lt;/p&gt;
&lt;p&gt;若以《水浒传》为为例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;武（松）十回&lt;/li&gt;
&lt;li&gt;林（冲）十回&lt;/li&gt;
&lt;li&gt;宋（江）十回&lt;/li&gt;
&lt;li&gt;第七十二至八十二回：受招安&lt;/li&gt;
&lt;li&gt;第八十三至九十回：平辽&lt;/li&gt;
&lt;li&gt;第九十一至一百回：平田虎&lt;/li&gt;
&lt;li&gt;第一百一至一百十回：平王庆&lt;/li&gt;
&lt;li&gt;第一百十一至一百二十回：平方腊&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若以《三国演义》为例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一至九回：董卓传，至董卓身死&lt;/li&gt;
&lt;li&gt;第十至十九回：吕布传，至吕布战死白门楼&lt;/li&gt;
&lt;li&gt;第二十至三十一回：曹操传，至大破袁绍&lt;/li&gt;
&lt;li&gt;第三十二至三十八回：刘备传，至隆中对&lt;/li&gt;
&lt;li&gt;第三十九至五十回：诸葛亮传，至华容道&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在这些十回的小故事中，又有相应的重心，这个重心大约是在三四回左右，在全书之中丝毫不乱，作者也基于这些结构上的巧妙安排而认定明清小说属于独运匠心的「才子书」，绝不可能脱胎于民间说书人口中。&lt;/p&gt;
&lt;p&gt;再将全书的十回目故事缀合起来，对于全书而言还有一个结构上的经典范式，那就是「二十-六十-二十」的叙述程式。《金瓶梅》就是具有这种对称美的典型，在前二十回与后二十回中，书中都是在描写西门庆院墙外的故事，在中间六十回转入展开这间深宅大院中离合，同时前二十回叙述家中新添金、瓶、梅三小妾，奠定全书格局，后二十回则是西门庆死去，树倒猢孙散，这种对称的安排不能说不是作者的心思。&lt;/p&gt;
&lt;p&gt;作者还发现，明清小说中的另一个叙事特点就是情节的高潮在终点前就已经发生了，大约在全书四分之三位置处，同时全书又可以分成上下两截相互照应。以《三国演义》为例子最为简单，第五十回赤壁之战确定了天下三分的格局，在此全书分为两截，而在第七十八回曹操死去，全书自此高潮突然收束，一路走向下坡。《金瓶梅》要更为规整，以第四十九回为上下截的分水岭，上截描述西门庆升官发财、步步高升的经历，下截则是其春风得意而加速自毁的过程，同样在第七十九回死去，达到故事的最高潮。&lt;/p&gt;
&lt;p&gt;作者认为这种绵延不绝的故事布局是中国叙事独特的「转轮式」布局，故事的发展正像一个不断旋转的法轮，暗给人天道循环的感受。正如《三国演义》所要强调的「合久并分，分久并合」，故事开始于东汉末年的纷乱，结束于一统于晋，故事从刘、关、张等主人公手中交递给他们的后辈，正像长江后浪推前浪，呈现出一种无了局的形式，作者想表达的思想也暗含其中了。&lt;/p&gt;
&lt;p&gt;作者在书的最后部分上述方法分析了四大奇书中的中心思想，对于只接触教科书式解读的我，其分析方法与结果都可谓是别开生面，例如《西游记》不能简单理解为明末政治的黑暗，在一定程度上它还与明末的思想主流「心学」有关，书中「心猿」等等用语肯定另有所指，作者将其解读为告诫人们「诚意正心」。但部分观点有些冒进而难免令人觉得略显穿凿，但我认为作者只是提出了他的观点，真正怎样理解和解读明清小说还是依赖于读者。我相信在看过作者的一系列剖析后，再去重新读一读四大奇书，一定会有新的理解，这些新收获才是作者浦安迪撰写此书想要传达给我们的东西。&lt;/p&gt;</content><category term="故纸堆"></category><category term="阅读"></category><category term="文学"></category></entry><entry><title>文献总结｜通过连接感知模版挖掘实现从头生成分子</title><link href="https://leonis.cc/sui-sui-nian/2023-03-25-summary-doi.org/10.48550/arXiv.2302.01129.html" rel="alternate"></link><published>2023-03-25T00:00:00+08:00</published><updated>2023-03-25T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-25:/sui-sui-nian/2023-03-25-summary-doi.org/10.48550/arXiv.2302.01129.html</id><summary type="html">&lt;p&gt;本文介绍由中科大于 2023 年发布在 ICLR 2023 上的一篇文章，文章原标题为 De Novo Molecular Generation via Connection-aware Motif Mining，文章提出了一种从分子数据集中挖掘模版结构的算法，同时设计了一种通过组合模版结构实现分子生成的模型。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2302.01129" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2302.01129&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍由中科大于 2023 年发布在 ICLR 2023 上的一篇文章，文章原标题为 De Novo Molecular Generation via Connection-aware Motif Mining，文章提出了一种从分子数据集中挖掘模版结构的算法，同时设计了一种通过组合模版结构实现分子生成的模型 MiCaM（Mined Connection-aware Motifs）。&lt;/p&gt;
&lt;h2 id="suan-fa"&gt;算法&lt;/h2&gt;
&lt;p&gt;文章中提出的连接感知模版挖掘能够从数据集中构建模版词汇，用于后续的分子生成，该算法包括两个主要步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;合并操作（Merging-operation Learning Phase）&lt;/li&gt;
&lt;li&gt;构建模版词汇（Motif-vocabulary Construction Phase）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="算法" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8702?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="算法" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8702?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="he-bing-cao-zuo"&gt;合并操作&lt;/h3&gt;
&lt;p&gt;数据集 &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 中的每个分子都可以表示为图 &lt;span class="math"&gt;\(\mathcal{G(V,E)}\)&lt;/span&gt;，结点 &lt;span class="math"&gt;\(\mathcal{V}\)&lt;/span&gt; 表示原子，边 &lt;span class="math"&gt;\(\mathcal{E}\)&lt;/span&gt; 表示化学键。接着定义与之类似的 &lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M(\mathcal{V}^{(k)}_M,\mathcal{E}^{(k)}_M)\)&lt;/span&gt; 表示第 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 次合并后的分子，其中 &lt;span class="math"&gt;\(\mathcal{F}\in\mathcal{V}_M\)&lt;/span&gt; 表示分子的子结构，可以由一个原子构成，也可以由多个原子构成，那么边 &lt;span class="math"&gt;\(\mathcal{E}_M\)&lt;/span&gt; 相应就表示子结构之间的连接方式。显然，&lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M\)&lt;/span&gt; 由 &lt;span class="math"&gt;\(\mathcal{G}\)&lt;/span&gt; 初始化得到，也就是 &lt;span class="math"&gt;\(\mathcal{G}^{(0)}_M(\mathcal{V}^{(0)}_M,\mathcal{E}^{(0)}_M)=\mathcal{G(V,E)}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;接下来介绍由 &lt;span class="math"&gt;\(\mathcal{G}^{(0)}_M\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M\)&lt;/span&gt; 的合并操作，文章将操作「&lt;span class="math"&gt;\(\oplus\)&lt;/span&gt;」定义将子结构合并为新的子结构，即 &lt;span class="math"&gt;\(\mathcal{F}_{ij}=\mathcal{F}_i\oplus\mathcal{F}_j\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\mathcal{F}_{ij}\)&lt;/span&gt; 就包含了 &lt;span class="math"&gt;\(\mathcal{F}_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\mathcal{F}_j\)&lt;/span&gt; 中所有的结点与边。&lt;span class="math"&gt;\(\mathcal{G}^{(0)}_M\)&lt;/span&gt; 包含了边 &lt;span class="math"&gt;\(\mathcal{E}^{(0)}_M\)&lt;/span&gt;，边又连接了相应的子结构，所以对于通过边连接的子结构 &lt;span class="math"&gt;\((\mathcal{F}^{(0)}_i,\mathcal{F}^{(0)}_j)\in\mathcal{E}^{(0)}_M\)&lt;/span&gt; 计算 &lt;span class="math"&gt;\(\mathcal{F}^{(0)}_{ij}=\mathcal{F}^{(0)}_i\oplus\mathcal{F}^{(0)}_j\)&lt;/span&gt;，其中出现频率最高的 &lt;span class="math"&gt;\(\mathcal{F}^{(0)}_{ij}\)&lt;/span&gt; 就记作 &lt;span class="math"&gt;\(\mathcal{M}^{(0)}\)&lt;/span&gt;。再次遍历子结构 &lt;span class="math"&gt;\((\mathcal{F}^{(0)}_i,\mathcal{F}^{(0)}_j)\in\mathcal{E}^{(0)}_M\)&lt;/span&gt;，只要 &lt;span class="math"&gt;\(\mathcal{F}^{(0)}_i\oplus\mathcal{F}^{(0)}_j==\mathcal{M}^{(0)}\)&lt;/span&gt;，就将这两个子结构合并，完成后得到的所有新子结构就是 &lt;span class="math"&gt;\(\mathcal{V}^{(1)}_M\)&lt;/span&gt;，相应的新连接边就是 &lt;span class="math"&gt;\(\mathcal{E}^{(1)}_M\)&lt;/span&gt;，二者构成了经过 1 次合并的分子 &lt;span class="math"&gt;\(\mathcal{G}^{(1)}_M\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;以上合并操作可以推广，在第 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 次合并中，可以从分子 &lt;span class="math"&gt;\(\mathcal{G}^{(k)}_M(\mathcal{V}^{(k)}_M,\mathcal{E}^{(k)}_M)\)&lt;/span&gt; 得到分子 &lt;span class="math"&gt;\(\mathcal{G}^{(k+1)}_M(\mathcal{V}^{(k+1)}_M,\mathcal{E}^{(k+1)}_M)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id="gou-jian-mo-ban-ci-hui"&gt;构建模版词汇&lt;/h3&gt;
&lt;p&gt;数据集中的所有分子在合并操作后都变为 &lt;span class="math"&gt;\(\mathcal{G}^{(K)}_M(\mathcal{V}^{(K)}_M,\mathcal{E}^{(K)}_M)\)&lt;/span&gt;，此时分子已经大大简化，将分子中的结点分割开来，并添加上连接位置的标记 &lt;code&gt;*&lt;/code&gt;，就得到了分子结构模版。&lt;/p&gt;
&lt;h2 id="mo-xing_1"&gt;模型&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="模型" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8703?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="模型" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8703?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;MiCaM 通过多个逐渐组合模版结构的步骤生成分子，这样的组合有两种方式，一种是直接在连接位置上连上另一个模版结构，另一种是分子中的两个连接位置相连（成环）。&lt;/p&gt;
&lt;p&gt;将第 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 步得到的分子记作 &lt;span class="math"&gt;\(\mathcal{G}_t\)&lt;/span&gt;，用 &lt;span class="math"&gt;\(\mathcal{C}_{\mathcal{G}_t}\)&lt;/span&gt; 表示该分子的所有连接位置，使用序列 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt; 记录 &lt;span class="math"&gt;\(\mathcal{C}_{\mathcal{G}_t}\)&lt;/span&gt; 中所有连接位置的顺序。在第 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 步取出 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt; 的首个元素 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt;，也就是当前操作的连接位置，在该处连接或者成环后，就得到了新分子。新分子中可能具有新的连接位置，所以还使用 RDKit 更新序列 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt;。分子生成步骤就是不断重复以上过程，直至 &lt;span class="math"&gt;\(\mathcal{Q}\)&lt;/span&gt; 为空，此时分子中的所有连接位置都被填满，就得到了输出分子。&lt;/p&gt;
&lt;p&gt;因为具有连接和成环两种组合方式，所以在每个生成步骤中还需要确定与当前操作的连接位置 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt; 相连的位置与子结构。&lt;/p&gt;
&lt;p&gt;文章为此设计了以下步骤，首先使用 GNN&lt;sub&gt;pmol&lt;/sub&gt; 编码当前步骤得到的分子 &lt;span class="math"&gt;\(\mathcal{G}_t\)&lt;/span&gt; 和当前操作的连接位置 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt;，分别得到相应的表示 &lt;span class="math"&gt;\(\boldsymbol{h}_{v_t}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\boldsymbol{h}_{\mathcal{G}_t}\)&lt;/span&gt;，使用 GNN&lt;sub&gt;motif&lt;/sub&gt; 编码模版库中的所有结构模版，得到所有连接位置 &lt;span class="math"&gt;\(v\in\mathcal{C}_\mathrm{Vocab}\)&lt;/span&gt; 的表示 &lt;span class="math"&gt;\(\boldsymbol{h}_{v}\)&lt;/span&gt;。使用神经网络将相应的表示转换为 key 向量与 query 向量，寻找与 query 向量最相似的 key 向量，也就是根据以下概率选择与当前操作的连接位置 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt; 相连的位置：&lt;/p&gt;
&lt;div class="math"&gt;$$P_v=\mathop{\mathrm{softmax}}_{v\in\mathcal{C}_\mathrm{Vocab}\cup\mathcal{C}_{\mathcal{G}_t}\backslash\{v_t\}}(\mathrm{NN_{query}}([\boldsymbol{z},\boldsymbol{h}_{\mathcal{G}_t},\boldsymbol{h}_{v_t}])\cdot\mathrm{NN_{key}}(\boldsymbol{h}_v))$$&lt;/div&gt;
&lt;p&gt;若候选的连接位置 &lt;span class="math"&gt;\(v\in\mathcal{C}_\mathrm{Vocab}\)&lt;/span&gt;，则在模版库中取得相应的 &lt;span class="math"&gt;\(\mathcal{F}^*\)&lt;/span&gt;，并将其接入分子 &lt;span class="math"&gt;\(\mathcal{G}_t\)&lt;/span&gt;，完成连接；若 &lt;span class="math"&gt;\(v\in\mathcal{C}_{\mathcal{G}_t}\)&lt;/span&gt;，那么就合并 &lt;span class="math"&gt;\(v_t\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(v\)&lt;/span&gt;，完成成环。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun"&gt;结果与讨论&lt;/h2&gt;
&lt;h3 id="fen-bu-xue-xi"&gt;分布学习&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8704?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8704?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章在 QM9、ZINC 和 GuacaMol 三个数据集上分别训练了 MiCaM，并使用 MiCaM 生成分子，检查生成的分子集是否接近训练集的分布。实验结果如上表所示，在 QM9 和 GuacaMol 数据集上，MiCaM 生成分子的 Uniqueness 和 Novelty 低于 MoLeR 和 GP-VAE 等模型，但在 KL Div 和 FCD score 上，MiCaM 完全优于其他模型。KL Div 与 FCD score 都衡量了生成分子集与训练集分布的相似程度，也就是说，MiCaM 生成的分子最接近训练集的分布。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8705?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8705?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在算法一节提到的参数 &lt;span class="math"&gt;\(K\)&lt;/span&gt; 决定了合并次数，文章在 QM9 数据集上测试了该参数对生成分子的影响。随着合并次数增加，KL Div 与 FCD score 都随之增加，而生成分子的 Novelty 却在下降。这是因为合并次数越多，分子简化程度越大，最后进入模版库中的模版结构也会更加复杂、更加接近训练集中的分子，最后使用这些模版结构构造的分子就会趋于与训练集分子「雷同」，这也一点程度解释了为什么 MiCaM 生成分子的 Uniqueness 和 Novelty 低于部分模型。&lt;/p&gt;
&lt;h3 id="die-dai-you-hua"&gt;迭代优化&lt;/h3&gt;
&lt;p&gt;文章还使用 MiCaM 进行了迭代目标增强（Iterative Target Augmentation, ITA）的分子生成。首先选出数据集中针对目标要求打分最高的 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个分子，接着使用模型在该训练集上微调并产生新分子，每次迭代过程中，在新分子与训练集中再选出打分最高的 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个分子，用作为下一次迭代的训练集。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8706?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8706?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在文章设计的 5 种任务中，MiCaM 优于其他所有模型。&lt;/p&gt;
&lt;p&gt;最后文章展示了 MiCaM 生成分子的过程，经过 5 个步骤，MiCaM 就能生成相当复杂的分子，同时分子的各性质分数也相应提高。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8707?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8707?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun_1"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种从分子数据中挖掘模版结构的算法，能够用于提取分子数据集中频繁出现的子结构并构建模版结构库，同时文章设计了基于该模版结构库的分子生成模型 MiCaM。MiCaM 通过逐步组合模版结构实现分子生成，很大程度解决了以往启发式分子生成随机性大、难以生成复杂结构的问题，使用模版结构组合生成的分子也更接近于现实中化学家对化合物改造的策略。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category><category term="VAE"></category></entry><entry><title>春日漫步小记</title><link href="https://leonis.cc/zai-lu-shang/2023-03-24-spring-trip-in-tianjin.html" rel="alternate"></link><published>2023-03-24T00:00:00+08:00</published><updated>2023-03-24T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-24:/zai-lu-shang/2023-03-24-spring-trip-in-tianjin.html</id><summary type="html">&lt;p&gt;五六天前刚起了几阵沙尘暴，入眼的一切都是黄蒙蒙的。黄沙中的行人都拉紧着衣服、戴着口罩，可能是为了防沙，也可能是为了防病毒。唯一不避风沙的只有两旁的行道树，经冬之后满是枯枝，更添了几分萧索。就是在这样一片了无生气之中，竟在 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;五六天前刚起了几阵沙尘暴，入眼的一切都是黄蒙蒙的。黄沙中的行人都拉紧着衣服、戴着口罩，可能是为了防沙，也可能是为了防病毒。唯一不避风沙的只有两旁的行道树，经冬之后满是枯枝，更添了几分萧索。就是在这样一片了无生气之中，竟在几天之中气温陡升，绽开了桃花，催绿了柳梢，赶在春分前酝酿好了春天的氛围。我想，也只有中原与华北的气候能如此依时令而行，不误物候，也只有在这时，看着次第开放的春花，才深感只有北方的花如此重诺而能被称为「花信」。&lt;/p&gt;
&lt;p&gt;虽是在尘埃满扑之中，但桃红柳绿的色彩也使春光亮丽了起来，现出了新鲜的气象。自新冠管控放松之后，趁着东风，京津间的通勤也恢复如常了，于是与北京的好友几人约在周末沿海河散散步。&lt;/p&gt;
&lt;p&gt;海河是天津最有风采的景致，虽说不如长江黄河之于中国，但若是失了海河，天津的趣味就要大打折扣。此行的起点就在三岔河口前的丁字沽，北运河、子牙河与海河在此交汇，形成「丁」形，因而得名「丁字沽」。朋友看着路边介绍京杭大运河的牌子，打趣这里该算天津的陆家嘴，运河虽是依旧却再难见到繁忙来往的船舶。&lt;/p&gt;
&lt;p&gt;丁字沽一带的桃花园与西沽公园是赏春的好去处，不用多言，其中最负盛名的自然是桃花。节临三月，占得春花头筹的就是桃花，连通往景点的路上也游人如织，非摩肩接朣不得过。也许是为了不打消游人赏花的兴致，原先告示的「预约入园」也成了君子协定，凡是到场即可入内，园中的人流便可以推测了。&lt;/p&gt;
&lt;p&gt;桃花园是北运河边的一条长堤，沿堤遍植桃树，因此没有「园」之实，称为桃花堤更为贴切。长堤上人头攒动，头顶上两边的桃树相互环合，拱成了一条花廊。游人在花廊下行走，而时不时几阵薰风吹过，就有几枚桃花脱了梢头，落入人流之中。人在花群中，花亦在人群中。&lt;/p&gt;
&lt;p&gt;花廊难以目力穷尽，只得顺人流向前。沿桃花堤前行，一旁的北运河也沿着桃花堤悠悠向前流淌。正当乱花迷眼而觉得乏味之时，桃花堤悄然南折，北运河也在此转而向南一路奔流汇注海河，我也在此转身向南。赫然映入眼中的是一株高三丈许的玉兰，玉兰的枝干上着满了白花，不杂一叶，开得是那样浓烈，温润洁白的玉兰浑如古玉，又是那样祥和。还不及细想缘何没有人驻足树下，才发觉前方悬挂着河北工业大学的匾额，同时提醒谢绝游人的参观。静静伫立在校园中，仅有的喧闹也只是汩汩的运河水与朗朗书声，悄悄花开花落，委心任乎去留，这株洁白的玉兰也变得更加洒脱起来。&lt;/p&gt;
&lt;p&gt;南折不远，便出了桃花堤，前方遥遥与之照应的是西沽公园。西沽公园与桃花堤不同，所占面积要大许多，所以尽管设置了巨大的广告牌招徕游客，园中也不至于像桃花堤那样拥挤。西沽公园中的游人更多是附近的居民，按天津人的闲致，家旁有如此山水湖林，自然免不了来此唱几句京戏，园中也得以檀板丝弦不绝于耳，平添了生活气息。&lt;/p&gt;
&lt;p&gt;午后日光稍晻，沿海河一路步行至旧意租界。随着渐渐临近春分，东风解冻了海河，虽没有鸭子来试试水温，但漂浮在河面上随波逐流的海鸥也提示着春水已暖。海河曾是繁忙的航道，如今常见的船只也只有观光轮渡，安静下来的海河于是成了海鸥休憩觅食的场所。每有渡船经过，稍谨慎的扑翅腾空而上，盘旋数周，待船只曳开的波澜平息后又落入水中，稍慵懒的也不动弹，宁随着艏波漂荡，一派春日融融的景象。&lt;/p&gt;
&lt;p&gt;与海河作别，进入旧意租界，眼见日头已经西颓，于是匆匆穿过条条欧式风情的街道寻觅晚餐，又匆匆送朋友往火车站，而后分别。一天的漫步果然让我收获了腰酸腿痛，但兴致、友人与良辰美景总是难以聚齐，难得能够这样共同游目聘怀，不仅幸甚，也算是不负如此韶光了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="桃花堤" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8688?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="桃花堤" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8688?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 游人如织的桃花堤&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="桃花堤" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8689?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="桃花堤" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8689?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="桃花堤" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8690?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="桃花堤" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8690?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="河北工业大学" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8691?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="河北工业大学" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8691?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 河工大中独自盛开的玉兰&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="海河" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8694?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="海河" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8694?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 海河中戏水的海鸥&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="海河" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8692?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="海河" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8692?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 与骑手相伴而飞&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="海河" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8693?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="海河" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8693?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="但丁广场" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8695?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="但丁广场" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8695?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 但丁握着哪部文集的稿子？&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="意式风情街" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8696?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="意式风情街" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8696?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;&lt;i class="fa fa-angle-double-up"&gt;&lt;/i&gt; 某无人居住的故居，窗前桃花已开，主人归未？&lt;/p&gt;</content><category term="在路上"></category><category term="随笔"></category><category term="游记"></category><category term="摄影"></category></entry><entry><title>文献总结｜使用等变扩散模型进行基于结构的药物设计</title><link href="https://leonis.cc/sui-sui-nian/2023-03-17-summary-doi.org/10.48550/arXiv.2210.13695.html" rel="alternate"></link><published>2023-03-17T00:00:00+08:00</published><updated>2023-03-17T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-17:/sui-sui-nian/2023-03-17-summary-doi.org/10.48550/arXiv.2210.13695.html</id><summary type="html">&lt;p&gt;本文介绍由洛桑联邦理工学院等研究单位于 2022 年发布在 arXiv 上的一篇文章，文章原标题为 Structure-based Drug Design with Equivariant Diffusion Models，文章首次将等变扩散模型用于基于结构的药物设计，实现针对特定的蛋白靶点生成多样且具有高亲合力的的配体分子。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2210.13695" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2210.13695&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍由洛桑联邦理工学院等研究单位于 2022 年发布在 arXiv 上的一篇文章，文章原标题为 Structure-based Drug Design with Equivariant Diffusion Models，文章首次将等变扩散模型用于基于结构的药物设计，实现针对特定的蛋白靶点生成多样且具有高亲合力的的配体分子。&lt;/p&gt;
&lt;p&gt;近年来，越来越多的深度学习模型被应于用基于结构的药物设计，但这些模型主要采用了序列生成的方式，这些方法所面临的一个重要问题是序列生成忽视了原子的几何顺序，可能并不能反映设计分子的真实原理，最后无法获取到目标分子的化学空间。&lt;/p&gt;
&lt;p&gt;针对于这个问题，文章考虑了分子中各原子的空间坐标，使用等变扩散模型完成了针对特定靶点的分子生成，称为 DiffSBDD（Equivariant Diffusion Model
for Structure-Based Drug Design），实验结果表明该模型能够获取指定蛋白活性口袋的信息并生成多样、具有类药性且具有高亲合力的分子。&lt;/p&gt;
&lt;h2 id="mo-xing"&gt;模型&lt;/h2&gt;
&lt;h3 id="qu-zao-sheng-kuo-san-gai-lu-mo-xing"&gt;去噪声扩散概率模型&lt;/h3&gt;
&lt;p&gt;去噪声扩散概率模型（Denoising Diffusion Probabilistic Models, DDPMs）是近年应用于多个领域的一类生成模型，DDPM 通过马尔可夫链逐次向样本数据上添加噪声，然后由神经网络学习该马尔可夫链的逆过程，实现从噪声中重建采样数据。&lt;/p&gt;
&lt;p&gt;就分子生成任务而言，样本数据是原子点云（简称分子） &lt;span class="math"&gt;\(\boldsymbol{z}_\mathrm{data}=[\boldsymbol{x},\boldsymbol{h}]\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(\boldsymbol{x}\in\mathbb{R}^{N\times3}\)&lt;/span&gt;，表示原子的空间坐标，&lt;span class="math"&gt;\(\boldsymbol{h}\in\mathbb{R}^{N\times d}\)&lt;/span&gt;，表示原子的特征。那么对原子点由 &lt;span class="math"&gt;\(t=0\)&lt;/span&gt; 至 &lt;span class="math"&gt;\(t=T\)&lt;/span&gt; 逐次加噪声的过程就可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$q(\boldsymbol{z}_t|\boldsymbol{z}_\mathrm{data})=\mathcal{N}(\boldsymbol{z}_t|\alpha_t\boldsymbol{z}_\mathrm{data},\sigma^2_t\boldsymbol{I})$$&lt;/div&gt;
&lt;p&gt;由 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 至 &lt;span class="math"&gt;\(s&amp;lt;t\)&lt;/span&gt; 的去噪声的过程同是马尔可夫链，记作 &lt;span class="math"&gt;\(q(\boldsymbol{z}_s|\boldsymbol{z}_\mathrm{data},\boldsymbol{z}_t)\)&lt;/span&gt;，可以看出去噪声的过程依赖于样本数据 &lt;span class="math"&gt;\(\boldsymbol{z}_\mathrm{data}\)&lt;/span&gt;，即标签数据。&lt;/p&gt;
&lt;p&gt;但当使用模型进行预测时，由于预测的分子 &lt;span class="math"&gt;\(\boldsymbol{z}_\mathrm{data}\)&lt;/span&gt; 是未知的，该模型使用神经网络 &lt;span class="math"&gt;\(\phi_\theta\)&lt;/span&gt; 拟合得到 &lt;span class="math"&gt;\(\hat{\boldsymbol{z}}_\mathrm{data}\)&lt;/span&gt;。具体来说，加噪声后的分子可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{z}_t=\alpha_t\boldsymbol{z}_\mathrm{data}+\sigma_t\boldsymbol{\epsilon},\ \epsilon\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})$$&lt;/div&gt;
&lt;p&gt;神经网络就是用于预测噪声 &lt;span class="math"&gt;\(\hat{\boldsymbol{\epsilon}}_\theta=\phi_\theta(\boldsymbol{z}_t,t)\)&lt;/span&gt;，那么显然有&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{\boldsymbol{z}}_\mathrm{data}=\frac{1}{\alpha_t}\boldsymbol{z}_t-\frac{\sigma_t}{\alpha_t}\hat{\boldsymbol{\epsilon}}_\theta$$&lt;/div&gt;
&lt;p&gt;所以训练的目标也就是最小化神经网络预测值 &lt;span class="math"&gt;\(\hat{\boldsymbol{\epsilon}}_\theta\)&lt;/span&gt; 与真实值 &lt;span class="math"&gt;\(\boldsymbol{\epsilon}\)&lt;/span&gt; 间的差距，损失函数为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}_\mathrm{train}=\frac{1}{2}||\boldsymbol{\epsilon}-\phi_\theta(\boldsymbol{z}_t,t)||^2$$&lt;/div&gt;
&lt;h3 id="en-deng-bian-tu-shen-jing-wang-luo"&gt;&lt;em&gt;E(n)&lt;/em&gt; - 等变图神经网络&lt;/h3&gt;
&lt;p&gt;在使生成的分子与蛋白活性口袋配合的过程中，需要重新打乱原子生成新的结构，所以需要对原子点云做置换、旋转等变换。传统的图神经网络无法很好处理这类具有等变对称性质的特征，因而提出的一类改进后用于处理等变性质的图神经网络就被称为等变图神经网络。&lt;/p&gt;
&lt;h3 id="diffsbdd"&gt;DiffSBDD&lt;/h3&gt;
&lt;p&gt;结合以上两种模型，就得到了文章中所设计的 DiffSBDD，DiffSBDD 具有两种分子生成方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;蛋白口袋条件分子生成（Conditional generation）&lt;/li&gt;
&lt;li&gt;通过联合分布实现分子生成（Inpainting）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8663?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8663?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h4&gt;Conditional generation&lt;/h4&gt;
&lt;p&gt;在 conditional generation 模式中，给模型的每个去噪声过程都指定了相同的蛋白口袋，简单来讲，就是在分子生成过程中，原子点云会不断发生置换、旋转等变换，而蛋白口袋不发生变化，使配体分子去「适配」蛋白口袋。&lt;/p&gt;
&lt;h3 id="inpainting"&gt;Inpainting&lt;/h3&gt;
&lt;p&gt;而在 inpainting 模式中，需要首先训练一个无指定条件的 DDPM 用于拟合配体分子与蛋白口袋的联合概率分布 &lt;span class="math"&gt;\(p(\boldsymbol{z}^{(L)}_\mathrm{data},\boldsymbol{z}^{(P)}_\mathrm{data})\)&lt;/span&gt;，该概率分布的主要用于从噪声数据中得到配合的配体分子与蛋白口袋对。&lt;/p&gt;
&lt;p&gt;在预测过程中，首先将加噪声后的样本 &lt;span class="math"&gt;\([\boldsymbol{z}^{(L)}_t,\boldsymbol{z}^{(P)}_t]\)&lt;/span&gt; 中的蛋白口袋掩盖掉，将得到其中的配体部分与加噪声后的蛋白口袋重新组合为 &lt;span class="math"&gt;\([\boldsymbol{z}^{(L)}_t,\boldsymbol{z}^{(P')}_t]\)&lt;/span&gt;，最后用已训练得到的联合概率分布 DDPM 为该组合去噪声，即生成目标分子。&lt;/p&gt;
&lt;h2 id="shu-ju_1"&gt;数据&lt;/h2&gt;
&lt;p&gt;文章分别使用了 CrossDocked 中 10 万个蛋白-配体对和 Binding MOAD 中 40354 个蛋白-配体对作为模型的数据集。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8664?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8664?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先文章对生成分子进行了评估，可以看出不管使用哪一个数据集训练模型、使用哪一个模式生成分子，最终结果中只有很小一部分生成的分子不合法，生成分子中大部分都满足新颖、合法的要求。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8665?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8665?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;接着文章将分子生成结果与目前在基于结构的药物设计上表现最好的 3D-SBDD 和 Pocaket2Mol 两种模型比较。从表中的数据可以看出，在 Vina Score 上，DiffSBDD 与另外两种模型都比较接近，也就是模型都识别到了配体与蛋白口袋间的相互作用，生成了具有亲合力的分子。DiffSBDD 在 QED、Lipinski 这些分子性质上并没有实现优化，而是与测试集保持相似。其中，DiffSBDD 在 SA 上显著低于其他两种模型，文章认为 SA 在一定程度上并不能反映分子真实的合成难度，低 SA 反而说明 DiffSBDD 能够探索更大的化学空间，因此具有最高的 Diversity。最后，DiffSBDD 所需的计算时间远远少于另外两种模型，相比之下更加高效。&lt;/p&gt;
&lt;p&gt;文章展示了针对靶点 2jjg 和 3kc1 生成的分子，其中为 3kc1 生成的第二个分子具有三环结构，该分子也曾在传统的基于结构的药物设计中被设计出来，说明 DiffSBDD 具有应用潜力。在生成的许多分子，还可以找到大环、三元环化合物，这些分子很难合成，所以 DiffSBDD 可能还需要考虑分子的可合成性。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8667?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8667?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章中所提出的 DiffSBDD 首次将等变扩散模型应用于基于结构的药物设计领域，实验证明了 DiffSBDD 在完成分子生成任务上不仅高效而且有效，能够针对给定的靶点生成多样且具有高亲合力的配体分子，该模型不需要再训练就可以直接应用于先导化合物优化等药物设计实践。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="GNN"></category></entry><entry><title>为 Pelican 博客加入搜索功能</title><link href="https://leonis.cc/sui-sui-nian/2023-03-14-deploy-pelican-search.html" rel="alternate"></link><published>2023-03-14T00:00:00+08:00</published><updated>2023-03-14T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-14:/sui-sui-nian/2023-03-14-deploy-pelican-search.html</id><summary type="html">&lt;p&gt;博客上的文章越来越多了，靠文章 tag 来检索文章总是有些麻烦，不妨为博客配置一个搜索功能吧。在中文互联网上搜索不到半点配置 Pelican Search 的相关信息，希望这篇文章能帮到后来人。&lt;/p&gt;</summary><content type="html">&lt;p&gt;在目前的博客生态圈中，静态博客占了相当大的比例。静态博客因为足够轻量、便于备份和迁移而受到包括我在内的许多用户喜欢，但由于静态博客没有数据库，在组织内容上就不免有所不足，这就涉及到题中所说的搜索功能了。为静态博客部署搜索功能比较麻烦，一个稍简单的方法就是借于「必应」「谷歌」等搜索引擎的 API，用它们来搜索站内的内容，但是这种方法的效果并不好，如果网页还未被收录或是相应头键字在网页中的占比太小，就很难搜索到目标信息。另一种方法就是在站内建立本地的搜索，例如 Hexo 等博客框架都提供了相应的插件，在本地生成搜索匹配所需的文件，将其一并推送至服务器实现全站搜索，这种方法的兼容性、准确性都要更好，也是我选择的方案。&lt;/p&gt;
&lt;p&gt;Pelican 也有类似的搜索插件 &lt;a href="https://github.com/pelican-plugins/search" rel="noopener" target="_blank"&gt;&lt;i class="fa fa-github"&gt;&lt;/i&gt; pelican-plugins/search&lt;/a&gt;，它主要是借助了 &lt;a href="https://stork-search.net/" rel="noopener" target="_blank"&gt;Stork&lt;/a&gt; 来实现搜索功能。Stork 在官方文档中指出，它可以用于为静态站点构建关速美观的搜索接口，所以理应可以用于所有类型的静态博客，我不了解其他博客框架是否使用了这个工具，但不得不提其搜索体验非常不错，十分值得一试。&lt;/p&gt;
&lt;h2 id="an-zhuang-stork"&gt;安装 Stork&lt;/h2&gt;
&lt;h3 id="an-zhuang-c-sheng-cheng-gong-ju"&gt;安装 C++ 生成工具&lt;/h3&gt;
&lt;p&gt;Stork 是基于 Rust 构建的工具，需要使用 Rust 的包管理器 Cargo 安装，若是在 macOS 或 Linux 系统上，直接按照官方文档给出的方法安装即可，而在 Windows 上就会比较麻烦，我在这里介绍一下 Windows 的操作方法。&lt;/p&gt;
&lt;p&gt;{note begin}后文所涉及的操作系统都是 Windows 10 系统，终端指的是 Windows 终端（Windows Terminal）。{note end}&lt;/p&gt;
&lt;p&gt;首先，在 Windows 上，Rust 需要某些 C++ 生成工具，可以选择安装 Visual Studio 或仅安装 Microsoft C++ 生成工具。安装 Visual Studio 的方法非常简单（推荐），按下不表，若仅安装 Microsoft C++ 生成工具，可以在终端中输入&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;winget install Microsoft.VisualStudio.2022.BuildTools
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;由于 winget 下载速度感人，实在不推荐这种方法。&lt;/p&gt;
&lt;p&gt;在安装好 Visual Studio 后，在开始界面搜索并打开 Visual Studio Installer，选择 &lt;code&gt;修改&lt;/code&gt; - &lt;code&gt;使用 C++ 的桌面开发&lt;/code&gt;，等待安装完成。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="Visual Studio" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8654?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="Visual Studio" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8654?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id="an-zhuang-rust"&gt;安装 Rust&lt;/h3&gt;
&lt;p&gt;Rust 也可以使用 winget 安装，命令很简单：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;winget install rustup
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是不太好用，所以我还是选择到 &lt;a href="https://www.rust-lang.org/zh-CN/tools/install" rel="noopener" target="_blank"&gt;Rust 官网&lt;/a&gt;下载。下载完成后打开安装程序，弹出的是命令行窗口，默认安装在 &lt;code&gt;C:\Users&lt;/code&gt; 路径下的目录中，如果不需要额外的设置，键入 &lt;code&gt;1&lt;/code&gt; 后按回车即可。&lt;/p&gt;
&lt;p&gt;但由于 C 盘空间不太够了，我需要修改一下安装的路径，在目标路径下创建以下两个文件夹并新建环境变量：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;CARGO_HOME: E:\RUST\.cargo
RUSTUP_HOME: E:\RUST\.rustup
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 &lt;code&gt;PATH&lt;/code&gt; 中添加变量 &lt;code&gt;%CARGO_HOME%&lt;/code&gt;、&lt;code&gt;%RUSTUP_HOME%&lt;/code&gt; 和 &lt;code&gt;%CARGO_HOME%\bin&lt;/code&gt;，然后再打开安装程序，默认路径就已经改变，键入 &lt;code&gt;1&lt;/code&gt; 按回车安装。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-txt"&gt;Rustup metadata and toolchains will be installed into the Rustup
home directory, located at:

  E:\RustT\.rustup

This can be modified with the RUSTUP_HOME environment variable.

The Cargo home directory is located at:

  E:\Rust\.cargo

This can be modified with the CARGO_HOME environment variable.

The cargo, rustc, rustup and other commands will be added to
Cargo's bin directory, located at:

  E:\Rust\.cargo\bin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装完成后，在终端中输入 &lt;code&gt;cargo --version&lt;/code&gt; 理应输出版本信息。&lt;/p&gt;
&lt;h3 id="tong-guo-cargo-an-zhuang-stork"&gt;通过 Cargo 安装 Stork&lt;/h3&gt;
&lt;p&gt;虽然安装好了 Cargo，但不出意外的话，与其他包管理器类似，从官方的源下载内容的速度非常之慢，所以需要修改配置使用镜像源。&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;.cargo&lt;/code&gt; 目录下创建文件 &lt;code&gt;config.toml&lt;/code&gt;，写入以下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-toml"&gt;[source.crates-io]
replace-with = 'ustc'

[source.ustc]
registry = "git://mirrors.ustc.edu.cn/crates.io-index"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;就使用了科大的镜像源，然后在终端中使用以下命令安装 Stork：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;cargo install stork-search --locked
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后可以使用 &lt;code&gt;stork --version&lt;/code&gt; 验证是否成功安装 Stork。&lt;/p&gt;
&lt;h2 id="bu-shu-pelican-search_1"&gt;部署 Pelican Search&lt;/h2&gt;
&lt;p&gt;安装好 Stork 后的步骤就很简单了，在 Pelican 的 Python 环境中安装插件：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sh"&gt;python -m pip install pelican-search
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后在 Pelican 设置中的 &lt;code&gt;PLUGINS&lt;/code&gt; 引入 &lt;code&gt;search&lt;/code&gt;，在主题的模板文件（一般是 &lt;code&gt;base.html&lt;/code&gt;）中引入 Stork CSS 的 CDN（当然也可以改写后自己部署）：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;link rel="stylesheet" href="https://files.stork-search.net/basic.css" /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以及 JavaScript：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;script src="https://files.stork-search.net/releases/v1.5.0/stork.js"&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;
    stork.register("sitesearch", "{{ SITEURL }}/search-index.st")
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后就可以在设计的搜索区域通过以下方式调用 Stork 搜索了：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;Search: &amp;lt;input data-stork="sitesearch" /&amp;gt;
&amp;lt;div data-stork="sitesearch-output"&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{warn begin}以上 CSS 应在页面的头部，例如 &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; 中引入，而 JavaScript 则应在 &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; 的尾部引入。这是因为 HTML 文件是自上至下顺序执行的，如果加载 JavaScript 的位置靠前，比如在 &lt;code&gt;&amp;lt;input data-stork="sitesearch" /&amp;gt;&lt;/code&gt; 元素之前引入，在执行时该元素还未生成，就会找不到匹配的搜索框，提示没有 &lt;code&gt;query selector `input[data-stork="sitesearch"]`&lt;/code&gt;，导致 &lt;code&gt;Uncaught StorkError&lt;/code&gt;。{warn end}&lt;/p&gt;
&lt;h3 id="pelican-search-de-she-zhi-xiang"&gt;Pelican Search 的设置项&lt;/h3&gt;
&lt;p&gt;pelican-search 的设置项只有两项，一项 &lt;code&gt;SEARCH_MODE&lt;/code&gt; 设定从 Markdown 文件建立索引还是从 HTML 建立索引，&lt;code&gt;SEARCH_HTML_SELECTOR&lt;/code&gt; 可以用于指定从 HTML 的哪些内容中建立索引。但 Stork 的设置项不止这些，将其整合进 pelican-search 应该也不太难，留到以后有精力的时候尝试一下。&lt;/p&gt;
&lt;p&gt;我浏览了一下 &lt;a href="https://stork-search.net/docs/config-ref" rel="noopener" target="_blank"&gt;Stork 官方文档&lt;/a&gt;中的内容，发现了很多很有意思的东西。例如 &lt;code&gt;minimum_indexed_substring_length&lt;/code&gt; 一项设置了建立索引的匹配项最短长度，默认值为 3，通俗来讲就是长于 3 的单词才会用于建立索引，那么在搜索时也需要起码键入 3 个字母才有结果。但这种做法对于中文来说就存在很大的问题，中文中最普遍的是双字词，所以文档中也提到 &lt;code&gt;minimum_index_ideographic_substring_length&lt;/code&gt; 一项设置，默认值是 1，对于 CJK 字符而言，长于 1 的词就可以建立索引。可惜在我的试验中这个设置貌似并没有效果，在我使用汉字搜索时，也必须输入 3 个汉字才有结果，若要搜索双字词，只好用两个汉字加上空格的方法将就一下。除了这个问题之外就是使用中文搜索的精度不高，很难找到匹配项，所以 Stork 的最大不足其实就在于对中文的支持不好。据作者的消息，他也很希望能够提高 Stork 在中文搜索上的表现，可以期待一下后续的更新。&lt;/p&gt;
&lt;p&gt;再来说说 Stork 的优点，那就是「快」。Stork 搜索的速度特别快，不论是汉字还是字母，击入三个字符秒出搜索结果，这一点的体验就特别好。另外 Stork 还支持包括英语在内的多种欧洲语言的词根检索，例如输入「get」，它亦能返回「getting」的检索结果，这个功能对于静态站点而言可谓强大。如果是英文语境下，Stork 搜索精度高、速度快、支持词根检索，简直是最为优雅的静态站点搜索插件，也期待一下后续它能否在中文搜索上也能提供如此流畅的体验。&lt;/p&gt;
&lt;p&gt;{note begin}不知是我的原因还是 Stork 的问题，生成的索引文件巨大，足足有 16 MB，完全不能在网页上使用。于是我花了一整天的时间升级 Nginx 并配置上了 Brotli 压缩传输，压缩后只有大约 900 KB 了，加载速度大大提升。{note end}&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references_1"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/maphew/95fb9e986edfab887e4ff36547d5da59" rel="noopener" target="_blank"&gt;Install Stork-search on Windows - GitHub Gist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/zh-cn/windows/dev-environment/rust/setup" rel="noopener" target="_blank"&gt;在 Windows 上针对 Rust 设置开发环境 - Microsoft Learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/skzxc/p/12129353.html" rel="noopener" target="_blank"&gt;Win10 Rust 语言安装与环境变量配置(+VSCode) - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://course.rs/first-try/slowly-downloading.html" rel="noopener" target="_blank"&gt;下载依赖太慢了？ - Rust语言圣经(Rust Course)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="Blog"></category><category term="Pelican"></category><category term="Rust"></category><category term="Windows"></category></entry><entry><title>文献总结｜为蛋白质口袋定制分子：用于基于结构药物设计的 Transformer 分子生成方法</title><link href="https://leonis.cc/sui-sui-nian/2023-03-11-summary-doi.org/10.48550/arXiv.2209.06158.html" rel="alternate"></link><published>2023-03-11T00:00:00+08:00</published><updated>2023-03-11T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-11:/sui-sui-nian/2023-03-11-summary-doi.org/10.48550/arXiv.2209.06158.html</id><summary type="html">&lt;p&gt;本文介绍由微软研究团队于 2022 年发布在 arXiv 上的一篇文章，文章原标题为 Tailoring Molecules for Protein Pockets: a Transformer-based Generative Solution for Structured-based Drug Design，文章使用 Transformer 构建了一种能够获取受体 3 维信息的分子生成模型 TamGent，其中分子生成部分使用了预训练模型，避免了训练数据有限的问题。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.48550/arXiv.2209.06158" rel="noopener" target="_blank"&gt;doi.org/10.48550/arXiv.2209.06158&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍由微软研究团队于 2022 年发布在 arXiv 上的一篇文章，文章原标题为 Tailoring Molecules for Protein Pockets: a Transformer-based Generative Solution for Structured-based Drug Design，文章使用 Transformer 构建了一种能够获取受体 3 维信息的分子生成模型 TamGent，其中分子生成部分使用了预训练模型，避免了训练数据有限的问题。&lt;/p&gt;
&lt;p&gt;随着人工智能技术的发展，深度学习也进入到基于结构的药物设计（Structure Based Drug Design, SBDD）领域。SBDD 基于受体蛋白的结构设计与之适配的分子，是药物化学中的重要方法，而在深度学习辅助下的 SBDD 也将大大提升药物设计的效率，但目前这一方向还存在两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用于训练模型的「靶点-药物分子对」有限；&lt;/li&gt;
&lt;li&gt;SBDD AI 模型还不能很好利用靶点活性口袋的 3 维信息。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;针对以上两个问题，文章首先使用分子数据预训练 Transformer 生成模型，使其学习到分子数据中更通用的特征，避免标签不足；其次，文章设计了一种变种的 Transformer encoder，通过 encoder 获得氨基酸序列中的 3 维结构信息，文章将最后得到的模型称为 TamGent（Target-aware molecule generator with Transformer）。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;文章使用来自于 PubChem 数据库中的 1000 万个分子的 SMILES 序列预训练用于分子生成的 Transformer decoder 模型，使用来源于文献（&lt;a href="https://arxiv.org/abs/2205.07249" rel="noopener" target="_blank"&gt;Luo et al.&lt;/a&gt;）的 12.3 万个靶点-配体对训练配体生成模型。&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8650?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8650?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;使用 &lt;span class="math"&gt;\(\boldsymbol{a}=(a_1,a_2,\cdots,a_N)\)&lt;/span&gt; 表示氨基酸序列，其中 &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; 为长度为 20 的 ont-hot 向量，可以用于表示 20 种氨基酸，使用 &lt;span class="math"&gt;\(\boldsymbol{r}=(r_1,r_2,\cdots,r_N)\)&lt;/span&gt; 表示相应的 3 维坐标，其中 &lt;span class="math"&gt;\(r_i\in\mathbb{R}^3\)&lt;/span&gt;。将配体分子的 SMILES 编码转化为向量 &lt;span class="math"&gt;\(\boldsymbol{y}=(y_1,y_2,\cdots,y_M)\)&lt;/span&gt;，那么模型训练的目标就是学习从 &lt;span class="math"&gt;\(\boldsymbol{x}=(\boldsymbol{a},\boldsymbol{r})\)&lt;/span&gt; 到 &lt;span class="math"&gt;\(\boldsymbol{y}\)&lt;/span&gt; 的映射。&lt;/p&gt;
&lt;p&gt;TamGent 的架构参考了变分自编码器的工作模式，也就是主要由活性口袋 encoder 和配体分子 decoder 构成，encoder 与 decoder 都使用了 Transformer 中的结构。&lt;/p&gt;
&lt;p&gt;配体分子 decoder 部分与 Transformer 完全一样，具有 self-attention 机制，能够根据生成的 toekn 生成下一个 token，完成分子生成，因此使用 1000 万个分子数据预训练该模型，使其能够根据数据集中分子的普遍特征生成分子。&lt;/p&gt;
&lt;p&gt;活性口袋 encoder 部分修改了其中的 attention 机制，文章中称为 distance-aware attention。具体来说，就是认为距离较远的氨基酸与配体的相互作用更小，所以将输入的氨基酸序列和坐标转化为特征矩阵后，再与 &lt;span class="math"&gt;\(\exp(-\mathrm{distance}^2/\tau)\)&lt;/span&gt; 相乘，距离越远的氨基酸的权重就会越小。&lt;/p&gt;
&lt;p&gt;在推断过程中，将氨基酸序列及其坐标输入模型，embedding 为特征矩阵后进入活性口袋 encoder 部分计算 distance-aware attention，得到活性口袋的表示，最后将其作为配体分子 decoder 部分中的 pocket-SMILES attention，生成分子得到预测的活性配体结果。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun_1"&gt;结果与讨论&lt;/h2&gt;
&lt;h3 id="sheng-cheng-fen-zi-jie-guo"&gt;生成分子结果&lt;/h3&gt;
&lt;p&gt;文章使用 DrugBank 数据库中 1641 个靶点-配体对的数据用于测试模型效果，随机抽取其中的 100 个靶点-配体对，使用 TamGent、3DGen 和 SECSE 三种模型针对每个靶点生成 20 个分子，对比生成效果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8651?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8651?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;三种模型生成分子的对接打分中，TamGent 生成的分子明显更低，说明分子与靶点具有更好的亲和力，同时其平均值也与标签数据最为接近。对比三种模型生成的分子与标签分子的相似性，同样是 TamGent 具有更大的相似性，生成的分子最接近标签分子。同时，在 QED、MD 和 SA 几项的分子指标上，TamGent 也都高于其他两种模型，以上几点可以表明 TamGent 在 DrugBank 数据上根据靶点生成分子具有明显的优势。&lt;/p&gt;
&lt;h3 id="an-li-yan-jiu"&gt;案例研究&lt;/h3&gt;
&lt;p&gt;接下来文章使用 TamGent 针对于具体的靶点生成配体，分析模型表现。文章选择 SARS-CoV-2 主糖蛋白酶（&lt;em&gt;M&lt;/em&gt; &lt;sup&gt;pro&lt;/sup&gt;）作为靶点生成分子，收集了 415 个高分辨率结构后，使用模型生成了 4563 个分子，其中找到了先前报道过的一种 &lt;em&gt;M&lt;/em&gt; &lt;sup&gt;pro&lt;/sup&gt; 候选抑制剂（GC-376）和 6 个可能的先导化合物片段。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8652?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8652?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;上图展示了相应分子的二维分布，其中灰色表示在 PubChem 中随机选取的 3 万个分子，蓝色表示 TamGent 生成的分子，黄色表示在先前报道中提到了可能的 &lt;em&gt;M&lt;/em&gt; &lt;sup&gt;pro&lt;/sup&gt; 抑制剂。&lt;/p&gt;
&lt;p&gt;明显可以看出 TamGent 生成的分子与随机选取的分子具有不同的分布并且成簇聚集，主要分为 ① 和 ② 两簇。在第 ① 簇中，生成分子与 GC-376 的谷本相似度达到 0.82，并且此前报道的 6 种候选抑制剂都位于该簇中。但在第 ② 簇中没有找到对接分数较好的分子，只在分子中找到了一些可能的活性片段。&lt;/p&gt;
&lt;p&gt;最后文章选出了第 ① 簇中两个结构不同的分子与 &lt;em&gt;M&lt;/em&gt; &lt;sup&gt;pro&lt;/sup&gt; 对接，两种分子都能很好地填充活性口袋，对接分数分别为 -10.2 和 -9.5，而先前的 GC-376 是 -9.4，说明 TamGent 能够根据口性口袋生成具有良好活性的分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8653?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8653?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun_1"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章参考变分自编码器的结构使用 Transformer 构建了一种能够获取受体 3 维信息的分子生成模型 TamGent，其中分子生成部分使用了预训练模型，避免模型依赖于有限的「靶点-药物分子对」。在分子生成任务中，TamGent 生成分子的效果优于以往的两种模型，使用 TamGent 针对 SARS-CoV-2 主糖蛋白酶生成活性分子，甚至找到了比先前报道的候选抑制剂具有更好对接打分的分子，表现出 TamGent 的优异性能。&lt;/p&gt;
&lt;p&gt;对于 TamGent，文章提出了 3 点改进措施，第一是使用更多实验测试得到的「靶点-药物分子对」进一步优化模型，第二是在模型中整合考虑 ADMET 待药理性质，第三是在具体靶点上微调模型，使其帮助提升针对相应靶点的药物研发效率。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category><category term="VAE"></category></entry><entry><title>文献总结｜MolGPT：使用 Transformer 解码器模型实现分子生成</title><link href="https://leonis.cc/sui-sui-nian/2023-03-03-summary-doi.org/10.1021/acs.jcim.1c00600.html" rel="alternate"></link><published>2023-03-03T00:00:00+08:00</published><updated>2023-03-03T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-03-03:/sui-sui-nian/2023-03-03-summary-doi.org/10.1021/acs.jcim.1c00600.html</id><summary type="html">&lt;p&gt;本文介绍于 2022 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 MolGPT: Molecular Generation Using a Transformer-Decoder Model，在 GPT 模型已经在自然语言处理领域得到了成功应用的背景下，这篇文章首次将 GPT 模型应用于完成分子生成的任务，实现了分子性质和结构两个方面的优化。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/acs.jcim.1c00600" rel="noopener" target="_blank"&gt;doi.org/10.1021/acs.jcim.1c00600&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2022 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 MolGPT: Molecular Generation Using a Transformer-Decoder Model，在生成预训练（Generative Pre-training, GPT）模型已经在自然语言处理领域得到了成功应用的背景下，这篇文章首次将 GPT 模型应用于完成分子生成的任务，实现了分子性质和结构两个方面的优化。&lt;/p&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;h3 id="shu-ju"&gt;数据&lt;/h3&gt;
&lt;p&gt;分子数据来自于 MOSES 和 GuacaMol 的数据集，其中包括源于 Zinc 的 190 万个类先导化合物与源于 ChEMBL 的 160 万个分子，分子为 SMILES 形式，使用 RDKit 提取分子的骨架用于模型训练。&lt;/p&gt;
&lt;p&gt;此外，使用 RDKit 计算出分子的logP、SA、拓扑极性表面积（TPSA）和 QED，用于训练具有性质约束的模型。&lt;/p&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8625?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8625?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在得到原始分子的分子骨架与性质信息后，将分子性质与分子骨架序列连接在一起，称为「条件」，那么原始分子就成为需要 MolGPT 根据条件生成的「目标分子」。&lt;/p&gt;
&lt;p&gt;在训练过程中，将条件和目标分子序列一同送入 MolGPT，使模型建立条件与目标分子的关系。GPT 模型通过顺序读取每个 token，由当前 token 预测下一个 token，从而获得采样的权重。&lt;/p&gt;
&lt;p&gt;具体来说，分子 SMILES 词嵌入为 256 维的向量后，将性质条件和骨架条件也分别词嵌入为 256 维的向量，将其直接拼接在 SMILES 向量的起始端，就构成了实际输入模型的信息。&lt;/p&gt;
&lt;p&gt;在推理过程中，模型对训练集中的所有 token 根据权重随机取样得到第一个字符，接着模型就根据输入的条件（即目标性质与需要改造的分子骨架）和第一个字符生成下一个字符，直至生成整个分子。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8626?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8626?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在 Transformer 中，encoder 模块对输入编码得到状态向量 &lt;span class="math"&gt;\(c\)&lt;/span&gt;，再由 decoder 模块对状态 &lt;span class="math"&gt;\(c\)&lt;/span&gt; 解码并运算产生输出，由于输入的情况是多种多样的，将其转化为等长的 &lt;span class="math"&gt;\(c\)&lt;/span&gt; 就有很大的局限性。GPT 模型减去了 Transformer 中的 encoder 模块，而保留了 Transformer 中例如 self-attention 在内的其他机制，具有更好的长文本处理能力。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun_1"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8632?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8632?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先使模型在不给定条件的情况下生成分子，即 MolGPT 根据从训练数据集中学习到的化学空间中的分子分布生成生成分子，生成的分子与训练集分子具有相似的特征。&lt;/p&gt;
&lt;p&gt;文章分析了 MolGPT 生成一个分子的过程，上图中的黑色横线表示当前步骤生成的字符（token），其他 token 上颜色的深浅表示了与生成该 token 之间的权重关系。&lt;/p&gt;
&lt;p&gt;可以看出，MolGPT 首先从已经学习到的分布中随机抽取出 &lt;code&gt;C&lt;/code&gt;，接着根据它继续生成后续 token，每个 token 都是由先前生成的 token 决定。同时还可以发现 MolGPT 在生成分子的过程中具有一定的「化学知识」，例如第一行中生成 &lt;code&gt;O&lt;/code&gt; 时，明显由前面的 &lt;code&gt;=&lt;/code&gt; 与 &lt;code&gt;N&lt;/code&gt; 决定，所以 MolGPT 不仅能够在双键上连接氧原子，还会构建酰胺结构使分子更稳定。&lt;/p&gt;
&lt;h3 id="xing-zhi-tiao-jian"&gt;性质条件&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8627?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8627?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;文章接着只使用性质条件作为输入分子的条件，测试模型是否能按照要求生成满足约束的分子，结果如上图所示，与训练集中分子的性质分布不同，生成的分子集中在设定的性质条件（黑线）两侧，评估生成分子，各组分子的 validity、unique 和 novelty 都在0.97 以上，具有很好的效果。&lt;/p&gt;
&lt;p&gt;此外文章还同时使用多种性质约束作为模型条件，在保持较高的 validity、unique 和 novelty 条件下，生成的分子散落在设置性质条件的周围，所以模型对也能很好地处理多性质约束的分子生成。&lt;/p&gt;
&lt;h3 id="xing-zhi-yu-gu-jia-tiao-jian"&gt;性质与骨架条件&lt;/h3&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8628?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8628?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;接下来文章测试了将性质与分子骨架同时作为模型的生成条件，分析了生成分子的性质分布以及生成分子与设定分子骨架的谷本相似度。与仅性质条件的结果相比，额外加入分子骨架条件后生成分子的性质虽然有一些偏移（如上图 g），但仍然能大致满足性质约束。同时生成的分子与设定的分子骨架具有极高的相似性，从这两个可以证明 MolGPT 可以用于对给定的分子骨架进行指定性质的优化。&lt;/p&gt;
&lt;p&gt;最后文章展示了使用 MolGPT 实现分子骨架优化的例子：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8631?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8631?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun_1"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章使用 GPT 构建了分子生成模型 MolGPT，MolGPT 生成的分子具有很高的 validity 和 uniqueness，在对 MolGPT 生成分子过程中的权重分析发现，MolGPT 能够很好学习到 SMILES 中所包含的化学语义。在实际应用上，MolGPT 可以根据指定的多种分子性质和（或）指定的分子骨架生成目标的分子，生成的分子能够很好满足预先设定的要求，有助于指导化合物优化的方向。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Transformer"></category><category term="GPT"></category></entry><entry><title>文献总结｜在大数据集中有效识别匹配分子对（MMPs）的算法</title><link href="https://leonis.cc/sui-sui-nian/2023-02-25-summary-doi.org/10.1021/ci900450m.html" rel="alternate"></link><published>2023-02-25T00:00:00+08:00</published><updated>2023-02-25T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:leonis.cc,2023-02-25:/sui-sui-nian/2023-02-25-summary-doi.org/10.1021/ci900450m.html</id><summary type="html">&lt;p&gt;本文介绍于 2010 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Computationally Efficient Algorithm to Identify Matched Molecular Pairs (MMPs) in Large Data Sets，文章介绍了一种在大规模数据中识别匹配分子对的算法，这种算法也就是目前用于生成匹配分子对的最常用方法。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; &lt;a href="https://doi.org/10.1021/ci900450m" rel="noopener" target="_blank"&gt;doi.org/10.1021/ci900450m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍于 2010 年发表在 &lt;em&gt;Journal of Chemical Information and Modeling&lt;/em&gt; 上的一篇文章，文章原标题为 Computationally Efficient Algorithm to Identify Matched Molecular Pairs (MMPs) in Large Data Sets，文章介绍了一种在大规模数据中识别匹配分子对的算法，这种算法也就是目前用于生成匹配分子对的最常用方法。&lt;/p&gt;
&lt;p&gt;匹配分子对（Matched Molecular Pairs, MMP）是指化合物 A 与化合物 B 之间只在单一位置具有结构差异的一对分子，所以 MMP 中的分子可以很容易地从一个化合物改变为另一个化合物。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8602?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8602?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;在面对先导化合物优化等问题时，传统且主流的做法仍然是依靠化学团队的经验。然而目前已经拥有化合物优化的海量数据，如果能得到其中蕴藏的经验和知识，将给予化学家很大的帮助并加快化合物优化的效率。MMP 就提供了从一批分子中获取化合物优化知识的一种方法，所以针对于识别分子数据集中 MMP 的算法，文章提出了两个要求：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;应当能识别数据集中所有的 MMP；&lt;/li&gt;
&lt;li&gt;计算效率高，能够适用于具有大量分子的数据集。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="fang-fa"&gt;方法&lt;/h2&gt;
&lt;p&gt;文章中提出的识别 MMP 算法可以归纳为以下几步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每次在一个化合中一处两个非氢原子间的非环单键处切断，在每个切断位点执行该操作，直至将数据集中所有分子分割为所有可能的片段。&lt;/li&gt;
&lt;li&gt;每次切断后，都要索引两个片段。具体来说，分子 &lt;span class="math"&gt;\(\rm{A&amp;mdash;B}\)&lt;/span&gt; 切断为 &lt;span class="math"&gt;\(\rm{A*}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\rm{* B}\)&lt;/span&gt; 后，需要建立 &lt;span class="math"&gt;\(\{\rm{key:value}\}\)&lt;/span&gt; 的索引，即 &lt;span class="math"&gt;\(\{\rm{A*:* B}\}\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\{\rm{* B:A*}\}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;下一次切断得到的片段若与已建立索引的 &lt;span class="math"&gt;\(\rm{key}\)&lt;/span&gt; 相同，就更新到相应的字典中。例如又得到了 &lt;span class="math"&gt;\(\rm{A* }\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\rm{* C}\)&lt;/span&gt;，那么索引就会更新为 &lt;span class="math"&gt;\(\{\rm{A*:* B,* C}\}\)&lt;/span&gt;，新增 &lt;span class="math"&gt;\(\{\rm{* C: A*}\}\)&lt;/span&gt;，保留 &lt;span class="math"&gt;\(\{\rm{* B: A*}\}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;在处理完所有分子后，相同 &lt;span class="math"&gt;\(\rm{key}\)&lt;/span&gt; 索引中的片段就揭示了 MMP 的化学转化，例如 &lt;span class="math"&gt;\(\rm{*B\rightarrow*C}\)&lt;/span&gt;，将键值对重新组合后同时也得到了目标的 MMP，例如 &lt;span class="math"&gt;\([\rm{A&amp;mdash;B,A&amp;mdash;C}]\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这种方法还可以推广到在同一分子中多次切断，识别单次切断可能会遗漏的 MMP。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8603?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8603?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;对于化学家而言，例如 &lt;span class="math"&gt;\(\rm{*B\rightarrow*C}\)&lt;/span&gt; 的转化规则，若该结构太大，那么通过这种转化得到的 MMP 也是没有实际意义的。所以为了避免用于替换的子结构过大，还可以设定只切断不超过 n 个非氢原子的子结构。&lt;/p&gt;
&lt;h2 id="jie-guo-yu-tao-lun"&gt;结果与讨论&lt;/h2&gt;
&lt;p&gt;文章所使用的分子数据库包含有 333332 个化合物，通过该算法使用单核 CPU 经过约 850 min 后，得到了 5310964 对 MMP。&lt;/p&gt;
&lt;p&gt;在得到的 MMP 中也存在一些问题，例如羧酸到酰胺是一类常见的结构转化，但在得到的结果中，羧酸到酰胺的转化的结果也会混入到醇到胺的转化中，因为这两种结构转化从分子表示上来看都是将羟基替换为氨基，而这两种结构转化的性质截然不同，就会导致下游的分子性质预测、结构改造任务出现问题。&lt;/p&gt;
&lt;p&gt;一种妥善的解决方法就是在切断步骤中，不切断官能团间的单键，保证 MMP 中的分子都具有相同的官能团，不互相混淆。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8604?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8604?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;另一个问题是一些化学转化是在环结构上进行结构改造，由于切断的操作只会在单键之间执行，所以对于这一类的化学转化，MMP 必须找到单键切下整个环结构。而对于一些稠环结构，很容易就超出了设定的 n 个非氢原子的约束，无法获得这一类化学转化。&lt;/p&gt;
&lt;p&gt;一个改进的方向就是设定若切下的子结构为纯粹的环结构，那么不计算原子数量直接建立索引，这样既获得到环结构上的化学转化，又避免了混入超过 n 原子的无意义转化。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8605?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8605?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;最后，文章只将在一个分子中切断 1 次的操作推广到了 3 次。实验结果发现，在一个分子中切断 3 次后，就可以找到数据集中几乎全部的 MMP，切断次数越多，效果提升越少，计算开销越大，所以在实际使用中，3 次切断就已经是可行的。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8606?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"&gt;&lt;img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8606?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="jie-lun"&gt;结论&lt;/h2&gt;
&lt;p&gt;文章提出了一种适用于在较大的分子数据集中识别 MMP 的算法，这种算法能够帮助人们从过去所累积的分子改造数据中获取化学改造的知识，结合分子的各种实际评估数据，这种方法能够更全面地构建出药物分子的构效关系，解决实际中先导化物优化方面的问题。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="Literature Summary"></category><category term="CADD"></category><category term="Algorithm"></category></entry></feed>