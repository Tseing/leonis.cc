<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Leo's blog</title><link href="https://tseing.github.io/" rel="alternate"></link><link href="https://tseing.github.io/feed.xml" rel="self"></link><id>https://tseing.github.io/</id><updated>2022-09-16T00:00:00+08:00</updated><subtitle>A nook to hoard my manuscripts.</subtitle><entry><title>《统计学习方法》第七章：支持向量机</title><link href="https://tseing.github.io/sui-sui-nian/2022-09-16-statistical-learning-chapter7.html" rel="alternate"></link><published>2022-09-16T00:00:00+08:00</published><updated>2022-09-16T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-09-16:/sui-sui-nian/2022-09-16-statistical-learning-chapter7.html</id><summary type="html">&lt;p&gt;《统计学习方法》第五章介绍了支持向量机模型以及支持向量机模型在处理线性可分、线性近似可分、线性不可分数据时的应用。支持向量机的学习过程同样采用对偶算法，此外还有 SMO 算法，大大提高了运算速度。&lt;/p&gt;</summary><content type="html">&lt;p&gt;支持向量机是一种与感知机相似的二分类模型，但感知机的学习策略仅仅是使线性可分的两类实例区分开来，而支持向量机使用的是间隔最大化策略。间隔最大化使支持向量机不仅能完成二分类任务，同时使支持向量机具有更加良好的可信度和预测功能。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;训练数据&lt;/th&gt;
&lt;th&gt;策略&lt;/th&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;线性可分&lt;/td&gt;
&lt;td&gt;硬间隔最大化&lt;/td&gt;
&lt;td&gt;线性可分支持向量机&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;近似线性可分&lt;/td&gt;
&lt;td&gt;软间隔最大化&lt;/td&gt;
&lt;td&gt;线性支持向量机&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;线性不可分&lt;/td&gt;
&lt;td&gt;核技巧、软间隔最大化&lt;/td&gt;
&lt;td&gt;非线性支持向量机&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="xian-xing-ke-fen-zhi-chi-xiang-liang-ji"&gt;线性可分支持向量机&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;支持向量机的模型与感知机类似，分离超平面为&lt;/p&gt;
&lt;div class="math"&gt;$$w^*\cdot x+b^*=0$$&lt;/div&gt;
&lt;p&gt;分类决策函数为&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=\mathrm{sign}(w^*\cdot x+b^*)$$&lt;/div&gt;
&lt;h3 id="ce-lue"&gt;策略&lt;/h3&gt;
&lt;h4&gt;函数间隔与几何间隔&lt;/h4&gt;
&lt;p&gt;实例到超平面的距离能&lt;em&gt;相对地&lt;/em&gt;用 &lt;span class="math"&gt;\(|w\cdot x+b|\)&lt;/span&gt; 衡量，称为函数间隔。样本点到超平面的函数间隔记作 &lt;span class="math"&gt;\(\hat{\gamma}_i\)&lt;/span&gt;，将数据集 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 中所有样本点函数间隔的最小值称为超平面关于数据集 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 的函数间隔，记作 &lt;span class="math"&gt;\(\hat{\gamma}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;函数间隔会随着超平面参数 &lt;span class="math"&gt;\(w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b\)&lt;/span&gt; 的改变而改变，但若 &lt;span class="math"&gt;\(w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b\)&lt;/span&gt; 等比例缩放，超平面没有变化（等式左右可同时约去比例），样本点到超平面距离没有变化，而函数间隔变化了。&lt;/p&gt;
&lt;p&gt;这说明需要将函数间隔规范化，也就得到了几何间隔 &lt;span class="math"&gt;\(\frac{w}{||w||}\)&lt;/span&gt;，这也就是样本点到超平面的实际（几何）距离，记作 &lt;span class="math"&gt;\(\gamma_i\)&lt;/span&gt;，类似地，超平面关于数据集的几何间隔记作 &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;，得到转化公式&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \gamma_i=\frac{\hat{\gamma}_i}{||w||}\\
    \gamma=\frac{\hat{\gamma}}{||w||}
\end{align}$$&lt;/div&gt;
&lt;h4&gt;间隔最大化&lt;/h4&gt;
&lt;p&gt;不同于感知机，间隔最大化的策略不仅用超平面将两类样本点分开，还要使不同类别的样本点的几何距离超平面最大，这样的做法使得超平面有足够的确信度将两类样本分开。&lt;/p&gt;
&lt;p&gt;再回忆一下感知机，感知机仅仅将线性可分的样本点分开，因此运算过程中取样本点的顺序不同，会得到不同的结果，当然这些不同的结果都能分开两类样本。但支持向量机采用了间隔最大化策略，几何间隔最大的分离超平面是唯一的，最后也就得到唯一且最优的模型。&lt;/p&gt;
&lt;p&gt;间隔最大化策略使得分离超平面的确定只依赖于最靠近超平面的样本点，这些实例点就称为支持向量。&lt;/p&gt;
&lt;p&gt;根据间隔最大化的思路，可以得到以下最优化问题：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \max_{w,b}&amp;amp;\quad \frac{\hat{\gamma}}{||w||}\\
    \mathrm{s.t.}&amp;amp;\quad y_i\frac{w\cdot x+b}{||w||}\geqslant \frac{\hat{\gamma}}{||w||}
\end{align}$$&lt;/div&gt;
&lt;p&gt;等比缩放 &lt;span class="math"&gt;\(w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b\)&lt;/span&gt; 将得到 &lt;span class="math"&gt;\(\lambda \hat{\gamma}\)&lt;/span&gt;，但超平面没有改变，几何间隔也没有改变，也就是说只需要考虑 &lt;span class="math"&gt;\(\frac{1}{||w||}\)&lt;/span&gt;，略去 &lt;span class="math"&gt;\(\hat{\gamma}\)&lt;/span&gt; 得到&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \max_{w,b}&amp;amp;\quad \frac{1}{||w||}\\
    \mathrm{s.t.}&amp;amp;\quad y_i(w\cdot x+b)\geqslant 1
\end{align}$$&lt;/div&gt;
&lt;p&gt;将该最优化问题转化为最小化问题：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \min_{w,b}&amp;amp;\quad \frac{1}{2}||w||^2\\
    \mathrm{s.t.}&amp;amp;\quad y_i(w\cdot x+b)-1\geqslant 0
\end{align}$$&lt;/div&gt;
&lt;p&gt;{note begin}最大化 &lt;span class="math"&gt;\(\frac{1}{||w||}\)&lt;/span&gt; 等价与最小化 &lt;span class="math"&gt;\(||w||^2\)&lt;/span&gt;，当然前面的常数项更是无所谓的。{note end}&lt;/p&gt;
&lt;h3 id="suan-fa"&gt;算法&lt;/h3&gt;
&lt;h4&gt;原始算法&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;算法 7.1&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：线性可分的数据集&lt;br/&gt;
输出：最大间隔分离超平面和分离决策函数&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;构造求解最优化问题得到最优解 &lt;span class="math"&gt;\(w^*\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b^*\)&lt;/span&gt;（解不等式组）；
    &lt;div class="math"&gt;$$\begin{align}
        \min_{w,b}&amp;amp;\quad \frac{1}{2}||w||^2\\
        \mathrm{s.t.}&amp;amp;\quad y_i(w\cdot x+b)-1\geqslant 0
    \end{align}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;得到分离超平面与决策函数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;对偶算法&lt;/h4&gt;
&lt;p&gt;对偶算法同样依赖于 Lagrange 函数（见&lt;a href="https://tseing.github.io/sui-sui-nian/2022-09-09-statistical-learning-chapter6.html#yan-yi-la-ge-lang-ri-han-shu"&gt;第六章&lt;/a&gt;），构造 lagrange 函数：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    L(w,b,\alpha)&amp;amp;=\frac{1}{2}||w||^2-\sum_i\alpha_i[y_i(w\cdot x+b)-1]\\
    &amp;amp;=\frac{1}{2}||w||^2-\sum_i\alpha_iy_i(w\cdot x+b)+\sum_i\alpha_i
\end{align}$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;求对偶问题的极小 &lt;span class="math"&gt;\(\min_{w,b}L(w,b,\alpha)\)&lt;/span&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;求 Lagrange 函数对 &lt;span class="math"&gt;\(w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b\)&lt;/span&gt; 的偏导并令其为零：&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial L}{w}=w-\sum_i\alpha_iy_ix_i=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\frac{\partial L}{b}=-\sum_i\alpha_iy_i=0$$&lt;/div&gt;
&lt;p&gt;将 &lt;span class="math"&gt;\(w=\sum_i\alpha_iy_ix_i\)&lt;/span&gt; 代入 Lagrange 函数，为简洁起见，先只考虑 &lt;span class="math"&gt;\(\frac{1}{2}||w||^2\)&lt;/span&gt; 一项：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \frac{1}{2}||w||^2&amp;amp;=\frac{1}{2}x^\mathrm{T}\cdot x\\
    &amp;amp;=\frac{1}{2}\left(\sum_i\alpha_iy_ix_i\right)\cdot\left(\sum_j\alpha_jy_jx_j\right)\\
    &amp;amp;=\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)
\end{align}$$&lt;/div&gt;
&lt;p&gt;再考虑到 &lt;span class="math"&gt;\(b\sum_i\alpha_iy_i=0\)&lt;/span&gt; 那么 Lagrange 函数应当为&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    L(w,b,\alpha)&amp;amp;=\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_i\alpha_iy_i\left(\sum_j\alpha_jy_jx_j\cdot x_i\right)+\sum_i\alpha_i\\
    &amp;amp;=-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_i\alpha_i
\end{align}$$&lt;/div&gt;
&lt;p&gt;对偶问题的极小也就是 &lt;span class="math"&gt;\(-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_i\alpha_i\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;求对偶问题极小的极大 &lt;span class="math"&gt;\(\max_{\alpha}\min_{w,b}L(w,b,\alpha)\)&lt;/span&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \max_{\alpha}&amp;amp;\quad -\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_i\alpha_i\\
    \mathrm{s.t.}&amp;amp;\quad \sum_i\alpha_iy_i=0,\ \alpha_i\geqslant0
\end{align}$$&lt;/div&gt;
&lt;p&gt;转化为极小问题&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \min_{\alpha}&amp;amp;\quad \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_i\alpha_i\\
    \mathrm{s.t.}&amp;amp;\quad \sum_i\alpha_iy_i=0,\ \alpha_i\geqslant0
\end{align}$$&lt;/div&gt;
&lt;p&gt;假设该问题的解为 &lt;span class="math"&gt;\(\alpha^*=(\alpha^*_1,\alpha^*_2,\cdots,\alpha^*_N)^\mathrm{T}\)&lt;/span&gt;，那么支持向量机的参数（从 KKT 条件导出）为&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    &amp;amp;w^*=\sum_i\alpha^*_iy_ix_i\\
    &amp;amp;b^*=y_j-\sum_i\alpha^*_iy_i(x_i\cdot x_j)
\end{align}$$&lt;/div&gt;
&lt;p&gt;可以看出，若 &lt;span class="math"&gt;\(\alpha_i=0\)&lt;/span&gt;，参数与该分量无关，也就是说该分量所对应的样本点不影响支持向量机。从另一方面看，支持向量机只与 &lt;span class="math"&gt;\(\alpha_i&amp;gt;0\)&lt;/span&gt; 对应的样本点有关，这些样本点就是支持向量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 7.2&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：线性可分的数据集&lt;br/&gt;
输出：最大间隔分离超平面和分离决策函数&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;构造并求解问题得到 &lt;span class="math"&gt;\(\alpha^*\)&lt;/span&gt;
&lt;div class="math"&gt;$$\begin{align}
        \min_{\alpha}&amp;amp;\quad \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_i\alpha_i\\
        \mathrm{s.t.}&amp;amp;\quad \sum_i\alpha_iy_i=0,\ \alpha_i\geqslant0
    \end{align}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;用 &lt;span class="math"&gt;\(\alpha^*\)&lt;/span&gt; 计算 &lt;span class="math"&gt;\(w^*\)&lt;/span&gt;，用 &lt;span class="math"&gt;\(\alpha^*\)&lt;/span&gt; 的正分量计算 &lt;span class="math"&gt;\(b^*\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;得到分离超平面与决策函数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;对偶算法案例&lt;/h4&gt;
&lt;p&gt;在算法 7.2 的第 1 步中，需要求解 &lt;span class="math"&gt;\(\alpha^*\)&lt;/span&gt;，这里容易令人困惑，以书中的例子说明计算方法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;正例点为 &lt;span class="math"&gt;\(x_1=(3,3)^\mathrm{T}\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(x_2=(4,3)^\mathrm{T}\)&lt;/span&gt;，负例点为 &lt;span class="math"&gt;\(x_3=(1,1)^\mathrm{T}\)&lt;/span&gt;，求线性可分支持向量机。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;先计算样本点的 Gram 矩阵，以便后续计算：&lt;/p&gt;
&lt;div class="math"&gt;$$G=\begin{bmatrix}
    18 &amp;amp;21 &amp;amp;6\\
    21 &amp;amp;25 &amp;amp;7\\
    6  &amp;amp;7  &amp;amp;2
\end{bmatrix}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\begin{align}
     \min_{w,b}&amp;amp;\quad \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_i\alpha_i\\
     &amp;amp;=\frac{1}{2}(18\alpha^2_1+25\alpha^2_2+2\alpha^2_3+42\alpha_1\alpha_2-12\alpha_1\alpha_3-14\alpha_2\alpha_3)-\alpha_1-\alpha_2-\alpha3\\
     \mathrm{s.t.}&amp;amp;\quad \alpha_1+\alpha_2-\alpha_3=0,\ \alpha_i\geqslant0
 \end{align}$$&lt;/div&gt;
&lt;p&gt;为了求解这一最优化问题，需要将约束代入目标问题，得到&lt;/p&gt;
&lt;div class="math"&gt;$$s(\alpha_1,\alpha_2)=4\alpha^2_1+\frac{13}{2}\alpha^2_2+10\alpha_1\alpha_2-2\alpha_1-2\alpha_2$$&lt;/div&gt;
&lt;p&gt;求其偏导并令其为零，得知 &lt;span class="math"&gt;\(s(\alpha_1,\alpha_2)\)&lt;/span&gt; 在 &lt;span class="math"&gt;\((\frac{3}{2},-1)^\mathrm{T}\)&lt;/span&gt; 处取得极值，但 &lt;span class="math"&gt;\(\alpha_2=-1\)&lt;/span&gt; 违反了 &lt;span class="math"&gt;\(\alpha_i\geqslant0\)&lt;/span&gt; 的约束，那么&lt;em&gt;最小值将在边界上取到&lt;/em&gt;。&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    &amp;amp;a_1=0,\quad s(0,\frac{2}{13})=-\frac{2}{13}\\
    &amp;amp;a_2=0,\quad s(\frac{1}{4},0)=-\frac{1}{4}
\end{align}$$&lt;/div&gt;
&lt;p&gt;所以计算得到最终的 &lt;span class="math"&gt;\(\alpha^*=(\frac{1}{4},0,\frac{1}{4})^\mathrm{T}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2 id="xian-xing-zhi-chi-xiang-liang-ji_1"&gt;线性支持向量机&lt;/h2&gt;
&lt;h3 id="mo-xing_1"&gt;模型&lt;/h3&gt;
&lt;p&gt;线性可分支持向量机是线性支持向量机的特例，所以线性支持向量机的模型与线性可分支持向量机相同。在现实情况中，很难遇到标准的线性可分的数据，这时候就需要使用更为普遍的线性支持向量机。&lt;/p&gt;
&lt;h3 id="ce-lue_1"&gt;策略&lt;/h3&gt;
&lt;p&gt;线性可分数据集与近似线性可分数据集的差别在于，近似线性可分数据集中存在一些特异点，若将这些特异点去除，那么数据集就变成了线性可分的。&lt;/p&gt;
&lt;p&gt;特异点无法被正常分类的原因是特异点不能满足支持向量机的分类条件&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    &amp;amp;正例点：w\cdot x+b\geqslant1\\
    &amp;amp;负例点：w\cdot x+b\leqslant-1
\end{align}$$&lt;/div&gt;
&lt;p&gt;从几何上来看，也就是特异点与分离超平面的距离不够远，不能满足函数间隔大于等于 1，因此引入一个松驰变量 &lt;span class="math"&gt;\(\xi_i\geqslant0\)&lt;/span&gt;，使得特异点的函数间隔加上松驰变量大于等于 1，那么最优化问题的约束就变为&lt;/p&gt;
&lt;div class="math"&gt;$$y_i(w\cdot x_i+b)\geqslant1-\xi_i$$&lt;/div&gt;
&lt;p&gt;原来的目标函数改为&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{2}||w||^2+C\sum_i\xi_i$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(C&amp;gt;0\)&lt;/span&gt; 称为惩罚参数，目标函数使 &lt;span class="math"&gt;\(\frac{1}{2}||w||^2\)&lt;/span&gt; 尽可能小，也就是间隔尽量大；&lt;span class="math"&gt;\(\xi_i\)&lt;/span&gt; 尽可能小，也就是误分类的点（补偿的间隔）尽量少；&lt;span class="math"&gt;\(C\)&lt;/span&gt; 就是在二种策略间权衡的权重值，调和二者关系。&lt;/p&gt;
&lt;h3 id="suan-fa_1"&gt;算法&lt;/h3&gt;
&lt;h4&gt;原始算法&lt;/h4&gt;
&lt;p&gt;线性支持向量机的原始问题为&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
     \min_{w,b}&amp;amp;\quad \frac{1}{2}||w||^2+C\sum_i\xi_i\\
     \mathrm{s.t.}&amp;amp;\quad y_i(w\cdot x_i+b)\geqslant1-\xi_i,\ \xi_i\geqslant0
\end{align}$$&lt;/div&gt;
&lt;p&gt;线性支持向量机的原始问题与线性可分支持向量机也相似，求解该问题得到分离超平面与分类决策函数。&lt;/p&gt;
&lt;h4&gt;对偶算法&lt;/h4&gt;
&lt;p&gt;从原始问题中导出对偶问题，使用同样的步骤构造 Lagrange 函数，并求其极大极小：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
     \min_{\alpha}&amp;amp;\quad \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_i\alpha_i\\
     \mathrm{s.t.}&amp;amp;\quad \sum_i\alpha_iy_i=0,\ 0\leqslant\alpha_i\color{orangered}{\leqslant C}
\end{align}$$&lt;/div&gt;
&lt;p&gt;可以看出线性支持向量机的原始问题只是比线性可分支持向量机多了一个约束条件，因此最终导出的结果也是相似的。求解该对偶问题得到 &lt;span class="math"&gt;\(\alpha^*\)&lt;/span&gt;，求得支持向量机参数&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    &amp;amp;w^*=\sum_i\alpha^*_iy_ix_i\\
    &amp;amp;b^*=y_j-\sum_i\alpha^*_iy_i(x_i\cdot x_j)
\end{align}$$&lt;/div&gt;
&lt;p&gt;这里需要注意的是，由于存在 &lt;span class="math"&gt;\(0\leqslant\alpha_i\leqslant C\)&lt;/span&gt; 的约束条件，需要保证 &lt;span class="math"&gt;\(\alpha^*\)&lt;/span&gt; 中各分量满足这一约束，选择其中满足 &lt;span class="math"&gt;\(0&amp;lt;\alpha_i&amp;lt;C\)&lt;/span&gt; 条件的分量计算支持向量机参数，很容易明白，满足 &lt;span class="math"&gt;\(0&amp;lt;\alpha_i&amp;lt;C\)&lt;/span&gt; 条件分量所对应的样本点就是该模型中的支持向量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 7.3&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：数据集&lt;br/&gt;
输出：分离超平面和分离决策函数&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;构造并求解问题得到 &lt;span class="math"&gt;\(\alpha^*\)&lt;/span&gt;
&lt;div class="math"&gt;$$\begin{align}
        \min_{\alpha}&amp;amp;\quad \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_i\alpha_i\\
        \mathrm{s.t.}&amp;amp;\quad \sum_i\alpha_iy_i=0,\ 0\leqslant\alpha_i\leqslant C
    \end{align}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;用 &lt;span class="math"&gt;\(\alpha^*\)&lt;/span&gt; 计算 &lt;span class="math"&gt;\(w^*\)&lt;/span&gt;，用 &lt;span class="math"&gt;\(\alpha^*\)&lt;/span&gt; 中满足条件 &lt;span class="math"&gt;\(0&amp;lt;\alpha_i&amp;lt;C\)&lt;/span&gt; 的分量计算 &lt;span class="math"&gt;\(b^*\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;得到分离超平面与决策函数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="fei-xian-xing-zhi-chi-xiang-liang-ji_1"&gt;非线性支持向量机&lt;/h2&gt;
&lt;h3 id="mo-xing_2"&gt;模型&lt;/h3&gt;
&lt;p&gt;在实际情况中，常常还会得到非线性的数据集，这时候若尝试用一个超平面将两类实例区分开，会得到大量的误分类点，这样的模型没有很好的确信度和预测性能。超平面的模型是简单的，若使用更复杂一些的超曲面，通常能取得更好的效果。&lt;/p&gt;
&lt;p&gt;我用一个简单的例子说明这个问题，回忆中学时代的线性回归，也就是用一条直线来拟合一系列数据点，如果数据点是由二次函数产生的，断然是无法找到这条合适的直线的。此时需要将数据点经过变换，经过变换后，在另一空间中得到适合拟合的数据。&lt;/p&gt;
&lt;p&gt;&lt;img alt="核技巧" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7606?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;核技巧也是同样的思路，将不适合使用超平面分类的数据集变换到另一空间中，在该空间中使用超平面分类，就相当于在原空间中使用超曲面而非超平面分类。&lt;/p&gt;
&lt;p&gt;将这个从输入空间到特征空间的映射记作 &lt;span class="math"&gt;\(\phi(x)\)&lt;/span&gt;，使得输入空间中的所有 &lt;span class="math"&gt;\(x,z\)&lt;/span&gt; 满足&lt;/p&gt;
&lt;div class="math"&gt;$$K(x,z)=\phi(x)\cdot\phi(z)$$&lt;/div&gt;
&lt;p&gt;就称 &lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt; 为核函数。考虑对偶问题&lt;/p&gt;
&lt;div class="math"&gt;$$\min_{\alpha}\quad \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(\color{orangered}{x_i\cdot x_j})-\sum_i\alpha_i$$&lt;/div&gt;
&lt;p&gt;将核函数代入就得到非线性支持向量机的对偶问题&lt;/p&gt;
&lt;div class="math"&gt;$$\min_{\alpha}\quad \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\color{orangered}{K(x_i,x_j)}-\sum_i\alpha_i$$&lt;/div&gt;
&lt;p&gt;所以非线性支持向量机的分类决策函数就是&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=\mathrm{sign}\left(\sum_i\alpha^*_iy_iK(x,x_i)+b^*\right)$$&lt;/div&gt;
&lt;p&gt;核函数一般不用自己计算，常见的核函数有&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;核函数&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;多项式核函数&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(K(x,z)=(x\cdot z+1)^p\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;高斯核函数&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(K(x,z)=\exp\left(-\frac{\|x-z\|^2}{2\sigma^2}\right)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="suan-fa_2"&gt;算法&lt;/h3&gt;
&lt;p&gt;非线性支持向量机的算法与线性支持向量机无异，不过是需要预先选择合适的核函数，构造最优化问题&lt;/p&gt;
&lt;div class="math"&gt;$$\min_{\alpha}\quad \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\K(x_i,x_j)-\sum_i\alpha_i$$&lt;/div&gt;
&lt;p&gt;最后用同样的方法求解该最优化问题得到非线性支持向量机。&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references_1"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://book.douban.com/subject/33437381/"&gt;李航, 2019. 统计学习方法（第2版）. 清华大学出版社.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/433150785"&gt;保姆级笔记-详细剖析SMO算法中的知识点 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="统计学习方法"></category><category term="Machine learning"></category><category term="Algorithm"></category></entry><entry><title>《统计学习方法》第六章：逻辑斯谛回归与最大熵模型</title><link href="https://tseing.github.io/sui-sui-nian/2022-09-09-statistical-learning-chapter6.html" rel="alternate"></link><published>2022-09-09T00:00:00+08:00</published><updated>2022-09-09T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-09-09:/sui-sui-nian/2022-09-09-statistical-learning-chapter6.html</id><summary type="html">&lt;p&gt;《统计学习方法》第五章主要介绍逻辑斯谛回归模型与最大熵模型，这两种模型具有类似的对数结构，都利用了极大似然估计原理。本章还介绍了广义拉格朗日函数和拟牛顿法。&lt;/p&gt;</summary><content type="html">&lt;h2 id="mo-xing"&gt;模型&lt;/h2&gt;
&lt;h3 id="luo-ji-si-di-hui-gui-mo-xing"&gt;逻辑斯谛回归模型&lt;/h3&gt;
&lt;p&gt;逻辑斯谛分布具有良好的性质，能够将 &lt;span class="math"&gt;\((-\infty,+\infty)\)&lt;/span&gt; 映射至 &lt;span class="math"&gt;\((-1，+1)\)&lt;/span&gt;，因此选用逻辑斯谛分布作为回归模型。逻辑斯谛分布函数与密度函数为&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    F(x)&amp;amp;=P(\leqslant x)=\frac{1}{1+\mathrm{e}^{-(x-\mu)/\gamma}}\\
    f(x)&amp;amp;=F'(x)=\frac{\mathrm{e}^{-(x-\mu)/\gamma}}{\gamma(1+\mathrm{e}^{-(x-\mu)/\gamma})^2}
\end{align}$$&lt;/div&gt;
&lt;p&gt;逻辑斯谛分布函数与密度函数的图像分别如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="逻辑斯谛分布" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7554?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;将逻辑斯谛分布函数简化，可以得到 Sigmoid 函数：&lt;/p&gt;
&lt;div class="math"&gt;$$S(x)=\frac{1}{1+\mathrm{e}^{-x}}=\frac{\mathrm{e}^x}{1+\mathrm{e}^x}$$&lt;/div&gt;
&lt;p&gt;回忆二分类的感知机 &lt;span class="math"&gt;\(w\cdot x+b\)&lt;/span&gt;，超平面将实例分作 &lt;span class="math"&gt;\(w\cdot x+b\geqslant 0\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(w\cdot x+b&amp;lt; 0\)&lt;/span&gt; 两类。可以看出，&lt;span class="math"&gt;\(w\cdot x+b\)&lt;/span&gt; 的值域为实数域，那么就可以利用逻辑斯谛分布将实数域映射到 &lt;span class="math"&gt;\((-1，+1)\)&lt;/span&gt;，实现分类。&lt;/p&gt;
&lt;p&gt;为了表述简洁，令 &lt;span class="math"&gt;\(w=(w^{(1)},w^{(2)},\cdots,w^{(n)},b)^{\mathrm{T}}\)&lt;/span&gt;，&lt;span class="math"&gt;\(x=(x^{(1)},x^{(2)},\cdots,x^{(n)},1)^{\mathrm{T}}\)&lt;/span&gt;，将 &lt;span class="math"&gt;\(w\cdot x\)&lt;/span&gt; 代入 Sigmoid 函数：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    P(Y=1|x)&amp;amp;=\frac{\exp(w\cdot x)}{1+\exp(w\cdot x)}\\
    P(Y=0|x)&amp;amp;=1-P(Y=1|x)=\frac{1}{1+\exp(w\cdot x)}
\end{align}$$&lt;/div&gt;
&lt;p&gt;这就是二项逻辑斯谛回归模型。从式中也能看到，若线性函数 &lt;span class="math"&gt;\(w\cdot x\)&lt;/span&gt;越大，&lt;span class="math"&gt;\(P(Y=1|x)\)&lt;/span&gt; 概率越大；若线性函数 &lt;span class="math"&gt;\(w\cdot x\)&lt;/span&gt;越小，&lt;span class="math"&gt;\(P(Y=0|x)\)&lt;/span&gt; 概率越大。最后就通过对比 &lt;span class="math"&gt;\(P(Y=1|x)\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(P(Y=0|x)\)&lt;/span&gt; 的大小来确定实例的类别。&lt;/p&gt;
&lt;h3 id="zui-da-shang-mo-xing"&gt;最大熵模型&lt;/h3&gt;
&lt;p&gt;最大熵原理认为，熵最大的模型是最好的模型。这是一个十分在「直觉」的原理，例如说，在等待公交车时，下一辆公交车只有两种情况&amp;mdash;&amp;mdash;「乘」或「不乘」，基于这种判断，通常会认为下一辆公交车有 50% 的概率可乘，50% 的概率不可乘。&lt;/p&gt;
&lt;p&gt;再例如，某事件有 &lt;span class="math"&gt;\(\{A,B,C,D,E\}\)&lt;/span&gt; 5 种情况，相应满足约束：&lt;/p&gt;
&lt;div class="math"&gt;$$P(A)+P(B)+P(C)+P(D)+P(E)=1$$&lt;/div&gt;
&lt;p&gt;在没有更多信息的情况下，根据最大熵原理，我们会认为&lt;/p&gt;
&lt;div class="math"&gt;$$P(A)=P(B)=P(C)=P(D)=P(E)=\frac{1}{5}$$&lt;/div&gt;
&lt;p&gt;如果额外获得了信息 &lt;span class="math"&gt;\(P(A)+P(B)=\frac{3}{10}\)&lt;/span&gt;，那么根据最大熵原理就会认为&lt;/p&gt;
&lt;div class="math"&gt;$$P(A)=P(B)=\frac{3}{20},\ P(C)=P(D)=P(E)=\frac{7}{30}$$&lt;/div&gt;
&lt;p&gt;可以看出，在缺少信息的情况下，最大熵原理将那些不确定的部分都视作等可能。等概率表示了对于事实的无知，但是没有更多信息，这种判断又是合理的。&lt;/p&gt;
&lt;p&gt;对于训练数据集，可以得到经验分布 &lt;span class="math"&gt;\(\tilde{P}(X=x,Y=y)\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\tilde{P}(X=x)\)&lt;/span&gt;，这里的经验分布是用频数估计得到的。但是使用频数估计联合分布是不准确的，这种不准确在最大熵模型中会造成约束条件的不准确，而约束条件又是最大熵模型的关键。例如上例中，约束条件为 &lt;span class="math"&gt;\(P(A)+P(B)=\frac{3}{10}\)&lt;/span&gt;，若该约束条件不准确，后续的概率估计也是无意义的。&lt;/p&gt;
&lt;p&gt;因此在最大熵模型中需要引入特征函数，特征函数用于人为选取「合适」的实例：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}
    f(x,y)=
    \begin{cases}
        1, &amp;amp;x\ 与\ y\ 满足某事实\\
        0, &amp;amp;否则
    \end{cases}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;特征函数 &lt;span class="math"&gt;\(f(x,y)\)&lt;/span&gt; 关于经验分布 &lt;span class="math"&gt;\(\tilde{P}(X,Y)\)&lt;/span&gt; 的期望为&lt;/p&gt;
&lt;div class="math"&gt;$$E_{\tilde{P}}(f)=\sum_{x,y}\tilde{P}(x,y)f(x,y)$$&lt;/div&gt;
&lt;p&gt;特征函数 &lt;span class="math"&gt;\(f(x,y)\)&lt;/span&gt; 关于模型 &lt;span class="math"&gt;\(P(X|Y)\)&lt;/span&gt; 与经验分布 &lt;span class="math"&gt;\(\tilde{P}(X)\)&lt;/span&gt; 的期望为&lt;/p&gt;
&lt;div class="math"&gt;$$E_P(f)=\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)$$&lt;/div&gt;
&lt;p&gt;若模型准确，二者理应相等，即&lt;/p&gt;
&lt;div class="math"&gt;$$E_{\tilde{P}}(f)=E_P(f)$$&lt;/div&gt;
&lt;p&gt;{note begin}回忆条件概率基本公式 &lt;span class="math"&gt;\(P(x,y)=P(y|x)P(x)\)&lt;/span&gt;。{note end}&lt;/p&gt;
&lt;p&gt;满足所有约束条件的模型构成的集合为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{C}\equiv \{P\in\mathcal{P}|E_P(f_i)=E_{\tilde{P}}(f_i)\}$$&lt;/div&gt;
&lt;p&gt;在条件概率分布 &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt; 上的条件熵为&lt;/p&gt;
&lt;div class="math"&gt;$$H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)$$&lt;/div&gt;
&lt;p&gt;在集合 &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt; 中选出 &lt;span class="math"&gt;\(H(P)\)&lt;/span&gt; 最大的模型即为最大熵模型。&lt;/p&gt;
&lt;h2 id="ce-lue_1"&gt;策略&lt;/h2&gt;
&lt;h3 id="luo-ji-si-di-hui-gui-mo-xing_1"&gt;逻辑斯谛回归模型&lt;/h3&gt;
&lt;p&gt;设数据集中的概率为&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=1|x)=\pi(x),\ P(Y=0|x)=1-\pi(x)$$&lt;/div&gt;
&lt;p&gt;构造对数似然函数：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    L(w)&amp;amp;=\log\prod_{i}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}\\
    &amp;amp;=\sum_i[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\
    &amp;amp;=\sum_i[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i)]
\end{align}$$&lt;/div&gt;
&lt;p&gt;那么求 &lt;span class="math"&gt;\(L(w)\)&lt;/span&gt; 的极大值，就能得到估计值 &lt;span class="math"&gt;\(\hat{w}\)&lt;/span&gt;，得到回归模型。也就是说，求解逻辑斯谛回归模型就是对于对数似然函数的最优化问题。&lt;/p&gt;
&lt;p&gt;{note begin}似然函数定义为 &lt;span class="math"&gt;\(L(p)=\prod_i p^{x_i}(1-p)^{1-x_i}\)&lt;/span&gt;，即抽样结果中各概率之积。由于每次抽样独立同分布的前提，可以认为似然函数为抽样结果（该事件）发生的概率。因为已经得到了该抽样结果，该事件发生的概率理应为 1，所以就要使似然函数最大化，这就是&lt;em&gt;最大似然估计&lt;/em&gt;的原理。{note end}&lt;/p&gt;
&lt;h3 id="yan-yi-la-ge-lang-ri-han-shu"&gt;广义拉格朗日函数&lt;/h3&gt;
&lt;p&gt;最大熵模型中使用了拉格朗日乘数法，因此有必要先介绍一下广义拉格朗日函数。回忆一下高等数学中的拉格朗日函数，若函数 &lt;span class="math"&gt;\(z=f(x,y)\)&lt;/span&gt; 有约束条件 &lt;span class="math"&gt;\(\varphi(x,y)=0\)&lt;/span&gt;，那么拉格朗日函数就为&lt;/p&gt;
&lt;div class="math"&gt;$$L(x,y)=f(x,y)+\lambda\varphi(x,y)$$&lt;/div&gt;
&lt;p&gt;欲寻找函数 &lt;span class="math"&gt;\(z=f(x,y)\)&lt;/span&gt; 的可能极值点，只需令拉格朗日函数的各一阶偏导数为零，联立求解。&lt;/p&gt;
&lt;p&gt;这里引入广义拉格朗日函数，若有约束最优化问题：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \min_x&amp;amp;\quad f(x)\\
    \mathrm{s.t.}&amp;amp;\quad c_i(x)\leqslant0,\ h_j(x)=0
\end{align}$$&lt;/div&gt;
&lt;p&gt;那么广义拉格朗日函数为&lt;/p&gt;
&lt;div class="math"&gt;$$L(x,\alpha,\beta)=f(x)+\color{teal}{\sum_i\alpha_ic_i(x)}+\color{steelblue}{\sum_j\beta_jh_j(x)}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(\beta_j\)&lt;/span&gt; 为拉格朗日乘子，&lt;span class="math"&gt;\(\alpha_i\geqslant0\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;可以看出，只要 &lt;span class="math"&gt;\(x\)&lt;/span&gt; 满足约束，&lt;span class="math"&gt;\(L(x,\alpha,\beta)\)&lt;/span&gt; 的第二项是递减的，第三项是不增的。那么就有&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}
    \max_{\alpha,\beta:\alpha_i\geqslant0}L(x,\alpha,\beta)=
    \begin{cases}
        f(x), &amp;amp;x\ 满足约束\\
        +\infty, &amp;amp;否则
    \end{cases}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;原来的约束最优化问题 &lt;span class="math"&gt;\(\min_xf(x)\)&lt;/span&gt; 在这里就可以改写为 &lt;/p&gt;
&lt;div class="math"&gt;$$\min_x\max_{\alpha,\beta:\alpha_i\geqslant0}L(x,\alpha,\beta)$$&lt;/div&gt;
&lt;p&gt;在满足 Karush-Kuhn-Tucker（KKT）条件下，原始问题的解与对偶问题的解相等，即&lt;/p&gt;
&lt;div class="math"&gt;$$\min_x\max_{\alpha,\beta:\alpha_i\geqslant0}L(x,\alpha,\beta)=\max_{\alpha,\beta:\alpha_i\geqslant0}\min_xL(x,\alpha,\beta)$$&lt;/div&gt;
&lt;h3 id="zui-da-shang-mo-xing_1"&gt;最大熵模型&lt;/h3&gt;
&lt;p&gt;最大熵模型的学习过程是有约束的最优化问题：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \max_{P\in\mathcal{C}}&amp;amp;\quad H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)\\
    \mathrm{s.t.}&amp;amp;\quad E_P(f_i)=E_{\tilde{P}}(f_i),\ \sum_yP(y|x)=1
\end{align}$$&lt;/div&gt;
&lt;p&gt;按照凸优化的习惯（求向下凸的函数的最小值），问题等价于&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \min_{P\in\mathcal{C}}&amp;amp;\quad -H(P)=\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)\\
    \mathrm{s.t.}&amp;amp;\quad E_P(f_i)=E_{\tilde{P}}(f_i),\ \sum_yP(y|x)=1
\end{align}$$&lt;/div&gt;
&lt;p&gt;构造拉格朗日函数 &lt;span class="math"&gt;\(L(P,w)\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$L(P,w)\equiv-H(P)+w_0\left[1-\sum_yP(y|x)\right]+\sum_iw_i[E_P(f_i)-E_{\tilde{P}}(f_i)]$$&lt;/div&gt;
&lt;p&gt;在这里，原始问题的解与对偶问题的解同样是等价的，即&lt;/p&gt;
&lt;div class="math"&gt;$$\underbrace{\min_{P\in\mathcal{C}}\max_wL(P,w)}_{原始形式}=\underbrace{\max_w\min_{P\in\mathcal{C}}L(P,w)}_{对偶形式}$$&lt;/div&gt;
&lt;p&gt;那么就可以通过求解对偶问题来求解原始问题，具体来说，就是先求解 &lt;span class="math"&gt;\(\min_{P\in\mathcal{C}}L(P,w)\)&lt;/span&gt;，固定 &lt;span class="math"&gt;\(w\)&lt;/span&gt;，令 &lt;span class="math"&gt;\(\frac{\partial L(P,w)}{\partial P(y|x)}=0\)&lt;/span&gt;，得到 &lt;span class="math"&gt;\(P(y|x)\)&lt;/span&gt; 的表达式后将其代入 &lt;span class="math"&gt;\(L(P,w)\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\(\min_{P\in\mathcal{C}}L(P,w)\)&lt;/span&gt;。再令 &lt;span class="math"&gt;\(\frac{\partial \min_{P\in\mathcal{C}}L(P,w)}{\partial w}=0\)&lt;/span&gt;，最终得到 &lt;span class="math"&gt;\(P(y|x)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2 id="suan-fa_1"&gt;算法&lt;/h2&gt;
&lt;p&gt;逻辑斯谛回归模型和最大熵模型都是光滑的凸函数，适用于多种最优化方法，常用的方法包括迭代尺度算法、梯度下降法、牛顿法或拟牛顿法。&lt;/p&gt;
&lt;h3 id="gai-jin-de-die-dai-chi-du-fa"&gt;改进的迭代尺度法&lt;/h3&gt;
&lt;p&gt;令 &lt;span class="math"&gt;\(\frac{\partial L(P,w)}{\partial P(y|x)}=0\)&lt;/span&gt;，可以得到&lt;/p&gt;
&lt;div class="math"&gt;$$P(y|x)=\frac{\exp(\sum_iw_if_i(x,y))}{\exp(1-w_0)}$$&lt;/div&gt;
&lt;p&gt;其中的 &lt;span class="math"&gt;\(\exp(1-w_0)\)&lt;/span&gt; 部分是个定值，不影响概率的相对大小，因此略去该项并归一化，得到&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    P_w(y|x)&amp;amp;=\frac{1}{Z_w(x)}\exp\left(\sum_iw_if_i(x,y)\right)\\
    Z_w(x)&amp;amp;=\sum_y\exp\left(w_if_i(x,y)\right)
\end{align}$$&lt;/div&gt;
&lt;p&gt;对数似然函数为&lt;/p&gt;
&lt;div class="math"&gt;$$L(w)=\log\prod_{x,y}P(y|x)^{\tilde{P}(x,y)}=\sum_{x,y}\tilde{P}(x,y)\log P(y|x)$$&lt;/div&gt;
&lt;p&gt;将 &lt;span class="math"&gt;\(P_w(y|x)\)&lt;/span&gt; 代入得到&lt;/p&gt;
&lt;div class="math"&gt;$$L(w)=\sum_{x,y}\tilde{P}(x,y)\sum_iw_if_i(x,y)-\sum_x\tilde{P}(x)\log Z_w(x)$$&lt;/div&gt;
&lt;p&gt;迭代尺度法的思路就是寻找一个新参数向量 &lt;span class="math"&gt;\(w+\delta=(w_i+\delta_1,w_2+\delta_2,\cdots,w_n+\delta_n)^\mathrm{T}\)&lt;/span&gt;，使 &lt;span class="math"&gt;\(L(w)\)&lt;/span&gt; 增大，并不断迭代更新 &lt;span class="math"&gt;\(w\rightarrow w+\delta\)&lt;/span&gt;，最终使 &lt;span class="math"&gt;\(L(w)\)&lt;/span&gt; 最大。&lt;/p&gt;
&lt;p&gt;为了简化计算，在该算法中需要引入一个量 &lt;span class="math"&gt;\(f^\#(x,y)\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$f^\#(x,y)=\sum_if_i(x,y)$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f^\#(x,y)\)&lt;/span&gt; 表示了所有特征出现的次数。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(w\)&lt;/span&gt; 更新前后似然函数的变化值为（证明略）&lt;/p&gt;
&lt;div class="math"&gt;$$L(w+\delta)-L(w)\geqslant B(\delta|w)$$&lt;/div&gt;
&lt;div class="math"&gt;$$B(\delta|w)=\sum_{x,y}\tilde{P}(x,y)\sum_i\delta_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x)\sum_i\left(\frac{f_i(x,y)}{f^\#(x,y)}\right)\exp(\delta_i,f^\#(x,y))$$&lt;/div&gt;
&lt;p&gt;我们需要找到 &lt;span class="math"&gt;\(B(\delta|w)\)&lt;/span&gt; 的极值，让每次迭代后 &lt;span class="math"&gt;\(L(w)\)&lt;/span&gt; 尽可能大，因此求 &lt;span class="math"&gt;\(B(\delta|w)\)&lt;/span&gt; 对 &lt;span class="math"&gt;\(\delta_i\)&lt;/span&gt; 的偏导，并令其为零，得到&lt;/p&gt;
&lt;div class="math"&gt;$$\underbrace{\sum_{x,y}\tilde{P}(x)P_w(y|x)f_i(x,y)}_{E_P(f_i)}\exp(\delta_if^\#(x,y))=E_{\tilde{P}}(f_i)$$&lt;/div&gt;
&lt;p&gt;解该方程即可得到用于每次迭代的 &lt;span class="math"&gt;\(\delta\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;若 &lt;span class="math"&gt;\(f^\#(x,y)\)&lt;/span&gt; 为常数，那么&lt;/p&gt;
&lt;div class="math"&gt;$$\delta_i=\frac{1}{f^\#(x,y)}\log\frac{E_{\tilde{P}}(f_i)}{E_P(f_i)}$$&lt;/div&gt;
&lt;p&gt;若 &lt;span class="math"&gt;\(f^\#(x,y)\)&lt;/span&gt; 不是常数，通常使用数值计算的方法求解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 6.1（IIS 算法）&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：特征函数 &lt;span class="math"&gt;\(f_i\)&lt;/span&gt;；经验分布 &lt;span class="math"&gt;\(\tilde{P}(X,Y)\)&lt;/span&gt;；模型 &lt;span class="math"&gt;\(P_w(y|x)\)&lt;/span&gt;；&lt;br/&gt;
输出：最优参数 &lt;span class="math"&gt;\(w^*_i\)&lt;/span&gt;；最优模型 &lt;span class="math"&gt;\(P_{w^*}\)&lt;/span&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;对所有 &lt;span class="math"&gt;\(i\)&lt;/span&gt;，取初值 &lt;span class="math"&gt;\(w_i=0\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;对每一个 &lt;span class="math"&gt;\(i\)&lt;/span&gt;，&lt;ol&gt;
&lt;li&gt;解以下方程得到 &lt;span class="math"&gt;\(\delta_i\)&lt;/span&gt;：
    &lt;div class="math"&gt;$$\sum_{x,y}\tilde{P}(x)P_w(y|x)f_i(x,y)\exp(\delta_if^\#(x,y))=E_{\tilde{P}}(f_i)$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;更新 &lt;span class="math"&gt;\(w_i\)&lt;/span&gt;：&lt;span class="math"&gt;\(w_i\leftarrow w_i+\delta_i\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;若不是所有 &lt;span class="math"&gt;\(w_i\)&lt;/span&gt; 收敛，重复第 2 步。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="niu-dun-fa-yu-ni-niu-dun-fa"&gt;牛顿法与拟牛顿法&lt;/h3&gt;
&lt;h4&gt;牛顿法&lt;/h4&gt;
&lt;p&gt;对于最优化的目标目标函数 &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;，若满足二阶可微的前提，可以在 &lt;span class="math"&gt;\(x=x^{(k)}\)&lt;/span&gt; 处二阶泰勒展开：&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=f(x^{(x)})+\nabla f(x^{(k)})\Delta x^{(k)}+\frac{1}{2}(\Delta x^{(k)})^\mathrm{T}\nabla^2f(x^{(k)})\Delta x^{(k)}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\Delta x^{(k)}=x-x^{(k)}\)&lt;/span&gt;。牛顿法的思路就是利用二阶泰勒展开近似，寻找合理的下降方向 &lt;span class="math"&gt;\(p_k\)&lt;/span&gt;，使 &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; 在数次迭代后到达极小值。显然当 &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; 达到极小值时梯度为 0。那么假设下一次（第 &lt;span class="math"&gt;\(k+1\)&lt;/span&gt; 次）迭代时 &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; 就能达到极小值，也就是令上式在 &lt;span class="math"&gt;\(x^{(k+1)}\)&lt;/span&gt; 处的梯度为零：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \nabla f(x)&amp;amp;=\nabla f(x^{(k)})+\nabla^2f(x^{(k)})\Delta x^{(k)}\\
    \nabla f(x^{(k+1)})&amp;amp;=\nabla f(x^{(k)})+\nabla^2f(x^{(k)})(x^{(k+1)}-x^{(k)})=0\\
    x^{(k+1)}&amp;amp;=x^{(k)}-\nabla^2f(x^{(k)})^{-1}\nabla f(x^{(k)})
\end{align}$$&lt;/div&gt;
&lt;p&gt;将其中的 &lt;span class="math"&gt;\(\nabla^2f(x^{(k)})^{-1}\)&lt;/span&gt; 记作 &lt;span class="math"&gt;\(H^{-1}_k\)&lt;/span&gt;，是黑塞矩阵的逆，将 &lt;span class="math"&gt;\(\nabla f(x^{(k)})\)&lt;/span&gt; 记作 &lt;span class="math"&gt;\(g_k\)&lt;/span&gt;。令 &lt;span class="math"&gt;\(p_k=-H^{-1}_kg_k\)&lt;/span&gt;，称为牛顿方向，那么每次迭代的过程就是&lt;/p&gt;
&lt;div class="math"&gt;$$x^{(k+1)}=x^{k}+p_k$$&lt;/div&gt;
&lt;p&gt;{note begin}这里的泰勒展开为矩阵形式，因此式中带有转置与逆等符号，若对求导过程有疑惑，可以参考&lt;a href="https://zhuanlan.zhihu.com/p/382683133"&gt;泰勒展开的矩阵形式&lt;/a&gt;。{note end}&lt;/p&gt;
&lt;h4&gt;拟牛顿法&lt;/h4&gt;
&lt;p&gt;考虑以上推导过程中得到的&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \nabla f(x^{(k+1)})&amp;amp;=\nabla f(x^{(k)})+\nabla^2f(x^{(k)})(x^{(k+1)}-x^{(k)})\\
    g_{k+1}-g_k&amp;amp;=H_k(x^{(k+1)}-x^{(k)})\\
\end{align}$$&lt;/div&gt;
&lt;p&gt;令 &lt;span class="math"&gt;\(y_k=g_{k+1}-g_k\)&lt;/span&gt;，&lt;span class="math"&gt;\(\delta_k=x^{(k+1)}-x^{(k)}\)&lt;/span&gt;，可以改写为&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    y_k&amp;amp;=H_k\delta_k\\
    H^{-1}_ky_k&amp;amp;=\delta_k
\end{align}$$&lt;/div&gt;
&lt;p&gt;由于牛顿法每次迭代都需要计算黑森矩阵的逆 &lt;span class="math"&gt;\(H^{-1}_k\)&lt;/span&gt;，逆矩阵的计算过程比较繁琐，拟牛顿法的思路是寻找一个矩阵 &lt;span class="math"&gt;\(G_k\)&lt;/span&gt;，使其代替逆黑森矩阵：&lt;/p&gt;
&lt;div class="math"&gt;$$G_{k+1}y_k=\delta_k$$&lt;/div&gt;
&lt;p&gt;或是用 &lt;span class="math"&gt;\(B_k\)&lt;/span&gt; 逼近黑塞矩阵：&lt;/p&gt;
&lt;div class="math"&gt;$$B_{k+1}\delta_k=y_k$$&lt;/div&gt;
&lt;p&gt;那么每次迭代的步骤就是更新该矩阵：&lt;/p&gt;
&lt;div class="math"&gt;$$B_{k+1}=B_k+\Delta B_k$$&lt;/div&gt;
&lt;p&gt;拟牛顿矩阵更新方式不同，会有不同的计算公式和算法，这里以 BFGS 算法为例推导矩阵更新公式。&lt;/p&gt;
&lt;p&gt;假设 &lt;span class="math"&gt;\(B_{k+1}=B_k+P_k+Q_k\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(P_k\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(Q_k\)&lt;/span&gt; 为待定矩阵：&lt;/p&gt;
&lt;div class="math"&gt;$$B_{k+1}\delta_k=\color{teal}{y_k}=\color{steelblue}{B_k\delta_k}+\color{teal}{P_k\delta_k}+\color{steelblue}{Q_k\delta_k}$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P_k\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(Q_k\)&lt;/span&gt; 在该式中可以有多种取法，这里用最简单的取法：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    &amp;amp;\color{teal}{P_k\delta_k=y_k}\\
    &amp;amp;\color{steelblue}{Q_k\delta_k=-B_k\delta_k}
\end{align}$$&lt;/div&gt;
&lt;p&gt;那么就可以得到迭代公式：&lt;/p&gt;
&lt;div class="math"&gt;$$B_{k+1}=B_k+\frac{y_ky^\mathrm{T}_k}{y^\mathrm{T}_k\delta_k}-\frac{B_k\delta_k\delta^\mathrm{T}_kB_k}{\delta^\mathrm{T}_kB_k\delta_k}$$&lt;/div&gt;
&lt;p&gt;{note begin}式中有许多 &lt;span class="math"&gt;\(y_ky^\mathrm{T}_k\)&lt;/span&gt; 类型结构，这是为了将向量转化为方阵，才有逆运算，否则向量不具有除法运算。{note end}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 6.2（最大熵模型的 BFGS 算法）&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：特征函数 &lt;span class="math"&gt;\(f_i\)&lt;/span&gt;；经验分布 &lt;span class="math"&gt;\(\tilde{P}(x,y)\)&lt;/span&gt;；目标函数 &lt;span class="math"&gt;\(f(w)=-L(w)\)&lt;/span&gt;，梯度 &lt;span class="math"&gt;\(g(w)=\nabla f(w)\)&lt;/span&gt;，精度 &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;；&lt;br/&gt;
输出：最优参数值 &lt;span class="math"&gt;\(w^*\)&lt;/span&gt;；最优模型 &lt;span class="math"&gt;\(P_{w^*}(y|x)\)&lt;/span&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;选定初始点 &lt;span class="math"&gt;\(w^{(0)}\)&lt;/span&gt;，取 &lt;span class="math"&gt;\(B_0\)&lt;/span&gt; 为正定对称矩阵，置 &lt;span class="math"&gt;\(k=0\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算 &lt;span class="math"&gt;\(g_k=g(w^{(k)})\)&lt;/span&gt;。若 &lt;span class="math"&gt;\(||g_k||&amp;lt;\varepsilon\)&lt;/span&gt;，则停止，&lt;span class="math"&gt;\(w^*=w^{(k)}\)&lt;/span&gt;，否则继续。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(B_kp_k=-g_k\)&lt;/span&gt;，求 &lt;span class="math"&gt;\(p_k\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;求 &lt;span class="math"&gt;\(\lambda_k\)&lt;/span&gt; 使得 &lt;span class="math"&gt;\(f(w^{(k)}+\lambda_kp_k)=\min_{\lambda\geqslant0}f(w^{(k)}+\lambda p_k)\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(w^{(k+1)}=w^{(k)}+\lambda_kp_k\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(g_{k+1}=g(w^{(k+1)})\)&lt;/span&gt;，若 &lt;span class="math"&gt;\(||g_{k+1}||\varepsilon\)&lt;/span&gt;，则停止，&lt;span class="math"&gt;\(w^*=w^{(k)}\)&lt;/span&gt;，否则按 BFGS 迭代公式求 &lt;span class="math"&gt;\(B_{k+1}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(k=k+1\)&lt;/span&gt;，回到步骤 3。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr/&gt;
&lt;h2 id="references_1"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://book.douban.com/subject/33437381/"&gt;李航, 2019. 统计学习方法（第2版）. 清华大学出版社.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://book.douban.com/subject/35258057/"&gt;刘浩洋, 户将, 李勇锋, 文再文, 2020. 最优化：建模、算法与理论. 高等教育出版社.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zhihu.com/question/24094554"&gt;如何理解最大熵模型里面的特征？ - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/38163970"&gt;Karush-Kuhn-Tucker (KKT)条件 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="统计学习方法"></category><category term="Machine learning"></category><category term="Algorithm"></category></entry><entry><title>国内部分网络无法访问 githui.io 的解决方案</title><link href="https://tseing.github.io/sui-sui-nian/2022-09-06-dns-forbidden-of-github-pages.html" rel="alternate"></link><published>2022-09-06T00:00:00+08:00</published><updated>2022-09-06T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-09-06:/sui-sui-nian/2022-09-06-dns-forbidden-of-github-pages.html</id><summary type="html">&lt;p&gt;和往常一样，在浏览器中键入我的 Github Pages 地址，Firefox 却提示「找不到服务器」。发现运营商自动提供的 DNS 服务不解析 github.io，需要通过更改 DNS 服务来解决。&lt;/p&gt;</summary><content type="html">&lt;p&gt;和往常一样，在浏览器中键入我的 Github Pages 地址，Firefox 却提示「找不到服务器」。我心头一紧，以为是 Github 的服务全部挂掉了，但发现 Github 可以正常访问。&lt;/p&gt;
&lt;p&gt;我还注意到，在键入 Github Pages 地址后，没有经过加载过程，立马给出了无法访问的信息，我怀疑这可能是带有 &lt;code&gt;github.io&lt;/code&gt; 后缀的域名都进了黑名单。后来我切换到手机流量访问 Github Pages 页面，网页解析一切正常，更是验证了这一猜测。&lt;/p&gt;
&lt;p&gt;正好最近在忙 ICP 备案的事，在工信部官网一查，&lt;code&gt;已经列入黑名单&lt;/code&gt; 赫然在目。&lt;/p&gt;
&lt;p&gt;&lt;img alt="黑名单" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7550?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;在网上看了一圈，原来 &lt;code&gt;github.io&lt;/code&gt; 早就进了黑名单，但还没有被屏蔽。各个网络运营商参考了这份黑名单，但采取了不同的措施，例如我以前使用联通宽带时，访问 Github Pages 时就一切正常，现在切换到移动的服务，却没法访问了。至于电信，暂时还不了解。&lt;/p&gt;
&lt;p&gt;既然不是被长城屏蔽，只是运营商提供的 DNS 不解析某些网址，那就好办了，只需更换 DNS 服务器即可。下面以阿里公共 DNS &lt;code&gt;223.5.5.5&lt;/code&gt; 与 &lt;code&gt;223.6.6.6&lt;/code&gt; 为例，更换设备的 DNS 服务器。&lt;/p&gt;
&lt;h2 id="windows-she-bei"&gt;Windows 设备&lt;/h2&gt;
&lt;p&gt;&lt;img alt="网络连接属性" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7551?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;打开网络连接的属性后，选择 IPv4 属性，输入相应的 DNS 地址后保存即可生效。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IPv4属性" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7552?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;h2 id="ios-she-bei"&gt;IOS 设备&lt;/h2&gt;
&lt;p&gt;IOS 设备与 Windows 类似，选择 &lt;code&gt;设置&lt;/code&gt; - &lt;code&gt;Wi-Fi&lt;/code&gt;，选择已连接的网络，下划选择&lt;code&gt;配置 DNS&lt;/code&gt; - &lt;code&gt;手动&lt;/code&gt;，最后添加 DNS 地址。&lt;/p&gt;
&lt;h2 id="android-she-bei"&gt;Android 设备&lt;/h2&gt;
&lt;p&gt;Android 设备稍有不同，因为在 &lt;code&gt;Wi-Fi&lt;/code&gt; - &lt;code&gt;高级选项&lt;/code&gt; 中，若选择手动配置 DNS，就必须关闭 DHCP 服务，改用静态 IP 地址，这可能会与路由配置冲突。&lt;/p&gt;
&lt;p&gt;在新版本的 Android 中，在 &lt;code&gt;Wi-Fi&lt;/code&gt; 界面中提供了一个新的选项 &lt;code&gt;私人 DNS&lt;/code&gt;。选择 &lt;code&gt;私人 DNS&lt;/code&gt;，打开该功能后，输入阿里 DNS 的域名 &lt;code&gt;dns.alidns.com&lt;/code&gt; 后保存。注意在此处不能输入 DNS 地址。&lt;/p&gt;
&lt;h2 id="linux-she-bei"&gt;Linux 设备&lt;/h2&gt;
&lt;p&gt;我的 Linux 设备是安装了 KDE Plasma 桌面的 Debian 11，想必没有安装桌面的 Linux 也没有用浏览器访问网页的需求。在 KDE 下更改 DNS 就和 Windows 一样容易了，在网络选项中选择 &lt;code&gt;IPv4&lt;/code&gt;，将方法改为 &lt;code&gt;自动（仅网络地址）&lt;/code&gt;后再添加 DNS 服务器地址。保存设置后需要注销再重新登录才能生效。&lt;/p&gt;
&lt;h2 id="hou-ji"&gt;后记&lt;/h2&gt;
&lt;p&gt;无论如何，修改 DNS 也只是权宜之计，过不了多久，可能其他 DNS 供应方也不再解析「黑名单」中的地址，或是被长城挡在外面，那时恐怕就无计可施了。&lt;/p&gt;
&lt;p&gt;因此我也一直想着把网站搬到国内的服务器上，但是国内的服务器需要各种备案手续，让我比较难接受。考虑种种因素之后，决定还是让我的博客继续在外面漂流一阵子吧&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;{warn begin}本文最后更新于 2022 年 09 月 06 日，请确定内容是否过时。{warn end}&lt;/p&gt;</content><category term="碎碎念"></category><category term="Blog"></category><category term="Github"></category><category term="DNS"></category></entry><entry><title>函数绘图工具的选择</title><link href="https://tseing.github.io/sui-sui-nian/2022-09-05-function-drawer.html" rel="alternate"></link><published>2022-09-05T00:00:00+08:00</published><updated>2022-09-05T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-09-05:/sui-sui-nian/2022-09-05-function-drawer.html</id><summary type="html">&lt;p&gt;不管是论文、博客文章还是 PPT，凡是有数学公式的地方，常常都需要伴有函数图象。绘制插图的工具多到难以计数，找到称心顺手的工具更是难上加难，以下就尝试过的工具给出我的评价，或许能提供一些参考。&lt;/p&gt;</summary><content type="html">&lt;p&gt;不管是论文、博客文章还是 PPT，凡是有数学公式的地方，常常都需要伴有函数图象。绘制插图的工具多到难以计数，找到称心顺手的工具更是难上加难，以下就尝试过的工具给出我的评价，或许能提供一些参考。&lt;/p&gt;
&lt;h2 id="powerpoint"&gt;PowerPoint&lt;/h2&gt;
&lt;p&gt;对，没有看错，正是 Office 家族的 PowerPoint，PowerPoint 的绘图工具十分强劲。&lt;/p&gt;
&lt;p&gt;一来是具有图形化界面，所见即所得的操作方式很适合完全没有绘图基础的用户，在一顿摸索之后总能画得出不错的图形。二是 PowerPoint 支持多种图片格式，不管是网络上常用的 JPEG，还是用于论文的 SVG，一键就能导出。此外，在 PowerPoint 原生环境中绘制的图形，插入到 PPT 中更是天衣无缝。再者，PowerPoint 是 Windows 系统中的预装软件，开箱即用，十分方便。&lt;/p&gt;
&lt;p&gt;在我看来，对于一些简单或是要求不高的图形，PowerPoint 就已经足够满足要求了。但是，若对排版有着较高要求，那么 PowerPoint 就难堪此任了。&lt;/p&gt;
&lt;p&gt;对于较为复杂的图形，在 PowerPoint 需要绘制大量图层，操作相当困难，建议还是交给专业的 Adobe Illustrator。再者，PowerPoint 很难直接放大整个图形。例如，一组包含文字与几何形状的图形，直接放大就会造成文字与几何形状间的错位，必须一个个调整。另一大缺点就是 Office 的公式编辑器相当难用，尽管新版的 Office 已经支持了 LaTex 表达式，但是字体等等问题真是一言难尽。令我放弃 PowerPoint 关键是 PowerPoint 不能按公式绘制函数，只能用曲线工具一点点去描图，这就不能满足我的需求了。&lt;/p&gt;
&lt;h2 id="tikz"&gt;TikZ&lt;/h2&gt;
&lt;p&gt;TikZ 是 LaTex 的绘图宏包，能够通过 LaTex 指令直接在 PDF 中绘制矢量图。TikZ 是在 LaTex 环境中用指令绘图的，因此 TikZ 绘制的图形特别能满足排版强迫症患者，同时绘图风格与 LaTex 文章一致，看起来十分舒服。&lt;/p&gt;
&lt;p&gt;我也尝试过使用 TikZ 绘图，我的感觉是操作较为繁琐。例如绘制平面坐标系，竟然需要绘制两根箭头线，再绘制线上的短线作为刻度。或许为了排版美观，这不算什么，「严谨」嘛。那么对会绘图工具来说，最致命的一点莫过于不能导出图片文件了吧，是的，TikZ 竟然只能导出 PDF。我还尝试了各种格式转换工具，总不能导出清晰的 JPEG，不适合放在网页上，遂放弃。&lt;/p&gt;
&lt;p&gt;TikZ 还有另一个小问题，也可能是我配置的问题，在我使用 TikZ 绘制函数时，若指定的定义域超过某个值，就会给出错误 &lt;code&gt;Dimension too large&lt;/code&gt;。网络上给出的原因是，TikZ 不支持计算过大的数值，这一点在使用上也让人备感掣肘。&lt;/p&gt;
&lt;h2 id="matplotlib"&gt;matplotlib&lt;/h2&gt;
&lt;p&gt;matplotlib 是我最早学习的专业绘图工具，过去几年，我也见证着它越来越完善。随着使用 matplotlib 越多，对于 matplotlib 的样式总会疲倦，于是我尝试了其他绘图工具，最终兜兜转转，又回到了 matplotlib 的怀抱，这大概就是「否定之否定」吧。&lt;/p&gt;
&lt;p&gt;matplotlib 是 Python 的绘图包，因为同时依赖于强力的 numpy 包，复杂的运算对它而言是轻而易举，这对于各种数据的处理非常方便。matplotlib 通过代码绘制图像，各种样式自然也可以自定义。matplotlib 的各种优点在此也不再赘述，回到本文的主题，那么如何绘制出教科书式的函数示意图呢？&lt;/p&gt;
&lt;p&gt;用以下代码绘制默认样式的函数图像：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(4,3))     # 新建画布
ax = fig.add_subplot(111)           # 新建坐标系
fig.add_axes(ax)                    # 将坐标系添加到画布

x = np.arange(-6, 6, 0.1)
y = lambda x: 1/(1+np.e**(-x))

ax.plot(x, y(x))

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="默认" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7548?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;matplotlib 的默认样式虽然也很美观，但是与我们想要的教科书样式差别很大，教科书样式的主要特点就是坐标轴位于原点、黑白配色，我的解决方案是使用以下代码的样式：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import matplotlib.pyplot as plt
import mpl_toolkits.axisartist as axisartist

fig = plt.figure(figsize=(10,4))
ax1 = axisartist.Subplot(fig, 121)      # 左侧子图
ax2 = axisartist.Subplot(fig, 122)      # 右侧子图
fig.add_axes(ax1)
fig.add_axes(ax2)

axes_list = [ax1, ax2]

for ax in axes_list:
    # 隐藏边框
    ax.axis[:].set_visible(False)
    # 在原点绘制 x, y 轴
    ax.axis["x"] = ax.new_floating_axis(0,0)
    ax.axis["y"] = ax.new_floating_axis(1,0)
    # 设置 x, y 轴的样式
    ax.axis["x"].set_axisline_style("-|&amp;gt;", size=1.5)
    ax.axis["y"].set_axisline_style("-|&amp;gt;", size=1.5)
    # 设置 x, y 轴的箭头设为黑色
    ax.axis["x"].line.set_facecolor("black")
    ax.axis["y"].line.set_facecolor("black")
    # 隐藏坐标轴刻度
    ax.set_xticks([])
    ax.set_yticks([])

x = np.arange(-6, 6, 0.1)
y = lambda x: 1/(1+np.e**(-x))
ax1.plot(x, y(x), lw=2, c="black")

x = np.arange(-6, 6, 0.1)
y = lambda x: np.e**(-x)/(1+np.e**(-x))**2
ax2.plot(x, y(x), lw=2, c="black")

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="教科书式" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7549?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;唯一的不足之处是坐标轴的箭头稍有些肥大，略显怪异，我没有找到改变这个箭头样式的方法，于是只好直接修改默认样式。在 &lt;code&gt;matplotlib.patches&lt;/code&gt; 中找到以下代码片段：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@_register_style(_style_list, name="-|&amp;gt;")
    class CurveFilledB(_Curve):
        """An arrow with filled triangle head at the end."""
        arrow = "-|&amp;gt;"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在后面添加代码修改箭头样式，修改后的代码片段是：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@_register_style(_style_list, name="-|&amp;gt;")
    class CurveFilledB(_Curve):
        """An arrow with filled triangle head at the end."""
        arrow = "-|&amp;gt;"
        # 修改默认箭头样式
        def __init__(self, head_length=.75, head_width=.125):
            super().__init__(head_length=head_length, head_width=head_width)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{warn begin}对于 &lt;code&gt;matplotlib.patches&lt;/code&gt; 的修改，在matplotlib 更新后会失效，需要重新修改。{warn end}&lt;/p&gt;
&lt;p&gt;最后使用以下代码绘制图像：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import matplotlib.pyplot as plt
import mpl_toolkits.axisartist as axisartist

def scale_axes(ax, x, y, xscale=0.2, yscale=0.2):
    dx = np.max(x) - np.min(x)
    ax.set_xlim([np.min(x)-xscale*dx, np.max(x)+xscale*dx])
    dy = np.max(y) - np.min(y)
    ax.set_ylim([np.min(y)-yscale*dy, np.max(y)+yscale*dy])

fig = plt.figure(figsize=(10,4))
ax1 = axisartist.Subplot(fig, 121)
ax2 = axisartist.Subplot(fig, 122)
fig.add_axes(ax1)
fig.add_axes(ax2)

axes_list = [ax1, ax2]

for ax in axes_list:
    ax.axis[:].set_visible(False)
    ax.axis["x"] = ax.new_floating_axis(0,0)
    ax.axis["y"] = ax.new_floating_axis(1,0)
    ax.axis["x"].set_axisline_style("-|&amp;gt;", size=1.5)
    ax.axis["y"].set_axisline_style("-|&amp;gt;", size=1.5)
    ax.axis["x"].line.set_facecolor("black")
    ax.axis["y"].line.set_facecolor("black")

    ax.set_xticks([])
    ax.set_yticks([])

x = np.arange(-6, 6, 0.1)
y = lambda x: 1/(1+np.e**(-x))
ax1.plot(x, y(x), lw=2, c="black")
scale_axes(ax1, x, y(x))

x = np.arange(-6, 6, 0.1)
y = lambda x: np.e**(-x)/(1+np.e**(-x))**2
ax2.plot(x, y(x), lw=2, c="black")
scale_axes(ax2, x, y(x))

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中我新增了 &lt;code&gt;scale_axes()&lt;/code&gt; 用于控制函数图像按比例自动缩放，能够让左右图的坐标轴对齐，风格更统一，这样的图像就非常美观了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="修改箭头" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7554?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;{warn begin}本文最后更新于 2022 年 09 月 07 日，请确定内容是否过时。{warn end}&lt;/p&gt;</content><category term="碎碎念"></category><category term="matplotlib"></category><category term="Python"></category></entry><entry><title>詞藪</title><link href="https://tseing.github.io/gu-zhi-dui/2022-08-31-hokciu-word-collect.html" rel="alternate"></link><published>2022-08-31T00:00:00+08:00</published><updated>2022-08-31T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-08-31:/gu-zhi-dui/2022-08-31-hokciu-word-collect.html</id><summary type="html">&lt;p&gt;輯錄閩東語散佚在民間的字詞。&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;p class="cite"&gt;洎夫醇醨旣異，步驟不同，一物多名，繫方俗之語；片言殊訓，滯今古之情，將使後生若爲之鑽仰？&lt;/p&gt;
&amp;mdash;&amp;mdash;《爾雅疏敘》&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;近年来出版了诸多闽东语词典，这是十分令人欣喜的事，所起到正字音、辨名实的作用不可胜量。但语言的灵魂终不在于纸面，而在于传诵人之口。倘一语尚存母语者，则仍能旧字递陈、新声迭出，有描类状物之用。若使无人传诵，则旧字漫漶、新声瘖然，终为死语矣。&lt;/p&gt;
&lt;p&gt;因此我将尚存于母语者之口而词典失收者，不论旧字新声，备列如下，毋使后来人寻之无路。若能确定正字，则以正字书之，若正字难辨，则谨录其音，以俟高明。其于所不知，盖阙如也。&lt;/p&gt;
&lt;h2 id="ming-wu-di-yi"&gt;名物第一&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;南京豆&lt;/strong&gt; /nang21 nging53 dau213/：豌豆。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;福馬林&lt;/strong&gt; /huk5 ma55 ling53/：福爾馬林（Formalin）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;凝凍&lt;/strong&gt; /ngik5 doeyng213/：一指凝固的过程。又指類似龜苓膏的凍類食品。例：「買仂囝凝凍食」。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="shi-shi-di-er"&gt;世事第二&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;生精&lt;/strong&gt; /ciang55 zing55/：喜愛喫生食的人（謔）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;糠餡&lt;/strong&gt; /koung55 ngang242/：餅或麵點長時間放置後，餡料的口感變差。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;搿喙&lt;/strong&gt; /ba53 cui213/：打呵欠。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;筋潤&lt;/strong&gt; /gyng55 noung213/：稍帶有韌性又軟糯的口感。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;凋尾&lt;/strong&gt; /d(i)eu55 mui33/：咽下食物后殘留的苦味。例：「者茶有仂囝凋尾。」&lt;/li&gt;
&lt;li&gt;/huak5/（滑？/乏？）：骨疽潰爛的樣子。例：「伊大腿上肉都huak5去。」&lt;/li&gt;
&lt;li&gt;/ie53/（栘？）相/suong213/：容貌好看，上相。&lt;/li&gt;
&lt;li&gt;/l(i)au53 gooyk24/（漏斞？）：小氣吝嗇。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="cheng-yu-di-san"&gt;成語第三&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;屎驚五色&lt;/strong&gt; /sai33 giang55 ngu33 laik24/：害怕得屁滾尿流。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;碗碟匙箸&lt;/strong&gt; /uang33 diek5 sie53 doey242/：鍋碗瓢盆。例：「蜀桌排滿碗碟匙箸。」&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;風時交北&lt;/strong&gt; /hung55 si53 gau55 booyk24/：狂風大作。例：「做風颱其辰候，風時交北。」&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;大男小女&lt;/strong&gt; /duai55 nang53 siu24 ny33/：男女老幼。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;斜踑倒砑&lt;/strong&gt; /sia55 gi55 do55 nga242/：形容坐姿歪斜、不端正。例：「學生著有學生樣，伓通坐會斜踑倒砑。」&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="li-yan-di-si"&gt;俚諺第四&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;熟熟食肥肉&lt;/strong&gt; /syk5 syk5 siek55 bui21 nyk5/：熟人宰客。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;有顧喙無顧身&amp;mdash;&amp;mdash;㑚記食&lt;/strong&gt; /u53 gu21 cui213 mo33 gu21 sing55 na53 gei213 siek5/：貪喫而邋遢。&lt;/li&gt;
&lt;/ul&gt;</content><category term="故纸堆"></category><category term="平话"></category></entry><entry><title>《统计学习方法》第五章：决策树</title><link href="https://tseing.github.io/sui-sui-nian/2022-08-23-statistical-learning-chapter5.html" rel="alternate"></link><published>2022-08-23T00:00:00+08:00</published><updated>2022-08-23T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-08-23:/sui-sui-nian/2022-08-23-statistical-learning-chapter5.html</id><summary type="html">&lt;p&gt;《统计学习方法》第五章主要介绍一种具有树形结构的分类与回归模型——决策树，使用不同的特征分类能力的衡量标准，存在不同的决策树生成算法。&lt;/p&gt;</summary><content type="html">&lt;h2 id="mo-xing"&gt;模型&lt;/h2&gt;
&lt;h3 id="jue-ce-shu"&gt;决策树&lt;/h3&gt;
&lt;p&gt;决策树是一种&lt;em&gt;分类&lt;/em&gt;与&lt;em&gt;回归&lt;/em&gt;的方法，它是由结点和有向边组成的树形结构。决策树的内部结点表示一个特征或属性，叶结点表示一个类。&lt;/p&gt;
&lt;p&gt;&lt;img alt="决策树模型" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7527?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;决策树学习的目标就是根据给定的训练集生成一个决策树模型，利用该模型对实例正确分类。也就是说决策树的生成在本质上是从训练集中归纳出一组分类规则，决策树中结点之间的路径正是对应了这种分类规则。&lt;/p&gt;
&lt;h3 id="fen-lei-yu-hui-gui-shu"&gt;分类与回归树&lt;/h3&gt;
&lt;p&gt;分类与回归树（CART）模型是一种应用广泛的&lt;em&gt;决策树&lt;/em&gt;模型，CART 假定决策树为&lt;em&gt;二叉树&lt;/em&gt;，左分支为&amp;ldquo;是&amp;rdquo;路径，右分支为&amp;ldquo;否&amp;rdquo;路径。CART 模型将每个特征不断二分，划分为有限个单元，最后就能在这些单元上进行预测。&lt;/p&gt;
&lt;h2 id="ce-lue_1"&gt;策略&lt;/h2&gt;
&lt;p&gt;决策树可以看作为一系列 if-then 规则的集合，那么如何选择规则的判断条件就十分重要了。选择判断条件就是在选择训练数据的特征，可以想象，如果利用一个特征作为分类条件得到的结果与&lt;em&gt;随机分类&lt;/em&gt;的结果没有很大差别，那么这个特征是没有分类能力的。为了生成精确的决策树模型，我们需要设立用于衡量所选特征分类能力的准则。&lt;/p&gt;
&lt;p&gt;常用选择特征的准则包括信息增益、信息增益比、平方误差和基尼指数。&lt;/p&gt;
&lt;h3 id="xin-xi-zeng-yi"&gt;信息增益&lt;/h3&gt;
&lt;p&gt;设一个离散随机分布为&lt;/p&gt;
&lt;div class="math"&gt;$$P(X=x_i)=p_i,\qquad i=1,2,\cdots,n$$&lt;/div&gt;
&lt;p&gt;那么随机变量 &lt;span class="math"&gt;\(X\)&lt;/span&gt; 的熵就定义为&lt;/p&gt;
&lt;div class="math"&gt;$$H(p)=-\sum_{i=1}^np_i\log p_i$$&lt;/div&gt;
&lt;p&gt;若 &lt;span class="math"&gt;\(p_i=0\)&lt;/span&gt;，规定 &lt;span class="math"&gt;\(0\log 0=0\)&lt;/span&gt;，其中的对数也可取 2 为底或取自然对数。熵是衡量随机变量不确定（混乱）程度的度量，熵值越大，则随机变量越不确定，也就是说各事件发生的概率值越接近，概率分布也更分散。&lt;/p&gt;
&lt;p&gt;&lt;img alt="熵与概率的关系" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7526?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;条件熵用于表示在某条件下随机变量的不确定性，条件熵具有条件期望的形式，定义为&lt;/p&gt;
&lt;div class="math"&gt;$$H(X|Y)=\sum_{i=1}^np_iH(Y|X=x_i)$$&lt;/div&gt;
&lt;p&gt;在实际操作中会使用频率估计概率，这时候熵就称为经验熵，条件熵称为经验条件熵。&lt;/p&gt;
&lt;p&gt;特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 对训练集 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 的信息增益就定义为训练集的经验熵与给定特征下的经验条件熵之差：&lt;/p&gt;
&lt;div class="math"&gt;$$g(D,A)=H(D)-H(D|A)$$&lt;/div&gt;
&lt;p&gt;可以直观地理解为，若特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 具有分类能力，在选定的特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 下，数据集表现出更大的确定性或有序性，熵值相应减小，那么熵值变化的大小就可以用来衡量数据集在应用该特征后确定性的增加程度，也就是该特征分类能力的强弱。因此，信息增益大的特征具有更强的分类能力。&lt;/p&gt;
&lt;p&gt;对于训练集 &lt;span class="math"&gt;\(D\)&lt;/span&gt;，用 &lt;span class="math"&gt;\(|D|\)&lt;/span&gt; 表示集合中元素个数，即样本个数。设有 &lt;span class="math"&gt;\(K\)&lt;/span&gt; 个类 &lt;span class="math"&gt;\(C_k\)&lt;/span&gt;，类 &lt;span class="math"&gt;\(C_k\)&lt;/span&gt; 中有 &lt;span class="math"&gt;\(|C_k|\)&lt;/span&gt; 个样本。依据特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 可将 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 划分为 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 个子集 &lt;span class="math"&gt;\(D_i\)&lt;/span&gt;，&lt;span class="math"&gt;\(D_i\)&lt;/span&gt; 中属于类 &lt;span class="math"&gt;\(C_k\)&lt;/span&gt; 的样本的集合记作 &lt;span class="math"&gt;\(D_{ik}\)&lt;/span&gt;。那么就按以下方法计算信息增益。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 5.1&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：训练集 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 和特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt;&lt;br/&gt;
输出：特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 对训练集 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 的信息增益&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;计算数据集 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 的经验熵
    &lt;div class="math"&gt;$$H(D)=-\sum_{k=1}^Kp_k\log_2p_k=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;计算特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 对数据集的经验条件熵
    &lt;div class="math"&gt;$$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;计算信息增益
    &lt;div class="math"&gt;$$g(D,A)=H(D)-H(D|A)$$&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="xin-xi-zeng-yi-bi"&gt;信息增益比&lt;/h3&gt;
&lt;p&gt;若参考信息增益比的大小选择特征，选择结果会偏向取值较多的特征。可以这么理解，特征取值越多，即划分条件越多，例如按年龄将人群分为青年、中年、老年（特征取值为 3）相较于按出生地将人群划分为是否当地居民（特征取值为 2），得到的划分结果会更「有序」。但这并不意味着年龄这一特征具有更好的分类能力，分类结果可能与被调查人的出生地有着更为密切的关系。&lt;/p&gt;
&lt;p&gt;当特征取值过多时，这个特征就像是精心制作的筛子，过筛后的结果总是「显得」更加有序，因此需要使用信息增益比校正这一问题。信息增益比定义为&lt;/p&gt;
&lt;div class="math"&gt;$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$&lt;/div&gt;
&lt;div class="math"&gt;$$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 为特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 的取值个数。&lt;/p&gt;
&lt;h3 id="ping-fang-wu-chai"&gt;平方误差&lt;/h3&gt;
&lt;p&gt;平方误差 &lt;span class="math"&gt;\(\sum(y_i-f(x_i))^2\)&lt;/span&gt; 是十分常用的误差函数，平方误差常用于回归树的构建。&lt;/p&gt;
&lt;p&gt;回归树将输入空间分割为若干单元，并在每个单元 &lt;span class="math"&gt;\(R_m\)&lt;/span&gt; 上有一个固定的输出 &lt;span class="math"&gt;\(c_m\)&lt;/span&gt;，可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=\sum c_mI(x\in R_m)$$&lt;/div&gt;
&lt;p&gt;在每个单元上的最优输出 &lt;span class="math"&gt;\(\hat{c}_m\)&lt;/span&gt; 自然是该单元中所有实例对应的输出 &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; 的均值：&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{c}_m=\mathrm{ave}(y_i|x_i\in R_m)$$&lt;/div&gt;
&lt;p&gt;目的是找到最优的切分特征 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 与最优切分点 &lt;span class="math"&gt;\(s\)&lt;/span&gt;，那么对于切分后得到的某一块区域 &lt;span class="math"&gt;\(R_m\)&lt;/span&gt; 内，&lt;span class="math"&gt;\(c_m\)&lt;/span&gt; 偏离于实例 &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; 的值都应该尽可能小：&lt;/p&gt;
&lt;div class="math"&gt;$$\min_{c_m}\sum_{x_i\in R_m}(y_i-c_m)^2$$&lt;/div&gt;
&lt;p&gt;实际上，这就是利用最小二乘法确定单元中的 &lt;span class="math"&gt;\(\hat{c}_m\)&lt;/span&gt; 并使其平方误差最小，这样回归树又称为最小二乘回归树。&lt;/p&gt;
&lt;p&gt;&lt;img alt="最小二乘回归树" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7547?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;因为回归树是二叉的，每次划分得到两个区域，对于整体而言。最优的划分条件 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(s\)&lt;/span&gt; 应使得到两个区域 &lt;span class="math"&gt;\(c_m\)&lt;/span&gt; 的平方误差之和尽可能小，完整求解问题表述为&lt;/p&gt;
&lt;div class="math"&gt;$$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_1}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]$$&lt;/div&gt;
&lt;h3 id="ji-ni-zhi-shu"&gt;基尼指数&lt;/h3&gt;
&lt;p&gt;基尼指数也是特征分类能力的度量，常用于分类树。样本点属于第 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 类的概率为 &lt;span class="math"&gt;\(p_k\)&lt;/span&gt;，基尼指数定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Gini}(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$$&lt;/div&gt;
&lt;p&gt;从基尼指数的定义可以看出，若样本点越分散，基尼指数越大。从下图可以看出，基尼指数与分类误差率、熵有着相似的变化关系，因此基尼指数同样可以用于衡量特征的分类能力。&lt;/p&gt;
&lt;p&gt;&lt;img alt="基尼指数" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7546?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 将样本 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 划分为两个部分，那么在特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 条件下的基尼指数就表示为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathrm{Gini}(D,A)=\frac{|D_1|}{|D|}\mathrm{Gini}(D_1)+\frac{|D_2|}{|D|}\mathrm{Gini}(D_2)$$&lt;/div&gt;
&lt;h2 id="suan-fa_1"&gt;算法&lt;/h2&gt;
&lt;h3 id="jue-ce-shu-de-sheng-cheng"&gt;决策树的生成&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;算法 5.2（ID3 算法）&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：训练集 &lt;span class="math"&gt;\(D\)&lt;/span&gt;，特征集 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 与阈值 &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;&lt;br/&gt;
输出：决策树 &lt;span class="math"&gt;\(T\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;若 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 中所有实例属于同一类（分类完成），将该类作为结节的类标记，返回 &lt;span class="math"&gt;\(T\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;若 &lt;span class="math"&gt;\(A=\varnothing\)&lt;/span&gt;（无特征可分），将 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 中实例数量最多的类作为该结点类标记，返回 &lt;span class="math"&gt;\(T\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;否则，计算各特征的信息增益，选择信息增益最大的特征 &lt;span class="math"&gt;\(A_g\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;若 &lt;span class="math"&gt;\(g(D,A_g)&amp;lt;\varepsilon\)&lt;/span&gt;（信息增益小于阈值），将 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 中实例数量最多的类作为该结点类标记，返回 &lt;span class="math"&gt;\(T\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;否则，使用 &lt;span class="math"&gt;\(A_g\)&lt;/span&gt; 的可能值 &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; 将训练集分割为若干 &lt;span class="math"&gt;\(D_i\)&lt;/span&gt;，将 &lt;span class="math"&gt;\(D_i\)&lt;/span&gt; 中实例数量最多的类作为类标记构建子结点，返回 &lt;span class="math"&gt;\(T\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;对于第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个结点，以 &lt;span class="math"&gt;\(D_i\)&lt;/span&gt; 为训练集，以 &lt;span class="math"&gt;\(A-\{A_g\}\)&lt;/span&gt; 为特征集，递归 1~5 步，返回子树 &lt;span class="math"&gt;\(T_i\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;算法 5.4（C4.5 算法）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用&lt;em&gt;信息增益比&lt;/em&gt;代替 ID3 算法中的信息增益，就是 C4.5 算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 5.4（最小二乘回归树生成算法）&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：训练数据集 &lt;span class="math"&gt;\(D\)&lt;/span&gt;&lt;br/&gt;
输出： 回归树 &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;选择最优切分变量 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 与切分点 &lt;span class="math"&gt;\(s\)&lt;/span&gt;，遍历 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(s\)&lt;/span&gt;，得到使下式最小的对 &lt;span class="math"&gt;\((j,s)\)&lt;/span&gt;
&lt;div class="math"&gt;$$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_1}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;用对 &lt;span class="math"&gt;\((j,s)\)&lt;/span&gt; 划分区域并确定 &lt;span class="math"&gt;\(\hat{c}_m\)&lt;/span&gt;
&lt;div class="math"&gt;$$\begin{gather}
        R_1(j,s)=\{x|x^{(j)}\leq s\},\quad R_2(j,s)=\{x|x^{(j)}&amp;gt;s\}\\
        \hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i,\quad x\in R_m,\ m=1,2
    \end{gather}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;对子区域继续调用步骤（1）~（2），直至停止；&lt;/li&gt;
&lt;li&gt;使用划分的区域生成决策树。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;算法 5.6（CART 生成算法）&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：训练数据集 &lt;span class="math"&gt;\(D\)&lt;/span&gt;&lt;br/&gt;
输出：CART 决策树&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;计算所有特征对于数据集 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 的基尼指数，对于所有可能特征 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 的所有可能取值 &lt;span class="math"&gt;\(a\)&lt;/span&gt;，使用是否满足 &lt;span class="math"&gt;\(A=a\)&lt;/span&gt; 作为判断条件将数据集划分为两个部分；&lt;/li&gt;
&lt;li&gt;选择基尼指数最小的特征与切分点，生成两个子结点；&lt;/li&gt;
&lt;li&gt;对于子结点继续调用 1~2 步，直至停止；&lt;/li&gt;
&lt;li&gt;生成 CART 决策树。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;{note begin}算法停止的条件可以参考算法 5.2，可以是样本点数据小于阈值，也可以特征的分类能力小于阈值，也可以是特征数量太少。{note end}&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references_1"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://book.douban.com/subject/33437381/"&gt;李航, 2019. 统计学习方法（第2版）. 清华大学出版社.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="统计学习方法"></category><category term="Machine learning"></category><category term="Algorithm"></category></entry><entry><title>为 Pelican 博客设置 Lightbox 效果</title><link href="https://tseing.github.io/sui-sui-nian/2022-08-17-pelican-lightbox.html" rel="alternate"></link><published>2022-08-17T00:00:00+08:00</published><updated>2022-08-17T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-08-17:/sui-sui-nian/2022-08-17-pelican-lightbox.html</id><summary type="html">&lt;p&gt;如何像社交媒体上发布的图片一样，让博客文章中的图片也能点击放大呢？使用 JavaScript 插件，在博客文章中添加这种 Lightbox 效果，再加上配套的 Python-Markdown 的拓展插件，让 Markdown 写作中的图片效果设置变得更加自由。&lt;/p&gt;</summary><content type="html">&lt;p&gt;Markdown 语法简洁而高效，使用 Markdown 撰写博客文章是十分通行的做法。若要在文章中插入图片，需要使用 &lt;code&gt;![标题](URL)&lt;/code&gt; 语法，Pelican 博客引擎将文章中的 &lt;code&gt;[标题](URL)&lt;/code&gt; 转换为 html 标签 &lt;code&gt;&amp;lt;img alt="标题" src="URL"&amp;gt;&lt;/code&gt;，就生成了用于发布的静态网页。&lt;/p&gt;
&lt;p&gt;这样生成的网页中，图片大小由 CSS 文件决定。设定的图片尺寸如果太小，难以看清图片细节，尺寸如果太大就会占据较大版面，也十分影响阅读。&lt;/p&gt;
&lt;p&gt;各类社交网站上的常规做法是，使用 CSS 文件决定合适的缩略图尺寸，点击缩略图后放大图片。点击缩略图放大的效果就像在暗室中使用的放映机，这个效果就被称为 Lightbox。&lt;/p&gt;
&lt;p&gt;Lightbox 功能非常常用，因此在网上有大量现成的插件，具有 Lightbox 功能的 Pelican 插件包括 &lt;a href="https://github.com/pelican-plugins/photos"&gt;photos&lt;/a&gt; 和 &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/gallery"&gt;Gallery&lt;/a&gt; 等。但是它们并不是纯粹的 Lightbox 插件，还具有图片处理、读取 EXIF 信息等功能，我觉得太「重」。&lt;/p&gt;
&lt;h2 id="qing-liang-de-lightgallery-markdown"&gt;轻量的 lightgallery-markdown&lt;/h2&gt;
&lt;p&gt;兜兜转转之下，我找到了一个实现 Lightbox 功能的 &lt;a href="https://github.com/g-provost/lightgallery-markdown"&gt;Python-Markdown 拓展&lt;/a&gt;。其实原理也很简单，这个拓展能将 &lt;code&gt;![!标题](URL)&lt;/code&gt; 转换为以下代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;div class="lightgallery"&amp;gt;
    &amp;lt;a href="URL" data-sub-html="标题"&amp;gt;
      &amp;lt;img alt="标题" src="URL" /&amp;gt;
    &amp;lt;/a&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再在 &lt;a href="https://github.com/sachinchoolur/lightgallery.js"&gt;lightgallery.js&lt;/a&gt; 的作用下就能实现 Lightbox 功能。所以只要是使用 Python-Markdown 作为 html 生成器的博客都可以使个方法设置 Lightbox 效果。&lt;/p&gt;
&lt;h2 id="an-zhuang-python-markdown-tuo-zhan"&gt;安装 Python-Markdown 拓展&lt;/h2&gt;
&lt;p&gt;Pelican 提供了 Python-Markdown 拓展的接口，先使用 &lt;code&gt;pip install lightgallery&lt;/code&gt; 安装 lightgallery-markdown，并在 &lt;code&gt;pelicanconf.py&lt;/code&gt; 中添加&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;MARKDOWN = {
    'extension_configs': {
        'markdown.extensions.codehilite': {'css_class': 'highlight'},
        'markdown.extensions.extra': {},
        'markdown.extensions.meta': {},
        'lightgallery': {},
    },
    'output_format': 'html5',
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="bu-shu-lightgalleryjs"&gt;部署 lightgallery.js&lt;/h2&gt;
&lt;p&gt;在 &lt;a href="https://github.com/sachinchoolur/lightgallery.js"&gt;lightgallery.js 项目仓库&lt;/a&gt;中下载以下文件并放置到相应位置：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dist/js/lightgallery.min.js&lt;/code&gt;&amp;emsp;&amp;rarr;&amp;emsp;&lt;code&gt;themes/{theme_name}/static/js/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dist/css/lightgallery.min.css&lt;/code&gt;&amp;emsp;&amp;rarr;&amp;emsp;&lt;code&gt;themes/{theme_name}/css/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dist/fonts/lg.*&lt;/code&gt;&amp;emsp;&amp;rarr;&amp;emsp;&lt;code&gt;themes/{theme_name}/font/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dist/img/loading.gif&lt;/code&gt;&amp;emsp;&amp;rarr;&amp;emsp;&lt;code&gt;themes/{theme_name}/images/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;{warn begin}由于文件目录结构不同，需要将 &lt;code&gt;lightgallery.min.css&lt;/code&gt; 中的字体、图片路径修改为相应路径。{warn end}&lt;/p&gt;
&lt;p&gt;修改 &lt;code&gt;themes/{theme_name}/templates/base.html&lt;/code&gt;，在其中添加以下代码&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;!-- 引入.css与.js文件--&amp;gt;
&amp;lt;link href="{{ SITEURL }}/{{ THEME_STATIC_DIR }}/css/lightgallery.min.css" type="text/css" rel="stylesheet" /&amp;gt;
&amp;lt;script type="text/javascript" src="{{ SITEURL }}/{{ THEME_STATIC_DIR }}/js/lightgallery.min.js"&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;!-- 图片计数 --&amp;gt;
&amp;lt;script&amp;gt;
  var elements = document.getElementsByClassName("lightgallery");
  for(var i=0; i&amp;lt;elements.length; i++) {
     lightGallery(elements[i]);
  }
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;到了这里，已经在博客上添加了 Lightbox 功能，但还缺少最重要的放大缩小功能。&lt;/p&gt;
&lt;h2 id="tian-jia-lightgalleryjs-cha-jian"&gt;添加 lightgallery.js 插件&lt;/h2&gt;
&lt;p&gt;下载 &lt;a href="https://github.com/sachinchoolur/lg-zoom.js"&gt;lg-zoom.js&lt;/a&gt;，将 &lt;code&gt;dist/lg_zoom.min.js&lt;/code&gt; 移入 &lt;code&gt;themes/{theme_name}/static/js/&lt;/code&gt;。同样在主题的 &lt;code&gt;base.html&lt;/code&gt; 中引入该 JavaScript 文件即可生效。lightgallery.js 项目还具有许多插件，都可以通过这样简单的方法使其生效。&lt;/p&gt;
&lt;h2 id="yi-xie-xiao-xiu-gai"&gt;一些小修改&lt;/h2&gt;
&lt;p&gt;若将图片标题留空，如 &lt;code&gt;![!](URL)&lt;/code&gt;，lightgallery.js 不会渲染查看图片时下方的图片信息，界面十分清爽舒服。但是这么一来，每次导出网页时Pelican 都会给出 &lt;code&gt;WARNING: Empty alt attribute for image content&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;于是修改 &lt;code&gt;lightgallery.js&lt;/code&gt; 文件，修改以下代码块：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;if (typeof subHtml !== 'undefined' &amp;amp;&amp;amp; subHtml !== null) {
            if (subHtml === '') {
                _lgUtils2.default.addClass(this.outer.querySelector(this.s.appendSubHtmlTo), 'lg-empty-html');
            } else {
                _lgUtils2.default.removeClass(this.outer.querySelector(this.s.appendSubHtmlTo), 'lg-empty-html');
            }
        }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将判断条件修改为：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;if (subHtml === '' || subHtml === 'NoCaption')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这时只要是标题设置为 &lt;code&gt;"NoCaption"&lt;/code&gt; 的图片就不会显示下方信息栏，Pelican 也不会因为缺少标题而给出警告。&lt;/p&gt;
&lt;p&gt;{note begin}&lt;code&gt;lightgallery.min.js&lt;/code&gt; 经过压缩，体积较小，加载速度更快，但代码可读性较差，不便于修改，可以先修改 &lt;code&gt;lightgallery.js&lt;/code&gt; 再压缩为 &lt;code&gt;lightgallery.min.js&lt;/code&gt;。{note end}&lt;/p&gt;
&lt;h2 id="demo"&gt;Demo 🥳&lt;/h2&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="{photo}135 mm&amp;emsp;f/5.6&amp;emsp;1/125&amp;emsp;ISO-800&amp;emsp;Photo by Leo" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7545?authkey=ALYpzW-ZQ_VBXTU"&gt;&lt;img alt="{photo}135 mm&amp;emsp;f/5.6&amp;emsp;1/125&amp;emsp;ISO-800&amp;emsp;Photo by Leo" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7545?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;{warn begin}本文最后更新于 2022 年 08 月 17 日，请确定内容是否过时。{warn end}&lt;/p&gt;</content><category term="碎碎念"></category><category term="Blog"></category><category term="Pelican"></category><category term="Markdown"></category><category term="JavaScript"></category></entry><entry><title>使用 prettymaps 生成精美地图</title><link href="https://tseing.github.io/sui-sui-nian/2022-08-14-prettymaps-generate-maps.html" rel="alternate"></link><published>2022-08-14T00:00:00+08:00</published><updated>2022-08-14T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-08-14:/sui-sui-nian/2022-08-14-prettymaps-generate-maps.html</id><summary type="html">&lt;p&gt;经过&lt;a href="https://tseing.github.io/sui-sui-nian/2022-08-11-prettymaps-install.html"&gt;上次&lt;/a&gt;的屡次踩坑，终于艰难地在 Windows 上安装好了 prettymaps。看着 &lt;code&gt;pip list&lt;/code&gt; 中的 &lt;code&gt;prettymaps&lt;/code&gt;，我迫不及待地想试着用 &lt;code&gt;prettymaps&lt;/code&gt; 生成一些地图，那么就开 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;经过&lt;a href="https://tseing.github.io/sui-sui-nian/2022-08-11-prettymaps-install.html"&gt;上次&lt;/a&gt;的屡次踩坑，终于艰难地在 Windows 上安装好了 prettymaps。看着 &lt;code&gt;pip list&lt;/code&gt; 中的 &lt;code&gt;prettymaps&lt;/code&gt;，我迫不及待地想试着用 &lt;code&gt;prettymaps&lt;/code&gt; 生成一些地图，那么就开始吧。&lt;/p&gt;
&lt;h2 id="attributeerror-cuo-wu"&gt;AttributeError 错误&lt;/h2&gt;
&lt;p&gt;先运行作者给出的示例代码试验一下：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;from prettymaps import *
from matplotlib import pyplot as plt
fig, ax = plt.subplots(figsize = (12, 12), constrained_layout = True)
layers = plot(
    'Pra&amp;ccedil;a Ferreira do Amaral, Macau', radius = 1100,
    ax = ax,
    layers = {
            'perimeter': {},
            'streets': {
                'custom_filter': '["highway"~"motorway|trunk|primary|secondary|tertiary|residential|service|unclassified|pedestrian|footway"]',
                'width': {
                    'motorway': 5,
                    'trunk': 5,
                    'primary': 4.5,
                    'secondary': 4,
                    'tertiary': 3.5,
                    'residential': 3,
                    'service': 2,
                    'unclassified': 2,
                    'pedestrian': 2,
                    'footway': 1,
                }
            },
            'building': {'tags': {'building': True, 'landuse': 'construction'}, 'union': False},
            'water': {'tags': {'natural': ['water', 'bay']}},
            'green': {'tags': {'landuse': 'grass', 'natural': ['island', 'wood'], 'leisure': 'park'}},
            'forest': {'tags': {'landuse': 'forest'}},
            'parking': {'tags': {'amenity': 'parking', 'highway': 'pedestrian', 'man_made': 'pier'}}
        },
        drawing_kwargs = {
            'background': {'fc': '#F2F4CB', 'ec': '#dadbc1', 'hatch': 'ooo...', 'zorder': -1},
            'perimeter': {'fc': '#F2F4CB', 'ec': '#dadbc1', 'lw': 0, 'hatch': 'ooo...',  'zorder': 0},
            'green': {'fc': '#D0F1BF', 'ec': '#2F3737', 'lw': 1, 'zorder': 1},
            'forest': {'fc': '#64B96A', 'ec': '#2F3737', 'lw': 1, 'zorder': 1},
            'water': {'fc': '#a1e3ff', 'ec': '#2F3737', 'hatch': 'ooo...', 'hatch_c': '#85c9e6', 'lw': 1, 'zorder': 2},
            'parking': {'fc': '#F2F4CB', 'ec': '#2F3737', 'lw': 1, 'zorder': 3},
            'streets': {'fc': '#2F3737', 'ec': '#475657', 'alpha': 1, 'lw': 0, 'zorder': 3},
            'building': {'palette': ['#FFC857', '#E9724C', '#C5283D'], 'ec': '#2F3737', 'lw': .5, 'zorder': 4},
        },
        osm_credit = {'color': '#2F3737'}
)
plt.savefig('macao.png')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是拋出了错误，提示 &lt;code&gt;AttributeError: 'DataFrame' object has no attribute 'crs'&lt;/code&gt;。在 &lt;a href="https://github.com/marceloprates/prettymaps/issues/88"&gt;Github&lt;/a&gt; 上有人给出了解决方案，原因是 &lt;code&gt;osmnx&lt;/code&gt; 的版本过低，直接 &lt;code&gt;pip install osmnx==1.2.1&lt;/code&gt; 就可以解决。&lt;/p&gt;
&lt;p&gt;需要注意的是，pip 可能会给出错误信息提示 &lt;code&gt;osmnx==1.2.1&lt;/code&gt; 与 &lt;code&gt;prettymaps&lt;/code&gt; 不兼容。但同样给出了已成功安装 &lt;code&gt;osmnx==1.2.1&lt;/code&gt; 的信息。经过我的尝试，&lt;code&gt;prettymaps&lt;/code&gt; 是可以正常工作的，所以这条错误信息可能没有什么影响。&lt;/p&gt;
&lt;h2 id="chu-kui-prettymapsplot-han-shu"&gt;初窥 prettymaps.plot() 函数&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;prettymaps&lt;/code&gt; 是在 &lt;code&gt;matplotlib&lt;/code&gt; 画布上绘制地图的，所以主要参数与 &lt;code&gt;matplotlib&lt;/code&gt; 的写法相同，用于调整颜色或文本，不甚重要。而用于生成地图的 &lt;code&gt;prettymaps.plot()&lt;/code&gt; 函数就比较关键，比较常用参数的就以下几项：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;plot(
    # 地图的中心点，可以是地名的字符串，也可以经纬度的元组
    'query',
    # 地图半径，单位为米
    radius = 100,
    # 将 x 轴绑定至画布 x 轴
    ax = ax,
    # OpenStreetMap 地图层信息，若不了解复制示例代码即可
    layers = {'perimeter': {}},
    # 图层样式，如颜色等
    drawing_kwargs = {},
    # 版权信息
    osm_credit = {}
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="yuan-xing-mo-shi"&gt;圆形模式&lt;/h2&gt;
&lt;p&gt;与示例代码相同，只要将 &lt;code&gt;perimeter&lt;/code&gt; 留空，默认的绘图模式就是圆形模式。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="NoCaption" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7539?authkey=ALYpzW-ZQ_VBXTU"&gt;&lt;img alt="NoCaption" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7539?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;{location}外滩  The Bund, Shanghai&lt;/p&gt;
&lt;h2 id="yuan-jiao-ju-xing-mo-shi"&gt;圆角矩形模式&lt;/h2&gt;
&lt;p&gt;圆角矩形模式下，需要新建一个变量 &lt;code&gt;dilate&lt;/code&gt; 用于控制圆角半径，并在每一个图层的参数中添加 &lt;code&gt;'circle': False&lt;/code&gt; 与 &lt;code&gt;'dilate': dilate&lt;/code&gt; 就能生成圆角矩形地图。&lt;/p&gt;
&lt;p&gt;在各个图层中添加键值时要注意括号的嵌套，特别容易出错，可以复制以下代码修改。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;dilate = 100
palette=['#F4A460', '#FA8072']
layers = plot(
    (26.08594,119.29199), radius = 400,
    ax = ax,
    layers = {
            'perimeter': {"circle": False, "dilate": dilate},
            'streets': {
                'custom_filter': '["highway"~"motorway|trunk|primary|secondary|tertiary|residential|service|unclassified|pedestrian|footway"]',
                'width': {
                    'motorway': 5,
                    'trunk': 5,
                    'primary': 4.5,
                    'secondary': 4,
                    'tertiary': 3.5,
                    'residential': 3,
                    'service': 3,
                    'unclassified': 3,
                    'pedestrian': 3,
                    'footway': 3,
                },
                'circle': False,
                'dilate': dilate,
            },
            'building': {
                'tags': {
                    'building': True,
                    'landuse': 'construction'
                },
                'union': False,
                'circle': False,
                'dilate': dilate
            },
            'water': {
                'tags': {
                    'natural': ['water', 'bay']
                },
                'circle': False,
                'dilate': dilate
            },
            'green': {
                'tags': {
                    'landuse': 'grass',
                    'natural': ['island', 'wood'],
                    'leisure': 'park'
                },
                'circle': False,
                'dilate': dilate,
            },
            'forest': {
                'tags': {'landuse': 'forest'},
                'circle': False,
                'dilate': dilate,
            },
            'parking': {
                'tags': {
                    'amenity': 'parking',
                    'highway': 'pedestrian',
                    'man_made': 'pier'},
                'circle': False,
                'dilate': dilate
            },
    },
        drawing_kwargs = {
            'background': {'fc': '#F2F4CB', 'ec': '#dadbc1', 'hatch': 'ooo...', 'zorder': -1},
            'perimeter': {'fc': '#F2F4CB', 'ec': '#dadbc1', 'lw': 0, 'hatch': 'ooo...',  'zorder': 0},
            'green': {'fc': '#D0F1BF', 'ec': '#2F3737', 'lw': 1, 'zorder': 1},
            'forest': {'fc': '#64B96A', 'ec': '#2F3737', 'lw': 1, 'zorder': 1},
            'water': {'fc': '#a1e3ff', 'ec': '#2F3737', 'hatch': 'ooo...', 'hatch_c': '#85c9e6', 'lw': 1, 'zorder': 2},
            'parking': {'fc': '#F2F4CB', 'ec': '#2F3737', 'lw': 1, 'zorder': 3},
            'streets': {'fc': '#2F3737', 'ec': '#475657', 'alpha': 1, 'lw': 0, 'zorder': 3},
            'building': {'palette': palette, 'ec': '#2F3737', 'lw': .5, 'zorder': 4},
        },
        osm_credit = {'color': '#2F3737'}
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="NoCaption" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7540?authkey=ALYpzW-ZQ_VBXTU"&gt;&lt;img alt="NoCaption" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7540?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;{location}三坊七巷  Sanfang Qixiang, Fuzhou&lt;/p&gt;
&lt;h2 id="fang-xing-mo-shi"&gt;方形模式&lt;/h2&gt;
&lt;p&gt;欲使地图布满整个方形画面，需要使用 &lt;code&gt;matplotlib&lt;/code&gt; 的命令，指定绘制出的 x 与 y 轴范围。只需要在圆角矩形的代码中末插入以下代码：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;xmin, ymin, xmax, ymax = layers['perimeter'].bounds
dx, dy = xmax-xmin, ymax-ymin
a = .2
ax.set_xlim(xmin+a*dx, xmax-a*dx)
ax.set_ylim(ymin+a*dy, ymax-a*dy)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;变量 &lt;code&gt;a&lt;/code&gt; 表示裁去的四周比例。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="NoCaption" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7541?authkey=ALYpzW-ZQ_VBXTU"&gt;&lt;img alt="NoCaption" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7541?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;{location}闽江  Min River, Fuzhou&lt;/p&gt;
&lt;h2 id="jin-jie-cao-zuo"&gt;进阶操作&lt;/h2&gt;
&lt;p&gt;但不止如此，我想讨论一些进阶的操作。&lt;code&gt;layers&lt;/code&gt; 参数指定了绘制的图层，在 &lt;code&gt;drawing_kwargs&lt;/code&gt; 中可以指定图层的样式，由底至上常见以下几个图层：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;background&lt;/code&gt; 画布的背景，可以参考圆形模式视图中四个角落的空白；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;perimeter&lt;/code&gt; 图层的底层，如果没有图层覆盖，就表现为该图层的颜色，可以参考方形模式视图中的浅黄色；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;green&lt;/code&gt; &lt;code&gt;forest&lt;/code&gt; &lt;code&gt;water&lt;/code&gt; 等按字面意思理解即可，若想确定各个图层中 &lt;code&gt;tags&lt;/code&gt; 所代表的具体事物，见 &lt;a href="https://wiki.openstreetmap.org/wiki/Zh-hans:Map_Features"&gt;OpenStreetMap Wiki&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;drawing_kwargs&lt;/code&gt; 中的样式参数包括以下几个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fc&lt;/code&gt; 图层的填充颜色；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ec&lt;/code&gt; 图层的轮廓颜色；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hatch&lt;/code&gt; 图层的填充纹理，具体设置见 &lt;a href="https://matplotlib.org/stable/gallery/shapes_and_collections/hatch_style_reference.html"&gt;matplolib 文档&lt;/a&gt;；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hatch_c&lt;/code&gt; 图层填充纹理的颜色；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt; 图层的透明度；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lw&lt;/code&gt; 图层轮廓线条的宽度；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;zorder&lt;/code&gt; 层数，决定了图层之间相互掩盖的关系。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有了以上认识之后，我们就能更加随心所欲地绘制想要的地图了。还有一个额外的小技巧，OpenStreetMap 提供了封闭元素，使用地址描述绘制地点，不设置 &lt;code&gt;radius&lt;/code&gt; 参数， &lt;code&gt;prettymaps&lt;/code&gt; 就会自动匹配封闭元素的边界，使用这个方式可以绘制指定场所的示意图、行政区的示意图。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;fig, ax = plt.subplots(figsize = (12, 12), constrained_layout = True)

# 画布颜色
fig.patch.set_facecolor('#eee')

# 边界向外扩张
def postprocessing(layers):
    layers['perimeter'] = layers['perimeter'].buffer(10)
    return layers

layers = plot(
    "北京大学, 5号, 颐和园路, 海淀区, 北京市, 100871, 中国",
    ax = ax,
    postprocessing = postprocessing,
    layers = {
            'perimeter': {},
            'streets': {
                'custom_filter': '["highway"~"residential|service|unclassified|pedestrian|footway"]',
                'width': {
                    'residential': 1.5,
                    'service': 1.5,
                    'unclassified': 1,
                    'pedestrian': 1,
                    'footway': 1,
                }
            },
            'building': {
                'tags': {
                    'building': True,
                    'landuse': 'construction'
                },
                'union': False
            },
            'water': {
                'tags': {
                    'natural': ['water']
                }
            },
            'green': {
                'tags': {
                    'landuse': ['grass'],
                    'natural': ['island', 'wood'],
                    'leisure': ['park', 'garden']
                }
            },
            'leisure': {
                'tags': {
                    'leisure': ['pitch', 'playground']
                }
            },
            'parking': {
                'tags': {
                    'amenity': 'parking',
                    'highway': 'pedestrian',
                    'man_made': 'pier'
                }
            },
        },
        drawing_kwargs = {
            'background': {'fc': '#eee', 'lw': 0 , 'zorder': -1},
            'perimeter': {'fc': '#F2F4CB', 'ec': '#2F3737', 'lw': 2, 'linestyle': 'dashed', 'zorder': 0},
            'green': {'fc': '#D0F1BF', 'ec': '#2F3737', 'lw': 0.5, 'zorder': 1},
            'leisure': {'fc': '#aae0cb', 'ec': '#2F3737', 'lw': 0.5, 'zorder': 1},
            'water': {'fc': '#a1e3ff', 'ec': '#2F3737', 'lw': 0.5, 'zorder': 2},
            'parking': {'fc': '#F2F4CB', 'ec': '#2F3737', 'lw': 0.5, 'zorder': 3},
            'streets': {'fc': '#2F3737', 'alpha': 0.7, 'lw': 0, 'zorder': 3},
            'building': {'palette': ['#FFC857', '#E9724C', '#C5283D'], 'ec': '#2F3737', 'lw': .5, 'zorder': 4},
        },
        osm_credit = {'color': '#2F3737'}
)
plt.savefig('pku.png')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;由于 &lt;code&gt;prettymaps&lt;/code&gt; 的封闭边界太过于靠近建筑，可以使用 &lt;code&gt;buffer()&lt;/code&gt; 将边界向外扩张一部分，能获得更好的视觉效果。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="NoCaption" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7542?authkey=ALYpzW-ZQ_VBXTU"&gt;&lt;img alt="NoCaption" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7542?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;{location}北京大学  Peking University, Beijing&lt;/p&gt;
&lt;p&gt;调用相应的图层标签，还可以绘制铁路、地铁线路。以下代码调用了了铁路、地铁、站台并将这些元素各自组合为图层：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-py"&gt;'railway': {
    'custom_filter': '["railway"~"rail"]',
    'width': 2,
    'circle': False,
    'dilate': dilate,
},
'subway': {
    'tags': {
        "railway": "subway",
    },
    'circle': False,
    'dilate': dilate,
},
'platform': {
    'tags': {
        'railway': 'platform'
    },
    'circle': False,
    'dilate': dilate,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="NoCaption" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7543?authkey=ALYpzW-ZQ_VBXTU"&gt;&lt;img alt="NoCaption" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7543?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p class="intro"&gt;{location}天津站  Tianjin Railway Station, Tianjin&lt;/p&gt;
&lt;h2 id="zi-ji-de-hua"&gt;自己的话&lt;/h2&gt;
&lt;p&gt;不得不说，&lt;code&gt;prettymaps&lt;/code&gt; 是一款十分优秀的 Python 绘图包，它的操作十分简单，足以让我这样不了解 GIS 的用户迅速入门，绘制出十分惊艳的地图。但在实际使用中，&lt;code&gt;prettymaps&lt;/code&gt; 还是存在着这样那样的问题，使得它的定位可能只能止步于一个发挥创意的工具，而不能成为一个合适的创作工具。&lt;/p&gt;
&lt;p&gt;譬如说，各种各样的用户因为各种各样的需求接触到 &lt;code&gt;prettymaps&lt;/code&gt;，其中不乏有些用户想为某些场所、学校绘制导览地图，但 &lt;code&gt;prettymaps&lt;/code&gt; 添加元素的方式不够灵活，这会让这些用户使用起来相当费劲。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;prettymaps&lt;/code&gt; 的数据基于 OpenStreetMap，因为国内这样那样的相关法律法规，注定了 OpenStreetMap 的国内贡献者特别少，这对于开源项目来说几乎是致命的。由于 OpenStreetMap 缺少国内数据，使用 &lt;code&gt;prettymaps&lt;/code&gt; 绘制国内城市的地图是相当力不从心的。&lt;/p&gt;
&lt;p&gt;这方面具体表现为大面积缺失建筑，绘制的地图上十分空旷；缺少海洋、海岸线数据，绘制近海区域时呈现为大片空白等等。&lt;/p&gt;
&lt;p&gt;OpenStreetMap 的封闭边界是非常亮眼的功能，在 &lt;code&gt;prettymaps&lt;/code&gt; 中用字符描述地点就能绘制出行政区、建筑群等等。但从另一个方面考虑，OpenStreetMap 的封闭边界是由用户贡献的，所以在涉边境线、涉敏感地区时务必小心。&lt;/p&gt;
&lt;p&gt;总而言之，&lt;code&gt;prettymaps&lt;/code&gt; 为我们提供了另一个视角，让我们俯瞰日常生活的这座城市，感受建筑布局的美。而 OpenStreetMap 可以说是一个伟大的项目，它借助所有人的力量去描绘我们所生活的这个世界的经纬，这个理念闪耀着国际主义与理想主义的光芒。&lt;/p&gt;
&lt;p&gt;{warn begin}本文最后更新于 2022 年 08 月 14 日，请确定内容是否过时。{warn end}&lt;/p&gt;</content><category term="碎碎念"></category><category term="prettymaps"></category><category term="Python"></category><category term="matplotlib"></category><category term="OpenStreetMap"></category></entry><entry><title>在 Windows 安装 prettymaps 的踩坑实录</title><link href="https://tseing.github.io/sui-sui-nian/2022-08-11-prettymaps-install.html" rel="alternate"></link><published>2022-08-11T00:00:00+08:00</published><updated>2022-08-11T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-08-11:/sui-sui-nian/2022-08-11-prettymaps-install.html</id><summary type="html">&lt;p&gt;prettymaps 是一个 Python 地图绘图包，在 matplotlib 绘图包的助力下能够绘制出十分精美的地图。但是 prettymaps 在 Windows 下的安装十分恼人，这篇笔记记录下了安装过程中的常见错误。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;code&gt;prettymaps&lt;/code&gt; 是一个 Python 地图绘图包，能够使用 OpenStreetMap 的地图数据，在 &lt;code&gt;matplotlib&lt;/code&gt; 绘图包的助力下能够绘制出十分精美、极具艺术感的地图。但是 &lt;code&gt;prettymaps&lt;/code&gt; 在 Windows 下的安装十分恼人，这篇笔记记录下了安装过程中的常见错误。&lt;/p&gt;
&lt;h2 id="chang-shi-zhi-jie-shi-yong-pip-an-zhuang"&gt;尝试直接使用 pip 安装&lt;/h2&gt;
&lt;p&gt;在项目的 &lt;a href="https://github.com/marceloprates/prettymaps"&gt;Github 仓库&lt;/a&gt;中，作者提供的方法是直接通过 &lt;code&gt;pip install prettymaps&lt;/code&gt; 安装，但是在 Windows 设备上貌似不起作用。命令行信息提示在安装 &lt;code&gt;Fiona&lt;/code&gt; 时发生错误，错误信息如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  Using cached Fiona-1.8.21.tar.gz (1.0 MB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  &amp;times; python setup.py egg_info did not run successfully.
  &amp;boxv; exit code: 1
  ╰&amp;boxh;&amp;gt; [1 lines of output]
      A GDAL API version must be specified. Provide a path to gdal-config using a GDAL_CONFIG environment variable or use a GDAL_VERSION environment variable.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

&amp;times; Encountered error while generating package metadata.
╰&amp;boxh;&amp;gt; See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里我们需要先明确一下 &lt;code&gt;prettymaps&lt;/code&gt; 的依赖：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- prettymaps
╰&amp;boxh;&amp;gt; - Fiona
    ╰&amp;boxh;&amp;gt; - GDAL
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再根据错误信息，也就是在安装 &lt;code&gt;Fiona&lt;/code&gt; 依赖时调用的 &lt;code&gt;GDAL&lt;/code&gt; 未正确配置，我的设备上没有安装 &lt;code&gt;GDAL&lt;/code&gt;，所以需要先安装 &lt;code&gt;GDAL&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id="an-zhuang-gdal-corebu-tui-jian"&gt;安装 GDAL core（不推荐）&lt;/h2&gt;
&lt;p&gt;可以先尝试直接 &lt;code&gt;pip install gdal&lt;/code&gt;，不出意外的话会有以下错误：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;extensions/gdalconst_wrap.c(2703): fatal error C1083: 无法打开包括文件: &amp;ldquo;gdal.h&amp;rdquo;: No such file or directory
extensions/gdal_array_wrap.cpp(2829): fatal error C1083: 无法打开包括文件: &amp;ldquo;gdal.h&amp;rdquo;: No such file or directory
extensions/gnm_wrap.cpp(2820): fatal error C1083: 无法打开包括文件: &amp;ldquo;gdal.h&amp;rdquo;: No such file or directory
extensions/ogr_wrap.cpp(2838): fatal error C1083: 无法打开包括文件: &amp;ldquo;gdal.h&amp;rdquo;: No such file or directory
extensions/osr_wrap.cpp(2879): fatal error C1083: 无法打开包括文件: &amp;ldquo;cpl_string.h&amp;rdquo;: No such file or directory
extensions/gdal_wrap.cpp(2883): fatal error C1083: 无法打开包括文件: &amp;ldquo;cpl_port.h&amp;rdquo;: No such file or directory
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这是因为 Windows 缺少 Linux 自带的 GDAL core，一个解决方法就是在 Windows 上安装 GDAL core。因为这个过程无比繁琐，而且我不是 GIS 从业者，没有使用其他依赖于 GDAL core 软件的需要，所以我最后没有采取这种方法。&lt;/p&gt;
&lt;p&gt;但一开始我并未意识到安装 GDAL core 如此麻烦，以至于我也折腾了半天 GDAL core 的安装，我把这一解决方案的整个流程总结如下：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;安装 GDAL core&lt;/code&gt; ⇨ &lt;code&gt;安装 GDAL bindings&lt;/code&gt; ⇨ &lt;code&gt;设置 GDAL core 环境变量&lt;/code&gt; ⇨ &lt;code&gt;安装 Fiona&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;安装 GDAL core 与 bindings，可以前往 &lt;a href="https://www.gisinternals.com/release.php"&gt;GISInternals&lt;/a&gt; 下载安装文件。因为我的设备上安装了 Visual Studio 2017，我就选择了 MSVC 2017 x64 版本。&lt;/p&gt;
&lt;p&gt;&lt;img alt="GISInternals首页" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7531?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;要下载 GDAL core 和 bindings 两个安装文件，其中 bindings 需要与自己的 Python 版本匹配，下载完成后先安装 core 再安装 bindings。&lt;/p&gt;
&lt;p&gt;&lt;img alt="GISInternals下载文件" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7533?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;安装完成后，使用 pip 可以查询到 &lt;code&gt;GDAL&lt;/code&gt; 的安装信息：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; pip list
Package           Version
----------------- -------
GDAL              3.5.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时已经在 Python 中安装了 &lt;code&gt;GDAL&lt;/code&gt;，但还需要配置环境变量后才能安装 &lt;code&gt;Fiona&lt;/code&gt;。详细步骤可以参考&lt;a href="https://zhuanlan.zhihu.com/p/141226948"&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id="shi-yong-whl-wen-jian-an-zhuang-tui-jian"&gt;使用 .whl 文件安装（推荐）&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;GDAL&lt;/code&gt; 与 &lt;code&gt;Fiona&lt;/code&gt; 不能直接通过 pip 安装是因为缺少 GDAL core，在不安装 GDAL core 的情况下，可以使用预编译的 &lt;code&gt;.whl&lt;/code&gt; 安装这两个依赖。UCI 的 &lt;a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal"&gt;Python 拓展包仓库&lt;/a&gt;提供了这两个依赖的 &lt;code&gt;.whl&lt;/code&gt; 文件。&lt;/p&gt;
&lt;p&gt;&lt;img alt="GDAL下载页面" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7530?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;要注意文件名中的两个参数，cp 指 Python 版本，win 字段指 CPU 架构，也就是常说的 32 位或 64 位系统。因此我选择的就是 &lt;code&gt;GDAL‑3.4.3‑cp39‑cp39‑win_amd64.whl&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;将下载文件移入 Python 安装目录下的 &lt;code&gt;Scripts&lt;/code&gt; 文件夹中，在该文件夹中打开终端，能过以下命令尝试安装：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python-repl"&gt;&amp;gt;&amp;gt;&amp;gt; pip install GDAL-3.4.3-cp39-cp39-win_amd64.whl
ERROR: Could not install packages due to an OSError: [Errno 22] Invalid argument
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;出现这个错误时我寻找了大量解决方法，当然，这个错误也没法解决，这纯粹是安装文件的问题。这个隐藏的坑浪费我大量时间，后来我发现将安装文件换成&lt;code&gt;3.3.3&lt;/code&gt;版本，就能成功安装了。&lt;/p&gt;
&lt;p&gt;但这还没结束，同样在 UCI 提供的仓库中下载 &lt;code&gt;Fiona&lt;/code&gt;，用同样的方法安装，这时就会发现 &lt;code&gt;Fiona&lt;/code&gt; 尝试卸载 &lt;code&gt;GDAL&lt;/code&gt; 并使用 pip 直接安装其他版本的 &lt;code&gt;GDAL&lt;/code&gt;。当然，结局就会和上面的情况一样，用 pip 直接安装是安装不上 &lt;code&gt;GDAL&lt;/code&gt; 的。&lt;/p&gt;
&lt;p&gt;这是因为这里还有一个隐藏的大坑，就是 &lt;code&gt;GDAL&lt;/code&gt; 与 &lt;code&gt;Fiona&lt;/code&gt; 的版本必须匹配，否则就会自动重新下载。经过大量尝试，Python 版本为 &lt;code&gt;3.9.10&lt;/code&gt; 的条件下，可以使用 &lt;code&gt;GDAL==3.3.2&lt;/code&gt; 与 &lt;code&gt;Fiona==1.8.20&lt;/code&gt;，这两个版本在 UCI 提供的仓库中都没有，我把下载链接放在后文。&lt;/p&gt;
&lt;p&gt;安装好这两个依赖后，就可以直接使用 &lt;code&gt;pip install prettymaps&lt;/code&gt; 安装 &lt;code&gt;prettymaps&lt;/code&gt; 了。在 Python 交互模式下输入 &lt;code&gt;import prettymaps&lt;/code&gt;，若无错误信息，就成功安装好 &lt;code&gt;prettymaps&lt;/code&gt; 了。&lt;/p&gt;
&lt;h2 id="zong-jie"&gt;总结&lt;/h2&gt;
&lt;p&gt;Windows 下安装 &lt;code&gt;prettymaps&lt;/code&gt; 的错误是 &lt;code&gt;GDAL&lt;/code&gt; 与 &lt;code&gt;Fiona&lt;/code&gt; 两个依赖未能成功安装造成的，本质原因是 Windows 缺少 GDAL core。&lt;/p&gt;
&lt;p&gt;由于以上原因在 Windows 下未能成功安装 &lt;code&gt;prettymaps&lt;/code&gt; 的推荐解决步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;安装 &lt;a href="http://1drv.stdfirm.com/u/s!AtseC45rsRhNunGGuDYayQdVADT3?e=RAJEsi"&gt;GDAL-3.3.2-cp39-cp39-win_amd64.whl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;安装 &lt;a href="http://1drv.stdfirm.com/u/s!AtseC45rsRhNunAkPiG4AOb9V8yi?e=fAgNhp"&gt;Fiona-1.8.20-cp39-cp39-win_amd64.whl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pip install prettymaps&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;{warn begin}本文最后更新于 2022 年 08 月 11 日，请确定内容是否过时。{warn end}&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/32224877"&gt;Python中GDAL的安装 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/141226948"&gt;windows10 环境中安装GDAL及其python绑定 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/389235808"&gt;window 操作系统安装python的第三方包Fiona报错时解决方法 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="prettymaps"></category><category term="Python"></category><category term="Windows"></category></entry><entry><title>《统计学习方法》第四章：朴素贝叶斯法</title><link href="https://tseing.github.io/sui-sui-nian/2022-08-05-statistical-learning-chapter4.html" rel="alternate"></link><published>2022-08-05T00:00:00+08:00</published><updated>2022-08-05T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-08-05:/sui-sui-nian/2022-08-05-statistical-learning-chapter4.html</id><summary type="html">&lt;p&gt;《统计学习方法》第四章介绍的贝叶斯法利用了条件概率原理，通过贝叶斯法可以从事件的统计数据中反推出产生随机事件的概率模型。&lt;/p&gt;</summary><content type="html">&lt;h2 id="mo-xing"&gt;模型&lt;/h2&gt;
&lt;h3 id="tiao-jian-gai-lu"&gt;条件概率&lt;/h3&gt;
&lt;p&gt;引入贝叶斯法模型前，首先回顾一下条件概率的基本公式。所谓条件概率，就是在某事件 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 发生的条件下，求另一事件 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 发生的概率，条件概率可以通过下式计算：&lt;/p&gt;
&lt;div class="math"&gt;$$P(A|B)=\frac{P(AB)}{P(B)}$$&lt;/div&gt;
&lt;h4&gt;乘法公式&lt;/h4&gt;
&lt;p&gt;根据条件概率定义，可以得到&lt;/p&gt;
&lt;div class="math"&gt;$$P(A|B)=\frac{P(AB)}{P(B)}$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(B|A)=\frac{P(AB)}{P(A)}$$&lt;/div&gt;
&lt;p&gt;显然有&lt;/p&gt;
&lt;div class="math"&gt;$$P(AB)=P(A)P(B|A)=P(B)P(A|B)$$&lt;/div&gt;
&lt;p&gt;该式就称为乘法公式。&lt;/p&gt;
&lt;h4&gt;全概率公式&lt;/h4&gt;
&lt;p&gt;若将&lt;em&gt;样本空间&lt;/em&gt; &lt;span class="math"&gt;\(\Omega\)&lt;/span&gt; 分割为&lt;em&gt;互不相容&lt;/em&gt;的各事件 &lt;span class="math"&gt;\(B_1,B_2,\cdots B_n\)&lt;/span&gt;，那么 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 事件的概率就应当是 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 的&lt;em&gt;所有&lt;/em&gt;条件概率与相应条件发生概率乘积之和：&lt;/p&gt;
&lt;div class="math"&gt;$$P(A)=\sum_{i=1}^nP(B_i)P(A|B_i)$$&lt;/div&gt;
&lt;p&gt;考虑 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 为两个事件的情况，利用全概系公式可以将事件 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 发生的概率写为&lt;/p&gt;
&lt;div class="math"&gt;$$P(A)=P(B)P(A|B)+P(\bar{B})P(A|\bar{B})$$&lt;/div&gt;
&lt;h4&gt;贝叶斯公式&lt;/h4&gt;
&lt;p&gt;假设 &lt;span class="math"&gt;\(Y_1,Y_2,\cdots Y_n\)&lt;/span&gt; 是对样本空间的划分，&lt;span class="math"&gt;\(X\)&lt;/span&gt; 为样本空间中的一个事件，那么根据条件概率的定义，有&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y_i|X)=\frac{P(XY_i)}{P(X)}$$&lt;/div&gt;
&lt;p&gt;利用乘法公式，该式可以写为&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y_i|X)=\frac{P(X|Y_i)P(Y_i)}{P(X)}$$&lt;/div&gt;
&lt;p&gt;利用全概率公式，得到&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y_i|X)=\frac{P(X|Y_i)P(Y_i)}{\sum_{j=1}^nP(X|Y_j)P(Y_j)}$$&lt;/div&gt;
&lt;p&gt;该式即为贝叶斯公式。&lt;/p&gt;
&lt;h3 id="po-su-bei-xie-si-fa-mo-xing"&gt;朴素贝叶斯法模型&lt;/h3&gt;
&lt;p&gt;假设训练集 &lt;span class="math"&gt;\(T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}\)&lt;/span&gt; 是由 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; &lt;em&gt;独立同分布&lt;/em&gt;产生，其中输入为特征向量，输出为类标记。&lt;/p&gt;
&lt;p&gt;先验概率分布为&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=c_k),\qquad k=1,2,\cdots,K$$&lt;/div&gt;
&lt;p&gt;条件概率分布为&lt;/p&gt;
&lt;div class="math"&gt;$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)$$&lt;/div&gt;
&lt;p&gt;对条件概率分布作&lt;em&gt;条件独立性&lt;/em&gt;的假设，即认为某特征向量事件的各分量事件相互独立，因此该向量代表事件的发生概率为该向量的各分量概率乘积：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    P(X=x|Y=c_k)&amp;amp;=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)\\
    &amp;amp;=\prod_{j=1}^nP(X^{j}=x^{(j)}|Y=c_k)
\end{align}$$&lt;/div&gt;
&lt;p&gt;朴素贝叶斯法的目的在于通过模型计算输入 &lt;span class="math"&gt;\(x\)&lt;/span&gt; 后的后验概率分布 &lt;span class="math"&gt;\(P(Y=c_k|X=x)\)&lt;/span&gt; 并输出后验概率最大的 &lt;span class="math"&gt;\(c_k\)&lt;/span&gt; 作为 &lt;span class="math"&gt;\(x\)&lt;/span&gt; 的类。&lt;/p&gt;
&lt;p&gt;类比推导得到的贝叶斯公式&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y_i|X)=\frac{P(X|Y_i)P(Y_i)}{\sum_{j=1}^nP(X|Y_j)P(Y_j)}$$&lt;/div&gt;
&lt;p&gt;后验概率分布可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}$$&lt;/div&gt;
&lt;p&gt;代入条件独立性假设，即有&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=c_k|X=x)=\frac{\color{orangered}{P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}}{\sum_k P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}$$&lt;/div&gt;
&lt;p&gt;其中不论 &lt;span class="math"&gt;\(c_k\)&lt;/span&gt; 为何值时，分母部分都是不变的，不影响发生概率大小的比较，因此朴素贝叶斯方法的模型可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$y=\arg \max_{c_k}\color{orangered}{P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}$$&lt;/div&gt;
&lt;h2 id="ce-lue_1"&gt;策略&lt;/h2&gt;
&lt;p&gt;损失函数可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}
    L(Y,f(X))=
    \begin{cases}
        1, &amp;amp;Y\neq f(X)\\
        0, &amp;amp;Y=f(X)
    \end{cases}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;期望风险函数为条件期望&lt;/p&gt;
&lt;div class="math"&gt;$$R_{\mathrm{exp}}(f)=E[L(Y,f(X))]=E_X\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)$$&lt;/div&gt;
&lt;p&gt;最小化期望风险就需要对 &lt;span class="math"&gt;\(X=x\)&lt;/span&gt; 逐个最小化：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    f(x)&amp;amp;=\arg \min_{y\in\mathcal{Y}}\sum_{k=1}^KL(c_k,y)P(c_k|X=x)\\
    &amp;amp;=\arg \min_{y\in\mathcal{Y}}\sum_{k=1}^KP(y\neq c_k|X=x)\\
    &amp;amp;=\arg \min_{y\in\mathcal{Y}}(1-P(y=c_k|X=x))\\
    &amp;amp;=\arg \max_{y\in\mathcal{Y}}P(y=c_k|X=x)\\
\end{align}$$&lt;/div&gt;
&lt;p&gt;也就是说后验概率最大的情况下期望风险就最小，这正是朴素贝叶斯法决定输出类别的方法。&lt;/p&gt;
&lt;h2 id="suan-fa"&gt;算法&lt;/h2&gt;
&lt;h3 id="ji-da-si-ran-gu-ji"&gt;极大似然估计&lt;/h3&gt;
&lt;p&gt;使用频率估计概率，先验概率为&lt;/p&gt;
&lt;div class="math"&gt;$$\color{teal}{P(Y=c_k)}=\frac{\sum_{i=1}^NI(y_i=c_k)}{N}$$&lt;/div&gt;
&lt;p&gt;特征向量的第 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 个分量 &lt;span class="math"&gt;\(x^{(j)}\)&lt;/span&gt; 可能取的值构成了集合 &lt;span class="math"&gt;\(\{a_{j1},a_{j2},\cdots,a_{jS_j}\}\)&lt;/span&gt;，那么条件概率为&lt;/p&gt;
&lt;div class="math"&gt;$$\color{steelblue}{P(X^{(j)}=a_{jl}|Y=c_k)}=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;算法 4.1&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：训练集 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 与实例 &lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;br/&gt;
输出：&lt;span class="math"&gt;\(x\)&lt;/span&gt; 的类别&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;计算先验概率 &lt;span class="math"&gt;\(\color{teal}{P(Y=c_k)}\)&lt;/span&gt; 与条件概率 &lt;span class="math"&gt;\(\color{steelblue}{P(X^{(j)}=a_{jl}|Y=c_k)}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;应用条件独立性假设，计算 &lt;span class="math"&gt;\(P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;确定 &lt;span class="math"&gt;\(x\)&lt;/span&gt; 的类别，&lt;span class="math"&gt;\(y=\arg \max_{c_k}P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="bei-xie-si-gu-ji"&gt;贝叶斯估计&lt;/h3&gt;
&lt;p&gt;在极大似然估计中可能会出现估计的概率为零，从而导致整个特征向量的估计概率也为零，影响估计结果。贝叶斯估计通过在频数上引入正数 &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; 从而避免了这种偏差：&lt;/p&gt;
&lt;div class="math"&gt;$$\color{teal}{P(Y=c_k)}=\frac{\sum_{i=1}^NI(y_i=c_k)+\color{orangered}{\lambda}}{N+\color{orangered}{K\lambda}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\color{steelblue}{P_{\lambda}(X^{j}=a_{jl}|Y=c_k)}=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\color{orangered}{\lambda}}{\sum_{i=1}^NI(y_i=c_k)+\color{orangered}{S_j\lambda}}$$&lt;/div&gt;
&lt;p&gt;若取 &lt;span class="math"&gt;\(\lambda=0\)&lt;/span&gt; 时就是极大似然估计，常取 &lt;span class="math"&gt;\(\lambda=1\)&lt;/span&gt;，称为拉普拉斯平滑。&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references_1"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://book.douban.com/subject/33437381/"&gt;李航, 2019. 统计学习方法（第2版）. 清华大学出版社.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://book.douban.com/subject/5998092/"&gt;茆诗松, 程依明, 濮晓龙, 2011. 概率论与数理统计教程. 高等教育出版社.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="统计学习方法"></category><category term="Machine learning"></category><category term="Algorithm"></category></entry><entry><title>《统计学习方法》第三章：k 近邻法</title><link href="https://tseing.github.io/sui-sui-nian/2022-08-02-statistical-learning-chapter3.html" rel="alternate"></link><published>2022-08-02T00:00:00+08:00</published><updated>2022-08-02T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-08-02:/sui-sui-nian/2022-08-02-statistical-learning-chapter3.html</id><summary type="html">&lt;p&gt;《统计学习方法》第三章介绍了 k 近邻模型，k 近邻模型原理十分简单，如何划分向量空间从而降低算法的时间复杂度就成为了更关键的问题。&lt;/p&gt;</summary><content type="html">&lt;h2 id="k-jin-lin-mo-xing"&gt;k 近邻模型&lt;/h2&gt;
&lt;p&gt;k 近邻法将输入实例的特征空间划分为若干子空间，子空间中的若干实例 &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 同属于 &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; 类别。具体来说，k 近邻法通过在训练集中找到与新输入实例最邻近的 k 个实例，这 k 个实例大部分属于 &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; 类别，就也将新输入实例归属为 &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; 类别。&lt;/p&gt;
&lt;p&gt;&lt;img alt="k近邻模型" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7519?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;训练集数据为&lt;/p&gt;
&lt;div class="math"&gt;$$
T=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}
$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(x_i\in\mathbf{R}^n\)&lt;/span&gt; 为特征向量，&lt;span class="math"&gt;\(y_i\in\{c_1,c_2,\cdots,c_K\}\)&lt;/span&gt; 为实例类别。k 近邻法根据&lt;em&gt;距离度量&lt;/em&gt;，在包括最邻近的 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 个点的邻域 &lt;span class="math"&gt;\(N_k(x)\)&lt;/span&gt; 中确定 &lt;span class="math"&gt;\(x\)&lt;/span&gt; 的 类别 &lt;span class="math"&gt;\(y\)&lt;/span&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$y=\arg \max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_i),\qquad i=1,2,\cdots,N;\ j=1,2,3,\cdots,K$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(I\)&lt;/span&gt; 为指示函数，条件为真时为 &lt;span class="math"&gt;\(1\)&lt;/span&gt;，条件为假时为 &lt;span class="math"&gt;\(0\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2 id="ce-lue"&gt;策略&lt;/h2&gt;
&lt;h3 id="ju-chi-du-liang"&gt;距离度量&lt;/h3&gt;
&lt;p&gt;在二维、三维空间中通常使用欧氏距离来衡量两点间的远近关系（相似程度）：&lt;/p&gt;
&lt;div class="math"&gt;$$d=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$$&lt;/div&gt;
&lt;div class="math"&gt;$$d=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2+(z_2-z_1)^2}$$&lt;/div&gt;
&lt;p&gt;在 &lt;span class="math"&gt;\(\mathbf{R}^n\)&lt;/span&gt; 空间中，更通常的距离度量是 &lt;span class="math"&gt;\(L_p\)&lt;/span&gt; 距离，&lt;span class="math"&gt;\(L_p\)&lt;/span&gt; 距离是由距离度量的概念通过推广得到的，因此同样具有衡量两点间远近关系（相似程度）的作用。&lt;/p&gt;
&lt;p&gt;设 &lt;span class="math"&gt;\(x_i,x_j\in\mathbf{R}^n\)&lt;/span&gt;，&lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(x_j\)&lt;/span&gt; 的 &lt;span class="math"&gt;\(L_p\)&lt;/span&gt; 距离由下式给出：&lt;/p&gt;
&lt;div class="math"&gt;$$L_p(x_i,x_j)=\left(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p\right)^{\frac{1}{p}}$$&lt;/div&gt;
&lt;h3 id="fen-lei-ce-lue"&gt;分类策略&lt;/h3&gt;
&lt;p&gt;k 近邻法的分类遵循多数表决规则，即输入实例附近 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 个邻近的训练实例的大多数类决定了预测结果。因此 k 近邻法中的 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 值决定了在一定的距离度量内选取分类「参考点」的数量，可以想知，若选取的 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 值太小，分类结果会对最近邻的几个点敏感，模型就趋于复杂，更容易发生过拟合。&lt;/p&gt;
&lt;h2 id="suan-fa_1"&gt;算法&lt;/h2&gt;
&lt;h3 id="gou-zao-kd-shu"&gt;构造 kd 树&lt;/h3&gt;
&lt;p&gt;kd 树是一种二叉树，kd 树通过对 &lt;span class="math"&gt;\(\mathbf{R}^n\)&lt;/span&gt; 空间中每一个维度&lt;em&gt;逐次地&lt;/em&gt;二分，最终将整个特征空间划分为若干超矩形，kd 树的每一个结点（训练实例）对应于一个超矩形。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法 3.2&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：数据集 &lt;span class="math"&gt;\(T\)&lt;/span&gt;&lt;br/&gt;
输出：kd 树&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;选择 &lt;span class="math"&gt;\(x^{(1)}\)&lt;/span&gt; 为坐标轴，以所有实例的 &lt;span class="math"&gt;\(x_i^{(1)}\)&lt;/span&gt; 坐标中位数（若中位有两个数则取其中之一）为切分点，将 &lt;span class="math"&gt;\(x^{(1)}\)&lt;/span&gt; 切分为两部分。&lt;/li&gt;
&lt;li&gt;重复切分：深度为 &lt;span class="math"&gt;\(j\)&lt;/span&gt; 的结点，选择 &lt;span class="math"&gt;\(x^{(l)}\)&lt;/span&gt; 为切分坐标轴，&lt;span class="math"&gt;\(l=j\ \mathrm{mod}\ k+1\)&lt;/span&gt;。简单来说，&lt;ul&gt;
&lt;li&gt;对于 &lt;span class="math"&gt;\(\mathbf{R}^2\)&lt;/span&gt; 空间，步骤为 &lt;span class="math"&gt;\(x^{(1)}\rightarrow x^{(2)}\rightarrow x^{(1)}\rightarrow\cdots\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;对于 &lt;span class="math"&gt;\(\mathbf{R}^3\)&lt;/span&gt; 空间，步骤为 &lt;span class="math"&gt;\(x^{(1)}\rightarrow x^{(2)}\rightarrow x^{(3)}\rightarrow x^{(1)}\rightarrow\cdots\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;直至将所有实例点切分完成。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="构造kd树" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7523?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;h3 id="sou-suo-kd-shu"&gt;搜索 kd 树&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;算法 3.3&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：kd 树，目标点 &lt;span class="math"&gt;\(S\)&lt;/span&gt;&lt;br/&gt;
输出：&lt;span class="math"&gt;\(S\)&lt;/span&gt; 的最近邻&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;首先在 kd 树中找出目标点 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 所属的区域，具体来说就是从根结点 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 开始逐层向下访问，直到目标点 &lt;span class="math"&gt;\(S\)&lt;/span&gt;。在访问过程的具体算法方面，可以通过判断点 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 的坐标与切分点的大小关系来快速准确地确定访问路径。&lt;/li&gt;
&lt;li&gt;到达点 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 的父结点，以此结点作为 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 的「当前最近点」。&lt;/li&gt;
&lt;li&gt;递归向上层访问，每次访问进行两个操作：&lt;ol&gt;
&lt;li&gt;如果该点距离 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 更近，将其作为新的「当前最近点」。&lt;/li&gt;
&lt;li&gt;因为 kd 为二叉树，该点必然存在另一分支子结点，那么就需要检查分支下是否存在更近的点。具体做法是判断分支子结点的区域是否与以点 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 为圆心、以点 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 与「当前最近点」距离为半径的圆相交。&lt;ul&gt;
&lt;li&gt;若相交，则访问分支子结点并进行 3 步骤；&lt;/li&gt;
&lt;li&gt;若不相交，回退到上一层。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;最终回到根结点时，搜索结束。最后的「当前最近点」即为 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 的最近邻。&lt;/li&gt;
&lt;/ol&gt;
&lt;video controls=""&gt;
&lt;source src="https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNumKI-2A9UTX-DXZs/root/content" type="video/mp4"/&gt;
&lt;/video&gt;
&lt;hr/&gt;
&lt;h2 id="references_1"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://book.douban.com/subject/33437381/"&gt;李航, 2019. 统计学习方法（第2版）. 清华大学出版社.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1GM4y1c78K?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=a5a1b5dd5c760997f9e16b7806d64651"&gt;向量的【范数】：模长的推广，柯西不等式 - 哔哩哔哩&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="统计学习方法"></category><category term="Machine learning"></category><category term="Algorithm"></category></entry><entry><title>《统计学习方法》第二章：感知机</title><link href="https://tseing.github.io/sui-sui-nian/2022-07-27-statistical-learning-chapter2.html" rel="alternate"></link><published>2022-07-27T00:00:00+08:00</published><updated>2022-07-27T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-07-27:/sui-sui-nian/2022-07-27-statistical-learning-chapter2.html</id><summary type="html">&lt;p&gt;《统计学习方法》第二章介绍了一种二分类模型——感知机，感知机具有直观的几何模型，有助于理解分类原理。感知机采用随机梯度下降策略选择最佳模型，随机梯度下降也是常用的一种最优化方法。&lt;/p&gt;</summary><content type="html">&lt;h2 id="mo-xing"&gt;模型&lt;/h2&gt;
&lt;h3 id="ding-yi"&gt;定义&lt;/h3&gt;
&lt;p&gt;感知机是&lt;em&gt;二分类&lt;/em&gt;的&lt;em&gt;线性&lt;/em&gt;分类模型，感知机定义为&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=\mathrm{sign}(w\cdot x+b)$$&lt;/div&gt;
&lt;div class="math"&gt;$$\begin{equation}
    \mathrm{sign}(x)=
    \begin{cases}
        +1, &amp;amp;x\geqslant 0\\
        -1, &amp;amp;x&amp;lt;0
    \end{cases}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;其中输入 &lt;span class="math"&gt;\(x\in \mathbf{R}^n\)&lt;/span&gt; 表示实例的特征向量，输出 &lt;span class="math"&gt;\(y\in \{+1,-1\}\)&lt;/span&gt;表示实例的类别。模型参数 &lt;span class="math"&gt;\(w\)&lt;/span&gt; 称为权值，参数 &lt;span class="math"&gt;\(b\)&lt;/span&gt; 称为偏置。&lt;/p&gt;
&lt;h3 id="ji-he-mo-xing"&gt;几何模型&lt;/h3&gt;
&lt;p&gt;若 &lt;span class="math"&gt;\(x\in \mathbf{R}^2\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    w\cdot x+b&amp;amp;=0\\
    (w^{(1)},w^{(2)})^\mathrm{T}\cdot (x^{(1)},x^{(2)})^\mathrm{T}+b&amp;amp;=0\\
\end{align}$$&lt;/div&gt;
&lt;p&gt;该形式满足平面中的直线方程：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    (i,j)^\mathrm{T}\cdot (x^{(1)},x^{(2)})^\mathrm{T}+b&amp;amp;=0\\
    ix^{(1)}+jx^{(2)}+b&amp;amp;=0
\end{align}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(w=(i,j)^\mathrm{T}\)&lt;/span&gt; 为直线法向量。&lt;/p&gt;
&lt;p&gt;类似地，当 &lt;span class="math"&gt;\(x\in \mathbf{R}^3\)&lt;/span&gt; 时，感知机模型满足空间中的平面方程：&lt;/p&gt;
&lt;div class="math"&gt;$$ix^{(1)}+jx^{(2)}+kx^{(3)}+b=0$$&lt;/div&gt;
&lt;p&gt;且 &lt;span class="math"&gt;\(w=(i,j,k)^\mathrm{T}\)&lt;/span&gt; 为平面法向量。&lt;/p&gt;
&lt;p&gt;因此，感知机在模型本质上是在特征空间 &lt;span class="math"&gt;\(\mathbf{R}^n\)&lt;/span&gt; 中的一个超平面 &lt;span class="math"&gt;\(S\)&lt;/span&gt;，代表特征向量的点被超平面 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 分隔成两部分，其中参数 &lt;span class="math"&gt;\(w\)&lt;/span&gt; 为超平面法向量。&lt;/p&gt;
&lt;p&gt;&lt;img alt="感知机的几何模型" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7509?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;超平面 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 也应符合 &lt;span class="math"&gt;\(\mathbf{R}^n\)&lt;/span&gt; 空间中的几何关系，所以在 &lt;span class="math"&gt;\(\mathbf{R}^n\)&lt;/span&gt; 空间中的任意一点 &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; 到超平面的距离为&lt;/p&gt;
&lt;div class="math"&gt;$$d=\frac{1}{||w||}|w\cdot x_0+b|$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(||w||\)&lt;/span&gt; 是 &lt;span class="math"&gt;\(w\)&lt;/span&gt; 的 &lt;span class="math"&gt;\(L_2\)&lt;/span&gt; 范数：&lt;/p&gt;
&lt;div class="math"&gt;$$||w||=\sqrt{\sum(w_i)^2}$$&lt;/div&gt;
&lt;p&gt;即法向量长度。&lt;/p&gt;
&lt;h4&gt;点到超平面距离&lt;/h4&gt;
&lt;p&gt;以 &lt;span class="math"&gt;\(x\in \mathbf{R}^2\)&lt;/span&gt; 为例，将超平面 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 方程转化为截距式：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
     ix^{(1)}+jx^{(2)}+b&amp;amp;=0\\
     \frac{x^{(1)}}{-\frac{b}{i}}+\frac{x^{(2)}}{-\frac{b}{j}}&amp;amp;=1
\end{align}$$&lt;/div&gt;
&lt;p&gt;超平面 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 在 &lt;span class="math"&gt;\(x^{(2)}\)&lt;/span&gt; 轴上的截距就为 &lt;span class="math"&gt;\(-\frac{b}{j}\)&lt;/span&gt;。过点 &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; 作平行于 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 的超平面 &lt;span class="math"&gt;\(S_0\)&lt;/span&gt;，同样得到截距式：&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{x^{(1)}}{-\frac{b_0}{i}}+\frac{x^{(2)}}{-\frac{b_0}{j}}=1$$&lt;/div&gt;
&lt;p&gt;超平面 &lt;span class="math"&gt;\(S_0\)&lt;/span&gt; 在 &lt;span class="math"&gt;\(x^{(2)}\)&lt;/span&gt; 轴上的截距为 &lt;span class="math"&gt;\(-\frac{b_0}{j}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="示意图" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7518?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;将 &lt;span class="math"&gt;\(d\)&lt;/span&gt; 平移至蓝色三角形中，&lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; 是超平面的法向量，&lt;span class="math"&gt;\(\boldsymbol{w}=w=(i,j)^\mathrm{T}\)&lt;/span&gt;，根据向量夹角余弦公式&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \cos\theta&amp;amp;=|\cos(\boldsymbol{w},\boldsymbol{j})|\\
    &amp;amp;=\frac{|\boldsymbol{w}\cdot\boldsymbol{j}|}{|\boldsymbol{w}||\boldsymbol{j}|}
\end{align}$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{j}\)&lt;/span&gt; 为 &lt;span class="math"&gt;\(x^{(2)}\)&lt;/span&gt; 方向的单位向量，因此有&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    \cos\theta&amp;amp;=\frac{|(i,j)^\mathrm{T}\cdot(0,1)^\mathrm{T}|}{|\boldsymbol{w}|}\\
    &amp;amp;=\frac{|j|}{|\boldsymbol{w}|}
\end{align}$$&lt;/div&gt;
&lt;p&gt;在蓝色三角形中，&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    d&amp;amp;=\left|\frac{b}{j}-\frac{b_0}{j}\right|\cos\theta\\
    &amp;amp;=\frac{|b-b_0|}{|j|}\frac{|j|}{|\boldsymbol{w}|}\\
    &amp;amp;=\frac{|b-b_0|}{|\boldsymbol{w}|}\\
\end{align}$$&lt;/div&gt;
&lt;p&gt;由 &lt;span class="math"&gt;\(w\cdot x_0+b_0=0\)&lt;/span&gt; 得到 &lt;span class="math"&gt;\(b_0=-w\cdot x_0\)&lt;/span&gt;，代入就可以证得&lt;/p&gt;
&lt;div class="math"&gt;$$d=\frac{|w\cdot x_0+b|}{||w||}$$&lt;/div&gt;
&lt;h2 id="ce-lue_1"&gt;策略&lt;/h2&gt;
&lt;p&gt;感知机的分类情况可以归为以下几类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(w\cdot x_i+b\geqslant0\)&lt;/span&gt;,&lt;span class="math"&gt;\(\ y_i=+1\)&lt;/span&gt;，分类正确；&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(w\cdot x_i+b\geqslant0\)&lt;/span&gt;,&lt;span class="math"&gt;\(\ y_i=-1\)&lt;/span&gt;，分类错误；&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(w\cdot x_i+b&amp;lt;0\)&lt;/span&gt;,&lt;span class="math"&gt;\(\ y_i=-1\)&lt;/span&gt;，分类正确；&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(w\cdot x_i+b&amp;lt;0\)&lt;/span&gt;,&lt;span class="math"&gt;\(\ y_i=+1\)&lt;/span&gt;，分类错误。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此分类错误的数据点 &lt;span class="math"&gt;\((x_i,y_i)\)&lt;/span&gt; 满足&lt;/p&gt;
&lt;div class="math"&gt;$$-y_i(w\cdot x_i+b)&amp;gt;0$$&lt;/div&gt;
&lt;p&gt;若超平面 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 误分类的点的集合为 &lt;span class="math"&gt;\(M\)&lt;/span&gt;，那么误分类点到超平面的距离之和为&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x_i\in M}\left[-\frac{1}{||w||}y_i(w\cdot x_i+b)\right]$$&lt;/div&gt;
&lt;p&gt;略去 &lt;span class="math"&gt;\(\frac{1}{||w||}\)&lt;/span&gt;（见第七章），得到&lt;em&gt;经验风险函数&lt;/em&gt;：&lt;/p&gt;
&lt;div class="math"&gt;$$L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$$&lt;/div&gt;
&lt;p&gt;感知机的学习策略就是在假设空间中选取 &lt;span class="math"&gt;\(L(w,b)\)&lt;/span&gt; 最小的模型参数 &lt;span class="math"&gt;\(w\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(b\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2 id="suan-fa"&gt;算法&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/Tseing/Statistical_Algorithms/blob/master/perceptron.py"&gt;&lt;i class="fa fa-github-alt"&gt;&lt;/i&gt; Code Here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;感知机的学习算法就是求解以下最优化问题的算法：&lt;/p&gt;
&lt;div class="math"&gt;$$\min_{w,b}L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$$&lt;/div&gt;
&lt;h3 id="sui-ji-ti-du-xia-jiang-fa"&gt;随机梯度下降法&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;任意选取超平面 &lt;span class="math"&gt;\(w_0\cdot x+b_0=0\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;随机选取误分类点使 &lt;span class="math"&gt;\(L(w,b)\)&lt;/span&gt; 梯度下降；&lt;/li&gt;
&lt;li&gt;不断更新 &lt;span class="math"&gt;\(w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b\)&lt;/span&gt;，直至 &lt;span class="math"&gt;\(L(w,b)=0\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;二元函数 &lt;span class="math"&gt;\(f(x,y)\)&lt;/span&gt; 在 &lt;span class="math"&gt;\((x_0,y_0)\)&lt;/span&gt; 处的梯度定义为&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbf{grad}\ f(x_0,y_0)=\nabla f(x_0,y_0)=f_x(x_0,y_0)\boldsymbol{i}+f_y(x_0,y_0)\boldsymbol{j}$$&lt;/div&gt;
&lt;p&gt;因此损失函数 &lt;span class="math"&gt;\(L(w,b)\)&lt;/span&gt; 的梯度为&lt;/p&gt;
&lt;div class="math"&gt;$$\nabla_wL(w,b)=\frac{\partial\left[-\sum_{x_i\in M}\color{orangered}{y_i}(w\cdot \color{orangered}{x_i}+b)\right]}{\partial w}=-\sum_{x_i\in M}\color{orangered}{y_ix_i}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\nabla_bL(w,b)=\frac{\partial\left[-\sum_{x_i\in M}\color{orangered}{y_i}(w\cdot x_i+b)\right]}{\partial b}=-\sum_{x_i\in M}\color{orangered}{y_i}$$&lt;/div&gt;
&lt;p&gt;随机选择误分类点 &lt;span class="math"&gt;\((x_i,y_i)\)&lt;/span&gt; 更新 &lt;span class="math"&gt;\(w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b\)&lt;/span&gt;，使 &lt;span class="math"&gt;\(L(w,b)\)&lt;/span&gt; 沿梯度方向下降：&lt;/p&gt;
&lt;div class="math"&gt;$$w\leftarrow w+\eta y_ix_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$b\leftarrow b+\eta y_i$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\eta\ (0&amp;lt;\eta\leqslant1)\)&lt;/span&gt; 称为步长，不断迭代至 &lt;span class="math"&gt;\(L(w,b)=0\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id="yuan-shi-xing-shi-suan-fa"&gt;原始形式算法&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;算法2.1&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：训练集 &lt;span class="math"&gt;\(T\)&lt;/span&gt;，学习率 &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;&lt;br/&gt;
输出：&lt;span class="math"&gt;\(w\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b\)&lt;/span&gt; ，感知机模型 &lt;span class="math"&gt;\(f(x)=\mathrm{sign}(w\cdot x+b)\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;设定步长 &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;，选取初值 &lt;span class="math"&gt;\(w_0\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b_0\)&lt;/span&gt;，一般为 &lt;span class="math"&gt;\(w_0=0\)&lt;/span&gt;，&lt;span class="math"&gt;\(b_0=0\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;选取点 &lt;span class="math"&gt;\((x_i,y_i)\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;若 &lt;span class="math"&gt;\(y_i(w\cdot x_i+b)\leqslant0\)&lt;/span&gt;，&lt;div class="math"&gt;$$w\leftarrow w+\eta y_ix_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$b\leftarrow b+\eta y_i$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;重复步骤（2），直至没有误分类点，即 &lt;span class="math"&gt;\(L(w,b)=0\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="dui-ou-xing-shi-suan-fa"&gt;对偶形式算法&lt;/h3&gt;
&lt;p&gt;经过数次以下迭代步骤：&lt;/p&gt;
&lt;div class="math"&gt;$$w\leftarrow w+\eta y_ix_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$b\leftarrow b+\eta y_i$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;最终的&lt;/em&gt; &lt;span class="math"&gt;\(w\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(b\)&lt;/span&gt; 可以表示为&lt;/p&gt;
&lt;div class="math"&gt;$$w=\sum_{i=1}^N \alpha_iy_ix_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$b=\sum_{i=1}^N\alpha_iy_i$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\alpha_i=n_i\eta\)&lt;/span&gt;，&lt;span class="math"&gt;\(n_i\)&lt;/span&gt; 表示点 &lt;span class="math"&gt;\((x_i,y_i)\)&lt;/span&gt; 被误分类的次数。也就是说，在迭代的每一次过程中，被误分类的点 &lt;span class="math"&gt;\((x_i,y_i)\)&lt;/span&gt; 都不断向超平面靠近，直至位于超平面的另一侧。&lt;/p&gt;
&lt;p&gt;最后得到的感知机模型应为&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=\mathrm{sign}\left(\sum_{i=1}^N\alpha_iy_ix_i\cdot x+b\right)$$&lt;/div&gt;
&lt;p&gt;为了与 &lt;span class="math"&gt;\(x=x_i\)&lt;/span&gt; 代入后的模型相区别，表示为&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=\mathrm{sign}\left(\sum_{j=1}^N\alpha_jy_jx_j\cdot x+b\right)$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;算法2.2&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;输入：训练集 &lt;span class="math"&gt;\(T\)&lt;/span&gt;，学习率 &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;&lt;br/&gt;
输出：&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b\)&lt;/span&gt; ，感知机模型 &lt;span class="math"&gt;\(f(x)=\mathrm{sign}\left(\sum_{j=1}^N\alpha_iy_jx_j\cdot x+b\right)\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;设定步长 &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;，选取初值 &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(b_0\)&lt;/span&gt;，一般为 &lt;span class="math"&gt;\(\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^\mathrm{T}=0\)&lt;/span&gt;，&lt;span class="math"&gt;\(b_0=0\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;选取点 &lt;span class="math"&gt;\((x_i,y_i)\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;若&lt;span class="math"&gt;\(y_i\left(\sum_{j=1}^N\alpha_jy_jx_j\cdot x_i+b\right)\leqslant0\)&lt;/span&gt;，&lt;div class="math"&gt;$$\alpha_i\leftarrow\alpha_i+\eta$$&lt;/div&gt;
&lt;div class="math"&gt;$$b\leftarrow b+\eta y_i$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;重复步骤 2，直至没有误分类点。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;考虑迭代的判断条件&lt;/p&gt;
&lt;div class="math"&gt;$$y_i\left(\sum_{j=1}^N\alpha_jy_j\vec{x_j}\cdot \vec{x_i}+b\right)\leqslant0$$&lt;/div&gt;
&lt;p&gt;每次判断都需要计算&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{j=1}^N\alpha_jy_j\color{orangered}{\vec{x_j}\cdot \vec{x_i}}=\alpha_1y_1\color{orangered}{\vec{x_1}\cdot\vec{x_i}}+\alpha_2y_2\color{orangered}{\vec{x_2}\cdot\vec{x_i}}+\cdots+\alpha_Ny_N\color{orangered}{\vec{x_N}\cdot\vec{x_i}}$$&lt;/div&gt;
&lt;p&gt;因此可以将内积以矩阵形式存放，即 Gram 矩阵：&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{G}=[x_i\cdot x_j]_{N\times N}=\begin{bmatrix}
    x_1\cdot x_1 &amp;amp;\cdots &amp;amp;x_1\cdot x_N\\
    \vdots       &amp;amp;\ddots &amp;amp;\vdots\\
    x_N\cdot x_1 &amp;amp;\cdots &amp;amp;x_N\cdot x_N
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;由于内积的性质，Gram 矩阵实际上是一个对称矩阵。&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references_1"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://book.douban.com/subject/33437381/"&gt;李航, 2019. 统计学习方法（第2版）. 清华大学出版社.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="统计学习方法"></category><category term="Machine learning"></category><category term="Algorithm"></category></entry><entry><title>《统计学习方法》第一章：概论</title><link href="https://tseing.github.io/sui-sui-nian/2022-07-21-statistical-learning-chapter1.html" rel="alternate"></link><published>2022-07-21T00:00:00+08:00</published><updated>2022-07-21T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-07-21:/sui-sui-nian/2022-07-21-statistical-learning-chapter1.html</id><summary type="html">&lt;p&gt;《统计学习方法》的第一章主要介绍了统计学习的基本概念与监督学习的分类。&lt;/p&gt;</summary><content type="html">&lt;h2 id="ji-ben-gai-nian"&gt;基本概念&lt;/h2&gt;
&lt;h3 id="tong-ji-xue-xi-guo-cheng"&gt;统计学习过程&lt;/h3&gt;
&lt;p&gt;统计学习主要基于数据概率构建概率统计模型并运用模型分析和预测数据，统计学习的主要过程可以归纳为：&lt;/p&gt;
&lt;div class="math"&gt;$$\boxed{\frac{训练数据集}{T=\{(x_i,y_i)\}}}\stackrel{学习}{\Longrightarrow}\boxed{\frac{模型}{P(Y|X)\ \mathrm{or}\ Y=f(X)}}\stackrel{预测}{\Longrightarrow}y_{N+1}$$&lt;/div&gt;
&lt;p&gt;其中，训练集表示为：&lt;/p&gt;
&lt;div class="math"&gt;$$
T=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}
$$&lt;/div&gt;
&lt;p&gt;其中输入变量 &lt;span class="math"&gt;\(X\)&lt;/span&gt; 的取值 &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 为特征向量：&lt;/p&gt;
&lt;div class="math"&gt;$$
x_i=(x^{(1)}_i,x^{(2)}_i,\cdots,x^{(n)}_i)^\mathrm{T}
$$&lt;/div&gt;
&lt;p&gt;输入变量 &lt;span class="math"&gt;\(X\)&lt;/span&gt; 与输出变量 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; 遵循联合概率分布 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt;，经过学习得到的模型可以是概率模型 &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt;，也可以是非概率模型 &lt;span class="math"&gt;\(Y=f(X)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id="gai-lu-mo-xing-yu-fei-gai-lu-mo-xing"&gt;概率模型与非概率模型&lt;/h3&gt;
&lt;p&gt;在监督学习中，概率模型为条件概率分布形式 &lt;span class="math"&gt;\(P(y|x)\)&lt;/span&gt;，非概率模型为决策函数形式 &lt;span class="math"&gt;\(y=f(x)\)&lt;/span&gt;；在无监督学习中，概率模型为 &lt;span class="math"&gt;\(P(z|x)\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(P(x|z)\)&lt;/span&gt;，非概率模型为隐函数形式 &lt;span class="math"&gt;\(z=g(x)\)&lt;/span&gt;。&lt;/p&gt;
&lt;div class="math"&gt;$$P(y|x)\underset{归一化}{\overset{最大化}{\rightleftharpoons}} y=f(x)$$&lt;/div&gt;
&lt;p&gt;由于 &lt;span class="math"&gt;\(P(y|x)\)&lt;/span&gt; 与 &lt;span class="math"&gt;\(y=f(x)\)&lt;/span&gt; 可以按照上述过程转化，所以概率模型与非概率模型不仅仅是在表现形式存在差异，更为重要的是在内部结构上，概率模型的变量、参数符合一定的联合概率分布，这也决定了概率模型符合以下基本概率公式：&lt;/p&gt;
&lt;div class="math"&gt;$$P(x)=\sum_y P(x,y)$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(x,y)=P(x)P(y|x)$$&lt;/div&gt;
&lt;h2 id="tong-ji-xue-xi-fang-fa-san-yao-su_1"&gt;统计学习方法三要素&lt;/h2&gt;
&lt;h3 id="mo-xing"&gt;模型&lt;/h3&gt;
&lt;p&gt;模型就是需要学习的条件分布概率或决策函数，由假设模型构成的集合称为假设空间，可以表示为：&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{F}=\{f|Y=f_\theta(X),\theta\in\mathbf{R}^n\}$$&lt;/div&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{F}=\{P|P_\theta(Y|X),\theta\in\mathbf{R}^n\}$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 为模型的参数向量，&lt;span class="math"&gt;\(\mathbf{R}^n\)&lt;/span&gt; 称为参数空间。&lt;/p&gt;
&lt;h3 id="ce-lue"&gt;策略&lt;/h3&gt;
&lt;p&gt;在假设空间中选择最优模型的方法称之为策略。&lt;/p&gt;
&lt;h4&gt;损失函数和风险函数&lt;/h4&gt;
&lt;p&gt;用损失函数能够衡量模型预测值 &lt;span class="math"&gt;\(f(X)\)&lt;/span&gt; 与真实值 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; 的差距，从而评估模型的优劣。损失函数记作 &lt;span class="math"&gt;\(L(Y,f(X))\)&lt;/span&gt;，例如常见的平方损失函数定义为：&lt;/p&gt;
&lt;div class="math"&gt;$$L(Y,f(X))=(Y-f(X))^2$$&lt;/div&gt;
&lt;p&gt;输入变量 &lt;span class="math"&gt;\(X\)&lt;/span&gt; 与输出变量 &lt;span class="math"&gt;\(Y\)&lt;/span&gt; 遵循联合概率分布 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt;，损失函数的期望为：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    R_{\mathrm{exp}}(f)&amp;amp;=E_P[L(Y,f(X))]\\
    &amp;amp;= \int_{\mathcal{X}\times\mathcal{Y}}L(y,f(x))P(x,y)\mathrm{d}x\mathrm{d}y\\
\end{align}$$&lt;/div&gt;
&lt;p&gt;该期望就是模型 &lt;span class="math"&gt;\(f(X)\)&lt;/span&gt; 在联合分布 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; 平均意义下的损失，称为风险函数。但由于 &lt;span class="math"&gt;\(P(X,Y)\)&lt;/span&gt; 无法确定，风险函数也无法计算，因此定义了能够确定的经验风险。经验风险是模型 &lt;span class="math"&gt;\(f(X)\)&lt;/span&gt; 在训练集 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 中的平均损失：&lt;/p&gt;
&lt;div class="math"&gt;$$R_{\mathrm{rmp}}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$&lt;/div&gt;
&lt;p&gt;当样本容量 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 趋向于无穷时，经验风险 &lt;span class="math"&gt;\(R_{\mathrm{rmp}}\)&lt;/span&gt; 趋于期望风险 &lt;span class="math"&gt;\(R_{\mathrm{exp}}\)&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;经验风险最小化与结构风险最小化&lt;/h4&gt;
&lt;p&gt;经验风险最小化策略通过寻找经验风险最小的模型作为最优模型，经验风险最小化策略需要足够大的样本容量，否则容易出现过拟合。经验风险最小化策本质为求解最优化问题：&lt;/p&gt;
&lt;div class="math"&gt;$$\min_{f\in\mathcal{F}}\frac{1}{N}L(y_i,f(x_i))$$&lt;/div&gt;
&lt;p&gt;结构风险最小化策略是防止过拟合的策略，其本质为求解最优化问题：&lt;/p&gt;
&lt;div class="math"&gt;$$\min_{f\in\mathcal{F}}\frac{1}{N}L(y_i,f(x_i))+\lambda J(f)$$&lt;/div&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(J(f)\)&lt;/span&gt; 是模型的复杂度，表示对复杂模型的惩罚，&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; 为系数。&lt;/p&gt;
&lt;h3 id="suan-fa"&gt;算法&lt;/h3&gt;
&lt;p&gt;算法是学习模型的具体计算方法。&lt;/p&gt;
&lt;h2 id="fan-hua_1"&gt;泛化&lt;/h2&gt;
&lt;h3 id="fan-hua-wu-chai"&gt;泛化误差&lt;/h3&gt;
&lt;p&gt;泛化能力是学习得到的模型对未知数据的预测能力，学习得到的模型对未知项进行预测的误差称为泛化误差，泛化误差等同于模型 &lt;span class="math"&gt;\(\hat{f}\)&lt;/span&gt; 的期望风险：&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
    R_{\mathrm{exp}}(\hat{f})&amp;amp;=E_P[L(Y,\hat{f}(X))]\\
    &amp;amp;= \int_{\mathcal{X}\times\mathcal{Y}}L(y,\hat{f}(x))P(x,y)\mathrm{d}x\mathrm{d}y\\
\end{align}$$&lt;/div&gt;
&lt;h3 id="er-fen-lei-wen-ti-de-fan-hua-wu-chai-shang-jie"&gt;二分类问题的泛化误差上界&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;定理 1.1&lt;/strong&gt; 对于二分类问题，当假设空间是有限个函数的集合
 &lt;span class="math"&gt;\(\mathcal{F}=\{ f_1,f_2,\cdots,f_d\}\)&lt;/span&gt; 时，对任意一个函数 &lt;span class="math"&gt;\(f\in \mathcal{F}\)&lt;/span&gt;，至少以概率 &lt;span class="math"&gt;\(1-\delta\)&lt;/span&gt;，&lt;span class="math"&gt;\(0&amp;lt;\delta&amp;lt;1\)&lt;/span&gt;，以下不等式成立（证明见参考）：&lt;/p&gt;
&lt;div class="math"&gt;$$R(f)\leq\hat{R}(f)+\varepsilon(d,N,\delta)$$&lt;/div&gt;
&lt;div class="math"&gt;$$\varepsilon(d,N\delta)=\sqrt{\frac{1}{2N}(\log d+\log\frac{1}{\delta})}$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(R(f)\)&lt;/span&gt; 为泛化误差，&lt;span class="math"&gt;\(\hat{R}(f)\)&lt;/span&gt; 为训练误差。&lt;/p&gt;
&lt;p&gt;泛化误差上界的性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;样本容量增加，泛化上界趋于0；&lt;/li&gt;
&lt;li&gt;假设空间越大，泛化上界越大。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="jian-du-xue-xi-de-ying-yong_1"&gt;监督学习的应用&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;输入变量 X&lt;/th&gt;
&lt;th&gt;输出变量 Y&lt;/th&gt;
&lt;th&gt;问题&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;离散变量&lt;/td&gt;
&lt;td&gt;分类&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;变量序列&lt;/td&gt;
&lt;td&gt;变量序列&lt;/td&gt;
&lt;td&gt;标注&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;连续变量&lt;/td&gt;
&lt;td&gt;连续变量&lt;/td&gt;
&lt;td&gt;回归&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr/&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://book.douban.com/subject/33437381/"&gt;李航, 2019. 统计学习方法（第2版）. 清华大学出版社.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/pastispast/p/12589078.html"&gt;二分类问题泛化误差上界的详细证明 - p_is_p - 博客园&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="碎碎念"></category><category term="统计学习方法"></category><category term="Machine learning"></category></entry><entry><title>在 Linux 上使用 OneDrive 作为博客图床</title><link href="https://tseing.github.io/sui-sui-nian/2022-07-14-onedrive-as-image-host.html" rel="alternate"></link><published>2022-07-14T00:00:00+08:00</published><updated>2022-07-14T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-07-14:/sui-sui-nian/2022-07-14-onedrive-as-image-host.html</id><summary type="html">&lt;p&gt;在博客中插入图片一直是一件头疼事，互联网上的各类图床工具都难令人完全满意。使用 OneDrive 作为图床或是一个很好的选择，这在 Windows 上已经有了成熟的方案，而在 Linux 上就有些麻烦，需要借助些额外的工具。&lt;/p&gt;</summary><content type="html">&lt;p&gt;在个人博客中，图片是不可或缺的，而在 &lt;code&gt;.md&lt;/code&gt; 文件中插入图片必须使用图片的直链，因此通常又需要图床等额外工具。由于国内市场的图床工具良莠不齐，没有精力仔细挑选，再就是把数据交付给第三方手中多少有些不放心。想&lt;/p&gt;
&lt;p&gt;到订阅 Microsoft Office 时附赠了 1 TB 的 OneDrive 容量，正好可以利用起来。使用 OneDrive 作为图床的好处就在于数据在自己的手中，不用担心某天突然挂掉，同时在多平台（Windows、iPad 和 Android）都有 OneDrive 应用，很方便同步数据。但是很遗憾，由于国内的环境，OneDrive 的网页版是打不开的，这就不能通过网页版直接生成图片链接，必须采用「绕道通行」的方法生成直链。&lt;/p&gt;
&lt;p&gt;Windows 系统自带 OneDrive 应用，可以直接使用桌面应用进行文件同步，借助 &lt;a href="https://github.com/Richasy/Img-Share"&gt;Img Share&lt;/a&gt; 生成图片链接。Img Share 后来被 &lt;a href="https://apps.microsoft.com/store/detail/picture-share/9PHWZ3QL0HN3?hl=en-us&amp;amp;gl=US"&gt;Picture Share&lt;/a&gt; 代替，在 Microsoft Store 中就可以直接下载到。Picture Share 十分容易上手，界面简洁且功能齐全，具体的设置方法可以参考&lt;a href="https://wzblog.fun/posts/b036879a/"&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;我撰写博客文章的工作环境是 Linux 系统，Linux 系统中没有 OneDrive 应用和上述 UWP 应用，也就不能使用上面的方法。本文就将介绍如何在 Linux 系统下使用 OneDrive 作为个人博客的图床并使用 ZFile 管理云盘文件。&lt;/p&gt;
&lt;h2 id="onedrive-wen-jian-lian-jie-de-sheng-cheng-fang-shi"&gt;OneDrive 文件链接的生成方式&lt;/h2&gt;
&lt;p&gt;OneDrive 的网页应用直接提供了嵌入代码，可以直接贴在文章中。但因为科学上网时上传速度相当感人，使用起来还是太过麻烦。虽然无法访问 OneDrive 网页，但 OneDrive 在国内的其他功能都是正常的，包括生成的分享链接，所以直接借用官方 API 生成链接的方案是可行的。发现有人已经写好了&lt;a href="https://github.com/harrisoff/onedrive-image-hosting"&gt;相关项目&lt;/a&gt;，点开项目右侧的链接登录自己的 OneDrive 账号后将生成的链接直接插入到 Markdown 文档就可以啦。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-markdown"&gt;![图片名称](https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNuUZNJKuT3c_gI4Jh/root/content)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="an-zhuang-zfile"&gt;安装 ZFile&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/zhaojun1998/zfile"&gt;ZFile&lt;/a&gt; 是一款在线网盘程序，支持包括 OneDrive 在内的多种存储源。ZFile 可以代替其他平台的 OneDrive 应用来管理云盘中的文件，实现同步、上传、下载等功能。ZFile 也能生成文件直链插入文章，但是这个功能需要云服务器，抱着能省则省的态度，就等以后再折腾，这里仅使用 ZFile 来管理 OneDrive。&lt;/p&gt;
&lt;p&gt;在 Linux 系统使用 ZFile 首先需要安装依赖：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;# Debian 10
$ apt update &amp;amp;&amp;amp; apt install -y apt-transport-https software-properties-common ca-certificates dirmngr gnupg
$ wget -qO - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public | apt-key add -
$ add-apt-repository --yes https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/
$ apt update &amp;amp;&amp;amp; apt install -y adoptopenjdk-8-hotspot-jre
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下载 ZFile：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ export ZFILE_INSTALL_PATH=~/zfile
$ mkdir -p $ZFILE_INSTALL_PATH &amp;amp;&amp;amp; cd $ZFILE_INSTALL_PATH
$ wget https://c.jun6.net/ZFILE/zfile-release.war
$ unzip zfile-release.war &amp;amp;&amp;amp; rm -rf zfile-release.war
$ chmod +x $ZFILE_INSTALL_PATH/bin/*.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ZFILE_INSTALL_PATH&lt;/code&gt; 指定了安装路径，可以自行修改。&lt;/p&gt;
&lt;h2 id="qi-dong-bing-pei-zhi-zfile"&gt;启动并配置 ZFile&lt;/h2&gt;
&lt;p&gt;通过以下命令启动 ZFile：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ ~/zfile/bin/start.sh       # 启动项目
$ ~/zfile/bin/stop.sh        # 停止项目
$ ~/zfile/bin/restart.sh     # 重启项目
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动项目后，默认开放在 8080 端口，使用 &lt;code&gt;localhost:8080&lt;/code&gt; 进入 ZFile：&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="NoCaption" href="https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNuUeZko02sAbyr5jh/root/content"&gt;&lt;img alt="NoCaption" src="https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNuUeZko02sAbyr5jh/root/content"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首次开启时需要注册管理员账号，登录进入系统后，首先配置存储源，选择存储策略为 &lt;code&gt;OneDrive&lt;/code&gt;，启用文件操作。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="NoCaption" href="https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNuUUBsSGYxpEV6Frp/root/content"&gt;&lt;img alt="NoCaption" src="https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNuUUBsSGYxpEV6Frp/root/content"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;点击链接登录 OneDrive 账号获取令牌，填写完成后即可保存设置。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="NoCaption" href="https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNuUPS4i5g5F_-nR4T/root/content"&gt;&lt;img alt="NoCaption" src="https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNuUPS4i5g5F_-nR4T/root/content"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;设置成功后在存储源中就可以看见 OneDrive 标志，并且显示刷新成功，这样 ZFile 就已经正常工作了。&lt;/p&gt;
&lt;p&gt;&lt;div class="lightgallery"&gt;&lt;a data-sub-html="NoCaption" href="https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNuUSMlwPi40T-1Um4/root/content"&gt;&lt;img alt="NoCaption" src="https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNuUSMlwPi40T-1Um4/root/content"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id="shi-yong-zfile-guan-li-onedrive"&gt;使用 ZFile 管理 OneDrive&lt;/h2&gt;
&lt;p&gt;在地址栏中输入 &lt;code&gt;localhost:8080&lt;/code&gt; 进入存储界面，在这里就理应能够看到 OneDrive 中存储的文件了，可以使用其他设备辅助测试是否能够正常上传或删除文件。&lt;/p&gt;
&lt;p&gt;如果有一台 VPS，在 VPS 上启动 ZFile 后，通过 &lt;code&gt;vps-ip:[port]&lt;/code&gt; 也能进入同样的管理界面。只需要将图片文件上传至 OneDrive，使用ZFile就可以得到文件的直链，不止是图片，这种方法还可以在 &lt;code&gt;.md&lt;/code&gt; 中插入音频或是视频文件，甚至搭建个人下载站，而且完全不占用服务器存储。值得注意的是，ZFile获得的文件「直链」并不是真正的直链，而是经过一次转发，可能会影响访问速度。&lt;/p&gt;
&lt;p&gt;&lt;img alt="ZFile获取直链流程" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7369?authkey=ALYpzW-ZQ_VBXTU"/&gt;&lt;/p&gt;
&lt;p&gt;因为 ZFile 向 OneDrive 请求得到是预览链接或临时下载链接 &lt;code&gt;1drv.com/...&lt;/code&gt;，该链接在一段时间后就会失效，也不能直接用作图床。当用户每次访问 ZFile 直链 &lt;code&gt;vps-ip:[port]/...&lt;/code&gt; 时，实际得到的都是 ZFile 转发得到的 OneDrive 临时链接。&lt;/p&gt;
&lt;p&gt;{warn begin}本文最后更新于 2022 年 07 月 14 日，请确定内容是否过时。{warn end}&lt;/p&gt;</content><category term="碎碎念"></category><category term="Blog"></category><category term="OneDrive"></category><category term="Linux"></category></entry><entry><title>使用 Pelican 发布文章的流程</title><link href="https://tseing.github.io/sui-sui-nian/2022-07-12-pelican-to-publish.html" rel="alternate"></link><published>2022-07-12T00:00:00+08:00</published><updated>2022-07-12T00:00:00+08:00</updated><author><name>Leo</name></author><id>tag:tseing.github.io,2022-07-12:/sui-sui-nian/2022-07-12-pelican-to-publish.html</id><summary type="html">&lt;p&gt;经过一番折腾，我的博客终于可以工作啦。但也所谓的「工作」也只是能显示罢了，后续的美化和插件部署免不了又是一通的麻烦事。暂且不论这些，先记录下使用 Pelican 在 Github Pages 上发布博客文章的流程。&lt;/p&gt;</summary><content type="html">&lt;h2 id="articles-or-pages"&gt;Articles or Pages?&lt;/h2&gt;
&lt;p&gt;Pelican 中有 articles 与 pages 的概念，在创建页面时应当首先区分二者。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;articles&lt;/strong&gt; 指具有时间戳的内容，例如博客文章等，直接创建在&lt;code&gt;content&lt;/code&gt;文件夹中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pages&lt;/strong&gt; 指与时间无关、展示固定内容的页面，需要创建在&lt;code&gt;content/pages&lt;/code&gt;文件夹下。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="zhuan-xie-wen-zhang"&gt;撰写文章&lt;/h2&gt;
&lt;h3 id="jupyter-notebook-fang-shi"&gt;Jupyter Notebook 方式&lt;/h3&gt;
&lt;p&gt;Jupyter Notebook 能够保存下代码的输入与输出信息，特别适合用于展示程序输出的图形。首先在&lt;code&gt;content&lt;/code&gt;目录中创建&lt;code&gt;.nbdata&lt;/code&gt;与&lt;code&gt;.ipynb&lt;/code&gt;的同名文件。&lt;code&gt;.nbdata&lt;/code&gt;文件中保存了文章的结构信息，而&lt;code&gt;.ipynb&lt;/code&gt;使用 Jupyter Notebook 保存了文章的具体内容。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;title: # 文章标题
slug: # 文章地址
date: # 时间
category: # 类别
tags: # 标签
author: # 作者
summary: # 概要

# 其他不常用信息
modified: # 修改时间
keywords: # 仅用于html内容
authors: # 多作者
lang: # 语言
translation: # 是否属于译文
status: # draft, hidden, or published
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="markdown-fang-shi"&gt;Markdown 方式&lt;/h3&gt;
&lt;p&gt;使用 Markdown 语言是写博客最为简单普遍的方式，在&lt;code&gt;content&lt;/code&gt;文件夹中创建&lt;code&gt;.md&lt;/code&gt;文件，在开始部分首先输入与&lt;code&gt;.nbdata&lt;/code&gt;相同的文章信息后，就可以直接开始撰写正文。&lt;/p&gt;
&lt;h2 id="sheng-cheng-jing-tai-wang-ye_1"&gt;生成静态网页&lt;/h2&gt;
&lt;p&gt;在撰写文章后，进入虚拟环境，在 &lt;code&gt;blog&lt;/code&gt; 文件夹中使用 &lt;code&gt;Pelican&lt;/code&gt; 生成 &lt;code&gt;.html&lt;/code&gt; 文件。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ source ./venv/bin/activate
$ pelican content -s publishconf.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后将 &lt;code&gt;output&lt;/code&gt; 文件夹同步至 Github 中 &lt;code&gt;&amp;lt;username&amp;gt;.github.io&lt;/code&gt; 仓库即完成文章的发布。&lt;/p&gt;
&lt;h2 id="fa-bu"&gt;发布&lt;/h2&gt;
&lt;p&gt;使用终端在 &lt;code&gt;blog&lt;/code&gt; 文件夹中输入 &lt;code&gt;pelican -lr&lt;/code&gt; 即可自动部署到本地服务器，默认端口为 8000，通过 &lt;code&gt;localhost:8000&lt;/code&gt; 访问。&lt;/p&gt;
&lt;p&gt;测试完成后将内容推送至 Github:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ git add .
$ git commit
$ git push
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="ke-long-yu-tong-bu"&gt;克隆与同步&lt;/h2&gt;
&lt;p&gt;由于我有 Windows 与 Linux 两个平台的设备，所以需要在两个平台上同步博客的内容，方便我在任意设备上都可以写文章。&lt;/p&gt;
&lt;p&gt;在终端中使用 &lt;code&gt;git clone --recursive&lt;/code&gt; 命令克隆仓库，&lt;code&gt;git clone&lt;/code&gt; 命令只会克隆主仓库，导致子模块失效，&lt;code&gt;--recursive&lt;/code&gt; 能递归地克隆包括子模块在内的整个仓库。使用 &lt;code&gt;git pull --recurse-submodules&lt;/code&gt; 命令能够拉取包含子模块在内的全部更新，即可完成同步。&lt;/p&gt;
&lt;p&gt;{warn begin}本文最后更新于 2022 年 08 月 14 日，请确定内容是否过时。{warn end}&lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.getpelican.com/en/latest/settings.html"&gt;Pelican Settings Document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zodiac911.github.io/blog/static-blog.html#%E7%B3%BB%E7%BB%9F%E8%A6%81%E6%B1%82"&gt;Pelican + GitHubPages 搭建个人博客 &amp;middot; Zodiac Wang&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="碎碎念"></category><category term="Blog"></category><category term="Pelican"></category></entry></feed>