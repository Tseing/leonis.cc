<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>ä»é›¶èµ·æ­¥çš„ Transformer ä¸ä»£ç æ‹†è§£ - Leoâ€™s blog</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html" rel="canonical" />
  <!-- Feed -->
  <link href="https://leonis.cc/feed.xml" type="application/atom+xml" rel="alternate"
    title="Leoâ€™s blog Full Atom Feed" />

  <link href="https://leonis.cc/theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- CSS specified by the user -->


  <link href="https://leonis.cc/theme/css/customize.css" type="text/css" rel="stylesheet" />


  <link href="https://leonis.cc/theme/css/plugins.css" type="text/css" rel="stylesheet" />


  <link href="https://leonis.cc/theme/css/lightgallery.min.css" type="text/css" rel="stylesheet" />


  <link href="https://leonis.cc/theme/css/bookshelf.css" type="text/css" rel="stylesheet" />


  <!-- font-awesome icons -->
  <link href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" rel="stylesheet" />
  <!-- Custom fonts -->
  <link href="https://leonis-fonts.deno.dev/leonis/ChiuKongGothic-CL-w4/result.css?updatedAt=1700222744424" rel="stylesheet" />
  <link href="https://leonis-fonts.deno.dev/leonis/ChiuKongGothic-CL-w5/result.css?updatedAt=1700275749020" rel="stylesheet" />
  <link href="https://leonis-fonts.deno.dev/leonis/ChiuKongGothic-CL-w7/result.css?updatedAt=1700275854743" rel="stylesheet" />
  <link href="https://cdnjs.loli.net/ajax/libs/fontsource-fira-sans/5.0.19/400.min.css" rel="stylesheet" />
  <link href="https://cdnjs.loli.net/ajax/libs/fontsource-fira-sans/5.0.19/400-italic.min.css" rel="stylesheet" />
  <link href="https://cdnjs.loli.net/ajax/libs/fontsource-fira-sans/5.0.19/500.min.css" rel="stylesheet" />
  <link href="https://cdnjs.loli.net/ajax/libs/fontsource-fira-sans/5.0.19/500-italic.min.css" rel="stylesheet" />
  <link href="https://cdnjs.loli.net/ajax/libs/fontsource-fira-sans/5.0.19/700.min.css" rel="stylesheet" />
  <link href="https://cdnjs.loli.net/ajax/libs/fontsource-fira-sans/5.0.19/700-italic.min.css" rel="stylesheet" />
  <link href="https://cdnjs.loli.net/ajax/libs/fontsource-lato/5.0.19/400.min.css" rel="stylesheet" />
  <link href="https://leonis-fonts.deno.dev/leonis/AdvocateAncientSerifSC-Bold/result.css?updatedAt=1700289637334" rel="stylesheet" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->

  <script>
    var siteUrl = 'https://leonis.cc';
  </script>

  <script>
    var localTheme = localStorage.getItem('attila_theme');
    switch (localTheme) {
      case 'dark':
        document.documentElement.classList.add('theme-dark');
        break;
      case 'light':
        document.documentElement.classList.add('theme-light');
        break;
      default:
        break;
    }
  </script>







    <meta name="description" content="è‡ª Google çš„è®ºæ–‡ Attention Is All You Need å‘å¸ƒåï¼Œå‡ å¹´å†…æ¶Œç°äº†å¤§é‡åŸºäº Transformer çš„æ¨¡å‹ï¼Œä¿¨ç„¶å½¢æˆäº† Transformer æ¨ªæ‰«äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ€åŠ¿ã€‚ ç½‘ç»œä¸Šä¹Ÿå‡ºç°äº†å¤§é‡è§£è¯»è®ºæ–‡æˆ–æ˜¯è®²è§£ Transformer...">

    <meta name="author" content="Leo">

    <meta name="tags" content="Python">
    <meta name="tags" content="PyTorch">
    <meta name="tags" content="Transformer">




<!-- Open Graph -->
<meta prefix="og: http://ogp.me/ns#" property="og:site_name" content="Leoâ€™s blog"/>
<meta prefix="og: http://ogp.me/ns#" property="og:title" content="ä»é›¶èµ·æ­¥çš„ Transformer ä¸ä»£ç æ‹†è§£"/>
<meta prefix="og: http://ogp.me/ns#" property="og:description" content="è‡ª Google çš„è®ºæ–‡ Attention Is All You Need å‘å¸ƒåï¼Œå‡ å¹´å†…æ¶Œç°äº†å¤§é‡åŸºäº Transformer çš„æ¨¡å‹ï¼Œä¿¨ç„¶å½¢æˆäº† Transformer æ¨ªæ‰«äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ€åŠ¿ã€‚ ç½‘ç»œä¸Šä¹Ÿå‡ºç°äº†å¤§é‡è§£è¯»è®ºæ–‡æˆ–æ˜¯è®²è§£ Transformer..."/>
<meta prefix="og: http://ogp.me/ns#" property="og:locale" content="en_US"/>
<meta prefix="og: http://ogp.me/ns#" property="og:url" content="https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html"/>
<meta prefix="og: http://ogp.me/ns#" property="og:type" content="article"/>
<meta prefix="og: http://ogp.me/ns#" property="article:published_time" content="2023-04-21 00:00:00+08:00"/>
<meta prefix="og: http://ogp.me/ns#" property="article:modified_time" content=""/>
<meta prefix="og: http://ogp.me/ns#" property="article:author" content="https://leonis.cc/author/leo.html">
<meta prefix="og: http://ogp.me/ns#" property="article:section" content="ç¢ç¢å¿µ"/>
<meta prefix="og: http://ogp.me/ns#" property="article:tag" content="Python"/>
<meta prefix="og: http://ogp.me/ns#" property="article:tag" content="PyTorch"/>
<meta prefix="og: http://ogp.me/ns#" property="article:tag" content="Transformer"/>
<meta prefix="og: http://ogp.me/ns#" property="og:image" content="https://img.leonis.cc/bg/header.webp">

<!-- Twitter Card -->

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "ä»é›¶èµ·æ­¥çš„ Transformer ä¸ä»£ç æ‹†è§£",
  "headline": "ä»é›¶èµ·æ­¥çš„ Transformer ä¸ä»£ç æ‹†è§£",
  "datePublished": "2023-04-21 00:00:00+08:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Leo",
    "url": "https://leonis.cc/author/leo.html"
  },
  "image": "https://leonis.cc/https://img.leonis.cc/bg/header.webp",
  "url": "https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html",
  "description": "è‡ª Google çš„è®ºæ–‡ Attention Is All You Need å‘å¸ƒåï¼Œå‡ å¹´å†…æ¶Œç°äº†å¤§é‡åŸºäº Transformer çš„æ¨¡å‹ï¼Œä¿¨ç„¶å½¢æˆäº† Transformer æ¨ªæ‰«äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ€åŠ¿ã€‚ ç½‘ç»œä¸Šä¹Ÿå‡ºç°äº†å¤§é‡è§£è¯»è®ºæ–‡æˆ–æ˜¯è®²è§£ Transformer..."
}
</script>

<link href="https://npm.elemecdn.com/@waline/client@2.14.9/dist/waline.css" rel="stylesheet" />
<script src="https://npm.elemecdn.com/@waline/client@2.14.9/dist/waline.js"></script>

</head>









<body class="category-template">

<div class="nav-header">
  <nav class="nav-wrapper" aria-label="Main">
<ul>

    <li class="nav-Home " role="presentation"><a href="/"><span>Home</span></a></li>
    <li class="nav-ç¢ç¢å”¸ active" role="presentation"><a href="/category/sui-sui-nian.html"><span>ç¢ç¢å”¸</span></a></li>
    <li class="nav-æ•…ç´™å † " role="presentation"><a href="/category/gu-zhi-dui.html"><span>æ•…ç´™å †</span></a></li>
    <li class="nav-åœ¨è·¯ä¸Š " role="presentation"><a href="/category/zai-lu-shang.html"><span>åœ¨è·¯ä¸Š</span></a></li>
    <li class="nav-å±±ç‰†é‚Š " role="presentation"><a href="/pages/shan-qiang-bian.html"><span>å±±ç‰†é‚Š</span></a></li>
    <li class="nav-ç ´æ«¥ç° " role="presentation"><a href="https://neodb.social/users/Leo/"><span>ç ´æ«¥ç°</span></a></li>
    <li class="nav-Archives " role="presentation"><a href="/archives.html"><span>Archives</span></a></li>
    <li class="nav-Tags " role="presentation"><a href="/tags.html"><span>Tags</span></a></li>
    <li class="nav-About " role="presentation"><a href="/about.html"><span>About</span></a></li>

</ul>
<ul class="nav-meta">
    <li class="nav-search">
      <a aria-label="Search" href="/search.html" target="_blank">
        <i class="icon icon-search" aria-hidden="true"></i>
    <span>search</span></a></li>
    <li class="nav-foreverblog">
      <a aria-label="Foreverblog" href="https://www.foreverblog.cn/go.html" target="_blank">
        <i aria-hidden="true"><img src="https://img.foreverblog.cn/wormhole_2_tp.gif" alt="foreverblog" title="ç©¿æ¢­è™«æ´-éšæœºè®¿é—®åå¹´ä¹‹çº¦å‹é“¾åšå®¢"></i>
    <span>foreverblog</span></a></li>
    <li class="nav-travellings">
      <a aria-label="Travellings" href="https://www.travellings.cn/go.html" target="_blank">
        <i aria-hidden="true"><img src="https://www.travellings.cn/assets/w.png" alt="travellings" title="å¼€å¾€-å‹é“¾æ¥åŠ›" aria-hidden="true"></i>
    <span>travellings</span></a></li>
  <li class="nav-search" style="display: none;">
    <a title="Search">
      <i class="icon icon-search" aria-hidden="true"></i>
      <span>Search</span>
    </a>
  </li>
</ul>  </nav>

  <div class="nav-wrapper-control">
    <div class="inner">
      <a class="nav-menu" role="button"><i class="icon icon-menu" aria-hidden="true"></i>Menu</a>
      <a class="nav-search" title="Search" role="button" style="display: none;"><i class="icon icon-search" aria-hidden="true"></i></a>
    </div>
  </div>
</div>
<div class="nav-close" role="button" aria-label="Close"></div>
  <section id="wrapper" class="page-wrapper">
    <!-- Progressbar -->
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="post-header  has-cover ">
      <div class="inner">
        <span class="post-info">
          <span class="post-type">Article</span>
          <span class="post-count">ç¢ç¢å¿µ</span>
        </span>
        <h1 class="post-title">ä»é›¶èµ·æ­¥çš„ Transformer ä¸ä»£ç æ‹†è§£</h1>
        <div class="post-meta">
          <div class="post-meta-avatars">


            <figure class="post-meta-avatar avatar">
              <a class="author-avatar" href="https://leonis.cc/author/leo.html">
                <img class="author-profile-image" src="https://cravatar.cn/avatar/95e31f6808fafa1f8ef3313b6f0b10e6?s=800" alt="Leo" />
              </a>
            </figure>
          </div>

          <h4 class="post-meta-author">
            Leo
          </h4>
          <time datetime="2023å¹´ 4æœˆ21æ—¥">2023å¹´ 4æœˆ21æ—¥</time>
        </div>
          <div class="post-cover cover">
            <img src="https://img.leonis.cc/bg/header.webp" alt="Category ç¢ç¢å¿µ" />
          </div>
      </div>
    </header>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
          <div class="toc-nav">
            <div id="toc"><ul><li><a class="toc-href" href="#xie-zi" title="æ¥”å­">æ¥”å­</a></li><li><a class="toc-href" href="#transformer-jie-gou" title="Transformer ç»“æ„">Transformer ç»“æ„</a><ul><li><a class="toc-href" href="#wei-zhi-bian-ma" title="ä½ç½®ç¼–ç ">ä½ç½®ç¼–ç </a></li><li><a class="toc-href" href="#encoder-yu-decoder" title="Encoder ä¸ Decoder">Encoder ä¸ Decoder</a></li><li><a class="toc-href" href="#transformer-ji-suan-bu-zou" title="Transformer è®¡ç®—æ­¥éª¤">Transformer è®¡ç®—æ­¥éª¤</a></li></ul></li><li><a class="toc-href" href="#dai-ma-chai-jie_1" title="ä»£ç æ‹†è§£">ä»£ç æ‹†è§£</a><ul><li><a class="toc-href" href="#positionembedding" title="PositionEmbedding">PositionEmbedding</a></li><li><a class="toc-href" href="#selfattention" title="SelfAttention">SelfAttention</a></li><li><a class="toc-href" href="#transformerblock" title="TransformerBlock">TransformerBlock</a></li><li><a class="toc-href" href="#encoder" title="Encoder">Encoder</a></li><li><a class="toc-href" href="#decoderblock" title="DecoderBlock">DecoderBlock</a></li><li><a class="toc-href" href="#decoder" title="Decoder">Decoder</a></li><li><a class="toc-href" href="#transformer" title="Transformer">Transformer</a></li><li><a class="toc-href" href="#train" title="Train">Train</a></li><li><a class="toc-href" href="#predict" title="Predict">Predict</a></li></ul></li><li><a class="toc-href" href="#hou-ji_1" title="åè®°">åè®°</a></li><li><a class="toc-href" href="#references" title="References">References</a></li></ul></div>
          </div>
        <div class="inner">
          <section class="post-content">
            <p>è‡ª Google çš„è®ºæ–‡ <a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank">Attention Is All You Need</a> å‘å¸ƒåï¼Œå‡ å¹´å†…æ¶Œç°äº†å¤§é‡åŸºäº Transformer çš„æ¨¡å‹ï¼Œä¿¨ç„¶å½¢æˆäº† Transformer æ¨ªæ‰«äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ€åŠ¿ã€‚</p>
<p>ç½‘ç»œä¸Šä¹Ÿå‡ºç°äº†å¤§é‡è§£è¯»è®ºæ–‡æˆ–æ˜¯è®²è§£ Transformer çš„æ–‡ç« ï¼Œå…¶ä¸­ä¹Ÿä¸ä¹è®¸å¤šé«˜æ°´å¹³äººå·¥æ™ºèƒ½ä»ä¸šè€…çš„è§£è¯»ã€‚è™½ç„¶æœ‰äº›å¯ä»¥ç§°å¾—ä¸Šæ˜¯é«˜å±‹å»ºç“´ï¼Œä½†ç›¸å½“å¤§éƒ¨åˆ†éš¾ä»¥é¿å…åœ°è½å…¥äº†çŸ¥è¯†çš„è¯…å’’ï¼ˆcurse of knowledgeï¼‰ï¼Œèµ·ç åœ¨æˆ‘åˆå¼€å§‹äº†è§£ Transformer æ—¶éš¾ä»¥è¯»æ‡‚è¿™äº›æ–‡ç« ã€‚</p>
<p>éšç€ Transformer å¹¿æ³›åº”ç”¨åˆ°å„é¢†åŸŸï¼Œå­¦ä¹  Transformer ä¹Ÿæˆäº†ä¸€é—¨ã€Œæ˜¾å­¦ã€ã€‚å°½ç®¡æˆ‘å·²ç»èƒ½è¯»æ‡‚ä¸€äº›æ›´æ·±å±‚æ¬¡çš„ Transformer å‰–æï¼Œä½†æˆ‘è¿˜æ˜¯æœªæ‰¾è§ä¸€ç¯‡åˆæˆ‘å¿ƒæ„çš„å…¥é—¨æ–‡ç« ï¼Œæ‰€ä»¥æˆ‘å¸Œæœ›èƒ½æ’°å†™ä¸€ç¯‡å°æ–‡ç« ï¼Œä»¥åˆå­¦è€…çš„è§’åº¦æ¥è®²è§£ Transformerï¼Œæ˜¯ä¸ºåºã€‚</p>
<h2 id="xie-zi">æ¥”å­</h2>
<p>Transformer æ˜¯è®¾è®¡ç”¨äº NLP çš„ä¸€ç§æ¨¡å‹ï¼Œå°½ç®¡ç›®å‰ Transformer æ‰€èƒ½å®Œæˆçš„ä»»åŠ¡å·²ç»å¤§å¤§æ‰©å±•ï¼Œä½†è¿™é‡Œè¿˜æ˜¯ä»¥æœ€åŸå§‹çš„ç¿»è¯‘ä»»åŠ¡ä¸ºä¾‹ã€‚</p>
<p>åœ¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæ‰€éœ€è¦çš„æ•°æ®åŒ…æ‹¬åŸå§‹è¯­å¥ä¸ç›®æ ‡è¯­å¥ï¼Œä¹Ÿå°±æ˜¯ Transformer åŸè®ºæ–‡ä¸­æ‰€æŒ‡çš„ã€Œinputã€å’Œã€Œoutputã€ï¼Œå› ä¸ºåå­—å¤ªå®¹æ˜“æ··æ·†ï¼Œè¿˜æ˜¯å°†å…¶åŸå§‹è¯­å¥ä¸ç›®æ ‡è¯­å¥æˆ–æ˜¯ã€Œsourceã€ä¸ã€Œtargetã€ã€‚</p>
<p>å‡è®¾ source ä¸º <code>ä½ å¥½ï¼Œä¸–ç•Œï¼</code>ï¼Œtarget ä¸º <code>Hello, world!</code>ï¼Œå®Œæˆè¿™ä¸ªä¸­è¯‘è‹±ä»»åŠ¡é¦–å…ˆè¦å°†æ–‡æœ¬è½¬åŒ–ä¸ºåˆ©äºæ¨¡å‹å¤„ç†çš„æ•°å€¼ï¼Œè¿™ä¸€æ­¥ç§°ä¸ºè¯åµŒå…¥ï¼ˆembeddingï¼‰ã€‚</p>
<p>å¸¸è§çš„è¯åµŒå…¥æ–¹æ³•æœ‰ word2vec ç­‰ç­‰ï¼Œåœ¨è¿™é‡Œä¸åšä»‹ç»ã€‚è¯åµŒå…¥æ­¥éª¤å¤§è‡´çš„æµç¨‹æ˜¯å…ˆå°† <code>ä½ å¥½ï¼Œä¸–ç•Œï¼</code> è½¬åŒ–ä¸º <code>&lt;start&gt; ä½ å¥½ ï¼Œ ä¸–ç•Œ ï¼ &lt;end&gt;</code>ï¼Œæ¯ä¸ªã€Œè¯ã€éƒ½ç”¨ç©ºæ ¼åˆ’åˆ†å¼€ï¼Œå…¶ä¸­ <code>&lt;start&gt;</code> ä¸ <code>&lt;end&gt;</code> åˆ†åˆ«è¡¨ç¤ºæ–‡æœ¬çš„èµ·è®«ï¼Œè¿™äº›ã€Œè¯ã€åœ¨ NLP é€šå¸¸ç§°ä¸ºã€Œtokenã€ã€‚æ¥ç€å†ä¸ºæ¯ä¸ª token åˆ†é…ç´¢å¼•ï¼Œä¾‹å¦‚ <code>&lt;start&gt;</code> ä¸º <code>1</code>ï¼Œ<code>&lt;end&gt;</code>ä¸º <code>0</code>ï¼Œç…§è¿™ä¸ªæ€è·¯ï¼Œæ–‡æœ¬å°±å¯ä»¥è½¬æ¢ä¸º <code>[1 2 3 4 5 0]</code> çš„è¡¨ç¤ºã€‚å½“ç„¶è¿™æ˜¯å¾ˆç®€å•çš„åšæ³•ï¼Œå®é™…ä¸Šï¼Œæ¯ä¸ª token éƒ½ä¼šè¢«è½¬åŒ–ä¸ºæŒ‡å®šç»´åº¦çš„å‘é‡ï¼Œç”¨è¿™ä¸€è¿ä¸²å‘é‡å°±å¯ä»¥è¡¨ç¤ºæ–‡æœ¬ã€‚</p>
<p>å°†ä¸Šè¿°è¿‡ç¨‹æŠ½è±¡å‡ºæ¥ï¼Œåœ¨è¯åµŒå…¥åï¼Œå¯ä»¥å¾—åˆ° source çš„è¡¨ç¤º <span class="math">\(\boldsymbol{X}=(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_t)\)</span> ä¸ target çš„è¡¨ç¤º <span class="math">\(\boldsymbol{Y}=(\boldsymbol{y}_1,\boldsymbol{y}_2,\cdots,\boldsymbol{y}_t)\)</span>ï¼Œå…¶ä¸­ <span class="math">\(\boldsymbol{x}_i\)</span> ä¸ <span class="math">\(\boldsymbol{y}_i\)</span> éƒ½æ˜¯æŒ‡å®šç»´åº¦ <span class="math">\(d\)</span> çš„å‘é‡ã€‚</p>
<p>é‚£ä¹ˆå¦‚ä½•ä½¿ç”¨ <span class="math">\(\boldsymbol{X}\)</span> ä¸ <span class="math">\(\boldsymbol{Y}\)</span> å®Œæˆç¿»è¯‘ä»»åŠ¡å‘¢ï¼Ÿ</p>
<p><strong>ç¬¬ä¸€ç§</strong>æ˜¯ä½¿ç”¨ RNN æ–¹æ³•ï¼Œä½¿ç”¨å½“å‰çš„ source token <span class="math">\(\boldsymbol{x}_t\)</span> ä¸å‰ä¸€æ­¥ä¸­ç”Ÿæˆçš„ token <span class="math">\(\hat{\boldsymbol{y}}_{t-1}\)</span> ç”Ÿæˆä¸‹ä¸€ä¸ª tokenï¼Œé€ä¸ªç”Ÿæˆç›´è‡³å¥å­æœ«å°¾ï¼š</p>
<div class="math">$$\hat{\boldsymbol{y}}_t=f(\hat{\boldsymbol{y}}_{t-1},\boldsymbol{x}_t)$$</div>
<p><strong>ç¬¬äºŒç§</strong>æ˜¯ä½¿ç”¨å·ç§¯çš„æ–¹æ³•ï¼Œå®šä¹‰ä¸€ä¸ªçª—å£é•¿åº¦å†é€šè¿‡å°èŒƒå›´ä¸­çš„å‡ ä¸ª <span class="math">\(\boldsymbol{x}_i\)</span> è®¡ç®—è¾“å‡ºï¼š</p>
<div class="math">$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_{t-1},\boldsymbol{x}_t,\boldsymbol{x}_{t+1})$$</div>
<p>å¯ä»¥çœ‹å‡ºï¼Œ<dot>RNN å¾ˆéš¾å­¦ä¹ åˆ°å…¨å±€çš„ä¿¡æ¯</dot>ï¼Œè€Œ<dot>å·ç§¯æ–¹æ³•åªèƒ½å­¦ä¹ åˆ°å°èŒƒå›´çš„å±€éƒ¨ä¿¡æ¯</dot>ã€‚</p>
<p>æ‰€ä»¥ Transformer ç»™å‡ºäº†<strong>ç¬¬ä¸‰ç§</strong>æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯è‡ªæ³¨æ„åŠ›æ–¹æ³•ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡å‹å°±å½“å‰çš„ source token <span class="math">\(\boldsymbol{x}_t\)</span> ä¸ <span class="math">\(\boldsymbol{X}\)</span> ä¸­å…¶ä»– token çš„å…³ç³»ç»™å‡ºè¾“å‡º <span class="math">\(\hat{\boldsymbol{y}}_t\)</span>ï¼š</p>
<div class="math">$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_t, \boldsymbol{X})$$</div>
<h2 id="transformer-jie-gou">Transformer ç»“æ„</h2>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>æ ‡å‡† Transformer çš„ç»“æ„å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¤§è‡´åˆ†ä¸ºå·¦ä¾§çš„ Encoder ä¸å³ä¾§çš„ Decoder ä¸¤ä¸ªéƒ¨åˆ†ã€‚Inputs ä¸ Outputs åˆ†åˆ«æ˜¯ä¸Šæ–‡æ‰€è¯´çš„ source ä¸ targetï¼ŒOutput Probabilities æ˜¯æ¨¡å‹è¾“å‡ºçš„å„ token æ¦‚ç‡ï¼Œå–å…¶ä¸­æœ€å¤§æ¦‚ç‡çš„ token å°±èƒ½ç»„ç»‡æˆæ¨¡å‹è¾“å‡ºç»“æœã€‚</p>
<h3 id="wei-zhi-bian-ma">ä½ç½®ç¼–ç </h3>
<p>Transformer å¹¶æ²¡æœ‰é‡‡ç”¨ RNN ä¸å·ç§¯æ–¹æ³•æ‰€ä½¿ç”¨çš„åºåˆ—å¤„ç† token çš„æ–¹æ³•ï¼Œå› è€Œèƒ½å¤Ÿå®ç°å¹¶è¡Œè®¡ç®—å¹¶ä¸”å¾ˆå¤§ç¨‹åº¦ä¸Šç¼“è§£äº†é•¿æœŸä¾èµ–é—®é¢˜ï¼ˆé¡ºåºå¤„ç†é•¿åºåˆ—å®¹æ˜“ä¸¢å¤±å¤šä¸ªæ­¥éª¤å‰çš„ä¿¡æ¯ï¼‰ã€‚æ–‡æœ¬ä¸­å¤šä¸ª token é—´æ˜¾ç„¶æœ‰å‰åçš„é¡ºåºå…³ç³»ï¼ŒTransformer ä½¿ç”¨ä½ç½®ç¼–ç çš„æ–¹å¼æ¥å¤„ç†é¡ºåºä¿¡æ¯ã€‚</p>
<p>source ä¸ target é€å…¥æ¨¡å‹ï¼Œç»è¿‡å¸¸è§„çš„è¯åµŒå…¥è¿‡ç¨‹åï¼Œè¿˜éœ€è¦åœ¨å¾—åˆ°çš„çŸ©é˜µä¸ŠåŠ ä¸Šä½ç½®ç¼–ç ï¼Œè®ºæ–‡å°†ä½ç½®ç¼–ç å®šä¹‰ä¸º</p>
<div class="math">$$\mathrm{PE}_{(\mathrm{pos},2i)}=\sin(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$</div>
<div class="math">$$\mathrm{PE}_{(\mathrm{pos},2i+1)}=\cos(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$</div>
<p>Transformer å°† <span class="math">\(\mathrm{pos}\)</span> ä½ç½®æ˜ å°„ä¸º <span class="math">\(d_\mathrm{model}\)</span> ç»´çš„å‘é‡ï¼Œå‘é‡ä¸­çš„ç¬¬ <span class="math">\(i\)</span> ä¸ªå…ƒç´ å³æŒ‰ä¸Šå¼è®¡ç®—ã€‚ä½ç½®ç¼–ç çš„è®¡ç®—å…¬å¼æ˜¯æ„é€ å‡ºçš„ç»éªŒå…¬å¼ï¼Œä¸å¿…æ·±ç©¶ï¼Œå½“ç„¶ä¹Ÿæœ‰è®¸å¤šæ–‡ç« åˆ†æäº†å¦‚æ­¤æ„é€ çš„åŸå› ï¼Œè¿™é‡Œä»ç•¥ã€‚</p>
<h3 id="encoder-yu-decoder">Encoder ä¸ Decoder</h3>
<p>è®¸å¤šå®Œæˆ seq2seq ä»»åŠ¡çš„æ¨¡å‹éƒ½é‡‡ç”¨äº† encoder-decoder æ¨¡å¼ï¼ŒTransformer ä¹Ÿä¸ä¾‹å¤–ã€‚ç®€å•æ¥è¯´ï¼Œencoder å°†è¾“å…¥ç¼–ç å¾—åˆ°ä¸€ä¸ªä¸­é—´å˜é‡ï¼Œdecoder è§£ç è¯¥ä¸­é—´å˜é‡å¾—åˆ°è¾“å‡ºã€‚</p>
<p>åœ¨ Transformer ä¸­ï¼Œsource ä¸ target åˆ†åˆ«é€å…¥ encoder ä¸ decoderï¼Œencoder è®¡ç®—å¾—åˆ°çš„ä¸­é—´ç»“æœå†é€å…¥ decoder ä¸­ä¸ target è¾“å…¥è¿›è¡Œè®¡ç®—ï¼Œå¾—åˆ°æœ€åçš„ç»“æœï¼Œè¿™å°±æ˜¯æ‰€è°“ã€Œç¼–ç -è§£ç ã€çš„å·¥ä½œæ–¹å¼ã€‚</p>
<p>ä» Transformer çš„ç»“æ„å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œæ¨¡å‹å…·æœ‰ <span class="math">\(N\)</span> å±‚ encoder ä¸ decoder å±‚ã€‚å…¶ä¸­ï¼Œencoder ä¸ decoder éƒ½å…·æœ‰ç›¸åŒçš„å¤šå¤´æ³¨æ„åŠ›å±‚ï¼ˆMulti-Head Attentionï¼‰ã€å‰é¦ˆå±‚ï¼ˆFeed Forwardï¼‰ã€‚encoder ä¸ decoder çš„ä¸åŒåœ¨äº decoder å¤šäº†ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼Œåœ¨è¿™ä¸€å±‚ä¸­ï¼Œencoder çš„è¾“å‡ºä¸ decoder çš„è¾“å…¥è®¡ç®—æ³¨æ„åŠ›ã€‚</p>
<p>è¿˜å¯ä»¥æ³¨æ„åˆ°ï¼Œåœ¨ encoder ä¸ decoder ä¸­ï¼Œæ¯ä¸€å±‚åéƒ½æœ‰ä¸€ä¸ª Add &amp; Norm å±‚ï¼Œç”¨äºå½’ä¸€åŒ–è®¡ç®—ç»“æœã€‚Add &amp; Norm å±‚çš„è®¡ç®—æ–¹å¼æ˜¯å°†å‰ä¸€å±‚çš„è¾“å…¥ä¸å‰ä¸€å±‚çš„è¾“å‡ºç›¸åŠ ï¼Œç„¶åå½’ä¸€åŒ–ï¼Œå¯ä»¥è¡¨ç¤ºä¸º <span class="math">\(\mathrm{LayerNorm}(\boldsymbol{x}+\mathrm{Sublayer}(\boldsymbol{x}))\)</span>ã€‚</p>
<h4>Attention æœºåˆ¶</h4>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>æ•°æ®è¿›å…¥ encoder ä¸ decoder çš„å†…éƒ¨ï¼Œé¦–å…ˆè¦é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œè®¡ç®—ï¼Œè¿™ä¹Ÿæ˜¯ Transformer çš„æ ¸å¿ƒã€‚</p>
<p>æ–‡ç« ä¸­å°†æ‰€ä½¿ç”¨çš„æ³¨æ„åŠ›ç§°ä¸ºç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆscaled dot-product attentionï¼‰ï¼Œå®šä¹‰ä¸º</p>
<div class="math">$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$</div>
<p>å…¶ä¸­ <span class="math">\(\boldsymbol{Q}_{n\times d_k}\)</span>ã€<span class="math">\(\boldsymbol{K}_{m\times d_k}\)</span>ã€<span class="math">\(\boldsymbol{V}_{m\times d_v}\)</span> åˆ†åˆ«æ˜¯è‹¥å¹²å‘é‡ <span class="math">\(\boldsymbol{q}\in\mathbb{R}^{d_k}\)</span>ã€<span class="math">\(\boldsymbol{k}\in\mathbb{R}^{d_k}\)</span>ã€<span class="math">\(\boldsymbol{v}\in\mathbb{R}^{d_v}\)</span> ç»„æˆçš„çŸ©é˜µã€‚</p>
<p>å•çœ‹çŸ©é˜µçš„ä¹˜æ³•ç¨æ˜¾å¤æ‚ï¼Œä¸å¦¨å…ˆç”¨å‘é‡è¯´æ˜è®¡ç®—æ­¥éª¤ã€‚é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯ä»¥ä»è¾“å…¥ <span class="math">\(\boldsymbol{x}\)</span> å¾—åˆ°å‘é‡ <span class="math">\(\boldsymbol{q}\)</span>ã€<span class="math">\(\boldsymbol{k}\)</span>ã€<span class="math">\(\boldsymbol{v}\)</span>ï¼š</p>
<div class="math">$$\boldsymbol{q}=\boldsymbol{x}\boldsymbol{W}^Q,\,\boldsymbol{k}=\boldsymbol{x}\boldsymbol{W}^K,\,\boldsymbol{v}=\boldsymbol{x}\boldsymbol{W}^V$$</div>
<p>å…¶ä¸­ï¼Œ<span class="math">\(\boldsymbol{W}^Q\)</span>ã€<span class="math">\(\boldsymbol{W}^K\)</span>ã€<span class="math">\(\boldsymbol{W}^V\)</span> åˆ†åˆ«è¡¨ç¤ºç›¸åº”çš„æƒé‡çŸ©é˜µã€‚<span class="math">\(\boldsymbol{q}\)</span> ä»£è¡¨ queryï¼Œ<span class="math">\(\boldsymbol{k}\)</span> ä»£è¡¨ keyï¼Œ<span class="math">\(\boldsymbol{v}\)</span> ä»£è¡¨ valueï¼Œç›®çš„æ˜¯<dot>ç”¨ query å»å¯»æ‰¾æ›´åŒ¹é…çš„ key-value å¯¹</dot>ã€‚</p>
<p>å› ä¸ºæ•°é‡ç§¯å¯ä»¥è¡¨ç¤ºä¸¤å‘é‡çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œä¸€ç§ç®€å•çš„åšæ³•æ˜¯ä½¿ç”¨ <span class="math">\(\boldsymbol{q}\)</span> ä¸è‹¥å¹²ä¸ª <span class="math">\(\boldsymbol{k}\)</span> è®¡ç®—æ•°é‡ç§¯ï¼Œå°†å…¶ä½œä¸ºåŒ¹é…åˆ†æ•°ï¼š</p>
<div class="math">$$\mathrm{score}=\boldsymbol{q}\cdot \boldsymbol{k}_i=\boldsymbol{q}\boldsymbol{k}^\top_i$$</div>
<p>ä½†è¿™æ ·çš„ã€Œæ³¨æ„åŠ›ã€å¤ªè¿‡äºç®€å•ï¼ŒGoogle ä»ä¸Šè¿°çš„æ•°é‡ç§¯å‡ºå‘ï¼Œè®¾è®¡äº†æ›´ä¸ºå¯é çš„æ³¨æ„åŠ›ï¼š</p>
<div class="math">$$\mathrm{Attention}(\boldsymbol{q},\boldsymbol{k}_i,\boldsymbol{v}_i)=\frac 1 Z\sum_i\exp\left(\frac{\boldsymbol{q}\boldsymbol{k}^\top_i}{\sqrt{d_k}}\right)\boldsymbol{v}_i$$</div>
<p>é¦–å…ˆï¼Œå¼ä¸­ <span class="math">\(1/Z\sum_i x_i\)</span> å½¢å¼çš„éƒ¨åˆ†æ˜¯ Softmax å‡½æ•°çš„ç®€å†™ï¼ŒSoftmax å‡½æ•°ç”±ä¸‹å¼å®šä¹‰ï¼š</p>
<div class="math">$$\mathrm{Softmax}(x_i)=\frac{\exp(x_i)}{\sum_j\exp(x_j)}$$</div>
<p>Softmax å‡½æ•°çš„ä½œç”¨æ˜¯å°†è‹¥å¹²æ•°å€¼ <span class="math">\(x_i\)</span> å½’ä¸€åŒ–ï¼Œå¾—åˆ°çš„ <span class="math">\(\mathrm{Softmax}(x_i)\)</span> å…·æœ‰</p>
<ul>
<li><span class="math">\(\sum_i\mathrm{Softmax}(x_i)=1\)</span></li>
<li><span class="math">\(\mathrm{Softmax}(x_i)\in[0, 1]\)</span></li>
</ul>
<p>ä¸¤ç‚¹æ€§è´¨ï¼Œæ‰€ä»¥ä¸æ¦‚ç‡å…·æœ‰ç›¸ä¼¼çš„ç‰¹å¾ï¼Œå¯ä»¥ç”¨ä½œæ¦‚ç‡å¤„ç†ã€‚</p>
<p>å…¶æ¬¡ï¼Œå¼ä¸­æ–°å¢çš„ <span class="math">\(\sqrt{d_k}\)</span> ç”¨äºè°ƒèŠ‚å†…ç§¯ <span class="math">\(\boldsymbol{q}\boldsymbol{k}^\top_i\)</span> çš„å¤§å°ã€‚å½“è‹¥å¹²å†…ç§¯çš„å¤§å°è¿‡äºæ‚¬æ®Šæ—¶ï¼ŒSoftmax å‡½æ•°å¾ˆå®¹æ˜“å°†å…¶æ¨å‘ <span class="math">\(0\)</span> æˆ– <span class="math">\(1\)</span> çš„è¾¹ç•Œå€¼ï¼Œè¿™æ ·çš„æ•°å€¼å¤„ç†èµ·æ¥æ²¡ä»€ä¹ˆæ„ä¹‰ã€‚</p>
<p>æœ€åï¼Œå†æ¬¡å›å¿† Transformer çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ç”¨ query å»å¯»æ‰¾æ›´åŒ¹é…çš„ key-value å¯¹ã€‚é‚£ä¹ˆä¸Šå¼çš„æ„ä¹‰å°±å¾ˆäº†ç„¶äº†ï¼Œå°±æ˜¯å°† query ä¸å„ä¸ª key çš„åŒ¹é…åˆ†æ•°è½¬åŒ–ä¸ºå„ä¸ªæ¦‚ç‡ï¼Œå†æŒ‰å„ä¸ªæ¦‚ç‡å–å„ä¸ª key æ‰€å¯¹åº”çš„ valueï¼Œç»„åˆå„ value åˆ†é‡å³å¾—åˆ°æ³¨æ„åŠ›ã€‚</p>
<p>ä»¥å…·æœ‰ä¸¤ä¸ª value çš„æƒ…å†µä¸ºä¾‹ï¼Œéœ€è¦å¾—åˆ°çš„ä¸­é—´é‡ <span class="math">\(\boldsymbol{z}\)</span>ï¼ˆç†è§£ä¸ºæ³¨æ„åŠ›äº¦å¯ï¼‰å¯ä»¥é€šè¿‡ä¸‹å¼è®¡ç®—ï¼š</p>
<div class="math">$$\begin{align}
    \boldsymbol{z}_1=\theta_{11}\boldsymbol{v}_1+\theta_{12}\boldsymbol{v}_2\\
    \boldsymbol{z}_2=\theta_{21}\boldsymbol{v}_1+\theta_{22}\boldsymbol{v}_2
\end{align}$$</div>
<p>æƒå€¼ <span class="math">\(\theta_{ij}\)</span>ï¼ˆå³ä¸Šæ–‡æ‰€è¯´æ¦‚ç‡ï¼‰é€šè¿‡ä¸‹å¼å¾—åˆ°ï¼š</p>
<div class="math">$$\theta_{ij}=\mathrm{Softmax}\left(\frac{\boldsymbol{q}_i\boldsymbol{k}^\top_j}{\sqrt{d_k}}\right)$$</div>
<p>å°†ä¸Šè¿°è¿ç®—è½¬ä¸ºçŸ©é˜µå½¢å¼ä¼šç®€æ´è®¸å¤šï¼š</p>
<div class="math">$$
\begin{pmatrix}
    \boldsymbol{z}_1 \\
    \boldsymbol{z}_2
\end{pmatrix}=
\begin{pmatrix}
    \theta_{11} &amp; \theta_{12} \\
    \theta_{21} &amp; \theta_{22}
\end{pmatrix}
\begin{pmatrix}
    \boldsymbol{v}_1 \\
    \boldsymbol{v}_2
\end{pmatrix}\\
$$</div>
<p>å¯ä»¥è®°ä½œ <span class="math">\(\boldsymbol{Z}=\boldsymbol{\theta}\boldsymbol{V}\)</span>ï¼Œä¹Ÿå°±æ˜¯</p>
<div class="math">$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$</div>
<h4>Multi-Head Attention</h4>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>å‰ä¸€èŠ‚ä¸­è§£é‡Šäº† Transformer ä¸­çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œä½†åœ¨æ¨¡å‹ä¸­å®é™…å¹¶éé€šè¿‡ä¸Šè¿°æ–¹å¼ç›´æ¥è®¡ç®—ï¼Œè€Œæ˜¯é€šè¿‡å¤šå¤´æ³¨æ„åŠ›çš„æ–¹å¼è®¡ç®—æ³¨æ„åŠ›ã€‚</p>
<p>å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¤šå¤´æ³¨æ„åŠ›åŒæ ·æ˜¯åœ¨è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œä½†ä¸çº¯ç²¹ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„ä¸åŒä¹‹å¤„åœ¨äºå¤šå¤´æ³¨æ„åŠ›å°†å¤šä¸ªæ³¨æ„åŠ›è®¡ç®—æ­¥éª¤å åŠ äº†èµ·æ¥ã€‚</p>
<p>å åŠ çš„æ¬¡æ•°ä¸º <span class="math">\(h\)</span>ï¼Œå³ä»£è¡¨ headï¼Œå¤šå°‘ä¸ª head è¡¨ç¤ºéœ€è¦è¿›è¡Œå¤šå°‘æ¬¡å åŠ è®¡ç®—ã€‚çŸ©é˜µ <span class="math">\(\boldsymbol{Q}\)</span>ã€<span class="math">\(\boldsymbol{K}\)</span>ã€<span class="math">\(\boldsymbol{V}\)</span> è¿›å…¥å¤šå¤´æ³¨æ„åŠ›è®¡ç®—æ­¥éª¤åï¼Œé¦–å…ˆè¦åˆ†åˆ«åœ¨ç¬¬ <span class="math">\(i\)</span> ä¸ª head ä¸­è¿›è¡Œçº¿æ€§å˜æ¢å¹¶è®¡ç®—æ³¨æ„åŠ›ï¼š</p>
<div class="math">$$\mathrm{head}_i=\mathrm{Attention}(\boldsymbol{Q}\boldsymbol{W}^Q_i,\boldsymbol{K}\boldsymbol{W}^K_i,\boldsymbol{V}\boldsymbol{W}^V_i)$$</div>
<p>å…¶ä¸­ <span class="math">\(\boldsymbol{W}^Q_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}\)</span>ï¼Œ<span class="math">\(\boldsymbol{W}^K_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}\)</span>ï¼Œ<span class="math">\(\boldsymbol{W}^V_i\in\mathbb{R}^{d_\mathrm{model}\times d_v}\)</span>ï¼Œæ³¨æ„ä¸åŒ head ä¸­çš„çº¿æ€§å˜æ¢å¹¶ä¸åŒï¼Œè¾“å‡ºä¹Ÿä¸åŒã€‚ç„¶åå°†æ‰€æœ‰è¾“å‡º <span class="math">\(\mathrm{head}_i\)</span> æ‹¼åˆåœ¨ä¸€èµ·ï¼Œç»çº¿æ€§å˜æ¢åä½œä¸ºæ³¨æ„åŠ›ï¼š</p>
<div class="math">$$\mathrm{MultiHead}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\mathrm{Concat}(\mathrm{head}_1,\mathrm{head}_2,\cdots,\mathrm{head}_h)\boldsymbol{W}^O$$</div>
<p>å…¶ä¸­ <span class="math">\(\boldsymbol{W}^O\in\mathbb{R}^{hd_v\times d_\mathrm{model}}\)</span>ã€‚</p>
<p>æ³¨æ„è¿™ä¸ªè¿‡ç¨‹ä¸­æ•°æ®ç»´æ•°çš„å˜åŒ– <span class="math">\(d_\mathrm{model}\)</span> ä¸ºå•å¤´æ³¨æ„åŠ›ä¸­æ¨¡å‹æ‰€å¤„ç†çš„ç»´æ•°ï¼Œ<span class="math">\(\boldsymbol{W}^Q_i\)</span>ï¼Œ<span class="math">\(\boldsymbol{W}^K_i\)</span>ï¼Œ<span class="math">\(\boldsymbol{W}^V_i\)</span> çš„çº¿æ€§å˜æ¢å°† queryã€key çš„ç»´æ•°ä» <span class="math">\(d_\mathrm{model}\)</span> æå‡åˆ° <span class="math">\(d_v\)</span>ï¼Œå°† value çš„ç»´æ•°ä» <span class="math">\(d_\mathrm{model}\)</span> æå‡è‡³ <span class="math">\(d_v\)</span>ã€‚æœ€åçš„ <span class="math">\(\boldsymbol{W}^O\)</span> åˆå°†æ‹¼åˆèµ·æ¥ç»´æ•°ä¸º <span class="math">\(hd_v\)</span> çš„æ³¨æ„åŠ›è½¬æ¢ä¸ºæ¨¡å‹æ‰€å¤„ç†çš„ç»´æ•° <span class="math">\(d_\mathrm{model}\)</span>ã€‚è¿™äº›çº¿æ€§å˜æ¢çŸ©é˜µ <span class="math">\(\boldsymbol{W}_i\)</span> å®é™…ä¸Šå°±æ˜¯æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å­¦ä¹ çš„ä¸€éƒ¨åˆ†å‚æ•°ã€‚</p>
<p>è‡³äºä¸ºä»€ä¹ˆè¦ç”¨å¤šå¤´çš„æ–¹å¼è®¡ç®—æ³¨æ„åŠ›ï¼Œè¿™å°±æ˜¯ä¸ªå¾ˆå¤æ‚çš„é—®é¢˜äº†ã€‚å°±æˆ‘çš„ç†è§£è€Œè¨€ï¼Œç”±äºæ¯ä¸ª head ä¸­çš„çº¿æ€§å˜æ¢çŸ©é˜µ <span class="math">\(\boldsymbol{W}_i\)</span>ï¼Œå¤šå¤´æ³¨æ„åŠ›å®é™…ä¸Šæ˜¯å°† queryã€keyã€value æ˜ å°„åˆ°ä¸åŒçš„å­ç©ºé—´ä¸­ï¼Œåœ¨å¤šä¸ªä¸åŒçš„å­ç©ºé—´ä¸­å¯»æ‰¾ä¸ query æœ€åŒ¹é…çš„ key-valueã€‚ç”±äºä¸åŒå­ç©ºé—´ä¸­å…·æœ‰ä¸åŒæ–¹é¢çš„ä¿¡æ¯ï¼Œæœ€åå°†å…¶æ‹¼æ¥èµ·æ¥ä½œä¸ºç»“æœï¼Œè¿™æ ·å¯ä»¥æ›´å¤šåœ°ä»å¤šä¸ªæ–¹é¢æ•è·æ•°æ®ä¸­çš„ä¿¡æ¯ã€‚</p>
<h4>Feed-Forward å±‚</h4>
<p>åœ¨å¤šå¤´æ³¨æ„åŠ›å±‚ä¹‹åï¼Œå°±æ˜¯å‰é¦ˆå±‚ï¼Œå‰é¦ˆå±‚åªåœ¨ä½ç½®æ–¹å‘ä¸Šè®¡ç®—ï¼Œæ‰€ä»¥åŸæ–‡æè¿°å…¶ä¸º position-wiseã€‚è¿›å…¥å‰é¦ˆå±‚çš„æ•°æ®åœ¨è¯¥å±‚ä¸­å…ˆåš 1 æ¬¡çº¿æ€§å˜æ¢ï¼Œç»´åº¦å‡é«˜ï¼Œå†ç»è¿‡ RELU æ¿€æ´»å‡½æ•°ï¼Œæœ€åå†åš 1 æ¬¡çº¿æ€§å˜æ¢ï¼Œç»´åº¦é™ä½ï¼Œè¾“å…¥ä¸è¾“å‡ºå‰é¦ˆå±‚çš„ç»´åº¦ç›¸åŒã€‚ä¸Šè¿°è¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸º</p>
<div class="math">$$\mathrm{FFN}(\boldsymbol{x})=\max(0,\boldsymbol{x}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2$$</div>
<p>RELU æ¿€æ´»å‡½æ•°å®šä¹‰ä¸º</p>
<div class="math">$$\mathrm{ReLU}(x)=x^+=\max(0,x)$$</div>
<p>å³å¼ä¸­çš„ <span class="math">\(\max\)</span>ï¼ŒæŒ‰åŸæ–‡ä¸­çš„ä¾‹å­ï¼Œ<span class="math">\(\boldsymbol{W}_1\)</span> ä½¿ <span class="math">\(\boldsymbol{x}\)</span> ç”± 512 ç»´å‡é«˜åˆ° 2048 ç»´ï¼Œ<span class="math">\(\boldsymbol{W}_2\)</span> ä½¿ <span class="math">\(\boldsymbol{x}\)</span> è®¡ç®—ç”± 2048 ç»´å†é™è‡³ 512 ç»´ï¼Œå‡ç»´ä¸é™ç»´çš„è¿‡ç¨‹ä¹Ÿæ˜¯ä¸ºäº†æ›´å¥½åœ°è·å¾—æ•°æ®ä¸­çš„ä¿¡æ¯ã€‚</p>
<h3 id="transformer-ji-suan-bu-zou">Transformer è®¡ç®—æ­¥éª¤</h3>
<p>Transformer æ¨¡å‹å¤§è‡´å°±ç”±ä¸Šè¿°çš„å‡ ä¸ªå±‚è¿æ¥åœ¨ä¸€èµ·æ„æˆï¼Œä½†æˆ–è®¸è¿˜æ˜¯è§‰å¾—æœ¦æœ¦èƒ§èƒ§ï¼Œæ¯”å¦‚ç©¶ç«Ÿä»€ä¹ˆæ‰æ˜¯ queryã€keyã€value ç­‰ç­‰ã€‚ä¸å¦¨å†æ¥çœ‹çœ‹ Transformer çš„ç»“æ„å›¾ï¼Œè¿™ä¸€æ¬¡å·²ç†ŸçŸ¥å¤§éƒ¨åˆ†æ¨¡å—çš„å·¥ä½œåŸç†äº†ï¼Œæ‰€ä»¥åªçœ‹æ•°æ®æµå…¥ä¸æµå‡ºå„æ¨¡å—çš„è·¯çº¿ã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>ä½œä¸º source çš„ <span class="math">\(\boldsymbol{X}\)</span> ä¸ä½œä¸º target çš„ <span class="math">\(\boldsymbol{Y}\)</span> åˆ†åˆ«ä»ä¸‹æ–¹çš„å·¦å³ä¸¤ä¾§è¿›å…¥æ¨¡å‹ã€‚<span class="math">\(\boldsymbol{X}\)</span> ä¸ <span class="math">\(\boldsymbol{Y}\)</span> éƒ½è¦ç»è¿‡è¯åµŒå…¥å¹¶åŠ ä¸Šä½ç½®ç¼–ç ï¼ŒæŒ‰ä»¥ä¸‹æ–¹å¼æ›´æ–°ï¼š</p>
<div class="math">$$
\begin{align}
    \boldsymbol{X}&amp;\leftarrow\mathrm{Embedding}(\boldsymbol{X})+\mathrm{PE}(\boldsymbol{X})\\
    \boldsymbol{Y}&amp;\leftarrow\mathrm{Embedding}(\boldsymbol{Y})+\mathrm{PE}(\boldsymbol{Y})
\end{align}
$$</div>
<p>æ¥ç€ <span class="math">\(\boldsymbol{X}\)</span> ä¸ <span class="math">\(\boldsymbol{Y}\)</span> åˆ†åˆ«è¿›å…¥ encoder ä¸ decoderï¼Œå¯ä»¥æ³¨æ„åˆ°æ•°æ®åˆ†ä½œ 4 æ¡è·¯çº¿ï¼Œè¿™æ„å‘³ç€å°†æ•°æ®å¤åˆ¶ 4 æ¬¡ã€‚å…ˆçœ‹è¿›å…¥å¤šå¤´æ³¨æ„åŠ›å±‚çš„ 3 æ¡æ•°æ®ï¼Œä»¥ encoder ä¸ºä¾‹ï¼Œåœ¨è¿™ä¸€å±‚ä¸­å°±æ˜¯åœ¨è®¡ç®—</p>
<div class="math">$$\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</div>
<p>ä¸è¨€è‡ªæ˜ï¼Œåœ¨è¿™é‡Œçš„ queryã€keyã€value ä¸‰è€…éƒ½æ˜¯ <span class="math">\(\boldsymbol{X}\)</span>ï¼Œæ˜¯åœ¨ <span class="math">\(\boldsymbol{X}\)</span> å†…éƒ¨è®¡ç®—æ³¨æ„åŠ›ï¼Œå› æ­¤ç§°å…¶ä¸º<strong>è‡ªæ³¨æ„åŠ›</strong>ï¼ˆself-attentionï¼‰ã€‚</p>
<p>åœ¨åç»­çš„ Add &amp; Norm å±‚ä¸­ï¼Œè®¡ç®—</p>
<div class="math">$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X}))$$</div>
<p>åœ¨å‰é¦ˆå±‚ä¸åç»­çš„ Add &amp; Norm å±‚è¾“çš„è¾“å‡ºç»“æœä¹Ÿå¯æƒ³è€ŒçŸ¥ï¼š</p>
<div class="math">$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\max(0,\boldsymbol{X}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2)$$</div>
<p>è¿™é‡Œçš„ <span class="math">\(\boldsymbol{X}\)</span> åˆ†ä½œä¸¤è·¯è¿›å…¥åˆ° decoder ä¸­ï¼Œåœ¨ decoder çš„è¯¥å¤šå¤´æ³¨æ„åŠ›å±‚ä¸­ï¼Œquery ä¸ key ä¸º <span class="math">\(\boldsymbol{X}\)</span>ï¼Œè€Œ value ä¸ºç±»ä¼¼æ­¥éª¤å¾—åˆ°çš„ <span class="math">\(\boldsymbol{Y}\)</span>ï¼Œè¯¥å±‚çš„è¾“å‡ºä¸º</p>
<div class="math">$$\boldsymbol{Z}=\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{Y})$$</div>
<p>è¿™ä¹Ÿæ˜¯ decoder ä¸ encoder çš„å…³é”®ä¸åŒã€‚è¾“å‡ºç»“æœ <span class="math">\(\boldsymbol{Z}\)</span> å®Œæˆåç»­çš„è®¡ç®—è¿‡ç¨‹åï¼Œå°±å¾—åˆ°å„ token çš„æ¦‚ç‡ï¼Œç”¨å„ token æ›¿æ¢å³å¯å¾—åˆ°æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬ç»“æœã€‚</p>
<p><div class="note-info"><p><span><i class="fa-solid fa-note-sticky"></i>&ensp;Note</span>&emsp;æœ‰å…´è¶£çš„è¯»è€…ä¸å¦¨æ ¹æ®å„çŸ©é˜µçš„å½¢çŠ¶å°è¯•è®¡ç®—ä¸€ä¸‹å„ä¸ªå˜é‡çš„ç»´åº¦åœ¨ Transformer åœ¨å„æ­¥éª¤ä¸­æ˜¯å¦‚ä½•å˜åŒ–çš„ï¼Œä¸€å®šä¼šå¯¹ Transformer çš„è®¡ç®—è¿‡ç¨‹æ”¶è·æ›´æ·±çš„äº†è§£ã€‚</p></div></p>
<h2 id="dai-ma-chai-jie_1">ä»£ç æ‹†è§£</h2>
<p>æœ‰äº†å¯¹ Transformer åŸç†çš„åŸºæœ¬è®¤è¯†ï¼Œå°±å¯ä»¥åŠ¨æ‰‹å®ç°ä¸€ä¸ª Transformer äº†ï¼Œé€šè¿‡ä»£ç æ›´æ·±å…¥äº†è§£ Transformer çš„ä¸€äº›ç»†èŠ‚ã€‚è¿™é‡Œä½¿ç”¨ PyTorch æ­å»ºä¸€ä¸ªæ ‡å‡†çš„ Transformerï¼Œå‚è€ƒä»£ç è§ <a href="https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py" rel="noopener" target="_blank"><i class="fa-brands fa-github"></i> aladdinpersson / Machine-Learning-Collection </a>ã€‚</p>
<p>ä»£ç ä¸­çš„å„æ¨¡å—å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ¥ä¸‹æ¥å¯¹å„æ¨¡å—é€ä¸ªæ‹†è§£ã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<h3 id="positionembedding">PositionEmbedding</h3>
<pre><code class="language-py">import math
import torch
import torch.nn as nn


class PositionEmbedding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        # d_model ä¸ºæ¨¡å‹å¤„ç†æ•°æ®çš„ç»´æ•°ï¼Œå³å…¬å¼ä¸­ d_k
        # max_len è¡¨ç¤ºæ¨¡å‹å¤„ç†çš„æœ€å¤§ token æ•°é‡
        super(PositionEmbedding, self).__init__()

        # ç”Ÿæˆå¤§å°ä¸º max_len * d_model çš„é›¶çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        # ç”Ÿæˆå¤§å°ä¸º max_len * 1 çš„ä½ç½®çŸ©é˜µ
        position = torch.arange(max_len).unsqueeze(1)
        # è®¡ç®—ä½ç½®ç¼–ç 
        div_term = torch.exp(torch.arange(0, d_model, 2) * - (math.log(10000.0) / d_model))
        x = position * div_term
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = self.pe[:, :x.size(1)]
        return x
</code></pre>
<p>é¦–å…ˆå®ç°ä½ç½®ç¼–ç æ¨¡å—ã€‚åœ¨ PyTorch ä¸­ï¼Œç”¨äºæ­å»ºç¥ç»ç½‘ç»œçš„æ¨¡å—éƒ½è¦ç»§æ‰¿ <code>nn.Module</code>ï¼ŒPyTorch ä¼šé€šè¿‡ <code>__call__()</code> è°ƒç”¨æ¨¡å—çš„ <code>forward()</code> çš„æ–¹æ³•è¿›è¡Œå‰å‘ä¼ æ’­ã€‚ç®€å•æ¥è®²å°±æ˜¯ï¼Œ<code>PositionEmbedding(x)</code> çš„åŠŸèƒ½ç­‰åŒäº <code>PositionEmbedding.forward(x)</code>ï¼Œä½†ä¸èƒ½ä½¿ç”¨ <code>PositionEmbedding.forward(x)</code>ï¼Œå› ä¸º PyTorch åšäº†è®¸å¤šæ¡ä»¶çš„åˆ¤å®šå’Œä¼˜åŒ–ã€‚</p>
<p><code>torch.arange(num)</code> çš„åŠŸèƒ½ç±»ä¼¼äº Python ä¸­çš„ <code>range(num)</code>ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å„ token çš„é¡ºåºä½ç½®ç´¢å¼•ã€‚<code>unsqueeze(dim)</code> ä¼šä»¤ Tensor åœ¨æŒ‡å®šçš„ç»´åº¦ <code>dim</code> ä¸Šæ‰©å¼  1 ç»´ï¼Œè¿™é‡Œæ˜¯ä¸ºäº†ä½¿ <code>pe</code> ä¸ <code>position</code> ä¸¤ä¸ªçŸ©é˜µçš„ç»´åº¦å¯¹é½ï¼Œä¾‹å¦‚ï¼š</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; torch.arange(5)
tensor([0, 1, 2, 3, 4])
&gt;&gt;&gt; torch.arange(5).unsqueeze(0)
tensor([[0, 1, 2, 3, 4]])
&gt;&gt;&gt; torch.arange(5).unsqueeze(1)
tensor([[0],
        [1],
        [2],
        [3],
        [4]])
&gt;&gt;&gt; torch.arange(5).size()
torch.Size([5])
&gt;&gt;&gt; torch.arange(5).unsqueeze(0).size()
torch.Size([1, 5])
&gt;&gt;&gt; torch.arange(5).unsqueeze(1).size()
torch.Size([5, 1])
</code></pre>
<p>ä»£ç ä¸­çš„ä½ç½®ç¼–ç å¹¶ä¸æ˜¯ç›´æ¥æŒ‰å…¬å¼è®¡ç®—çš„ï¼Œè€Œæ˜¯åšäº†ä¸€äº›å˜æ¢ï¼Œå…ˆè®¡ç®—ä¸€ä¸ªä¸­é—´é‡ <code>div_term</code>ï¼Œå…¶ä¸­ <code>torch.arange(0, d_model, 2)</code> å³ä¸º <span class="math">\(2i\)</span>ï¼Œå¯ä»¥æ•´ç†å‡º</p>
<div class="math">$$\begin{align}
    \mathrm{div\_term}_i&amp;=\exp\left[2i\times(-\frac{\ln10000}{d_k})\right]\\
    &amp;=\left[\exp(-\frac{\ln10000}{d_k})\right]^{2i}\\
    &amp;=\left[10000^{-\frac{1}{d_k}}\right]^{2i}\\
    &amp;=10000^{-2i/d_k}
\end{align}
$$</div>
<p>æ‰€ä»¥ <code>position * div_term</code> å°±å¯ä»¥å¾—åˆ°</p>
<div class="math">$$\mathrm{position}\times \mathrm{div\_term}_i=\mathrm{pos}/10000^{2i/d_k}$$</div>
<p>å°±æ˜¯ä½ç½®ç¼–ç ä¸­çš„ä¸€é¡¹ã€‚</p>
<p><code>pe[:, 0::2]</code> ä¸ <code>pe[:, 1::2]</code> æ˜¯ Pytorch ä¸­çš„é«˜çº§ç´¢å¼•æ“ä½œã€‚ç´¢å¼•ä¸­ç”¨ <code>,</code> åˆ†éš”ä¸åŒç»´åº¦ï¼Œä¾‹ä¸­ä»¥ <code>,</code> ä¸ºåˆ†ç•Œï¼Œå‰é¢æ˜¯å¯¹ç¬¬ 1 ç»´çš„ç´¢å¼•ï¼Œåé¢æ˜¯å¯¹ç¬¬ 2 ç»´çš„ç´¢å¼•ã€‚ç´¢å¼•æ“ä½œä¹Ÿéµå®ˆ Python çš„è§„åˆ™ï¼Œå³ <code>a:b:c</code> ä¸­ <code>a</code> ä¸ºèµ·å§‹ï¼Œ<code>b</code> ä¸ºæœ«å°¾ï¼Œ<code>c</code> ä¸ºæ­¥é•¿ã€‚</p>
<p>æ‰€ä»¥ <code>pe[:, 0::2]</code> ä¸ <code>pe[:, 1::2]</code> å–å‡ºå…¨éƒ¨ç¬¬ 1 ç»´ä¸­çš„å…ƒç´ ï¼Œå³è¡Œæ–¹å‘ä¸Šä¸æ“ä½œï¼Œå†åœ¨ç¬¬ 2 ç»´ä¸­åˆ†åˆ«ä» <code>0</code> æˆ– <code>1</code> å¼€å§‹ä»¥æ­¥é•¿ <code>2</code> å–å‡ºå…ƒç´ ï¼Œå³å–å‡ºç¬¬ <span class="math">\(2i\)</span> æˆ–ç¬¬ <span class="math">\(2i+1\)</span> åˆ—ã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>åœ¨ <code>forward()</code> éƒ¨åˆ†ï¼Œè¾“å‡ºçš„ä½ç½®ç¼–ç ä¸º <code>pe[:, :x.size(1)]</code>ï¼Œè¿™ä¸»è¦æ˜¯ä¸ºäº†ç¡®ä¿çŸ©é˜µå½¢çŠ¶åœ¨åŠ æ³•è¿‡ç¨‹ä¸­ä¸ä¼šå› éæ³•è¾“å…¥çš„å¹¿æ’­è€Œæ”¹å˜ã€‚å…¶å®åœ¨è¾“å…¥åˆæ³•çš„æƒ…å†µä¸‹ï¼Œ<code>x.size(1)</code> å°±æ˜¯ <code>d_model</code>ï¼Œç­‰ä»·äº <code>pe[:, :]</code>ï¼Œä¹Ÿç­‰ä»·äº <code>pe</code>ã€‚</p>
<!-- æŒ‡å®š `requires_grad_(False)` æ˜¯å› ä¸º PyTorch ä¼šè‡ªåŠ¨ä¿å­˜ Tensor çš„æ¥æºï¼Œç”¨äºæ›´å¿«åœ°è®¡ç®—æ¢¯åº¦ï¼Œè€Œè¿™é‡Œçš„åŠ æ³•è®¡ç®—å¹¶ä¸æ˜¯è®­ç»ƒè¿‡ç¨‹ï¼Œå–æ¶ˆä¿å­˜èƒ½èŠ‚çœä¸€éƒ¨åˆ†èµ„æºã€‚ -->
<h3 id="selfattention">SelfAttention</h3>
<p>åœ¨è¿›å…¥ Transformer æ ¸å¿ƒéƒ¨åˆ†ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å†æ¬¡æ˜ç¡®ä¸€ä¸‹è¾“å…¥æ¨¡å‹çš„æ•°æ®æ ¼å¼ã€‚ä¸Šæ–‡ä¸­ä»…ä»¥è¾“å…¥æ¨¡å‹ä¸€æ¡æ•°æ®ï¼ˆç”±è‹¥å¹² token ç»„æˆçš„ä¸€æ¡å¥å­ï¼‰ä¸ºä¾‹ï¼Œåœ¨å®é™…æ“ä½œä¸­ï¼Œä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼Œä¼šåŒæ—¶è¾“å…¥è‹¥å¹²æ¡æ•°æ®ï¼Œåœ¨æ„å»ºæ¨¡å‹æ—¶ä¹Ÿè¦è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä¸€æ¬¡è¾“å…¥æ¨¡å‹çš„æ•°æ®æ¡æ•°å°±ç§°ä¸º batch sizeï¼Œæ‰€ä»¥æ¨¡å‹æ‰€å¤„ç†çš„å…¶å®æ˜¯ä¸€ä¸ª <span class="math">\(\mathrm{batch\_size}\times\mathrm{max\_len}\times\mathrm{d\_model}\)</span> çš„é«˜ç»´çŸ©é˜µã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ<code>x.size()</code> çš„ç»“æœæ˜¯ <code>[batch_size, max_len, d_model]</code>ï¼ŒåŠ¡å¿…æ³¨æ„ä¸‰è€…é¡ºåºã€‚</p>
<pre><code class="language-py">class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        # ç¡®ä¿ embed_size èƒ½è¢« heads æ•´é™¤
        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(embed_size, embed_size)
        self.keys = nn.Linear(embed_size, embed_size)
        self.queries = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)
</code></pre>
<p>å…ˆçœ‹ <code>SelfAttention</code> çš„åˆå§‹åŒ–éƒ¨åˆ†ï¼Œæ˜ç™½äº†æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹å°±ä¸éš¾ç†è§£ä¸Šé¢çš„å„ä¸ªå±æ€§äº†ã€‚<code>head_dim</code> æ˜¯æ¯ä¸€ä¸ª head ä¸­æ³¨æ„åŠ›çš„ç»´åº¦ï¼Œ<code>embeds_size</code> å¿…é¡»èƒ½è¢« <code>heads</code> æ•´é™¤ï¼Œå¦åˆ™å°†å¤šå¤´æ³¨æ„åŠ›æ‹¼æ¥åœ¨ä¸€èµ·çš„ç»´æ•°ä¸ç­‰äºæ¨¡å‹å¤„ç†çš„ç»´æ•°å°±ä¼šå‡ºç°é—®é¢˜ã€‚</p>
<p><code>values</code>ã€<code>keys</code>ã€<code>queries</code> éƒ½æ˜¯è®¡ç®—å¤šå¤´æ³¨æ„åŠ›å‰çš„çº¿æ€§å˜æ¢ï¼Œ<code>fc_out</code> æ˜¯æ‹¼æ¥å¤šå¤´æ³¨æ„åŠ›åçš„çº¿æ€§å˜æ¢ã€‚çº¿æ€§å˜æ¢å¯ä»¥ç›´æ¥è°ƒç”¨ <code>nn.Linear(in_dim, out_dim)</code>ï¼Œåªéœ€è¦æŒ‡å®šçº¿æ€§å˜æ¢å‰åçš„ç»´æ•°å³å¯ï¼Œè¿™é‡Œçº¿æ€§å˜æ¢å‰åç»´æ•°æ²¡æœ‰å˜åŒ–ã€‚</p>
<p>å¯èƒ½ä¼šæœ‰è¯»è€…ç–‘æƒ‘ä¸ºä»€ä¹ˆè¿™é‡Œæ‰€è®¾å®šçš„çº¿æ€§å˜æ¢ä¸æ”¹å˜ç»´æ•°ï¼ŒåŸæ–‡ä¸­æ‰€æè¿°çš„æ­¥éª¤ä¸æ˜¯åº”è¯¥å°† <span class="math">\(d_\mathrm{model}\)</span> å‡è‡³ <span class="math">\(d_v\)</span> å†è®¡ç®—æ³¨æ„åŠ›å—ï¼Ÿè¿™æ˜¯æ­£ç¡®çš„ï¼ŒåŸæ–‡ä¸­çš„è®¡ç®—æµç¨‹ç¡®å®å¦‚æ­¤ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨çº¿æ€§å˜æ¢åå¤åˆ¶ <code>h</code> ä»½ï¼ˆä¾‹ä¸­ä¸º 2ï¼‰ <span class="math">\(\boldsymbol{Q}\)</span>ï¼Œç”¨è‹¥å¹²ä»½ <span class="math">\(\boldsymbol{Q}\)</span> åˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›å†æ‹¼åˆèµ·æ¥ï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„ç»´æ•°è‡ªç„¶å°±æ˜¯ <code>h * d_v</code> ï¼ˆä¾‹ä¸­ä¸º 2 * 6ï¼‰ï¼Œå†ç”¨ä¸€ä¸ªçº¿æ€§å˜æ¢å°†å…¶è½¬åŒ–å›æ¨¡å‹æ‰€å¤„ç†çš„ç»´æ•° <code>d_model</code>ï¼ˆä¾‹ä¸­ä¸º 5ï¼‰ã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>ä½†ä»£ç ä¸­ä¼˜åŒ–äº†ä¸€éƒ¨åˆ†æ¯”è¾ƒç¹ççš„æ“ä½œï¼Œä¹Ÿæœ‰å…¶ä»–ç‰ˆæœ¬çš„ä»£ç ä½¿ç”¨äº†æ›´æ¥è¿‘åŸæ–‡çš„å®ç°æ–¹å¼ï¼Œå¦‚  <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py" rel="noopener" target="_blank"><i class="fa-brands fa-github"></i> jadore801120 / attention-is-all-you-need-pytorch </a>ï¼Œæµç¨‹å°±å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå‹‰å¼ºç§°ä¹‹ä¸ºã€Œå•å¤´æ³¨æ„åŠ›å˜å¤šå¤´æ³¨æ„åŠ›ã€çš„ä¸€ç§ä»£ç å®ç°å§ã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>ä¾‹ä¸­ <code>d_model</code> ä¹Ÿå°±æ˜¯è¯åµŒå…¥çš„ç»´æ•°è¿˜æ˜¯ 5ï¼Œ<code>heads</code> ä»ä¸º 2ï¼Œ<code>d_value</code> ä»ä¸º 6ï¼Œä½†æ¨¡å‹ä¸å†æ˜¯å°† <span class="math">\(d_\mathrm{model}\)</span> å‡è‡³ <span class="math">\(d_v\)</span>ï¼Œè€Œæ˜¯å°† <span class="math">\(d_\mathrm{model}\)</span> ç›´æ¥å‡è‡³ <span class="math">\(hd_v\)</span>ï¼Œç„¶åå°† <span class="math">\(\boldsymbol{Q}\)</span> åˆ†æˆ <code>h</code> ä»½ï¼Œæ¯ä»½åˆ†åˆ«ç”¨äºè®¡ç®—å¹¶æ‹¼æ¥ä¸ºæ³¨æ„åŠ›ã€‚ä¸ä¸Šä¾‹ç›¸æ¯”ï¼Œæœ¬è´¨ä¸Šå…¶å®å¹¶æ— åŒºåˆ«ï¼ŒåŒºåˆ«ä»…ä»…æ˜¯ä¸Šä¾‹å…ˆå¤åˆ¶å¤šä¸ªçŸ©é˜µå†åˆ†åˆ«åšçº¿æ€§å˜æ¢ï¼Œè€Œè¯¥ä¾‹åªä½¿ç”¨äº†ä¸€ä¸ªæ›´å¤§çš„çŸ©é˜µä¹˜æ³•å°±å®Œæˆäº†ä¸Šè¿°æ“ä½œï¼Œæ•ˆç‡ä¸Šæ›´ä¼˜ã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>å¤šå¤´æ³¨æ„åŠ›è¿˜æœ‰ä¸€ç§å®ç°æ–¹æ³•ï¼Œä¹Ÿæ˜¯è¿™é‡Œå±•ç¤ºä»£ç æ‰€ä½¿ç”¨çš„æ–¹æ³•ã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè¿™ç§æ–¹æ³•å¯¹è¯åµŒå…¥çš„ç»´æ•°æœ‰è¦æ±‚ï¼Œåœ¨è¯åµŒå…¥çš„æ­¥éª¤ä¸­å°±å°† token è¡¨ç¤ºä¸º <code>d_v * h</code> ç»´ï¼Œè¿™ä¹Ÿæ˜¯å‰æ–‡ä»£ç åœ¨åˆå§‹åŒ–ä¸­ä½¿ç”¨ <code>assert</code> è¯­å¥ç¼˜ç”±ã€‚åç»­çš„çº¿æ€§å˜æ¢ä¸æ”¹å˜ç»´æ•°ï¼Œè®¡ç®—å¤šå¤´æ³¨æ„åŠ›æ—¶ç›´æ¥å°† <code>d_v * h</code> ç»´åˆ‡åˆ†ä¸º <code>h</code> ä»½ä½œä¸ºæ¯ä¸ª head è®¡ç®—çš„å¯¹è±¡ã€‚æ‹¼æ¥å„ head çš„æ³¨æ„åŠ›åï¼Œæœ€åçš„çº¿æ€§å˜æ¢ä¹Ÿä¸æ”¹å˜ç»´æ•°ã€‚</p>
<p>åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™ç§æ–¹æ³•åº”è¯¥æ˜¯å¯¹å‰ä¸¤ç§æ–¹æ³•çš„ç®€åŒ–ï¼Œä¸‰ä¸ªä¾‹å­ä¸­ç”¨äºè®¡ç®—å¤šå¤´æ³¨æ„åŠ›çš„ <code>d_value</code> éƒ½ä¸º 6ï¼Œè®¡ç®—é‡ç›¸åŒã€‚ç¬¬ 3 ç§æ–¹æ³•éœ€è¦æ›´å¤§çš„ <code>d_model</code>ï¼Œè€Œä¸”è®¡ç®—å¤šå¤´æ³¨æ„åŠ›æ—¶æ²¡æœ‰ä½¿ç”¨åˆ°å…¨éƒ¨çš„ embeddingï¼Œè™½è¯´æ•ˆæœç±»ä¼¼ï¼Œä½†æ€»è§‰æœ‰äº›å¥‡æ€ªã€‚è¿™æˆ–è®¸æ˜¯ä¸ºäº†è®¡ç®—ä¸Šçš„æ–¹ä¾¿ï¼Œä¸ç”¨åšè¿‡å¤šçš„çŸ©é˜µå˜æ¢ ğŸ¤”</p>
<pre><code class="language-py"># class SelfAttention(nn.Module):
    def forward(self, values, keys, query, mask):
        # è·å– batch_size
        N = query.shape[0]
        # d_v, d_k, d_q
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # å¯¹ query, key, value åšçº¿æ€§å˜æ¢
        values = self.values(values)    # (N, value_len, embed_size)
        keys = self.keys(keys)          # (N, key_len, embed_size)
        queries = self.queries(query)   # (N, query_len, embed_size)

        # å°† token çš„è¯åµŒå…¥åˆ’åˆ†ä¸º heads ä»½
        # d_model = embed_size = d_v * heads
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = queries.reshape(N, query_len, self.heads, self.head_dim)

        # queries: (N, query_len, heads, heads_dim),
        # keys: (N, key_len, heads, heads_dim)
        # energy: (N, heads, query_len, key_len)
        energy = torch.einsum("nqhd,nkhd-&gt;nhqk", [queries, keys])

        # å°†æ©ç çŸ©é˜µä¸­ä¸º 0 çš„å¯¹åº”é¡¹è®¾ä¸º -infï¼Œä¸å‚ä¸è®¡ç®—
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        # å¾—åˆ°çš„ç‚¹ç§¯é™¤ä»¥ sqrt(d_k) å¹¶ç”¨ Softmax å½’ä¸€åŒ–
        # attention: (N, heads, query_len, key_len)
        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)

        # attention: (N, heads, query_len, key_len)
        # values: (N, value_len, heads, heads_dim)
        # out after matrix multiply: (N, query_len, heads, head_dim), then
        # we reshape and flatten the last two dimensions.
        out = torch.einsum("nhql,nlhd-&gt;nqhd", [attention, values]).reshape(
            N, query_len, self.heads * self.head_dim
        )

        # æ‹¼æ¥å¤šå¤´æ³¨æ„åŠ›åçš„çº¿æ€§å˜æ¢
        # out: (N, query_len, embed_size)
        out = self.fc_out(out)

        return out
</code></pre>
<p><code>forward()</code> éƒ¨åˆ†æè¿°äº†ä¸Šè¿°è®¡ç®—å¤šå¤´é‡æ„åŠ›çš„è¿‡ç¨‹ã€‚çº¿æ€§å˜æ¢åï¼Œä½¿ç”¨ <code>reshape()</code> æ–¹æ³•å°† Tensor è½¬åŒ–åŒ–ä¸ºæŒ‡å®šç»´åº¦ï¼Œä¹Ÿå°±æ˜¯å°†è¯åµŒå…¥åˆ’åˆ†ä¸º <code>heads</code> ä»½çš„æ“ä½œï¼ŒTensor çš„å½¢çŠ¶ç”± <code>[N, query_len, embed_size]</code> å˜ä¸º <code>[N, query_len, self.heads, self.head_dim]</code>ï¼ŒæŠŠ <code>embed_size</code> æ‹†æˆ <code>heads * head_dim</code>ã€‚</p>
<p>æ¥ç€ä½¿ç”¨ <code>torch.einsum()</code> å¾—åˆ°æ³¨æ„åŠ›è®¡ç®—çš„ä¸€ä¸ªä¸­é—´é‡ <code>energy</code>ã€‚<code>torch.einsum()</code> ç§°ä¸ºçˆ±å› æ–¯å¦æ±‚å’Œçº¦å®šï¼Œå¯ä»¥éå¸¸ç®€æ´åœ°è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€è½¬ç½®å¾…æ“ä½œï¼Œä½†ä¼šæœ‰äº›éš¾ä»¥ç†è§£ã€‚</p>
<p>ä¾‹å¦‚çŸ©é˜µä¹˜æ³• <span class="math">\(\boldsymbol{A}_{i\times j}\boldsymbol{B}_{j\times k}=\boldsymbol{C}_{i\times k}\)</span>ï¼Œå¯ä»¥è¡¨ç¤ºä¸º <code>"ij,jk-&gt;ik"</code>ï¼š</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; A = torch.randn(3, 4)
&gt;&gt;&gt; B = torch.randn(4, 5)
&gt;&gt;&gt; C = torch.einsum("ij,jk-&gt;ik", [A, B])
&gt;&gt;&gt; C.size()
torch.Size([3, 5])
</code></pre>
<p>ä¾‹å¦‚çŸ©é˜µè½¬ç½® <span class="math">\((\boldsymbol{A}_{i\times j})^\top=\boldsymbol{B}_{j\times i}\)</span>ï¼Œå¯ä»¥è¡¨ç¤ºä¸º <code>"ij-&gt;ji"</code>ï¼š</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; A = torch.randn(3, 4)
&gt;&gt;&gt; B = torch.einsum("ij-&gt;ji", [A])
&gt;&gt;&gt; B.size()
torch.Size([4, 3])
</code></pre>
<p>å®šä¹‰äº†çŸ©é˜µä¹˜æ³•çš„è¡¨ç¤ºåï¼Œç›¸åº”çš„æ•°é‡ç§¯ä¸å‘é‡ç§¯å°±ä¹Ÿèƒ½è¡¨ç¤ºäº†ï¼Œä¸å†èµ˜è¿°ã€‚æ±‚å’Œæ“ä½œå°†çŸ©é˜µè½¬åŒ–ä¸ºæ•°å€¼ï¼Œè¡Œä¸åˆ—éƒ½ä¼šæ¶ˆå¤±ï¼Œæ‰€ä»¥ <span class="math">\(\sum a_{ij}\in\boldsymbol{A}_{i\times j}\)</span> å¯ä»¥è®°ä½œ <code>"ij-&gt;"</code>ï¼š</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; A = torch.randn(3, 4)
&gt;&gt;&gt; torch.einsum("ij-&gt;", [A])
tensor(0.5634)
</code></pre>
<p>æ­¤å¤–ï¼Œçˆ±å› æ–¯å¦æ±‚å’Œçº¦å®šè¿˜å¯ä»¥è¡¨ç¤ºåœ¨æŒ‡å®šç»´åº¦ä¸Šæ±‚å’Œã€åšæ•°é‡ç§¯ç­‰ä¸€ç³»åˆ—çš„å¤æ‚æ“ä½œï¼Œè¯»è€…å¯ä»¥è‡ªè¡Œè¯•éªŒã€‚</p>
<p>ä»£ç ä¸­ <code>queries</code> çš„å½¢çŠ¶ä¸º <code>[N, query_len, heads, heads_dim]</code>ï¼Œè®°ä½œ <span class="math">\(\boldsymbol{Q}_{N\times q\times h \times d}\)</span>ï¼Œ<code>keys</code> çš„å½¢çŠ¶ä¸º <code>[N, key_len, heads, heads_dim]</code>ï¼Œè®°ä½œ <span class="math">\(\boldsymbol{K}_{N\times k\times h \times d}\)</span>ï¼Œé‚£ä¹ˆ <code>torch.einsum("nqhd,nkhd-&gt;nhqk", [queries, keys])</code> æ‰€åšçš„æ“ä½œå°±æ˜¯ï¼š</p>
<ol>
<li>å°† <span class="math">\(\boldsymbol{Q}_{N\times q\times h \times d}\)</span> è½¬ç½®ä¸º <span class="math">\(\boldsymbol{Q}_{N\times h \times q\times d}\)</span>ï¼Œå°† <span class="math">\(\boldsymbol{K}_{N\times k\times h \times d}\)</span> è½¬ç½®ä¸º <span class="math">\(\boldsymbol{K}_{N\times h\times k \times d}\)</span>ï¼›</li>
<li>ä¸¤ä¸ªçŸ©é˜µä¸­çš„ <span class="math">\(N\times h\)</span> æ˜¯ <code>batch_size</code> ä¸ <code>heads</code> çš„ä¹˜ç§¯ï¼Œä»…ä»…æ˜¯è¡¨ç¤ºæ•°é‡ï¼Œæ‰€ä»¥ <span class="math">\(\boldsymbol{K}_{N\times h \times k\times d}\)</span> å¯ä»¥è§†ä½œç”± <span class="math">\(N\times h\)</span> ä¸ª <span class="math">\((\boldsymbol{K}_i)_{\ k\times d}\)</span> å­çŸ©é˜µæ„æˆçš„å¤§çŸ©é˜µã€‚é‚£ä¹ˆå›ºå®šå‰ä¸¤ç»´ä¸å˜ï¼Œè½¬ç½®åä¸¤ç»´ï¼Œç›¸å½“äº<strong>è½¬ç½®</strong>æ‰€æœ‰å­çŸ©é˜µï¼Œå¾—åˆ° <span class="math">\(\boldsymbol{K}_{N\times h \times d\times k}\)</span>ï¼›</li>
<li>å›ºå®šå‰ä¸¤ç»´ï¼Œä»¤ <span class="math">\(\boldsymbol{Q}_{N\times h \times q\times d}\)</span> ä¸ <span class="math">\(\boldsymbol{K}_{N\times h \times d\times k}\)</span> åœ¨åä¸¤ç»´ä¸Šåšä¹˜æ³•ï¼Œå¾—åˆ° <span class="math">\((\boldsymbol{QK})_{N\times h \times q \times k}\)</span>ã€‚</li>
</ol>
<p>ä»”ç»†æ€è€ƒä¸Šè¿°çš„è½¬ç½®å’Œä¹˜æ³•è¿‡ç¨‹ï¼Œå®é™…ä¸Šå°±æ˜¯åœ¨åšå¤šå¤´æ³¨æ„åŠ›ä¸­çš„ <span class="math">\(\boldsymbol{Q}\boldsymbol{K}^\top\)</span>ã€‚</p>
<p>æ©ç éƒ¨åˆ†çš„æ“ä½œå…ˆç•¥è¿‡ã€‚æ¥ç€ <code>torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)</code> å…ˆå°†å‰ä¸€æ­¥ä¸­å¾—åˆ° <code>energy</code> é™¤ä»¥ <span class="math">\(\sqrt{d_k}\)</span> å†ç”¨ Softmax å½’ä¸€åŒ–ã€‚æŒ‡å®šçš„ <code>dim=3</code> ä¸ <code>dim=-1</code> ç­‰ä»·ï¼Œå…¶ç›®çš„æ˜¯åœ¨æœ€åä¸€ç»´çš„æ–¹å‘ä¸Šå½’ä¸€åŒ–ã€‚</p>
<p>ä»¥ä¸€ä¸ªç®€å•çš„ <span class="math">\(\boldsymbol{Q}\boldsymbol{K}^\top\)</span> ä¹˜æ³•ä¸ºä¾‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ<span class="math">\(\boldsymbol{Q}\)</span> ä¸ <span class="math">\(\boldsymbol{K}\)</span> çš„æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ª token çš„è¯åµŒå…¥è¡¨ç¤ºã€‚è®¡ç®—å¾—åˆ° <span class="math">\(\boldsymbol{Q}\boldsymbol{K}^\top\)</span> åéœ€è¦å½’ä¸€åŒ–ï¼Œ<code>softmax(dim=0)</code> æ˜¯åœ¨è¡Œæ–¹å‘ä¸Šå½’ä¸€åŒ–ï¼Œåœ¨å¾—åˆ°çš„ç»“æœä¸­ï¼Œå…¨éƒ¨è¡ŒåŠ èµ·æ¥ï¼Œå„å…ƒç´ ä¸º 1ï¼›<code>softmax(dim=1)</code> æ˜¯åœ¨åˆ—æ–¹å‘ä¸Šå½’ä¸€åŒ–ï¼Œç»“æœä¸­çš„å…¨éƒ¨åˆ—åŠ èµ·æ¥ï¼Œå„å…ƒç´ ä¸º 1ã€‚</p>
<p>è®¡ç®—æ³¨æ„åŠ›è¿˜æ˜¯ä¸ºäº†å¾—åˆ°æ›´å‡†ç¡®çš„ token è¡¨ç¤ºï¼Œæ‰€ä»¥å½’ä¸€åŒ–çš„æ–¹å‘åº”è¯¥ä¸åŸå§‹çš„ <span class="math">\(\boldsymbol{Q}\)</span> æ–¹å‘ç›¸åŒï¼Œå³ <code>softmax(dim=1)</code>ã€‚ä»£ç ä¸­ä¹Ÿæ˜¯ä¸€æ ·ï¼Œ<span class="math">\((\boldsymbol{QK})_{N\times h \times q \times k}\)</span> æ˜¯ <span class="math">\(N\times h\)</span> ä¸ª <span class="math">\((\boldsymbol{Q}\boldsymbol{K}_i)_{q\times k}\)</span> å­çŸ©é˜µï¼Œè¦åœ¨æ‰€æœ‰å­çŸ©é˜µçš„åˆ—æ–¹å‘ä¸Šåšå½’ä¸€åŒ–ï¼Œé‚£ä¹ˆå°±æ˜¯åœ¨ç¬¬ 4 ä¸ªç»´åº¦ä¸Šåš Softmaxï¼Œå³ <code>softmax(dim=3)</code>ã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>æ­¤æ—¶ï¼Œä¸Šè¿°è¿‡ç¨‹å·²ç»å®Œæˆäº†å¤šå¤´æ³¨æ„åŠ›ä¸­çš„ <span class="math">\(\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\)</span>ï¼Œå°†ç»“æœè®°ä½œ <span class="math">\(\boldsymbol{A}_{N\times h\times q\times k}\)</span>ã€‚</p>
<p>åœ¨ä¸‹ä¸€æ­¥ä¸­ï¼Œç”¨ <code>"nhql,nlhd-&gt;nqhd"</code> è¡¨ç¤ºäº† <span class="math">\(\boldsymbol{A}\)</span> ä¸ <span class="math">\(\boldsymbol{V}\)</span> çš„ä¹˜æ³•ï¼Œå…·ä½“æ“ä½œæ˜¯ï¼š</p>
<ol>
<li>å°† <span class="math">\(\boldsymbol{V}_{N\times v\times h\times d}\)</span> è½¬ç½®ä¸º <span class="math">\(\boldsymbol{V}_{N\times h\times v\times d}\)</span>ï¼›</li>
<li>å›ºå®šå‰ä¸¤ç»´ï¼Œä»¤ <span class="math">\(\boldsymbol{A}_{N\times h\times q\times k}\)</span> ä¸ <span class="math">\(\boldsymbol{V}_{N\times h\times v\times d}\)</span> åœ¨åä¸¤ç»´ä¸Šåšä¹˜æ³•ï¼Œè¿™é‡Œæœ‰ <span class="math">\(q=k=v\)</span>ï¼Œæ‰€ä»¥ç»“æœä¸º <span class="math">\((AV)_{N\times h \times q\times d}\)</span>ï¼Œåˆ°è¿™ä¸€æ­¥å·²ç»è®¡ç®—äº† <span class="math">\(\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\boldsymbol{V}\)</span>ï¼›</li>
<li>å°†ç»“æœè½¬ç½®ä¸º <span class="math">\((AV)_{N\times q \times h\times d}\)</span>ã€‚</li>
</ol>
<p>æœ€åä»£ç ä½¿ç”¨ <code>reshape()</code> åˆå¹¶åä¸¤ç»´ï¼Œå°†ç»“æœè½¬åŒ–ä¸º <span class="math">\((AV)_{N\times q \times hd}\)</span>ï¼Œå¾ˆå·§å¦™åœ°æ‹¼æ¥äº†å¤šä¸ª head çš„æ³¨æ„åŠ›ï¼Œæœ€åé€šè¿‡çº¿æ€§å±‚å†è¾“å‡ºç»“æœã€‚</p>
<p>è‡³æ­¤ï¼ŒTransformer ä¸­çš„ <code>SelfAttention</code> éƒ¨åˆ†å·²ç»ç»“æŸï¼Œè¯»è€…æˆ–è®¸ä¼šè§‰å¾—å¤´æ˜è„‘èƒ€ã€‚ä¸å¿…æ‹…å¿ƒï¼Œæœ€ä¸ºè‰°æ¶©çš„ä¸€éƒ¨åˆ†å·²ç»è¿‡å»ï¼Œæ¥ä¸‹æ¥æ˜¯ä¸€è·¯ä¸‹å¡ ğŸš©</p>
<h3 id="transformerblock">TransformerBlock</h3>
<pre><code class="language-py">class TransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TransformerBlock, self).__init__()
        # å‰ä¸€å±‚çš„å¤šå¤´æ³¨æ„åŠ›
        self.attention = SelfAttention(embed_size, heads)
        # Add &amp; Norm å±‚
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        # å‰é¦ˆå±‚
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, value, key, query, mask):
        attention = self.attention(value, key, query, mask)

        x = self.dropout(self.norm1(attention + query))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out
</code></pre>
<p><code>TransformerBlock</code> æ¨¡å—åŒ…æ‹¬å¤šå¤´æ³¨æ„åŠ›ä¸åæ¥çš„ Add &amp; Normã€Feed Forwardã€Add &amp; Norm ä¸‰å±‚ã€‚</p>
<p>åˆå§‹åŒ–éƒ¨åˆ†ä½¿ç”¨ <code>nn.Sequential()</code> å°† <code>nn.Linear()</code>ã€<code>nn.ReLU()</code>ã€<code>nn.Linear</code> ä¾æ¬¡è¿æ¥èµ·æ¥å½¢æˆå‰é¦ˆå±‚ï¼Œæ­£å¦‚å‰æ–‡æ‰€è¯´çš„ï¼Œæ•°æ®è¿›å…¥å‰é¦ˆå±‚å…ˆå‡ç»´å†æ¿€æ´»ï¼Œæœ€åå†é™å›åŸæ¥ç»´åº¦ï¼Œ<code>forward_expansion</code> å†³å®šå‡ç»´çš„å€æ•°ã€‚<code>dropout</code> ç”¨äºéšæœºå¼ƒç”¨ä¸€éƒ¨åˆ†æ•°æ®é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œç›´æ¥è°ƒç”¨ <code>nn.Dropout()</code> ç±»ï¼Œæ¥æ”¶çš„æ•°å€¼å†³å®šäº†å¼ƒç”¨æ•°æ®çš„æ¯”ä¾‹ã€‚</p>
<p><code>forward()</code> éƒ¨åˆ†ä¹Ÿå¾ˆç®€å•ï¼Œè®¡ç®—çš„å¤šå¤´æ³¨æ„åŠ›ä¾æ¬¡åš Add &amp; Normã€Feed Forwardã€Add &amp; Norm ä¸‰å±‚åè¾“å‡ºæ•°æ®ã€‚</p>
<h3 id="encoder">Encoder</h3>
<pre><code class="language-py">class Encoder(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        embed_size,
        num_layers,
        heads,
        device,
        forward_expansion,
        dropout,
        max_length,
    ):

        super(Encoder, self).__init__()
        self.embed_size = embed_size
        # CPU or GPU
        self.device = device
        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)
        self.position_embedding = PositionalEncoding(embed_size, max_length)

        self.layers = nn.ModuleList(
            [
                TransformerBlock(
                    embed_size,
                    heads,
                    dropout=dropout,
                    forward_expansion=forward_expansion,
                )
                for _ in range(num_layers)
            ]
        )

        self.dropout = nn.Dropout(dropout)
</code></pre>
<p>Encoder æ˜¯ Transformer ä¸­çš„å·¦è¾¹éƒ¨åˆ†ï¼ŒTransformer ä¸­æœ‰ <span class="math">\(N\)</span> ä¸ª <code>TransformerBlock</code> é¡ºåºå æ”¾åœ¨ä¸€èµ·ç»„æˆ encoderã€‚æ‰€ä»¥åœ¨åˆå§‹åŒ–éƒ¨åˆ†ï¼Œä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼åœ¨ <code>layers</code> ä¸­æ”¾ç½®äº† <code>num_layers</code> å±‚ <code>TransformerBlock</code>ã€‚</p>
<pre><code class="language-py"># class Encoder(nn.Module):
    def forward(self, x, mask):
        # è¾“å…¥æ•°æ®çš„ batch_size ä¸é•¿åº¦
        N, seq_length = x.shape
        # ä»è¾“å…¥æ•°æ®è®¡ç®—ä½ç½®ç´¢å¼•
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        # ç”±ä½ç½®ç´¢å¼•å¾—åˆ°ä½ç½®ç¼–ç ï¼Œå¹¶ dropout ä¸€éƒ¨åˆ†æ•°æ®
        out = self.dropout(
            (self.word_embedding(x) + self.position_embedding(positions))
        )

        # è®©æ•°æ®é€å±‚ç»è¿‡ encoderï¼Œè®¡ç®—è‡ªæ³¨æ„åŠ›
        for layer in self.layers:
            out = layer(out, out, out, mask)

        return out
</code></pre>
<p>åœ¨ <code>forward()</code> éƒ¨åˆ†ä¸­ï¼Œä½¿ç”¨ <code>torch.arange()</code> å¾—åˆ°ä½ç½®ç´¢å¼•ï¼Œå†ç”¨ <code>expand()</code> æ–¹æ³•å°†ä½ç½®ç´¢å¼•çŸ©é˜µçš„å½¢çŠ¶å˜ä¸ºä¸è¾“å…¥æ•°æ®ç›¸åŒï¼Œ<code>expand()</code> æ–¹æ³•çš„ä¸»è¦ä½œç”¨æ˜¯å¤åˆ¶ï¼Œä¾‹å¦‚ï¼š</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; torch.arange(0, 5)
tensor([0, 1, 2, 3, 4])
&gt;&gt;&gt; torch.arange(0, 5).expand(2, 5)
tensor([[0, 1, 2, 3, 4],
        [0, 1, 2, 3, 4]])
</code></pre>
<p><code>to()</code> æ–¹æ³•ç”¨äºæŒ‡å®š Tensor å­˜å‚¨çš„è®¾å¤‡ï¼Œä¾‹å¦‚ <code>"CPU"</code> æˆ– <code>"GPU"</code>ã€‚å°†è¯åµŒå…¥åŠ ä¸Šä½ç½®ç¼–ç å¾—åˆ° <code>out</code>ï¼Œå†å°† <code>out</code> é€å…¥ encoder ä¸­è®¡ç®—ç»“æœã€‚</p>
<p><code>layer(out, out, out)</code> çœ‹èµ·æ¥æˆ–è®¸æœ‰äº›å¥‡æ€ªï¼Œè¯·ç•™æ„ï¼Œå‰æ–‡å·²ç»è®¨è®ºè¿‡ï¼Œåœ¨ encoder ä¸­è®¡ç®—çš„æ˜¯<strong>è‡ªæ³¨æ„åŠ›</strong>ï¼Œæ‰€ä»¥æ­¤æ—¶çš„ queryã€keyã€value éƒ½æ˜¯ç›¸åŒçš„ï¼Œè€Œåœ¨ decoder ä¸­å°±ä¼šæœ‰æ‰€ä¸åŒäº†ã€‚</p>
<h3 id="decoderblock">DecoderBlock</h3>
<pre><code class="language-py">class DecoderBlock(nn.Module):
    def __init__(self, embed_size, heads, forward_expansion, dropout, device):
        super(DecoderBlock, self).__init__()
        self.norm = nn.LayerNorm(embed_size)
        self.attention = SelfAttention(embed_size, heads=heads)
        self.transformer_block = TransformerBlock(
            embed_size, heads, dropout, forward_expansion
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, value, key, src_mask, trg_mask):
        attention = self.attention(x, x, x, trg_mask)
        query = self.dropout(self.norm(attention + x))
        out = self.transformer_block(value, key, query, src_mask)
        return out
</code></pre>
<p>ç±»ä¼¼åœ°ï¼ŒDecoder æ˜¯ Transformer ç»“æ„å›¾ä¸­çš„å³ä¾§éƒ¨åˆ†ï¼Œä¹Ÿæ˜¯ç”± <span class="math">\(N\)</span> å±‚ <code>DecoderBlock</code> ç»„æˆã€‚decoder åªæ¯” encoder å¤šäº†ä¸€ä¸ªæ©ç æ³¨æ„åŠ›å±‚ï¼Œå…¶ä»–ç»“æ„ç›¸åŒï¼Œæ‰€ä»¥ <code>DecoderBlock</code> çš„åˆå§‹åŒ–ä¸­ç›´æ¥è°ƒç”¨äº†å…ˆå‰å®šä¹‰çš„ <code>TransformerBlock</code>ã€‚</p>
<p><code>forward()</code> ä¸­ï¼Œtarget è¿›å…¥ decoder åï¼Œå…ˆè®¡ç®—<strong>è‡ªæ³¨æ„åŠ›</strong>ï¼ˆ<code>attention(x, x, x)</code>ï¼‰ï¼Œå†ç»è¿‡ Add &amp; Norm å±‚å¾—åˆ° <code>query</code>ï¼Œå†ä¸ encoder ä¸­çš„ç»“æœåšå¤šå¤´æ³¨æ„åŠ›ï¼ˆ<code>attention(value, key, query)</code>ï¼‰ï¼Œè¾“å‡ºç»“æœã€‚ç•™æ„ä¸¤ç§æ³¨æ„åŠ›è®¡ç®—çš„ä¸åŒï¼Œå‚è€ƒ Transformer ç»“æ„å›¾ç†è§£ä¸€ä¸‹å°±ä¼šå¾ˆæ˜ç¡®ã€‚</p>
<h3 id="decoder">Decoder</h3>
<pre><code class="language-py">class Decoder(nn.Module):
    def __init__(
        self,
        trg_vocab_size,
        embed_size,
        num_layers,
        heads,
        forward_expansion,
        dropout,
        device,
        max_length,
    ):
        super(Decoder, self).__init__()
        self.device = device
        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)
        self.position_embedding = PositionEmbedding(embed_size,max_length)

        self.layers = nn.ModuleList(
            [
                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)
                for _ in range(num_layers)
            ]
        )
        self.fc_out = nn.Linear(embed_size, trg_vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, src_mask, trg_mask):
        N, seq_length = x.shape
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))

        for layer in self.layers:
            x = layer(x, enc_out, enc_out, src_mask, trg_mask)

        out = self.fc_out(x)

        return out
</code></pre>
<p>å®ç°äº† <code>DecoderBlock</code> åï¼Œ<code>Decoder</code> å°±æ²¡æœ‰ä»€ä¹ˆå†…å®¹äº†ï¼Œä¸ encoder ç±»ä¼¼ï¼Œå°±æ˜¯å°†å¤šä¸ª <code>DecoderBlock</code> ç»„è£…èµ·æ¥ï¼ŒæŒ‰æ¥å£ä¼ å…¥æ•°æ®è¿›è¡Œè®¡ç®—ã€‚</p>
<h3 id="transformer">Transformer</h3>
<p>æœ€åçš„ <code>Transformer</code> å°†å„ä¸ªæ¨¡å—éƒ½ç»„åˆèµ·æ¥ï¼š</p>
<pre><code class="language-py">class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        trg_vocab_size,
        src_pad_idx,
        trg_pad_idx,
        embed_size=512,
        num_layers=6,
        forward_expansion=4,
        heads=8,
        dropout=0,
        device="cpu",
        max_length=100,
    ):

        super(Transformer, self).__init__()

        self.encoder = Encoder(
            src_vocab_size,
            embed_size,
            num_layers,
            heads,
            device,
            forward_expansion,
            dropout,
            max_length,
        )

        self.decoder = Decoder(
            trg_vocab_size,
            embed_size,
            num_layers,
            heads,
            forward_expansion,
            dropout,
            device,
            max_length,
        )

        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        self.device = device

    def make_src_mask(self, src):
        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)
        # (N, 1, 1, src_len)
        return src_mask.to(self.device)

    def make_trg_mask(self, trg):
        N, trg_len = trg.shape
        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(
            N, 1, trg_len, trg_len
        )

        return trg_mask.to(self.device)

    def forward(self, src, trg):
        src_mask = self.make_src_mask(src)
        trg_mask = self.make_trg_mask(trg)
        enc_src = self.encoder(src, src_mask)
        out = self.decoder(trg, enc_src, src_mask, trg_mask)
        return out
</code></pre>
<p>åˆå¦‚åŒ–éƒ¨åˆ†ä¸»è¦æ˜¯è®¾å®šäº†é»˜è®¤çš„å‚æ•°ï¼Œå¹¶å¼•å…¥å‰é¢å®šä¹‰å¥½çš„ <code>Encoder</code> ä¸ <code>Decoder</code> æ¨¡å—ã€‚<code>Transformer</code> ä¸­è¿˜å¤šäº† <code>make_src_mask()</code> ä¸ <code>make_trg_mask()</code> ä¸¤ä¸ªå‡½æ•°ï¼Œè¿™å°±ä¸å¾—ä¸è°ˆè°ˆ Transformer ä¸­çš„æ©ç æœºåˆ¶äº†ã€‚</p>
<p>è€ƒè™‘ä¸€ä¸ªæƒ…å¢ƒï¼Œéœ€è¦ä½¿ç”¨ Transformer ç¿»è¯‘ä¸€æ‰¹ï¼ˆè‹¥å¹²æ¡ï¼‰å¥å­ï¼Œå„å¥å­çš„é•¿åº¦è‡ªç„¶æ˜¯ä¸åŒçš„ï¼Œé‚£ä¹ˆè¾“å…¥æ¨¡å‹çš„æ•°æ®çš„å½¢çŠ¶ä¹Ÿæ˜¯ä¸åŒçš„ï¼Œè¿™åœ¨åç»­æ­¥éª¤ä¸­å°±ä¼šå‡ºç°å¾ˆå¤šé—®é¢˜ã€‚åœ¨å®é™…ä¸­ï¼Œé€šå¸¸ä¼šæ‰¾åˆ°æ–‡æœ¬ä¸­æœ€é•¿çš„å¥å­ï¼ˆ<code>max_len</code>ï¼‰ï¼Œå†å°†æ‰€æœ‰å¥å­éƒ½å˜ä¸ºè¯¥é•¿åº¦ï¼Œè¿™ç§æ“ä½œç§°ä¸º paddingã€‚</p>
<p>å…·ä½“åšæ³•å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåˆ†åˆ«ç”¨ <code>&lt;s&gt;</code> ä¸ <code>&lt;e&gt;</code> æ ‡è®°å¥å­çš„èµ·è®«ï¼Œç”¨ <code>&lt;p&gt;</code> å¡«å…… <code>&lt;e&gt;</code> åçš„ç©ºä½ï¼Œå„æ•°æ®çš„é•¿åº¦å°±ä¼šä¸€è‡´ã€‚ç„¶åæ ¹æ®è®¾å®šçš„è¯å…¸ï¼Œå°† token è½¬åŒ–ä¸ºç´¢å¼•ï¼Œæ¥ç€å†åšè¯åµŒå…¥ã€‚<code>make_src_mask()</code> å°±æ˜¯æ ¹æ® <code>&lt;p&gt;</code> çš„ç´¢å¼•ï¼Œå°† <code>&lt;p&gt;</code> æ‰€åœ¨ä½ç½®éƒ½æ ‡è®°ä¸º <code>False</code>ï¼Œå…¶ä»–ä½ç½®æ ‡è®°ä¸º <code>True</code>ã€‚</p>
<p>åç»­ <code>unsqueeze()</code> çš„æ“ä½œæ¯”è¾ƒè´¹è§£ï¼Œå…¶å®å®ƒæ˜¯åˆ©ç”¨äº† PyTorch çš„å¹¿æ’­æœºåˆ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ¹é…çŸ©é˜µçš„å½¢çŠ¶ã€‚å›¾ä¸­çš„ä¾‹å­å¯ä»¥çœ‹ä½œæ˜¯å°†çŸ©é˜µç¿»è½¬å†åœ¨ç¬¬ 3 ä¸ªæ–¹å‘ä¸Šæ‹‰é•¿ã€‚å› ä¸ºä»£ç ä¸­çš„æ©ç è¦ç”¨äºæ©ç›–å½¢çŠ¶ä¸º <code>[N, heads, query_len, key_len]</code> å…·æœ‰ 4 ä¸ªæ–¹å‘çš„ <code>energy</code>ï¼Œæ‰€ä»¥è¦é¢å¤–å†åšä¸€æ¬¡ <code>unsqueeze()</code>ã€‚æœ€åå°†æ©ç ç”¨äºæ©ç›–è¯åµŒå…¥æ•°æ®ï¼Œæ©ç å°±åƒä¸€ä¸ªç½©å­ç›–åœ¨è¯åµŒå…¥æ•°æ®ä¸Šï¼Œæ¨¡å‹åªè®¡ç®— <code>True</code> ä½ç½®ä¸Šçš„æ•°æ®ã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>ä½¿ç”¨æ©ç å¯ä»¥è®©æ¨¡å‹çµæ´»åœ°å¤„ç†ä¸åŒé•¿åº¦çš„æ•°æ®ï¼Œæ•°æ®çš„é•¿åº¦ç”±æ©ç å†³å®šï¼Œæ”¹å˜æ©ç å°±ç›¸å½“äºæ”¹å˜å¤„ç†çš„æ•°æ®ï¼Œè€Œä¸å»æ”¹å˜å­˜å‚¨åœ¨ç¡¬ä»¶ä¸­çš„æ•°æ®ï¼Œè¿™å¯¹äºè®¡ç®—æ›´æœ‰åˆ©ã€‚</p>
<p><code>make_trg_mask()</code> å‡½æ•°äº§ç”Ÿç”¨äº target æ•°æ®çš„æ©ç ï¼Œåœ¨ target ä¸Šä½¿ç”¨æ©ç çš„åŸå› ä¸ source ä¸åŒã€‚åœ¨ decoder ä¸­ï¼Œæ¨¡å‹è¦æ ¹æ®è¾“å…¥æ•°æ®çš„è®¡ç®—ç»“æœç»™å‡ºæ–° tokenï¼Œè€Œç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹æ˜¯é¡ºåºçš„ï¼Œä¾èµ–äºå‰ä¸€æ­¥ç”Ÿæˆçš„ç»“æœã€‚å…·ä½“æ¥è¯´å°±æ˜¯ï¼Œ</p>
<ol>
<li>åºåˆ—ä»¥ <code>&lt;s&gt;</code> æ ‡è®°èµ·å§‹ï¼›</li>
<li>æ ¹æ®å·²æœ‰çš„ <code>&lt;s&gt;</code> ç”Ÿæˆ <code>A</code>ï¼›</li>
<li>æ ¹æ®ç”Ÿæˆçš„ <code>&lt;s&gt; A</code> ç”Ÿæˆ <code>B</code>ï¼›</li>
<li>æ ¹æ®ç”Ÿæˆçš„ <code>&lt;s&gt; A B</code> ç”Ÿæˆ <code>C</code>ï¼›</li>
<li>ä»¥æ­¤ç±»æ¨ï¼Œç›´è‡³æ¨¡å‹ç”Ÿæˆ <code>&lt;e&gt;</code>ï¼Œå¥å­ç»“æŸã€‚</li>
</ol>
<p>å‰æ–‡å·²ç»è®¨è®ºè¿‡ï¼Œè¿™ç§æ–¹æ³•æœ‰å¾ˆå¤šå±€é™æ€§ï¼Œè€Œ Transformer çš„å·§å¦™ä¹‹å¤„å°±åœ¨äºèƒ½å¤Ÿå¹¶è¡Œå®Œæˆè¿™ä¸ªè¿‡ç¨‹ã€‚</p>
<p>æˆ‘ä»¬å¯ä»¥è€ƒè™‘è®­ç»ƒè¿‡ç¨‹ï¼Œå®é™…ä¸Šä¸ç”Ÿæˆè¿‡ç¨‹ç±»ä¼¼ï¼Œè®­ç»ƒè¿‡ç¨‹å°±æ˜¯è¦æ ¹æ®å·²ç»ç”Ÿæˆçš„ <code>&lt;s&gt;</code> å»ºç«‹ä¸ä¸‹ä¸€ä¸ª token <code>A</code> çš„å…³ç³»ï¼Œè€Œä¸èƒ½æ˜¯ä¸åç»­ <code>B</code> æˆ– <code>C</code> çš„å…³ç³»ï¼Œå°†è¿™ç§å…³ç³»ä»¥å‚æ•°çš„å½¢å¼å­˜å‚¨åˆ°æ¨¡å‹ä¸­ï¼Œæ¨ç†é˜¶æ®µå°±èƒ½é¡ºåˆ©åœ°æ ¹æ® <code>&lt;s&gt;</code> ç”Ÿæˆ <code>A</code>ã€‚è¿™æ ·çš„è®­ç»ƒè¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>Transformer ä¸éœ€è¦é€ä¸ª token ç”Ÿæˆå†å»ºç«‹å…³ç³»ï¼Œå¯ä»¥é€šè¿‡ä¸‹ä¸‰è§’çŸ©é˜µä¸€æ¬¡ç›´æ¥å–å‡º <code>&lt;s&gt;</code>ã€<code>&lt;s&gt; A</code>ã€<code>&lt;s&gt; A B</code> ç­‰ token åºåˆ—ï¼Œå¹¶è¡Œåœ°è®­ç»ƒæ¨¡å‹ä¸å¯¹åº”çš„ä¸‹ä¸€ä¸ª token å»ºç«‹å…³ç³»ã€‚æœ€åå°† <code>&lt;s&gt;</code> ä¸æ¯ä¸€æ­¥éª¤ä¸­æ–°ç”Ÿæˆ token <code>A</code>ã€<code>B</code>ã€<code>C</code>ã€<code>&lt;e&gt;</code> æ‹¼åˆèµ·æ¥ï¼Œå³å¾—åˆ°ç”Ÿæˆçš„æ–‡æœ¬ã€‚</p>
<p><code>make_trg_mask()</code> å°±æ˜¯åœ¨æ„å»ºè¿™ä¸ªä¸‹ä¸‰è§’çš„æ©ç ã€‚<code>torch.ones()</code> ç”¨äºç”ŸæˆæŒ‡å®šå¤§å°å…ƒç´ å…¨ä¸º <code>1</code> çš„çŸ©é˜µï¼Œç„¶åç”¨ <code>torch.tril()</code> å–è¯¥çŸ©é˜µçš„ä¸‹ä¸‰è§’ï¼Œå†ç”¨ <code>expand()</code> æ–¹æ³•å°†è¯¥çŸ©é˜µå¤åˆ¶åˆ°ä¸ <code>batch_size</code> åŒ¹é…ã€‚</p>
<h3 id="train">Train</h3>
<p>ä»å‰é¢è®¨è®ºçš„æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹è¿˜å¯ä»¥çŸ¥é“çš„ä¸€ç‚¹æ˜¯ï¼Œæ¨¡å‹æ°¸è¿œä¸ä¼šç”Ÿæˆ <code>&lt;s&gt;</code>ï¼Œæ‰€ä»¥ target ä¸­æ²¡æœ‰ <code>&lt;s&gt;</code>ï¼Œè€Œ source åˆ™å¿…é¡»ç”± <code>&lt;s&gt;</code> èµ·å§‹ã€‚åœ¨å®é™…ä¸­ï¼Œä¸€ç§åšæ³•æ˜¯ï¼Œç”¨é¢„å¤„ç†çš„è„šæœ¬åœ¨åŸå§‹è®­ç»ƒæ•°æ®ï¼ˆä¾‹å¦‚ <code>.csv</code>ã€<code>.txt</code> æ–‡ä»¶ï¼‰ä¸­æ ‡ä¸Šæ ‡è®°ï¼›å¦ä¸€ç§æ–¹æ³•æ˜¯ï¼Œåœ¨è®­ç»ƒä»£ç ä¸­åŠ å…¥é¢„å¤„ç†çš„åŠŸèƒ½ï¼Œè¯»å–æ•°æ®æ—¶åˆ†åˆ«ä¸ºæ•°æ®åšä¸Šç›¸åº”æ ‡è®°ã€‚ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæœ¬æ–‡å°±ä¸å®ç°è¿™ä¸€éƒ¨åˆ†åŠŸèƒ½ï¼Œä½¿ç”¨ Transformer å¯ä»¥ç›´æ¥å¤„ç†çš„æ•°æ®ã€‚</p>
<p>ç”Ÿæˆè®­ç»ƒæ•°æ®çš„å‡½æ•°ä¸º</p>
<pre><code class="language-py">def generate_random_batch(batch_size, max_length=16):
    src = []
    for i in range(batch_size):
        # éšæœºæŒ‡å®šæœ‰æ•ˆæ•°æ®çš„é•¿åº¦
        random_len = random.randint(1, max_length - 2)
        # åœ¨æ•°æ®èµ·è®«å¤„åŠ ä¸Šæ ‡è®°ï¼Œ"&lt;s&gt;": 0, "&lt;e&gt;": 1
        random_nums = [0] + [random.randint(3, 9) for _ in range(random_len)] + [1]
        # padding å¡«æ»¡æ•°æ®é•¿åº¦ï¼Œ"&lt;p&gt;": [2]
        random_nums = random_nums + [2] * (max_length - random_len - 2)
        src.append(random_nums)

    src = torch.LongTensor(src)
    # tgt å»é™¤æœ«å°¾çš„ token
    tgt = src[:, :-1]
    # tgt_y å»é™¤é¦–ä¸ª &lt;s&gt;ï¼Œå³æ¨¡å‹éœ€è¦é¢„æµ‹çš„ tokenï¼Œç”¨äºè®¡ç®—æŸå¤±
    tgt_y = src[:, 1:]
    # æ¨¡å‹éœ€è¦é¢„æµ‹çš„ token æ•°é‡ï¼ˆä¸è®¡ &lt;p&gt;ï¼‰ï¼Œç”¨äºè®¡ç®—æŸå¤±å‡½æ•°
    n_tokens = (tgt_y != 2).sum()

    return src, tgt, tgt_y, n_tokens
</code></pre>
<p><code>generate_random_batch()</code> èƒ½å¤Ÿç”Ÿæˆ Transformer å¯ä»¥ç›´æ¥è®¡ç®—çš„ç›¸åŒçš„ source ä¸ targetï¼Œè¯¥æ¨¡å‹çš„ä»»åŠ¡ç›®æ ‡å°±æ˜¯ç”Ÿæˆä¸è¾“å…¥ç›¸åŒçš„åºåˆ—ã€‚æ¨¡å‹ä¸ä¼šç”Ÿæˆ <code>&lt;s&gt;</code>ï¼Œæ‰€ä»¥<code>tgt_y</code> å»é™¤ <code>&lt;s&gt;</code> ç”¨äºä¸ç”Ÿæˆçš„åºåˆ—å¯¹æ¯”è®¡ç®—æŸå¤±ï¼Œè¿™å¾ˆå®¹æ˜“ç†è§£ã€‚ä½†ä¸ºä»€ä¹ˆ <code>tgt</code> éœ€è¦å»é™¤æœ€åä¸€ä¸ª token å‘¢ï¼Ÿè¿™ä¸€ç‚¹æˆ‘å°†åœ¨åæ–‡ç”Ÿæˆåºåˆ—çš„ Predict ä¸€èŠ‚è®¨è®ºã€‚è®­ç»ƒä¸æµ‹è¯•æ¨¡å‹çš„ä»£ç å¦‚ä¸‹ï¼š</p>
<pre><code class="language-py">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# &lt;p&gt; ç´¢å¼•
src_pad_idx = 2
trg_pad_idx = 2
# è¯è¡¨å¤§å°ï¼Œå³å…¨éƒ¨ token æ•°é‡ï¼ŒåŒ…æ‹¬ &lt;s&gt; &lt;e&gt; &lt;p&gt; ç­‰æ ‡è®°
src_vocab_size = 10
trg_vocab_size = 10
# æ–‡æœ¬æœ€å¤§é•¿åº¦
max_len = 16

model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,
                    embed_size=128, num_layers=2, dropout=0.1, max_length=max_len,
                    device=device).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
criteria = nn.CrossEntropyLoss()
total_loss = 0

for step in range(2000):
    src, tgt, tgt_y, n_tokens = generate_random_batch(batch_size=2, max_length=max_len)
    optimizer.zero_grad()
    out = model(src, tgt)

    # contiguous() ä¸ view() å°†çŸ©é˜µåœ¨å„è¡Œé¦–å°¾ç›¸è¿ä¸ºä¸€è¡Œï¼ˆå³å‘é‡ï¼‰
    # åœ¨ä¸¤å‘é‡é—´è®¡ç®—æŸå¤±å‡½æ•°
    # tgt_y ä¸­å…ƒç´ çš„å€¼æ˜¯ç´¢å¼•ï¼Œé™¤ä»¥ n_tokens å°†å…¶ç¼©æ”¾åˆ° [0, 1]
    loss = criteria(out.contiguous().view(-1, out.size(-1)),
                    tgt_y.contiguous().view(-1)) / n_tokens
    loss.backward()
    optimizer.step()

    total_loss += loss

    if step != 0 and step % 40 == 0:
        print(f"Step {step}, total_loss: {total_loss}")
        total_loss = 0

# Predict
copy_test(model, max_len)
</code></pre>
<p>PyTorch ä½¿ç”¨ <code>torch.optim</code> å®šä¹‰æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…¶ä¸­å¯ä»¥é€‰æ‹©éå¸¸å¤šç§çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œè¿™é‡Œé€‰æ‹©äº† <code>Adam()</code>ï¼Œ<code>lr=3e-4</code> æŒ‡å®šäº†è®­ç»ƒæ­¥éª¤çš„å­¦ä¹ ç‡ã€‚<code>nn.CrossEntropyLoss()</code> ç”¨äºè®¡ç®—ä¸¤ä¸ªå‘é‡çš„äº¤å‰ç†µæŸå¤±ï¼Œä½œä¸ºè®­ç»ƒè¿‡ç¨‹çš„æŸå¤±å‡½æ•°ã€‚</p>
<p>åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæ¯ä¸€ä¸ªå¾ªç¯å¤„ç† 1 ä¸ª batch çš„æ•°æ®ï¼Œåœ¨åŒä¸€ä¸ª batch ä¸­ PyTorch è‡ªåŠ¨è®¡ç®—æ¢¯åº¦çš„åå‘ä¼ æ’­å¹¶æ›´æ–°å‚æ•°ã€‚ä½†åœ¨æ–°çš„ batch ä¸­ï¼Œå› ä¸ºå·²ç»æ›´æ–°åˆ°å‚æ•°ä¸­äº†ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›ä¿ç•™ä¸Šä¸€ä¸ª batch çš„æ¢¯åº¦ï¼Œæ‰€ä»¥ç”¨ <code>optimizer.zero_grad()</code> å°†æ¢¯åº¦æ¸…ç©ºã€‚</p>
<p>å°† <code>src</code> ä¸ <code>tgt</code> ä¼ å…¥æ¨¡å‹ï¼Œ<code>out</code> å°±æ˜¯ Transformer çš„è®¡ç®—ç»“æœã€‚<code>loss.backward()</code> ä¸ <code>optimizer.step()</code> ä¸¤è¡Œä»£ç å°±æ˜¯å‰é¢æ‰€è¯´çš„è®© PyTorch è‡ªåŠ¨è®¡ç®—æ¢¯åº¦çš„åå‘ä¼ æ’­å¹¶æ›´æ–°å‚æ•°ã€‚</p>
<h3 id="predict">Predict</h3>
<p>è®­ç»ƒç»“æŸåï¼Œæˆ‘ç”¨ <code>copy_test()</code> å‡½æ•°æµ‹è¯•æ¨¡å‹çš„æ•ˆæœï¼Œè¿™ä¸ªæµ‹è¯•å‡½æ•°å®šä¹‰ä¸º</p>
<pre><code class="language-py">def copy_test(model, max_len):
    model = model.eval()
    src = torch.LongTensor([[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]])
    # æ¨¡å‹ä» &lt;s&gt; å¼€å§‹ç”Ÿæˆåºåˆ—ï¼Œä½†ä¸ä¼šç”Ÿæˆ &lt;s&gt;ï¼Œæ‰€ä»¥æŒ‡å®šèµ·å§‹çš„ &lt;s&gt;
    tgt = torch.LongTensor([[0]])

    for i in range(max_len):
        # outï¼š (1, i + 1, 10)
        # i + 1 æ¨¡å‹è¾“å‡ºçš„ token æ•°é‡
        # 10 ä¸º vocab_sizeï¼Œæ˜¯è¯è¡¨ä¸­ token æ•°é‡ï¼Œout æ˜¯è¯è¡¨ä¸­å„ token åœ¨æ­¤å¤„å‡ºç°çš„æ¦‚ç‡
        out = model(src, tgt)
        # å–è¾“å‡ºçš„ i + 1 ä¸ª token ä¸­çš„æœ€åä¸€ä¸ª
        # predict: (1, 10)
        predict = out[:, -1]
        # å–å¾—æ¦‚ç‡æœ€å¤§çš„ token ç´¢å¼•
        # y: (1, )
        y = torch.argmax(predict, dim=1)
        # é€ä¸ªæ‹¼åˆ token ç´¢å¼•
        # y.unsqueeze(0): (1, 1)
        # tgt: (1, i + 1 )
        tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)
        # è‹¥ç”Ÿæˆ token &lt;e&gt;ï¼Œè¡¨ç¤ºå¥å­ç»“æŸï¼Œé€€å‡ºå¾ªç¯
        if y == 1:
            break
    print(tgt)
</code></pre>
<p><code>eval()</code> æ–¹æ³•ä»¤æ¨¡å‹é€€å‡ºè®­ç»ƒæ¨¡å¼ï¼Œä¼šå…³é—­ dropout ç­‰è®­ç»ƒè¿‡ç¨‹ä¸­æ‰éœ€è¦çš„åŠŸèƒ½ã€‚åœ¨å¾ªç¯ä¸­é€ä¸ªæ‹¼åˆç”Ÿæˆçš„ tokenï¼Œå°±èƒ½å¾—åˆ°ç”Ÿæˆçš„å¥å­ã€‚å¾ªç¯ä¸­çš„æ“ä½œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨ç¬¬ 1 æ¬¡å¾ªç¯ä¸­ï¼Œ<code>tgt</code> ä¸º <code>&lt;s&gt;</code>ï¼Œé€šè¿‡ä¸ <code>src</code> çš„æ³¨æ„åŠ›ä¸ä¸‹ä¸‰è§’çŸ©é˜µå¾—åˆ°è®¡ç®—ç»“æœ <code>out</code> ä¸º <code>A</code>ï¼Œç„¶åå°† <code>tgt</code> æ›´æ–°ä¸º <code>&lt;s&gt; A</code>ï¼Œåœ¨ç¬¬ 2 æ¬¡å¾ªç¯ä¸­ï¼Œå¾—åˆ°çš„ <code>out</code> ä¸º <code>A B</code>ï¼Œæ‰€ä»¥åœ¨æ¯æ¬¡å¾ªç¯ä¸­éƒ½åªå–æ–°ç”Ÿæˆçš„ <code>out[-1]</code> æ›´æ–° <code>tgt</code>ï¼Œæœ€åå°†ç»“æœæ‹¼æ¥èµ·æ¥å¾—åˆ°å®Œæ•´çš„è¾“å‡ºç»“æœã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>æˆ–è®¸è¯»è€…ä¼šæœ‰ç–‘æƒ‘ï¼Œæ—¢ç„¶ä½¿ç”¨ä¸‹ä¸‰è§’çŸ©é˜µå¹¶è¡Œè®¡ç®—æ˜¯ Transformer çš„ä¼˜åŠ¿ï¼Œä¸ºä»€ä¹ˆè¿™é‡Œå´æ˜¯ç”¨å¾ªç¯é¡ºåºåœ°ç”Ÿæˆå‘¢ï¼Ÿä¸ºä»€ä¹ˆè®¡ç®—ä¸Šå›¾ä¸­æœ€åä¸€ä¸ªçŸ©é˜µçš„ <code>out</code>ï¼Œè€Œæ˜¯è¦ç”¨ä¸€ä¸ªä¸ªçš„ <code>out[-1]</code> å‘¢ï¼Ÿ</p>
<p>è¦æ³¨æ„çš„æ˜¯ï¼Œè®­ç»ƒä¸ç”Ÿæˆæœ‰é‡è¦çš„ä¸€ä¸ªä¸åŒï¼Œå°±æ˜¯ç”Ÿæˆä¸­çš„ <code>tgt</code> æ˜¯ç©ºç™½çš„ã€æ¨¡å‹ä¸å¯çŸ¥çš„ï¼Œè€Œè®­ç»ƒä¸­çš„ <code>tgt</code> æ˜¯å®Œæ•´çš„ã€æ¨¡å‹å¯çŸ¥çš„ã€‚å¦‚ä¸Šå›¾ä¸­ï¼Œ<code>tgt</code> åœ¨æ¯ä¸ªå¾ªç¯ä¸­éƒ½åœ¨å˜é•¿ï¼Œåªæœ‰ <code>tgt</code> å˜æˆäº† <code>&lt;s&gt; A B C &hellip;</code> æ‰ä¼šæœ‰æœ€åä¸€ä¸ªçŸ©é˜µä¸­çš„ <code>out</code>ã€‚å¦‚æœè¯´åªè¦æœ€åä¸€ä¸ªçŸ©é˜µä¸­çš„ <code>out</code> è€Œä¸è¦å‰é¢çš„æ­¥éª¤ï¼Œå°±å˜æˆäº†ã€Œåƒä¸¤ä¸ªé¦’å¤´åƒé¥±ï¼Œæ‰€ä»¥åªåƒåä¸€ä¸ªèƒ½åƒå¾—é¥±çš„é¦’å¤´ã€çš„ç¬‘è¯ã€‚</p>
<p>æ‰€ä»¥<dot>ç”Ÿæˆè¿‡ç¨‹å¹¶ä¸æ˜¯å¹¶è¡Œçš„ï¼ŒTransformer çš„å¹¶è¡ŒæŒ‡çš„æ˜¯è®­ç»ƒè¿‡ç¨‹</dot>ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ Transformer åªéœ€è¦åšä¸€æ¬¡ä¸‹ä¸‰è§’çŸ©é˜µçš„è¿ç®—å°±å¯ä»¥å»ºç«‹å¤šä¸ª token é—´çš„å…³ç³»ã€‚è¿™å¼ å›¾è¿˜è§£é‡Šäº†æ¨¡å‹æ°¸è¿œä¸ä¼šç”Ÿæˆ <code>&lt;s&gt;</code> ä½† <code>tgt</code> å¿…é¡»ä»¥ <code>&lt;s&gt;</code> èµ·å§‹çš„åŸå› ã€‚å›¾ä¸­è¿˜å¯ä»¥å¾ˆæ˜ç™½çš„çœ‹å‡ºä¸ºä»€ä¹ˆå…ˆå‰çš„è®­ç»ƒä»£ç è¦å»é™¤ <code>tgt</code> æœ«å°¾çš„ tokenï¼Œå› ä¸º Transformer çš„è¾“å‡º <code>out</code> è®¡ç®—çš„æ˜¯ <code>tgt</code> ä¸‹ä¸€ä¸ª tokenï¼ˆåŠæ­¤å‰ï¼‰çš„è®¡ç®—ç»“æœï¼Œè‹¥ä¸å»é™¤æœ«ä½å°±è¶…å‡ºèŒƒå›´äº†ã€‚</p>
<p><div class="lightgallery"><a data-sub-html="n" href="https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU" rel="noopener" target="_blank"><img alt="n" src="https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU"/></a></div></p>
<p>æœ€åè®­ç»ƒä¸æµ‹è¯•çš„ç»“æœä¸º</p>
<pre><code class="language-python-repl">cpu
Step 40, total_loss: 4.021485328674316
Step 80, total_loss: 2.8817126750946045
&hellip;&hellip;
Step 1920, total_loss: 0.9760974049568176
Step 1960, total_loss: 0.8644390106201172
tensor([[0, 6, 3, 4, 5, 7, 6, 4, 3, 1]])
</code></pre>
<p>è¾“å‡ºçš„ç»“æœæ²¡æœ‰è¾“å‡º source <code>[[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]]</code> ä¸­æœ«å°¾ä»£è¡¨ <code>&lt;p&gt;</code> çš„ <code>2</code>ï¼Œå‰é¢çš„ token ç´¢å¼•ä¹Ÿä¸ source ç›¸å·®æ— å‡ ï¼Œè¯´æ˜æ¨¡å‹æ­£ç¡®å¤åˆ¶äº†è¾“å…¥åºåˆ—ï¼Œè®­ç»ƒæ˜¯æˆåŠŸçš„ã€‚</p>
<h2 id="hou-ji_1">åè®°</h2>
<p>è‡³æ­¤ï¼Œè¿™ç¯‡ Transformer çš„ä»‹ç»ç»ˆäºå‘Šä¸€æ®µè½äº†ã€‚ä»èµ·è‰ã€ç»˜å›¾å†åˆ°æœ€åçš„ä»£ç æ¢³ç†ï¼Œå‰åèŠ±äº†ä¸€å‘¨å¤šçš„æ—¶é—´ã€‚è™½åä¸ºä»‹ç»ï¼Œå…¶å®è¿˜æ˜¯ä¸ºè‡ªå·±åœ¨åšæ¢³ç†ï¼Œè¾¹å†™è¾¹æƒ³ã€è¾¹æƒ³è¾¹æŸ¥ï¼Œç»ˆäºæŠŠ Transformer ä¸­çš„ä¸€äº›ç»†èŠ‚å¼„æ˜ç™½äº†ï¼Œè¿™ç¯‡ç¬”è®°ä¹Ÿèƒ½ä¸ºè¯»è€…å‹¾å‹’å‡ºä¸€ä¸ªå¤§è‡´çš„å›¾æ™¯ã€‚</p>
<p>å½“ç„¶ï¼Œé™äºç¯‡å¹…ï¼Œé™äºã€Œä»é›¶èµ·æ­¥ã€çš„åˆè¡·ï¼Œä¹Ÿé™äºç¬”åŠ›ï¼Œè¿˜æœ‰è®¸å¤šæ›´æ·±å±‚æ¬¡é—®é¢˜éƒ½æ²¡æœ‰æ¢è®¨ï¼Œä½†æˆ‘ç›¸ä¿¡ï¼Œåœ¨çœ‹æ‡‚äº†è¿™ç¯‡ç¬”è®°ä¹‹åï¼Œå†å»é˜…è¯»é‚£äº›æ–‡ç« å·²ç»ä¸æˆé—®é¢˜äº†ï¼Œè¿™ä¹Ÿç¬¦åˆæˆ‘çš„åˆå¿ƒã€‚</p>
<p>æˆ–è®¸è¯»è€…è¿˜å¾ˆå›°æƒ‘ï¼Œç–‘æƒ‘ä¸ºä»€ä¹ˆæ•°å­¦æ¨å¯¼ä¸Šå¹¶ä¸é‚£ä¹ˆä¸¥è°¨çš„æ¨¡å‹å±…ç„¶èƒ½æœ‰æ•ˆï¼Œç”šè‡³å…·æœ‰æå¥½çš„è¡¨ç°ï¼Œé‚£å°±è¯´æ˜éœ€è¦é’»å…¥ç ”ç©¶ Transformer çš„åº•å±‚äº†ï¼Œä¸å¯ä¸å†è¯»äº›æ›´ä¸“ä¸šçš„æ–‡ç« ã€‚æˆ‘ä¹ŸæŠŠå†™è¿™ç¯‡æ–‡ç« æ—¶æ‰€å‚è€ƒä»¥åŠè¾ƒå¥½çš„ç›¸å…³èµ„æ–™ç½—åˆ—äºåï¼Œä»¥é£¨è¯»è€…ã€‚</p>
<hr/>
<h2 id="references">References</h2>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank">Vaswani, A. et al. Attention Is All You Need (2017) - arXiv</a></li>
<li><a href="https://spaces.ac.cn/archives/4765" rel="noopener" target="_blank">ã€ŠAttention is All You Needã€‹æµ…è¯»ï¼ˆç®€ä»‹+ä»£ç ï¼‰- ç§‘å­¦ç©ºé—´</a></li>
<li><a href="https://spaces.ac.cn/archives/6933" rel="noopener" target="_blank">ä»è¯­è¨€æ¨¡å‹åˆ° Seq2Seqï¼šTransformer å¦‚æˆï¼Œå…¨é  Mask - ç§‘å­¦ç©ºé—´</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" rel="noopener" target="_blank">Language Modeling with nn.Transformer and torchtext - PyTorch</a></li>
<li><a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener" target="_blank">The Illustrated Transformer - Jay Alammar</a></li>
<li><a href="https://www.cnblogs.com/wevolf/p/12484972.html" rel="noopener" target="_blank">Transformer æºç ä¸­ Mask æœºåˆ¶çš„å®ç° - åšå®¢å›­</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/434232512" rel="noopener" target="_blank">torch.einsum è¯¦è§£ - çŸ¥ä¹</a></li>
<li><a href="https://blog.csdn.net/zhaohongfei_358/article/details/126019181" rel="noopener" target="_blank">Pytorch ä¸­ nn.Transformer çš„ä½¿ç”¨è¯¦è§£ä¸ Transformer çš„é»‘ç›’è®²è§£ - CSDN åšå®¢</a></li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
          </section>


          <section class="post-footer" style="margin: 12rem 0 0;">
            <div class="post-share">
              <span class="post-info-label">Share</span>
                <a title="å¤åˆ¶æ–‡ç« é“¾æ¥" aria-label="Share-link" class="share-link"
                  href="#" onclick='copyURL(event, "ä»é›¶èµ·æ­¥çš„ Transformer ä¸ä»£ç æ‹†è§£ - Leoâ€™s blog https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html");return false;'>
                  <i class="icon icon-link"
                  aria-hidden="true"></i><span class="hidden">Share-link</span>
                </a>
                <a title="å˜Ÿå˜Ÿåˆ° Mastodon" aria-label="Mastodon" class="mastodon"
                  href="https://tootpick.org/#text=ä»é›¶èµ·æ­¥çš„ Transformer ä¸ä»£ç æ‹†è§£ - Leoâ€™s blog https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html" target="_blank" rel="noopener noreferrer">
                  <i class="fa-brands fa-mastodon"
                  aria-hidden="true"></i><span class="hidden">Mastodon</span>
                </a>
                <a title="åˆ†äº«åˆ°å¾®åš" aria-label="Weibo" class="weibo"
                  href="http://service.weibo.com/share/share.php?url=https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html&title=ä»é›¶èµ·æ­¥çš„ Transformer ä¸ä»£ç æ‹†è§£&nbsp;-&nbsp;Leoâ€™s blog"
                  target="_blank" rel="noopener noreferrer">
                  <i class="fa-brands fa-weibo"
                  aria-hidden="true"></i><span class="hidden">Weibo</span>
                </a>
                <a title="é€šè¿‡ Email è½¬å‘" aria-label="Email" class="email"
                  href="mailto:?subject=ä»é›¶èµ·æ­¥çš„ Transformer ä¸ä»£ç æ‹†è§£&amp;body=https://leonis.cc/sui-sui-nian/2023-04-21-transformer-from-scratch.html">
                  <i class="icon icon-mail"
                  aria-hidden="true"></i><span class="hidden">Email</span>
                </a>
              <div class="clear"></div>
            </div>

            <aside class="post-tags">
<a href="https://leonis.cc/tag/python.html"># Python</a><a href="https://leonis.cc/tag/pytorch.html"># PyTorch</a><a href="https://leonis.cc/tag/transformer.html"># Transformer</a>            </aside>

            <div class="clear"></div>

            <aside class="post-author">


                <figure class="post-author-avatar">
                  <img src="https://cravatar.cn/avatar/95e31f6808fafa1f8ef3313b6f0b10e6?s=800" alt="Leo" />
                </figure>
                <div class="post-author-bio">
                  <h4 class="post-author-name"><a href="https://leonis.cc/author/leo.html">Leo</a></h4>
                    <p class="post-author-about">A chemist who doesnâ€™t know about classical literature isnâ€™t a good programmer. Cool, huh?</p>
                    <span class="post-author-location">
                      <a rel="noopener" target="_blank" href="https://www.bing.com/maps?cp=39.116572%7E117.361669&lvl=10.1"
                      title="Tientsin">
                      <i class="fa-solid fa-earth-asia fa-fw"></i></a>
                    </span>
                  <!-- Social linkes in alphabet order. -->
                    <span class="post-author-mastodon">
                      <a rel="noopener" target="_blank" href="https://mast.dragon-fly.club/@leonis"
                      title="@leonis@dragon-fly.club">
                        <i class="fa-brands fa-mastodon fa-fw"></i></a>
                    </span>
                    <span class="post-author-github">
                      <a rel="noopener" target="_blank" href="https://github.com/Tseing"
                      title="@Tseing">
                        <i class="fa-brands fa-github fa-fw"></i></a>
                    </span>
                    <span class="post-author-email">
                      <a rel="noopener" target="_blank" href="mailto:im.yczeng@outlook.com"
                      title="im.yczeng@outlook.com">
                        <i class="fa-solid fa-envelope fa-fw"></i></a>
                    </span>
                    <span class="post-author-rss">
                      <a rel="noopener" target="_blank" href="/feed.xml"
                      title="RSS Feed">
                        <i class="fa-solid fa-rss fa-fw"></i></a>
                    </span>
                </div>
                <div class="clear"></div>
            </aside>

          </section>


          <aside class="post-nav">
            <a class="post-nav-next" href="https://leonis.cc/sui-sui-nian/2023-04-22-summary-openreview.html">
              <section class="post-nav-teaser">
                <i class="icon icon-arrow-left"></i>
                  <h2 class="post-nav-title">æ–‡çŒ®æ€»ç»“ï½œä½¿ç”¨ä¸Šä¸‹æ–‡å¢å¼ºçš„åˆ†å­è¡¨ç¤ºæå‡å°‘æ ·æœ¬è¯ç‰©å‘ç°çš„æ•ˆæœ</h2>
                <p class="post-nav-excerpt">æœ¬æ–‡ä»‹ç»äº 2023 å¹´å‘è¡¨åœ¨ ICLR 2023 ä¸Šçš„ä¸€ç¯‡æ–‡ç« ï¼Œæ–‡ç« åŸæ ‡é¢˜ä¸º Context-enriched molecule representations...</p>
                <p class="post-nav-meta"><time datetime="2023å¹´ 4æœˆ22æ—¥">2023å¹´ 4æœˆ22æ—¥</time></p>
              </section>
            </a>
            <a class="post-nav-prev" href="https://leonis.cc/sui-sui-nian/2023-04-15-summary-doi.org/10.1021/acs.jmedchem.2c01787.html">
              <section class="post-nav-teaser">
                <i class="icon icon-arrow-right"></i>
                  <h2 class="post-nav-title">æ–‡çŒ®æ€»ç»“ï½œè¯ç‰©å‘ç°ä¸­çš„åŒ¹é…åˆ†å­å¯¹åˆ†æï¼šæ–¹æ³•ä¸å½“å‰åº”ç”¨</h2>
                <p class="post-nav-excerpt">æœ¬æ–‡ä»‹ç» 2023 å¹´ç”±æ›¹ä¸œå‡ä¸ä¾¯å»·å†›ç ”ç©¶å›¢é˜Ÿå‘è¡¨åœ¨ Journal of Medicinal Chemistry ä¸Šçš„ä¸€ç¯‡å±•æœ›ï¼Œæ–‡ç« åŸæ ‡é¢˜ä¸º Matched...</p>
                <p class="post-nav-meta"><time datetime="2023å¹´ 4æœˆ15æ—¥">2023å¹´ 4æœˆ15æ—¥</time></p>
              </section>
            </a>
            <div class="clear"></div>
          </aside>

<div id="waline"></div>
        </div>
      </article>
    </main>
    <div class="nav-footer">
      <nav class="nav-wrapper" aria-label="Footer">
        <span class="nav-copy">Leoâ€™s blog &copy; 2024
        </span>
        <span class="nav-credits">



          Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a> &bull; Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a> &bull;
          <a class="menu-item js-theme" href="#" data-system="System theme" data-dark="Dark theme" data-light="Light theme">
            <span class="theme-icon"></span><span class="theme-text">System theme</span>
          </a>
        </span>
      </nav>
    </div>

  </section>

  <script src="https://cdnjs.loli.net/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
  <!-- code highlight -->
  <script src="https://cdnjs.loli.net/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
  <script src="https://cdnjs.loli.net/ajax/libs/highlight.js/11.7.0/languages/django.min.js"></script>
  <script src="https://cdnjs.loli.net/ajax/libs/highlight.js/11.7.0/languages/dockerfile.min.js"></script>
  <script src="https://cdnjs.loli.net/ajax/libs/highlight.js/11.7.0/languages/markdown.min.js"></script>
  <script src="https://cdnjs.loli.net/ajax/libs/highlight.js/11.7.0/languages/nginx.min.js"></script>
  <script src="https://cdnjs.loli.net/ajax/libs/highlight.js/11.7.0/languages/pgsql.min.js"></script>
  <script type="text/javascript" src="https://leonis.cc/theme/js/jquery.fitvids.js"></script>
  <script type="text/javascript" src="https://leonis.cc/theme/js/script.js"></script>

  <!-- lightbox -->
  <script type="text/javascript" src="https://leonis.cc/theme/js/lightgallery.min.js"></script>
  <script type="text/javascript" src="https://leonis.cc/theme/js/lg-zoom.min.js"></script>
  <script>
    var elements = document.getElementsByClassName("lightgallery");
    for(var i=0; i<elements.length; i++) {
       lightGallery(elements[i]);
    }
  </script>
    <!-- umami analytics -->
    <script async defer src="https://analytics.umami.is/script.js" data-website-id="b508982a-f7bf-4c24-a948-8de93b0cb81d"></script>


  <!-- 	The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in article.html, but it needs to be included down here, after jQuery has already loaded. -->

<script>
  $(document).ready(function () {
    var viewport = $(window);
    var post = $('.post-content');
    // Responsive videos with fitVids
    post.fitVids();

    var mdSelector="pre code";
    var rstSelector="pre code";
    var selector=mdSelector;
    // Format code blocks and add line numbers
    function codestyling() {
      $(selector).each(function(i, e) {
        // Code highlight
        hljs.highlightElement(e);
        // No lines for plain text blocks
        if (!$(this).hasClass('language-text')) {
          var code = $(this);
          // Calculate amount of lines
          var lines = code.html().split(/\n(?!$)/g).length;
          var numbers = [];
          if (lines > 1) {
            lines++;
          }
          for (i = 1; i < lines; i++) {
            numbers += '<span class="line" aria-hidden="true">' + i + '</span>';
          }
          code.parent().append('<div class="lines">' + numbers + '</div>');
        }
      });
    }

    // Format code blocks only
    function codestylingWithoutLineNumbers() {
      $(selector).each(function(i, e) {
        // Code highlight
        hljs.highlightElement(e);
      });
    }

    codestylingWithoutLineNumbers();
    // Reading progress bar on window top
    function readingProgress() {
      var postBottom = post.offset().top + post.height();
      var viewportHeight = viewport.height();
      var progress = 100 - (((postBottom - (viewport.scrollTop() + viewportHeight) + viewportHeight / 3) / (postBottom - viewportHeight + viewportHeight / 3)) * 100);
      $('.progress-bar').css('width', progress + '%');
      (progress > 100) ? $('.progress-container').addClass('complete'): $('.progress-container').removeClass('complete');
    }
    readingProgress();
    // Trigger reading progress
    viewport.on({
      'scroll': function() {
        readingProgress();
      },
      'resize': function() {
        readingProgress();
      },
      'orientationchange': function() {
        readingProgress();
      }
    });

  });

  Waline.init({
      el: '#waline',
      serverURL: "https://waline-1-d9689975.deta.app/",
      emoji: false,
      search: false,
      imageUploader: false,
      locale: {placeholder: "æ¬¢è¿è¯„è®ºï¼Œå¡«å†™é‚®ç®±å¯ä»¥è·å–å¤´åƒå’Œæ”¶åˆ°å›å¤é€šçŸ¥ï½"},
  });
</script>
</body>

</html>