<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>《统计学习方法》第六章：逻辑斯谛回归与最大熵模型</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="https://leonis.cc/sui-sui-nian/2022-09-09-statistical-learning-chapter6.html" rel="canonical" />
  <!-- Feed -->
        <link href="https://leonis.cc/feed.xml" type="application/atom+xml" rel="alternate" title="Leo's blog Full Atom Feed" />

  <link href="https://leonis.cc/theme/css/style.css" type="text/css" rel="stylesheet" />
  <link href="https://leonis.cc/theme/css/lightgallery.min.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="https://leonis.cc/theme/css/code_blocks/atom-one-light.min.css" rel="stylesheet">

    <!-- CSS specified by the user -->


    <link href="https://leonis.cc/theme/css/plugins.css" type="text/css" rel="stylesheet" />


    <link href="https://leonis.cc/theme/css/bookshelf.css" type="text/css" rel="stylesheet" />

  <!-- fonts -->
  <link href="https://ik.imagekit.io/leonis/ChiuKongGothic-CL-w4/result.css?updatedAt=1700222744424" rel="stylesheet" />
  <link href="https://ik.imagekit.io/leonis/ChiuKongGothic-CL-w5/result.css?updatedAt=1700275749020" rel="stylesheet" />
  <link href="https://ik.imagekit.io/leonis/ChiuKongGothic-CL-w7/result.css?updatedAt=1700275854743" rel="stylesheet" />
  <link href="https://code.cdn.mozilla.net/fonts/fira.css" rel="stylesheet">
  <link href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/hack-font/3.003/web/hack.min.css" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Livvic:wght@400&display=swap" rel="stylesheet" />
  <link href="https://ik.imagekit.io/leonis/AdvocateAncientSerifSC-Bold/result.css?updatedAt=1700289637334" rel="stylesheet" />




    <meta name="description" content="《统计学习方法》第五章主要介绍逻辑斯谛回归模型与最大熵模型，这两种模型具有类似的对数结构，都利用了极大似然估计原理。本章还介绍了广义拉格朗日函数和拟牛顿法。">

    <meta name="author" content="Leo">

    <meta name="tags" content="统计学习方法">
    <meta name="tags" content="Machine learning">
    <meta name="tags" content="Algorithm">




<!-- Open Graph -->
<meta prefix="og: http://ogp.me/ns#" property="og:site_name" content="Leo's blog"/>
<meta prefix="og: http://ogp.me/ns#" property="og:title" content="《统计学习方法》第六章：逻辑斯谛回归与最大熵模型"/>
<meta prefix="og: http://ogp.me/ns#" property="og:description" content="《统计学习方法》第五章主要介绍逻辑斯谛回归模型与最大熵模型，这两种模型具有类似的对数结构，都利用了极大似然估计原理。本章还介绍了广义拉格朗日函数和拟牛顿法。"/>
<meta prefix="og: http://ogp.me/ns#" property="og:locale" content="en_US"/>
<meta prefix="og: http://ogp.me/ns#" property="og:url" content="https://leonis.cc/sui-sui-nian/2022-09-09-statistical-learning-chapter6.html"/>
<meta prefix="og: http://ogp.me/ns#" property="og:type" content="article"/>
<meta prefix="og: http://ogp.me/ns#" property="article:published_time" content="2022-09-09 00:00:00+08:00"/>
<meta prefix="og: http://ogp.me/ns#" property="article:modified_time" content=""/>
<meta prefix="og: http://ogp.me/ns#" property="article:author" content="https://leonis.cc/author/leo.html">
<meta prefix="og: http://ogp.me/ns#" property="article:section" content="碎碎念"/>
<meta prefix="og: http://ogp.me/ns#" property="article:tag" content="统计学习方法"/>
<meta prefix="og: http://ogp.me/ns#" property="article:tag" content="Machine learning"/>
<meta prefix="og: http://ogp.me/ns#" property="article:tag" content="Algorithm"/>
<meta prefix="og: http://ogp.me/ns#" property="og:image" content="https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNwFfSnZ1Pc1osKbni/root/content">

<!-- Twitter Card -->

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "《统计学习方法》第六章：逻辑斯谛回归与最大熵模型",
  "headline": "《统计学习方法》第六章：逻辑斯谛回归与最大熵模型",
  "datePublished": "2022-09-09 00:00:00+08:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Leo",
    "url": "https://leonis.cc/author/leo.html"
  },
  "image": "https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNwFfSnZ1Pc1osKbni/root/content",
  "url": "https://leonis.cc/sui-sui-nian/2022-09-09-statistical-learning-chapter6.html",
  "description": "《统计学习方法》第五章主要介绍逻辑斯谛回归模型与最大熵模型，这两种模型具有类似的对数结构，都利用了极大似然估计原理。本章还介绍了广义拉格朗日函数和拟牛顿法。"
}
</script>

<!-- waline comment -->
<link href="https://npm.elemecdn.com/@waline/client@2.14.9/dist/waline.css" rel="stylesheet" />
<script src="https://npm.elemecdn.com/@waline/client@2.14.9/dist/waline.js"></script></head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>
          <li><a href="https://leonis.cc/category/sui-sui-nian.html" role="presentation">碎碎念</a></li>
          <li><a href="https://leonis.cc/category/gu-zhi-dui.html" role="presentation">故纸堆</a></li>
          <li><a href="https://leonis.cc/category/zai-lu-shang.html" role="presentation">在路上</a></li>
          <li><a href="https://leonis.cc/pages/shan-qiang-bian.html" role="presentation">山墙边</a></li>
          <li><a href="https://neodb.social/users/Leo/" role="presentation">破橱簏</a></li>
          <li><a href="https://leonis.cc/tags.html" role="presentation">Tags</a></li>
          <li><a href="https://leonis.cc/pages/about.html" role="presentation">About</a></li>


    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" class="has-cover">
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <a class="home-button" href="https://leonis.cc/" aria-label="Home" title="Home">
                  <i class="ic ic-arrow-left" aria-hidden="true"></i>
                  <span>Home</span>
                </a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button" aria-label="Menu">
              <i class="ic ic-menu" aria-hidden="true"></i>
              <span>Menu</span>
            </a>
          </span>
        </nav>
        <h1 class="post-title">《统计学习方法》第六章：逻辑斯谛回归与最大熵模型</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="https://leonis.cc/author/leo.html">Leo</a>
            | <time datetime="2022年 9月09日">2022年 9月09日</time>
        </span>
            <div class="post-cover cover" style="background-image: url('https://api.onedrive.com/v1.0/shares/s!AtseC45rsRhNwFfSnZ1Pc1osKbni/root/content')">
      </div>
    </header>

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
            <div class="toc-nav">
              <div id="toc"><ul><li><a class="toc-href" href="#mo-xing" title="模型">模型</a><ul><li><a class="toc-href" href="#luo-ji-si-di-hui-gui-mo-xing" title="逻辑斯谛回归模型">逻辑斯谛回归模型</a></li><li><a class="toc-href" href="#zui-da-shang-mo-xing" title="最大熵模型">最大熵模型</a></li></ul></li><li><a class="toc-href" href="#ce-lue_1" title="策略">策略</a><ul><li><a class="toc-href" href="#luo-ji-si-di-hui-gui-mo-xing_1" title="逻辑斯谛回归模型">逻辑斯谛回归模型</a></li><li><a class="toc-href" href="#yan-yi-la-ge-lang-ri-han-shu" title="广义拉格朗日函数">广义拉格朗日函数</a></li><li><a class="toc-href" href="#zui-da-shang-mo-xing_1" title="最大熵模型">最大熵模型</a></li></ul></li><li><a class="toc-href" href="#suan-fa_1" title="算法">算法</a><ul><li><a class="toc-href" href="#gai-jin-de-die-dai-chi-du-fa" title="改进的迭代尺度法">改进的迭代尺度法</a></li><li><a class="toc-href" href="#niu-dun-fa-yu-ni-niu-dun-fa" title="牛顿法与拟牛顿法">牛顿法与拟牛顿法</a></li></ul></li><li><a class="toc-href" href="#references_1" title="References">References</a></li></ul></div>
            </div>
        <div class="inner">
            <section class="post-content">
                <h2 id="mo-xing">模型</h2>
<h3 id="luo-ji-si-di-hui-gui-mo-xing">逻辑斯谛回归模型</h3>
<p>逻辑斯谛分布具有良好的性质，能够将 <span class="math">\((-\infty,+\infty)\)</span> 映射至 <span class="math">\((-1，+1)\)</span>，因此选用逻辑斯谛分布作为回归模型。逻辑斯谛分布函数与密度函数为</p>
<div class="math">$$\begin{align}
    F(x)&amp;=P(\leqslant x)=\frac{1}{1+\mathrm{e}^{-(x-\mu)/\gamma}}\\
    f(x)&amp;=F'(x)=\frac{\mathrm{e}^{-(x-\mu)/\gamma}}{\gamma(1+\mathrm{e}^{-(x-\mu)/\gamma})^2}
\end{align}$$</div>
<p>逻辑斯谛分布函数与密度函数的图像分别如下：</p>
<p><img alt="逻辑斯谛分布" src="https://storage.live.com/items/4D18B16B8E0B1EDB!7554?authkey=ALYpzW-ZQ_VBXTU"/></p>
<p>将逻辑斯谛分布函数简化，可以得到 Sigmoid 函数：</p>
<div class="math">$$S(x)=\frac{1}{1+\mathrm{e}^{-x}}=\frac{\mathrm{e}^x}{1+\mathrm{e}^x}$$</div>
<p>回忆二分类的感知机 <span class="math">\(w\cdot x+b\)</span>，超平面将实例分作 <span class="math">\(w\cdot x+b\geqslant 0\)</span> 与 <span class="math">\(w\cdot x+b&lt; 0\)</span> 两类。可以看出，<span class="math">\(w\cdot x+b\)</span> 的值域为实数域，那么就可以利用逻辑斯谛分布将实数域映射到 <span class="math">\((-1，+1)\)</span>，实现分类。</p>
<p>为了表述简洁，令 <span class="math">\(w=(w^{(1)},w^{(2)},\cdots,w^{(n)},b)^{\mathrm{T}}\)</span>，<span class="math">\(x=(x^{(1)},x^{(2)},\cdots,x^{(n)},1)^{\mathrm{T}}\)</span>，将 <span class="math">\(w\cdot x\)</span> 代入 Sigmoid 函数：</p>
<div class="math">$$\begin{align}
    P(Y=1|x)&amp;=\frac{\exp(w\cdot x)}{1+\exp(w\cdot x)}\\
    P(Y=0|x)&amp;=1-P(Y=1|x)=\frac{1}{1+\exp(w\cdot x)}
\end{align}$$</div>
<p>这就是二项逻辑斯谛回归模型。从式中也能看到，若线性函数 <span class="math">\(w\cdot x\)</span>越大，<span class="math">\(P(Y=1|x)\)</span> 概率越大；若线性函数 <span class="math">\(w\cdot x\)</span>越小，<span class="math">\(P(Y=0|x)\)</span> 概率越大。最后就通过对比 <span class="math">\(P(Y=1|x)\)</span> 与 <span class="math">\(P(Y=0|x)\)</span> 的大小来确定实例的类别。</p>
<h3 id="zui-da-shang-mo-xing">最大熵模型</h3>
<p>最大熵原理认为，熵最大的模型是最好的模型。这是一个十分在「直觉」的原理，例如说，在等待公交车时，下一辆公交车只有两种情况&mdash;&mdash;「乘」或「不乘」，基于这种判断，通常会认为下一辆公交车有 50% 的概率可乘，50% 的概率不可乘。</p>
<p>再例如，某事件有 <span class="math">\(\{A,B,C,D,E\}\)</span> 5 种情况，相应满足约束：</p>
<div class="math">$$P(A)+P(B)+P(C)+P(D)+P(E)=1$$</div>
<p>在没有更多信息的情况下，根据最大熵原理，我们会认为</p>
<div class="math">$$P(A)=P(B)=P(C)=P(D)=P(E)=\frac{1}{5}$$</div>
<p>如果额外获得了信息 <span class="math">\(P(A)+P(B)=\frac{3}{10}\)</span>，那么根据最大熵原理就会认为</p>
<div class="math">$$P(A)=P(B)=\frac{3}{20},\ P(C)=P(D)=P(E)=\frac{7}{30}$$</div>
<p>可以看出，在缺少信息的情况下，最大熵原理将那些不确定的部分都视作等可能。等概率表示了对于事实的无知，但是没有更多信息，这种判断又是合理的。</p>
<p>对于训练数据集，可以得到经验分布 <span class="math">\(\tilde{P}(X=x,Y=y)\)</span> 与 <span class="math">\(\tilde{P}(X=x)\)</span>，这里的经验分布是用频数估计得到的。但是使用频数估计联合分布是不准确的，这种不准确在最大熵模型中会造成约束条件的不准确，而约束条件又是最大熵模型的关键。例如上例中，约束条件为 <span class="math">\(P(A)+P(B)=\frac{3}{10}\)</span>，若该约束条件不准确，后续的概率估计也是无意义的。</p>
<p>因此在最大熵模型中需要引入特征函数，特征函数用于人为选取「合适」的实例：</p>
<div class="math">$$\begin{equation}
    f(x,y)=
    \begin{cases}
        1, &amp;x\ 与\ y\ 满足某事实\\
        0, &amp;否则
    \end{cases}
\end{equation}$$</div>
<p>特征函数 <span class="math">\(f(x,y)\)</span> 关于经验分布 <span class="math">\(\tilde{P}(X,Y)\)</span> 的期望为</p>
<div class="math">$$E_{\tilde{P}}(f)=\sum_{x,y}\tilde{P}(x,y)f(x,y)$$</div>
<p>特征函数 <span class="math">\(f(x,y)\)</span> 关于模型 <span class="math">\(P(X|Y)\)</span> 与经验分布 <span class="math">\(\tilde{P}(X)\)</span> 的期望为</p>
<div class="math">$$E_P(f)=\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)$$</div>
<p>若模型准确，二者理应相等，即</p>
<div class="math">$$E_{\tilde{P}}(f)=E_P(f)$$</div>
<p><div class="note-info"><p><i class="fa-solid fa-note-sticky"></i>&ensp;<b>Note</b>&emsp;回忆条件概率基本公式 <span class="math">\(P(x,y)=P(y|x)P(x)\)</span>。</p></div></p>
<p>满足所有约束条件的模型构成的集合为</p>
<div class="math">$$\mathcal{C}\equiv \{P\in\mathcal{P}|E_P(f_i)=E_{\tilde{P}}(f_i)\}$$</div>
<p>在条件概率分布 <span class="math">\(P(Y|X)\)</span> 上的条件熵为</p>
<div class="math">$$H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)$$</div>
<p>在集合 <span class="math">\(\mathcal{C}\)</span> 中选出 <span class="math">\(H(P)\)</span> 最大的模型即为最大熵模型。</p>
<h2 id="ce-lue_1">策略</h2>
<h3 id="luo-ji-si-di-hui-gui-mo-xing_1">逻辑斯谛回归模型</h3>
<p>设数据集中的概率为</p>
<div class="math">$$P(Y=1|x)=\pi(x),\ P(Y=0|x)=1-\pi(x)$$</div>
<p>构造对数似然函数：</p>
<div class="math">$$\begin{align}
    L(w)&amp;=\log\prod_{i}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}\\
    &amp;=\sum_i[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\
    &amp;=\sum_i[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i)]
\end{align}$$</div>
<p>那么求 <span class="math">\(L(w)\)</span> 的极大值，就能得到估计值 <span class="math">\(\hat{w}\)</span>，得到回归模型。也就是说，求解逻辑斯谛回归模型就是对于对数似然函数的最优化问题。</p>
<p><div class="note-info"><p><i class="fa-solid fa-note-sticky"></i>&ensp;<b>Note</b>&emsp;似然函数定义为 <span class="math">\(L(p)=\prod_i p^{x_i}(1-p)^{1-x_i}\)</span>，即抽样结果中各概率之积。由于每次抽样独立同分布的前提，可以认为似然函数为抽样结果（该事件）发生的概率。因为已经得到了该抽样结果，该事件发生的概率理应为 1，所以就要使似然函数最大化，这就是<dot>最大似然估计</dot>的原理。</p></div></p>
<h3 id="yan-yi-la-ge-lang-ri-han-shu">广义拉格朗日函数</h3>
<p>最大熵模型中使用了拉格朗日乘数法，因此有必要先介绍一下广义拉格朗日函数。回忆一下高等数学中的拉格朗日函数，若函数 <span class="math">\(z=f(x,y)\)</span> 有约束条件 <span class="math">\(\varphi(x,y)=0\)</span>，那么拉格朗日函数就为</p>
<div class="math">$$L(x,y)=f(x,y)+\lambda\varphi(x,y)$$</div>
<p>欲寻找函数 <span class="math">\(z=f(x,y)\)</span> 的可能极值点，只需令拉格朗日函数的各一阶偏导数为零，联立求解。</p>
<p>这里引入广义拉格朗日函数，若有约束最优化问题：</p>
<div class="math">$$\begin{align}
    \min_x&amp;\quad f(x)\\
    \mathrm{s.t.}&amp;\quad c_i(x)\leqslant0,\ h_j(x)=0
\end{align}$$</div>
<p>那么广义拉格朗日函数为</p>
<div class="math">$$L(x,\alpha,\beta)=f(x)+\color{teal}{\sum_i\alpha_ic_i(x)}+\color{steelblue}{\sum_j\beta_jh_j(x)}$$</div>
<p>其中 <span class="math">\(\alpha_i\)</span> 与 <span class="math">\(\beta_j\)</span> 为拉格朗日乘子，<span class="math">\(\alpha_i\geqslant0\)</span>。</p>
<p>可以看出，只要 <span class="math">\(x\)</span> 满足约束，<span class="math">\(L(x,\alpha,\beta)\)</span> 的第二项是递减的，第三项是不增的。那么就有</p>
<div class="math">$$\begin{equation}
    \max_{\alpha,\beta:\alpha_i\geqslant0}L(x,\alpha,\beta)=
    \begin{cases}
        f(x), &amp;x\ 满足约束\\
        +\infty, &amp;否则
    \end{cases}
\end{equation}$$</div>
<p>原来的约束最优化问题 <span class="math">\(\min_xf(x)\)</span> 在这里就可以改写为 </p>
<div class="math">$$\min_x\max_{\alpha,\beta:\alpha_i\geqslant0}L(x,\alpha,\beta)$$</div>
<p>在满足 Karush-Kuhn-Tucker（KKT）条件下，原始问题的解与对偶问题的解相等，即</p>
<div class="math">$$\min_x\max_{\alpha,\beta:\alpha_i\geqslant0}L(x,\alpha,\beta)=\max_{\alpha,\beta:\alpha_i\geqslant0}\min_xL(x,\alpha,\beta)$$</div>
<h3 id="zui-da-shang-mo-xing_1">最大熵模型</h3>
<p>最大熵模型的学习过程是有约束的最优化问题：</p>
<div class="math">$$\begin{align}
    \max_{P\in\mathcal{C}}&amp;\quad H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)\\
    \mathrm{s.t.}&amp;\quad E_P(f_i)=E_{\tilde{P}}(f_i),\ \sum_yP(y|x)=1
\end{align}$$</div>
<p>按照凸优化的习惯（求向下凸的函数的最小值），问题等价于</p>
<div class="math">$$\begin{align}
    \min_{P\in\mathcal{C}}&amp;\quad -H(P)=\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)\\
    \mathrm{s.t.}&amp;\quad E_P(f_i)=E_{\tilde{P}}(f_i),\ \sum_yP(y|x)=1
\end{align}$$</div>
<p>构造拉格朗日函数 <span class="math">\(L(P,w)\)</span>：</p>
<div class="math">$$L(P,w)\equiv-H(P)+w_0\left[1-\sum_yP(y|x)\right]+\sum_iw_i[E_P(f_i)-E_{\tilde{P}}(f_i)]$$</div>
<p>在这里，原始问题的解与对偶问题的解同样是等价的，即</p>
<div class="math">$$\underbrace{\min_{P\in\mathcal{C}}\max_wL(P,w)}_{原始形式}=\underbrace{\max_w\min_{P\in\mathcal{C}}L(P,w)}_{对偶形式}$$</div>
<p>那么就可以通过求解对偶问题来求解原始问题，具体来说，就是先求解 <span class="math">\(\min_{P\in\mathcal{C}}L(P,w)\)</span>，固定 <span class="math">\(w\)</span>，令 <span class="math">\(\frac{\partial L(P,w)}{\partial P(y|x)}=0\)</span>，得到 <span class="math">\(P(y|x)\)</span> 的表达式后将其代入 <span class="math">\(L(P,w)\)</span> 得到 <span class="math">\(\min_{P\in\mathcal{C}}L(P,w)\)</span>。再令 <span class="math">\(\frac{\partial \min_{P\in\mathcal{C}}L(P,w)}{\partial w}=0\)</span>，最终得到 <span class="math">\(P(y|x)\)</span>。</p>
<h2 id="suan-fa_1">算法</h2>
<p>逻辑斯谛回归模型和最大熵模型都是光滑的凸函数，适用于多种最优化方法，常用的方法包括迭代尺度算法、梯度下降法、牛顿法或拟牛顿法。</p>
<h3 id="gai-jin-de-die-dai-chi-du-fa">改进的迭代尺度法</h3>
<p>令 <span class="math">\(\frac{\partial L(P,w)}{\partial P(y|x)}=0\)</span>，可以得到</p>
<div class="math">$$P(y|x)=\frac{\exp(\sum_iw_if_i(x,y))}{\exp(1-w_0)}$$</div>
<p>其中的 <span class="math">\(\exp(1-w_0)\)</span> 部分是个定值，不影响概率的相对大小，因此略去该项并归一化，得到</p>
<div class="math">$$\begin{align}
    P_w(y|x)&amp;=\frac{1}{Z_w(x)}\exp\left(\sum_iw_if_i(x,y)\right)\\
    Z_w(x)&amp;=\sum_y\exp\left(w_if_i(x,y)\right)
\end{align}$$</div>
<p>对数似然函数为</p>
<div class="math">$$L(w)=\log\prod_{x,y}P(y|x)^{\tilde{P}(x,y)}=\sum_{x,y}\tilde{P}(x,y)\log P(y|x)$$</div>
<p>将 <span class="math">\(P_w(y|x)\)</span> 代入得到</p>
<div class="math">$$L(w)=\sum_{x,y}\tilde{P}(x,y)\sum_iw_if_i(x,y)-\sum_x\tilde{P}(x)\log Z_w(x)$$</div>
<p>迭代尺度法的思路就是寻找一个新参数向量 <span class="math">\(w+\delta=(w_i+\delta_1,w_2+\delta_2,\cdots,w_n+\delta_n)^\mathrm{T}\)</span>，使 <span class="math">\(L(w)\)</span> 增大，并不断迭代更新 <span class="math">\(w\rightarrow w+\delta\)</span>，最终使 <span class="math">\(L(w)\)</span> 最大。</p>
<p>为了简化计算，在该算法中需要引入一个量 <span class="math">\(f^\#(x,y)\)</span>：</p>
<div class="math">$$f^\#(x,y)=\sum_if_i(x,y)$$</div>
<p><span class="math">\(f^\#(x,y)\)</span> 表示了所有特征出现的次数。</p>
<p><span class="math">\(w\)</span> 更新前后似然函数的变化值为（证明略）</p>
<div class="math">$$L(w+\delta)-L(w)\geqslant B(\delta|w)$$</div>
<div class="math">$$B(\delta|w)=\sum_{x,y}\tilde{P}(x,y)\sum_i\delta_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x)\sum_i\left(\frac{f_i(x,y)}{f^\#(x,y)}\right)\exp(\delta_i,f^\#(x,y))$$</div>
<p>我们需要找到 <span class="math">\(B(\delta|w)\)</span> 的极值，让每次迭代后 <span class="math">\(L(w)\)</span> 尽可能大，因此求 <span class="math">\(B(\delta|w)\)</span> 对 <span class="math">\(\delta_i\)</span> 的偏导，并令其为零，得到</p>
<div class="math">$$\underbrace{\sum_{x,y}\tilde{P}(x)P_w(y|x)f_i(x,y)}_{E_P(f_i)}\exp(\delta_if^\#(x,y))=E_{\tilde{P}}(f_i)$$</div>
<p>解该方程即可得到用于每次迭代的 <span class="math">\(\delta\)</span>。</p>
<p>若 <span class="math">\(f^\#(x,y)\)</span> 为常数，那么</p>
<div class="math">$$\delta_i=\frac{1}{f^\#(x,y)}\log\frac{E_{\tilde{P}}(f_i)}{E_P(f_i)}$$</div>
<p>若 <span class="math">\(f^\#(x,y)\)</span> 不是常数，通常使用数值计算的方法求解。</p>
<p><strong>算法 6.1（IIS 算法）</strong></p>
<blockquote>
<p>输入：特征函数 <span class="math">\(f_i\)</span>；经验分布 <span class="math">\(\tilde{P}(X,Y)\)</span>；模型 <span class="math">\(P_w(y|x)\)</span>；<br/>
输出：最优参数 <span class="math">\(w^*_i\)</span>；最优模型 <span class="math">\(P_{w^*}\)</span>。</p>
</blockquote>
<ol>
<li>对所有 <span class="math">\(i\)</span>，取初值 <span class="math">\(w_i=0\)</span>。</li>
<li>对每一个 <span class="math">\(i\)</span>，<ol>
<li>解以下方程得到 <span class="math">\(\delta_i\)</span>：
    <div class="math">$$\sum_{x,y}\tilde{P}(x)P_w(y|x)f_i(x,y)\exp(\delta_if^\#(x,y))=E_{\tilde{P}}(f_i)$$</div>
</li>
<li>更新 <span class="math">\(w_i\)</span>：<span class="math">\(w_i\leftarrow w_i+\delta_i\)</span>。</li>
</ol>
</li>
<li>若不是所有 <span class="math">\(w_i\)</span> 收敛，重复第 2 步。</li>
</ol>
<h3 id="niu-dun-fa-yu-ni-niu-dun-fa">牛顿法与拟牛顿法</h3>
<h4>牛顿法</h4>
<p>对于最优化的目标目标函数 <span class="math">\(f(x)\)</span>，若满足二阶可微的前提，可以在 <span class="math">\(x=x^{(k)}\)</span> 处二阶泰勒展开：</p>
<div class="math">$$f(x)=f(x^{(x)})+\nabla f(x^{(k)})\Delta x^{(k)}+\frac{1}{2}(\Delta x^{(k)})^\mathrm{T}\nabla^2f(x^{(k)})\Delta x^{(k)}$$</div>
<p>其中 <span class="math">\(\Delta x^{(k)}=x-x^{(k)}\)</span>。牛顿法的思路就是利用二阶泰勒展开近似，寻找合理的下降方向 <span class="math">\(p_k\)</span>，使 <span class="math">\(f(x)\)</span> 在数次迭代后到达极小值。显然当 <span class="math">\(f(x)\)</span> 达到极小值时梯度为 0。那么假设下一次（第 <span class="math">\(k+1\)</span> 次）迭代时 <span class="math">\(f(x)\)</span> 就能达到极小值，也就是令上式在 <span class="math">\(x^{(k+1)}\)</span> 处的梯度为零：</p>
<div class="math">$$\begin{align}
    \nabla f(x)&amp;=\nabla f(x^{(k)})+\nabla^2f(x^{(k)})\Delta x^{(k)}\\
    \nabla f(x^{(k+1)})&amp;=\nabla f(x^{(k)})+\nabla^2f(x^{(k)})(x^{(k+1)}-x^{(k)})=0\\
    x^{(k+1)}&amp;=x^{(k)}-\nabla^2f(x^{(k)})^{-1}\nabla f(x^{(k)})
\end{align}$$</div>
<p>将其中的 <span class="math">\(\nabla^2f(x^{(k)})^{-1}\)</span> 记作 <span class="math">\(H^{-1}_k\)</span>，是黑塞矩阵的逆，将 <span class="math">\(\nabla f(x^{(k)})\)</span> 记作 <span class="math">\(g_k\)</span>。令 <span class="math">\(p_k=-H^{-1}_kg_k\)</span>，称为牛顿方向，那么每次迭代的过程就是</p>
<div class="math">$$x^{(k+1)}=x^{k}+p_k$$</div>
<p><div class="note-info"><p><i class="fa-solid fa-note-sticky"></i>&ensp;<b>Note</b>&emsp;这里的泰勒展开为矩阵形式，因此式中带有转置与逆等符号，若对求导过程有疑惑，可以参考<a href="https://zhuanlan.zhihu.com/p/382683133" rel="noopener" target="_blank">泰勒展开的矩阵形式</a>。</p></div></p>
<h4>拟牛顿法</h4>
<p>考虑以上推导过程中得到的</p>
<div class="math">$$\begin{align}
    \nabla f(x^{(k+1)})&amp;=\nabla f(x^{(k)})+\nabla^2f(x^{(k)})(x^{(k+1)}-x^{(k)})\\
    g_{k+1}-g_k&amp;=H_k(x^{(k+1)}-x^{(k)})\\
\end{align}$$</div>
<p>令 <span class="math">\(y_k=g_{k+1}-g_k\)</span>，<span class="math">\(\delta_k=x^{(k+1)}-x^{(k)}\)</span>，可以改写为</p>
<div class="math">$$\begin{align}
    y_k&amp;=H_k\delta_k\\
    H^{-1}_ky_k&amp;=\delta_k
\end{align}$$</div>
<p>由于牛顿法每次迭代都需要计算黑森矩阵的逆 <span class="math">\(H^{-1}_k\)</span>，逆矩阵的计算过程比较繁琐，拟牛顿法的思路是寻找一个矩阵 <span class="math">\(G_k\)</span>，使其代替逆黑森矩阵：</p>
<div class="math">$$G_{k+1}y_k=\delta_k$$</div>
<p>或是用 <span class="math">\(B_k\)</span> 逼近黑塞矩阵：</p>
<div class="math">$$B_{k+1}\delta_k=y_k$$</div>
<p>那么每次迭代的步骤就是更新该矩阵：</p>
<div class="math">$$B_{k+1}=B_k+\Delta B_k$$</div>
<p>拟牛顿矩阵更新方式不同，会有不同的计算公式和算法，这里以 BFGS 算法为例推导矩阵更新公式。</p>
<p>假设 <span class="math">\(B_{k+1}=B_k+P_k+Q_k\)</span>，其中 <span class="math">\(P_k\)</span> 与 <span class="math">\(Q_k\)</span> 为待定矩阵：</p>
<div class="math">$$B_{k+1}\delta_k=\color{teal}{y_k}=\color{steelblue}{B_k\delta_k}+\color{teal}{P_k\delta_k}+\color{steelblue}{Q_k\delta_k}$$</div>
<p><span class="math">\(P_k\)</span> 与 <span class="math">\(Q_k\)</span> 在该式中可以有多种取法，这里用最简单的取法：</p>
<div class="math">$$\begin{align}
    &amp;\color{teal}{P_k\delta_k=y_k}\\
    &amp;\color{steelblue}{Q_k\delta_k=-B_k\delta_k}
\end{align}$$</div>
<p>那么就可以得到迭代公式：</p>
<div class="math">$$B_{k+1}=B_k+\frac{y_ky^\mathrm{T}_k}{y^\mathrm{T}_k\delta_k}-\frac{B_k\delta_k\delta^\mathrm{T}_kB_k}{\delta^\mathrm{T}_kB_k\delta_k}$$</div>
<p><div class="note-info"><p><i class="fa-solid fa-note-sticky"></i>&ensp;<b>Note</b>&emsp;式中有许多 <span class="math">\(y_ky^\mathrm{T}_k\)</span> 类型结构，这是为了将向量转化为方阵，才有逆运算，否则向量不具有除法运算。</p></div></p>
<p><strong>算法 6.2（最大熵模型的 BFGS 算法）</strong></p>
<blockquote>
<p>输入：特征函数 <span class="math">\(f_i\)</span>；经验分布 <span class="math">\(\tilde{P}(x,y)\)</span>；目标函数 <span class="math">\(f(w)=-L(w)\)</span>，梯度 <span class="math">\(g(w)=\nabla f(w)\)</span>，精度 <span class="math">\(\varepsilon\)</span>；<br/>
输出：最优参数值 <span class="math">\(w^*\)</span>；最优模型 <span class="math">\(P_{w^*}(y|x)\)</span>。</p>
</blockquote>
<ol>
<li>选定初始点 <span class="math">\(w^{(0)}\)</span>，取 <span class="math">\(B_0\)</span> 为正定对称矩阵，置 <span class="math">\(k=0\)</span>；</li>
<li>计算 <span class="math">\(g_k=g(w^{(k)})\)</span>。若 <span class="math">\(||g_k||&lt;\varepsilon\)</span>，则停止，<span class="math">\(w^*=w^{(k)}\)</span>，否则继续。</li>
<li><span class="math">\(B_kp_k=-g_k\)</span>，求 <span class="math">\(p_k\)</span>。</li>
<li>求 <span class="math">\(\lambda_k\)</span> 使得 <span class="math">\(f(w^{(k)}+\lambda_kp_k)=\min_{\lambda\geqslant0}f(w^{(k)}+\lambda p_k)\)</span>。</li>
<li><span class="math">\(w^{(k+1)}=w^{(k)}+\lambda_kp_k\)</span>。</li>
<li><span class="math">\(g_{k+1}=g(w^{(k+1)})\)</span>，若 <span class="math">\(||g_{k+1}||\varepsilon\)</span>，则停止，<span class="math">\(w^*=w^{(k)}\)</span>，否则按 BFGS 迭代公式求 <span class="math">\(B_{k+1}\)</span>。</li>
<li><span class="math">\(k=k+1\)</span>，回到步骤 3。</li>
</ol>
<hr/>
<h2 id="references_1">References</h2>
<ul>
<li><a href="https://book.douban.com/subject/33437381/" rel="noopener" target="_blank">李航, 2019. 统计学习方法（第2版）. 清华大学出版社.</a></li>
<li><a href="https://book.douban.com/subject/35258057/" rel="noopener" target="_blank">刘浩洋, 户将, 李勇锋, 文再文, 2020. 最优化：建模、算法与理论. 高等教育出版社.</a></li>
<li><a href="https://www.zhihu.com/question/24094554" rel="noopener" target="_blank">如何理解最大熵模型里面的特征？ - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/38163970" rel="noopener" target="_blank">Karush-Kuhn-Tucker (KKT)条件 - 知乎</a></li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </section>

            <section class="post-info">
                <!-- <div class="post-share">
                    <a title="Twitter" aria-label="Twitter" class="twitter" href="https://twitter.com/share?text=《统计学习方法》第六章：逻辑斯谛回归与最大熵模型&amp;url=https://leonis.cc/sui-sui-nian/2022-09-09-statistical-learning-chapter6.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                      <i class="ic ic-twitter" aria-hidden="true"></i><span class="hidden">Twitter</span>
                    </a>
                    <a title="Facebook" aria-label="Facebook" class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://leonis.cc/sui-sui-nian/2022-09-09-statistical-learning-chapter6.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                      <i class="ic ic-facebook" aria-hidden="true"></i><span class="hidden">Facebook</span>
                    </a>
                    <a title="LinkedIn" aria-label="LinkedIn" class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://leonis.cc/sui-sui-nian/2022-09-09-statistical-learning-chapter6.html&amp;title=《统计学习方法》第六章：逻辑斯谛回归与最大熵模型" onclick="window.open(this.href, 'linkedin-share', 'width=930,height=720');return false;">
                      <i class="ic ic-linkedin" aria-hidden="true"></i><span class="hidden">LinkedIn</span>
                    </a>
                    <a title="Email" aria-label="Email" class="email" href="mailto:?subject=《统计学习方法》第六章：逻辑斯谛回归与最大熵模型&amp;body=https://leonis.cc/sui-sui-nian/2022-09-09-statistical-learning-chapter6.html">
                      <i class="ic ic-mail" aria-hidden="true"></i><span class="hidden">Email</span>
                    </a>
                    <div class="clear"></div>
                </div> -->

                <aside class="post-tags">
<a href="https://leonis.cc/tag/tong-ji-xue-xi-fang-fa.html">统计学习方法</a><a href="https://leonis.cc/tag/machine-learning.html">Machine learning</a><a href="https://leonis.cc/tag/algorithm.html">Algorithm</a>                </aside>

                <div class="clear"></div>

                <aside class="post-author">


                        <figure class="post-author-avatar">
                            <img src="https://cravatar.cn/avatar/95e31f6808fafa1f8ef3313b6f0b10e6?s=800" alt="Leo" />
                        </figure>
                    <div class="post-author-bio">
                        <h4 class="post-author-name"><a href="https://leonis.cc/author/leo.html">Leo</a></h4>
                            <p class="post-author-about">A biochemist who doesn't know about classical literature isn't a good programmer. Cool, huh?</p>
                            <span class="post-author-location"><i class="ic ic-location"></i> Tientsin</span>
                        <!-- Social linkes in alphabet order. -->
                            <span class="post-author-mastodon"><a rel="noopener" target="_blank" href="https://mast.dragon-fly.club/@leonis"><i class="fa-brands fa-mastodon fa-fw"></i></i> @leonis@dragon-fly.club</a></span>
                            <span class="post-author-github"><a rel="noopener" target="_blank" href="https://github.com/Tseing"><i class="ic ic-github"></i> GitHub</a></span>
                            <span class="post-author-email"><a rel="noopener" target="_blank" href="mailto:im.yczeng@outlook.com"><i class="ic ic-mail"></i>  im.yczeng@outlook.com</a></span>
                    </div>
                    <div class="clear"></div>
                </aside>

                </section>


                <aside class="post-nav">
                    <a class="post-nav-next" href="https://leonis.cc/sui-sui-nian/2022-09-16-statistical-learning-chapter7.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-left"></i>
                                <h2 class="post-nav-title">《统计学习方法》第七章：支持向量机</h2>
                            <p class="post-nav-excerpt">《统计学习方法》第五章介绍了支持向量机模型以及支持向量机模型在处理线性可分、线性近似可分、线性...</p>
                        </section>
                    </a>
                    <a class="post-nav-prev" href="https://leonis.cc/sui-sui-nian/2022-09-06-dns-forbidden-of-github-pages.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-right"></i>
                                <h2 class="post-nav-title">国内部分网络无法访问 githui.io 的解决方案</h2>
                            <p class="post-nav-excerpt">和往常一样，在浏览器中键入我的 GitHub Pages 地址，Firefox...</p>
                        </section>
                    </a>
                    <div class="clear"></div>
                </aside>

                <div id="waline"></div>
                <script>
                    Waline.init({
                      el: '#waline',
                      serverURL: 'https://waline-1-d9689975.deta.app/',
                      emoji: false,
                      imageUploader: false,
                      locale: {placeholder: '欢迎评论，填写邮箱支持获取头像和收到回复通知。'},
                    });
                </script>
            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer">
      <div class="inner">
        <section class="credits">


          <span class="credits-theme"><a rel="noopener" target="_blank" href="https://github.com/Tseing/Pelican_blog"><i class="fa-brands fa-github fa-fw"></i><b> Source</b></a> &bull;
               <a href="https://leonis.cc/feed.xml"><i class="fa-solid fa-rss fa-fw"></i><b> Atom</b></a> &bull;
               <a href= "https://leonis.cc/sitemap.xml"><i class="fa-solid fa-sitemap fa-fw"></i><b> Sitemap</b></a></span>
          <span class="credits-software">Theme <a rel="noopener" target="_blank" href="https://github.com/arulrajnet/attila"><b>Attila</b></a> &bull; Published with <a rel="noopener" target="_blank" href="https://github.com/getpelican/pelican"><b>Pelican</b></a></span>
        </section>
      </div>
    </footer>
  </section>
  <script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script type="text/javascript" src="https://leonis.cc/theme/js/script.js"></script>
  <!-- awesome-font icon -->
  <script src="https://kit.fontawesome.com/d5180ce512.js" crossorigin="anonymous"></script>
  <!-- code highlight -->
  <script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/highlight.js/11.4.0/highlight.min.js"></script>
  <script src="https://lf9-cdn-tos.bytecdntp.com/cdn/expire-1-M/highlight.js/11.4.0/languages/django.min.js"></script>
  <script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/highlight.js/11.4.0/languages/dockerfile.min.js"></script>
  <script src="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/highlight.js/11.4.0/languages/markdown.min.js"></script>
  <script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/highlight.js/11.4.0/languages/nginx.min.js"></script>
  <script>hljs.highlightAll();</script>
  <!-- lightbox -->
  <script type="text/javascript" src="https://leonis.cc/theme/js/lightgallery.min.js"></script>
  <script type="text/javascript" src="https://leonis.cc/theme/js/lg-zoom.min.js"></script>
  <script>
    var elements = document.getElementsByClassName("lightgallery");
    for(var i=0; i<elements.length; i++) {
       lightGallery(elements[i]);
    }
  </script>
    <!-- umami analytics -->
    <script async defer src="https://analytics.umami.is/script.js" data-website-id="b508982a-f7bf-4c24-a948-8de93b0cb81d"></script>
  <!-- visitor count -->
  <!-- <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> -->
  <!--  -->
</body>
</html>